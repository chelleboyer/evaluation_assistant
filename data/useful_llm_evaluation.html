<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A Survey of Useful LLM Evaluation</title>
<!--Generated on Mon Jun  3 02:19:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2406.00936v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S1" title="In A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S1.SS1" title="In 1 Introduction ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Artificial Intelligence and Large Language Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S1.SS2" title="In 1 Introduction ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.2 </span>Why Evaluating LLMs is Important</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S1.SS3" title="In 1 Introduction ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.3 </span>The Roadmap of Useful LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S1.SS4" title="In 1 Introduction ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.4 </span>Study Overview</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2" title="In A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Core Ability Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS1" title="In 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Reasoning</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS1.SSS1" title="In 2.1 Reasoning ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Logical Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS1.SSS2" title="In 2.1 Reasoning ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Mathematical Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS1.SSS3" title="In 2.1 Reasoning ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Commonsense Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS1.SSS4" title="In 2.1 Reasoning ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.4 </span>Multi-hop Reasoning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS1.SSS5" title="In 2.1 Reasoning ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.5 </span>Structured Data Reasoning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS2" title="In 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Societal Impact</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS2.SSS1" title="In 2.2 Societal Impact ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Safety</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS2.SSS1.Px1" title="In 2.2.1 Safety ‣ 2.2 Societal Impact ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title">Content Safety</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS2.SSS1.Px2" title="In 2.2.1 Safety ‣ 2.2 Societal Impact ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title">Security</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS2.SSS1.Px3" title="In 2.2.1 Safety ‣ 2.2 Societal Impact ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title">Ethical Consideration</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS2.SSS2" title="In 2.2 Societal Impact ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Truthfulness</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS2.SSS2.Px1" title="In 2.2.2 Truthfulness ‣ 2.2 Societal Impact ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title">Hallucination</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS2.SSS2.Px2" title="In 2.2.2 Truthfulness ‣ 2.2 Societal Impact ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title">Bias Mitigation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS3" title="In 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Domain Knowledge</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS3.SSS1" title="In 2.3 Domain Knowledge ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Finance</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS3.SSS2" title="In 2.3 Domain Knowledge ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Legislation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS3.SSS3" title="In 2.3 Domain Knowledge ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.3 </span>Psychology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS3.SSS4" title="In 2.3 Domain Knowledge ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.4 </span>Medicine</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS3.SSS5" title="In 2.3 Domain Knowledge ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.5 </span>Education</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3" title="In A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Agent Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS1" title="In 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Planning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS2" title="In 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Application Scenarios</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS2.SSS1" title="In 3.2 Application Scenarios ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Web Grounding</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS2.SSS1.Px1" title="In 3.2.1 Web Grounding ‣ 3.2 Application Scenarios ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title">Search Engine</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS2.SSS1.Px2" title="In 3.2.1 Web Grounding ‣ 3.2 Application Scenarios ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title">Onlineshopping</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS2.SSS2" title="In 3.2 Application Scenarios ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Code Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS2.SSS3" title="In 3.2 Application Scenarios ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Database Queries</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS2.SSS4" title="In 3.2 Application Scenarios ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.4 </span>API Calls</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS2.SSS5" title="In 3.2 Application Scenarios ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.5 </span>Tool Creation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS2.SSS6" title="In 3.2 Application Scenarios ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.6 </span>Robotic Navigation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS2.SSS7" title="In 3.2 Application Scenarios ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.7 </span>Robotic Manipulation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS3" title="In 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Benchmark</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S4" title="In A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Future Directions</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S4.SS1" title="In 4 Future Directions ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Dynamic Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S4.SS2" title="In 4 Future Directions ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>LLMs as Evaluators</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S4.SS3" title="In 4 Future Directions ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Root Cause Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S4.SS4" title="In 4 Future Directions ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Fine-grained LLM Agent Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S4.SS5" title="In 4 Future Directions ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Robot Benchmark Development</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S5" title="In A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">A Survey of Useful LLM Evaluation</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Ji-Lun Peng<sup class="ltx_sup" id="id6.6.id1">∗</sup> Sijia Cheng<sup class="ltx_sup" id="id7.7.id2">∗</sup> Egil Diau<sup class="ltx_sup" id="id8.8.id3">∗</sup> Yung-Yu Shih<sup class="ltx_sup" id="id9.9.id4">∗</sup> 
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id5.5.1">Po-Heng Chen<sup class="ltx_sup" id="id5.5.1.1"><span class="ltx_text ltx_font_medium" id="id5.5.1.1.1">∗</span></sup> Yen-Ting Lin Yun-Nung Chen</span>
<br class="ltx_break"/>National Taiwan University, Taipei, Taiwan 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id10.10.id5">{b09207002, r11922184, r12922a03, r12944007, r11922044}@ntu.edu.tw
<br class="ltx_break"/>{ytl, y.v.chen}ieee.org</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id11.id1">LLMs have gotten attention across various research domains due to their exceptional performance on a wide range of complex tasks. Therefore, refined methods to evaluate the capabilities of LLMs are needed to determine the tasks and responsibility they should undertake. Our study mainly discussed how LLMs, as useful tools, should be effectively assessed. We proposed the two-stage framework: from “core ability” to “agent”, clearly explaining how LLMs can be applied based on their specific capabilities, along with the evaluation methods in each stage. Core ability refers to the capabilities that LLMs need in order to generate high-quality natural language texts. After confirming LLMs possess core ability, they can solve real-world and complex tasks as agent. In the "core ability" stage, we discussed the reasoning ability, societal impact, and domain knowledge of LLMs. In the “agent” stage, we demonstrated embodied action, planning, and tool learning of LLMs agent applications. Finally, we examined the challenges currently confronting the evaluation methods for LLMs, as well as the directions for future development.<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/MiuLab/EvalLLM-Survey" title="">https://github.com/MiuLab/EvalLLM-Survey</a></span></span></span>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">*</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">*</sup><span class="ltx_note_type">footnotetext: </span>Equal contribution.</span></span></span></p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.5">
<p class="ltx_p" id="p1.5.6"><span class="ltx_text ltx_font_bold" id="p1.5.6.1">A Survey of Useful LLM Evaluation</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.5.5" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.5.5.5" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.5.5.5.5">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.4.4.4.4.4">
<span class="ltx_td ltx_align_center" id="p1.4.4.4.4.4.4"><span class="ltx_text ltx_font_bold" id="p1.4.4.4.4.4.4.4">
Ji-Lun Peng<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.1"><span class="ltx_text ltx_font_medium" id="p1.4.4.4.4.4.4.4.1.1">∗</span></sup> Sijia Cheng<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.2"><span class="ltx_text ltx_font_medium" id="p1.4.4.4.4.4.4.4.2.1">∗</span></sup> Egil Diau<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.3"><span class="ltx_text ltx_font_medium" id="p1.4.4.4.4.4.4.4.3.1">∗</span></sup> Yung-Yu Shih<sup class="ltx_sup" id="p1.4.4.4.4.4.4.4.4"><span class="ltx_text ltx_font_medium" id="p1.4.4.4.4.4.4.4.4.1">∗</span></sup></span></span></span>
<span class="ltx_tr" id="p1.5.5.5.5.5">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.5.1"><span class="ltx_text ltx_font_bold" id="p1.5.5.5.5.5.1.1">Po-Heng Chen<sup class="ltx_sup" id="p1.5.5.5.5.5.1.1.1"><span class="ltx_text ltx_font_medium" id="p1.5.5.5.5.5.1.1.1.1">∗</span></sup> Yen-Ting Lin Yun-Nung Chen</span></span></span>
<span class="ltx_tr" id="p1.5.5.5.5.6.1">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.6.1.1">National Taiwan University, Taipei, Taiwan</span></span>
<span class="ltx_tr" id="p1.5.5.5.5.7.2">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.7.2.1"><span class="ltx_text ltx_font_typewriter" id="p1.5.5.5.5.7.2.1.1">{b09207002, r11922184, r12922a03, r12944007, r11922044}@ntu.edu.tw</span></span></span>
<span class="ltx_tr" id="p1.5.5.5.5.8.3">
<span class="ltx_td ltx_align_center" id="p1.5.5.5.5.8.3.1"><span class="ltx_text ltx_font_typewriter" id="p1.5.5.5.5.8.3.1.1">{ytl, y.v.chen}ieee.org</span></span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="259" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The two-stage framework of our LLMs evaluation.</figcaption>
</figure>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Artificial Intelligence and Large Language Model</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">Artificial intelligence (AI) simulates human behavior to complete multiple tasks needing human intelligence. The first models of AI tried to simulate the function of a single neuron with feedforward, simple input-output functions <cite class="ltx_cite ltx_citemacro_cite">Muthukrishnan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib95" title="">2020</a>)</cite>. As time has progressed, a variety of machine learning (ML) and deep learning (DL) models have been developed. They are not only capable of identifying patterns from vast amounts of data, but they can also make predictions, and even handle unstructured data such as text, images, and audio. Recently, the Transformer architecture <cite class="ltx_cite ltx_citemacro_cite">Vaswani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib144" title="">2017</a>)</cite> was proposed, allowing word embeddings to be context-dependent, and model training to be scaled up <cite class="ltx_cite ltx_citemacro_cite">Min et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib93" title="">2023</a>)</cite>. Therefore, researchers gradually increased the parameters in pre-trained language models trying to reach better performance. Using the Generative Pre-trained Transformer (GPT) series as an illustration, the progression in complexity and models’ capability is marked by a significant increase in the number of parameters: GPT-1 <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib110" title="">2018</a>)</cite> has 117 million parameters, GPT-2 <cite class="ltx_cite ltx_citemacro_cite">Radford et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib111" title="">2019</a>)</cite> expands this to 1.5 billion parameters, and GPT-3 <cite class="ltx_cite ltx_citemacro_cite">Mann et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib87" title="">2020</a>)</cite> further escalates to 175 billion parameters. Moreover, GPT-4 released by OpenAI with much larger model size could accept image and text inputs and produce text outputs, and exhibited human-level performance on various professional and academic benchmarks <cite class="ltx_cite ltx_citemacro_cite">Achiam et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib2" title="">2023</a>)</cite>. The models mentioned above, due to their tremendous size, are referred to as LLMs. They have gotten attention across various research domains due to their exceptional performance on a wide range of complex tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Why Evaluating LLMs is Important</h3>
<div class="ltx_para" id="S1.SS2.p1">
<p class="ltx_p" id="S1.SS2.p1.1">The early works testing model’s intelligence referred to as the Turing Test, raising the question of whether machines could imitate human intelligence and made people fail to differentiate <cite class="ltx_cite ltx_citemacro_cite">Pinar Saygin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib105" title="">2000</a>)</cite>. Evaluating AI is vital as it helps us gauge the real-world capabilities and limitations of AI systems. As AI technologies improve, particularly in areas like software testing and structural engineering, they can sometimes perform better than humans. However, we need clear benchmarks to make sure these technologies are both reliable and effective <cite class="ltx_cite ltx_citemacro_cite">Salehi and Burgueño (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib117" title="">2018</a>)</cite>. With the rapid evolution of LLMs, refined methods to evaluate the capabilities of LLMs is needed to determine the tasks and responsibility they should undertake. Because LLMs exhibit a broad spectrum of capabilities beyond the specific task they are trained for: predicting the next words of human-written texts <cite class="ltx_cite ltx_citemacro_cite">Nolfi (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib98" title="">2023</a>)</cite>, such as formal linguistic competence <cite class="ltx_cite ltx_citemacro_cite">Mahowald et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib84" title="">2023</a>)</cite>, factual knowledge <cite class="ltx_cite ltx_citemacro_cite">Petroni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib104" title="">2019</a>)</cite>, and even theory of mind skills <cite class="ltx_cite ltx_citemacro_cite">Kosinski (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib62" title="">2023</a>)</cite>, we should design benchmarks or evaluation methods specific to each task or domain. In current benchmarks, the comprehensive abilities of LLMs are automatically evaluated through tasks spanning multiple domains such as HELM <cite class="ltx_cite ltx_citemacro_cite">Liang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib72" title="">2022</a>)</cite> and BIG-Bench <cite class="ltx_cite ltx_citemacro_cite">Srivastava et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib131" title="">2022</a>)</cite>, or by generating human feedback automatically like AlpacaFarm <cite class="ltx_cite ltx_citemacro_cite">Dubois et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib32" title="">2024</a>)</cite> and MT-bench <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib183" title="">2024</a>)</cite>. However, when LLMs are required to perform specific tasks, the existence of evaluation methods tailored to those tasks becomes potential. This allows for a comparison of different models’ capabilities under identical tasks to select the best performer. In this study, we categorizes LLMs’ distinct abilities, systematically reviews existing evaluation methods under each category, and discusses how LLMs, as "useful" tools, should be effectively assessed.</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.3 </span>The Roadmap of Useful LLMs</h3>
<div class="ltx_para" id="S1.SS3.p1">
<p class="ltx_p" id="S1.SS3.p1.1">To determine whether LLMs are capable to become useful tools, we should split LLMs’ capabilities into "core ability" and "agent", and discuss them respectively. Core ability refers to the capabilities that LLMs need in order to generate high-quality natural language texts, which are the foundation of performing complex behaviors.</p>
</div>
<div class="ltx_para" id="S1.SS3.p2">
<p class="ltx_p" id="S1.SS3.p2.1">Firstly, LLMs must possess the capability for reasoning, as during interactions with humans, they are required to deduce arguments step by step to engage in effective discussion. Furthermore, the societal impact of LLMs needs significant attention, for LLMs must be perceived as safe and trustworthy for humans to believe in and actively use them. Lastly, LLMs should have knowledge across various domains, and they can assist humans in solving problems occurring withing diverse fields.</p>
</div>
<div class="ltx_para" id="S1.SS3.p3">
<p class="ltx_p" id="S1.SS3.p3.1">Upon confirming that LLMs possess these core abilities, we can utilize LLMs to perform complex behaviors to deal with real-world problems, which we define as agent. For instance, LLMs agents can perform planning, generating an explicit deliberation process that chooses and organizes actions by anticipating their expected outcomes <cite class="ltx_cite ltx_citemacro_cite">Ghallab et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib38" title="">2004</a>)</cite>. Then, LLMs agent can solve tasks in various scenarios such as using tools, creating tools, navigating embodied robots, and so on.</p>
</div>
<div class="ltx_para" id="S1.SS3.p4">
<p class="ltx_p" id="S1.SS3.p4.1">Even though LLMs can display the aforementioned capabilities, comprehensive evaluation methods are necessary to ensure that LLMs achieve as satisfactory level of performance in executing each task. Existing papers on LLM evaluation methods, including <cite class="ltx_cite ltx_citemacro_citet">Guo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib40" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Chang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib17" title="">2023</a>)</cite>, provide a thorough review of evaluation approaches for various aspects of LLMs, yet no study has offered a phased framework to explore the usability of LLMs. Hence, this paper proposes a two-stage framework to examine whether LLMs are sufficiently useful tools (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.00936v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S1.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.4 </span>Study Overview</h3>
<div class="ltx_para" id="S1.SS4.p1">
<p class="ltx_p" id="S1.SS4.p1.1">In this study, we first introduce the evaluation methods of the core ability of LLMs (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.00936v1#S2.F2" title="Figure 2 ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>), including <span class="ltx_text ltx_font_bold" id="S1.SS4.p1.1.1">Reasoning</span> with 5 subsections, <span class="ltx_text ltx_font_bold" id="S1.SS4.p1.1.2">Societal Impact</span> with 2 subsections, and <span class="ltx_text ltx_font_bold" id="S1.SS4.p1.1.3">Domain Knowledge</span> with 5 subsections. Then, for LLMs agent (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2406.00936v1#S3.F3" title="Figure 3 ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>), we introduce evaluation methods of the agent application of LLMs, including <span class="ltx_text ltx_font_bold" id="S1.SS4.p1.1.4">Planning</span>, <span class="ltx_text ltx_font_bold" id="S1.SS4.p1.1.5">Application Scenarios</span> with 7 subsections, and <span class="ltx_text ltx_font_bold" id="S1.SS4.p1.1.6">Benchmark</span>. In these subsections, we present applications of LLMs, evaluation methods, and datasets. Lastly, we give our point of view on the usability of LLMs and suggest future directions and challenges in evaluating LLMs.</p>
</div>
<div class="ltx_para" id="S1.SS4.p2">
<p class="ltx_p" id="S1.SS4.p2.1">The contributions of this paper are as follows:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S1.I1.ix1.p1">
<p class="ltx_p" id="S1.I1.ix1.p1.1">We provide a two-stage framework: from core ability to agent to examine whether LLMs are sufficiently useful tools.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S1.I1.ix2.p1">
<p class="ltx_p" id="S1.I1.ix2.p1.1">In each section, we elucidate the applications of LLMs pertaining to the specific capability, along with the evaluation methods. Furthermore, we provide an analysis of the current performance levels of LLMs in these domains.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S1.I1.ix3.p1">
<p class="ltx_p" id="S1.I1.ix3.p1.1">We examine the challenges currently confronting the evaluation methods for LLMs, as well as the directions for future development.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Core Ability Evaluation</h2>
<figure class="ltx_figure" id="S2.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S2.F2.1">{forest}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S2.F2.2">for tree=
grow=east,
reversed=true,
anchor=base west,
parent anchor=east,
child anchor=west,
base=left,
font=,
rectangle,
draw,
rounded corners,align=left,
inner xsep=4pt,
inner ysep=1pt,
,
where level=1font=,fill=pink!50,
where level=2font=,fill=green!10,
where level=3font=,fill=gray!20,
[Core Ability Evaluation
<br class="ltx_break"/>(Sec. <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2" title="2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">2</span></a>),fill=yellow!20,font=[Reasoning
<br class="ltx_break"/>(Sec. <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS1" title="2.1 Reasoning ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">2.1</span></a>)
[Logical Reasoning
[<cite class="ltx_cite ltx_citemacro_citet">Weston et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib155" title="">2015</a>)</cite><span class="ltx_text" id="S2.F2.2.1">, </span> <cite class="ltx_cite ltx_citemacro_citet">Bhagavatula et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib10" title="">2019</a>)</cite>]
]
[Mathematical Reasoning
[<cite class="ltx_cite ltx_citemacro_citet">Cobbe et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib21" title="">2021</a>)</cite><span class="ltx_text" id="S2.F2.2.2">, </span><cite class="ltx_cite ltx_citemacro_citet">Hendrycks et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib42" title="">2021</a>)</cite>]
]
[Commonsense Reasoning
[<cite class="ltx_cite ltx_citemacro_citet">Talmor et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib138" title="">2018</a>)</cite><span class="ltx_text" id="S2.F2.2.3">, </span><cite class="ltx_cite ltx_citemacro_citet">Mihaylov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib92" title="">2018</a>)</cite>]
]
[Multi-hop Reasoning
[<cite class="ltx_cite ltx_citemacro_citet">Geva et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib37" title="">2021</a>)</cite><span class="ltx_text" id="S2.F2.2.4">, </span><cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib166" title="">2018</a>)</cite>]
]
[Structured Data Reasoning
[<cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib18" title="">2020</a>)</cite><span class="ltx_text" id="S2.F2.2.5">, </span><cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib180" title="">2018</a>)</cite>]
]
]
[Societal Impact
<br class="ltx_break"/>(Sec. <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS2" title="2.2 Societal Impact ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">2.2</span></a>)
[Safety[<cite class="ltx_cite ltx_citemacro_citet">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib77" title="">2023</a>)</cite><span class="ltx_text" id="S2.F2.2.6">, </span><cite class="ltx_cite ltx_citemacro_citet">Kim et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib61" title="">2024b</a>)</cite><span class="ltx_text" id="S2.F2.2.7">, </span><cite class="ltx_cite ltx_citemacro_citet">Yuan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib174" title="">2024</a>)</cite><span class="ltx_text" id="S2.F2.2.8">, </span><cite class="ltx_cite ltx_citemacro_citet">Scherrer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib118" title="">2023</a>)</cite>]
]
[Truthfulness
[<cite class="ltx_cite ltx_citemacro_citet">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib52" title="">2024</a>)</cite><span class="ltx_text" id="S2.F2.2.9">, </span><cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib178" title="">2024b</a>)</cite><span class="ltx_text" id="S2.F2.2.10">, </span><cite class="ltx_cite ltx_citemacro_citet">Hort et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib44" title="">2021</a>)</cite><span class="ltx_text" id="S2.F2.2.11">, </span><cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib176" title="">2023</a>)</cite>]
]
]
[Domain Knowledge
<br class="ltx_break"/>(Sec. <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS3" title="2.3 Domain Knowledge ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">2.3</span></a>)
[Finance
[<cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib157" title="">2023</a>)</cite><span class="ltx_text" id="S2.F2.2.12">, </span><cite class="ltx_cite ltx_citemacro_citet">Xie et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib158" title="">2023</a>)</cite><span class="ltx_text" id="S2.F2.2.13">, </span><cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib69" title="">2023b</a>)</cite>]
]
[Legislation
[<cite class="ltx_cite ltx_citemacro_citet">Blair-Stanek et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib13" title="">2023</a>)</cite><span class="ltx_text" id="S2.F2.2.14">, </span><cite class="ltx_cite ltx_citemacro_citet">Engel and Mcadams (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib33" title="">2024</a>)</cite><span class="ltx_text" id="S2.F2.2.15">, </span><cite class="ltx_cite ltx_citemacro_citet">Liga and Robaldo (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib74" title="">2023</a>)</cite><span class="ltx_text" id="S2.F2.2.16">, </span><cite class="ltx_cite ltx_citemacro_citet">Deroy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib28" title="">2023</a>)</cite>
]
]
[Psychology
[<cite class="ltx_cite ltx_citemacro_citet">Lu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib83" title="">2024</a>)</cite><span class="ltx_text" id="S2.F2.2.17">, </span><cite class="ltx_cite ltx_citemacro_citet">Demszky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib25" title="">2023</a>)</cite><span class="ltx_text" id="S2.F2.2.18">, </span><cite class="ltx_cite ltx_citemacro_citet">Demszky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib25" title="">2023</a>)</cite>
]
]
[Medicine
[<cite class="ltx_cite ltx_citemacro_citet">Agrawal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib3" title="">2022</a>)</cite><span class="ltx_text" id="S2.F2.2.19">, </span><cite class="ltx_cite ltx_citemacro_citet">Sharma and Thakur (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib124" title="">2023</a>)</cite><span class="ltx_text" id="S2.F2.2.20">, </span><cite class="ltx_cite ltx_citemacro_citet">Benoit (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib9" title="">2023</a>)</cite><span class="ltx_text" id="S2.F2.2.21">, </span><cite class="ltx_cite ltx_citemacro_citet">Kumar (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib63" title="">2023</a>)</cite><span class="ltx_text" id="S2.F2.2.22">, </span><cite class="ltx_cite ltx_citemacro_citet">Thirunavukarasu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib141" title="">2023</a>)</cite>
]
]
[Education
[<cite class="ltx_cite ltx_citemacro_citet">Abdelghani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib1" title="">2023</a>)</cite><span class="ltx_text" id="S2.F2.2.23">, </span><cite class="ltx_cite ltx_citemacro_citet">Jia et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib51" title="">2021</a>)</cite><span class="ltx_text" id="S2.F2.2.24">, </span><cite class="ltx_cite ltx_citemacro_citet">Menick et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib90" title="">2022</a>)</cite><span class="ltx_text" id="S2.F2.2.25">, </span><cite class="ltx_cite ltx_citemacro_citet">Dijkstra et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib29" title="">2022</a>)</cite><span class="ltx_text" id="S2.F2.2.26">, </span><cite class="ltx_cite ltx_citemacro_citet">Kasneci et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib57" title="">2023</a>)</cite>
]
]
]
]</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The overview of core ability evaluation.</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The evaluation of core abilities in LLMs thoroughly examines their linguistic capabilities across three essential dimensions: reasoning, societal impact, and domain-specific knowledge.
This essential evaluation emphasizes LLMs’ proficiency in complex cognitive reasoning processes in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS1" title="2.1 Reasoning ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">2.1</span></a>, their commitment to truthfulness and safety standards in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS2" title="2.2 Societal Impact ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">2.2</span></a>, and their adeptness in applying specialized knowledge across a wide range of domains in Section <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.SS3" title="2.3 Domain Knowledge ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">2.3</span></a>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">By confirming that LLMs possess these core abilities, we recognize the potential of these skills to evolve into more complex behaviors.
This development emphasizes the adaptability and scalability of LLMs as tools for advanced applications, indicating that the focus will be on enhancing these foundational abilities further in the future.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Reasoning</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Proficiency in reasoning empowers both humans and machines to make well-founded decisions, derive logical conclusions, and adeptly tackle problems.
Recent research <cite class="ltx_cite ltx_citemacro_citep">(Huang and Chang, <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib46" title="">2023</a>; Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib136" title="">2024</a>)</cite> has increasingly emphasized the augmentation of reasoning capacities in LLMs, aiming to attain human-level or even surpass human-level reasoning prowess within specialized domains.
In this section, our attention is directed towards evaluating the various reasoning abilities of LLMs.
The reasoning task can be categorized into the following groups:
logical reasoning, mathematical reasoning, commonsense reasoning, multi-hop reasoning and structured data reasoning.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Logical Reasoning</h4>
<figure class="ltx_table" id="S2.T1">
<div class="ltx_inline-block ltx_transformed_outer" id="S2.T1.1" style="width:867.2pt;height:174pt;vertical-align:-1.5pt;"><span class="ltx_transformed_inner" style="transform:translate(137.7pt,-27.4pt) scale(1.46509166128663,1.46509166128663) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.2.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.2.1.1.1">
<span class="ltx_p" id="S2.T1.1.1.2.1.1.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.2.1.1.1.1.1">Type</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.2.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.2.1.2.1.1" style="width:170.7pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.2.1.2.1.1.1">Example Source</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S2.T1.1.1.2.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.2.1.3.1">
<span class="ltx_p" id="S2.T1.1.1.2.1.3.1.1" style="width:256.1pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.2.1.3.1.1.1">Input</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S2.T1.1.1.2.1.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.2.1.4.1">
<span class="ltx_p" id="S2.T1.1.1.2.1.4.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.2.1.4.1.1.1">answer</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.3.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.3.1.1.1">
<span class="ltx_p" id="S2.T1.1.1.3.1.1.1.1" style="width:56.9pt;">Inductive Reasoning</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.3.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.3.1.2.1.1" style="width:170.7pt;">bAbI-15 <cite class="ltx_cite ltx_citemacro_citep">(Weston et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib155" title="">2015</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.1.3.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.3.1.3.1">
<span class="ltx_p" id="S2.T1.1.1.3.1.3.1.1" style="width:256.1pt;">Sheep are afraid of wolves.
Cats are afraid of dogs.
Mice are afraid of cats.
Gertrude is a sheep.
What is Gertrude afraid of?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.3.1.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.3.1.4.1">
<span class="ltx_p" id="S2.T1.1.1.3.1.4.1.1" style="width:56.9pt;">wolves</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.1.4.2.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.2.1.1">
<span class="ltx_p" id="S2.T1.1.1.4.2.1.1.1" style="width:56.9pt;">Deductive Reasoning</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.1.4.2.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.2.2.1">
<span class="ltx_p" id="S2.T1.1.1.4.2.2.1.1" style="width:170.7pt;">bAbI-16 <cite class="ltx_cite ltx_citemacro_citep">(Weston et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib155" title="">2015</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S2.T1.1.1.4.2.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.2.3.1">
<span class="ltx_p" id="S2.T1.1.1.4.2.3.1.1" style="width:256.1pt;">Lily is a swan.
Lily is white.
Bernhard is green.
Greg is a swan.
What color is Greg?</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S2.T1.1.1.4.2.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.4.2.4.1">
<span class="ltx_p" id="S2.T1.1.1.4.2.4.1.1" style="width:56.9pt;">white</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.2.1">
<span class="ltx_p" id="S2.T1.1.1.1.2.1.1" style="width:56.9pt;">Abductive Reasoning</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.1.1">
<span class="ltx_p" id="S2.T1.1.1.1.1.1.1" style="width:170.7pt;"><math alttext="\alpha" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.1.1.m1.1a"><mi id="S2.T1.1.1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.1.1.m1.1d">italic_α</annotation></semantics></math>-NLI <cite class="ltx_cite ltx_citemacro_citep">(Bhagavatula et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib10" title="">2019</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="S2.T1.1.1.1.3">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.3.1">
<span class="ltx_p" id="S2.T1.1.1.1.3.1.1" style="width:256.1pt;">obs1: I walked into my math class.
obs2: I ended up failing.
hyp1: I saw the string by the door.
hyp2: I didn’t study for the test.</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="S2.T1.1.1.1.4">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.1.1.1.4.1">
<span class="ltx_p" id="S2.T1.1.1.1.4.1.1" style="width:56.9pt;">hyp2</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Examples for different types of logical reasoning.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.2">Based on concepts from philosophy and logic, logical reasoning can further be divided to three different types:
1) <span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p1.2.1">Inductive reasoning</span> involves inferring general conclusions based on observed patterns or regularities in specific instances. bAbI-15 <cite class="ltx_cite ltx_citemacro_citep">(Weston et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib155" title="">2015</a>)</cite> and EntailmentBank <cite class="ltx_cite ltx_citemacro_citep">(Dalvi et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib22" title="">2021</a>)</cite> are common benchmarks for inductive reasoing.
2) <span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p1.2.2">Deductive reasoning</span> is the process of deriving necessary conclusions based on known premises and logical rules. bAbI-16 <cite class="ltx_cite ltx_citemacro_citep">(Weston et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib155" title="">2015</a>)</cite> is an common benchmark for testing deductive reasoning.
3) <span class="ltx_text ltx_font_bold" id="S2.SS1.SSS1.p1.2.3">abductive reasoning</span> is a form of reasoning where possible explanations or hypotheses are inferred based on given observations and known information. <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.1.m1.1"><semantics id="S2.SS1.SSS1.p1.1.m1.1a"><mi id="S2.SS1.SSS1.p1.1.m1.1.1" xref="S2.SS1.SSS1.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.1.m1.1b"><ci id="S2.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS1.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.1.m1.1d">italic_α</annotation></semantics></math>-NLI, <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS1.SSS1.p1.2.m2.1"><semantics id="S2.SS1.SSS1.p1.2.m2.1a"><mi id="S2.SS1.SSS1.p1.2.m2.1.1" xref="S2.SS1.SSS1.p1.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS1.p1.2.m2.1b"><ci id="S2.SS1.SSS1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS1.p1.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS1.p1.2.m2.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS1.p1.2.m2.1d">italic_α</annotation></semantics></math>-NLG <cite class="ltx_cite ltx_citemacro_citep">(Bhagavatula et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib10" title="">2019</a>)</cite> and AbductiveRules <cite class="ltx_cite ltx_citemacro_citep">(Young et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib171" title="">2022</a>)</cite> are several benchmarks for abductive reasoning.
Table <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S2.T1" title="Table 1 ‣ 2.1.1 Logical Reasoning ‣ 2.1 Reasoning ‣ 2 Core Ability Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">1</span></a> show several examples of each type of logical reasoning task.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS1.p2">
<p class="ltx_p" id="S2.SS1.SSS1.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib159" title="">2023a</a>)</cite> is a comprehensive study on logical reasoning in several LLMs including text-davinci-003, ChatGPT and BARD. They found that BARD perform best generally among three models and ChatGPT performs worse in deductive and inductive settings. Besides, they also show that ChatGPT falls short in generation tasks since it is tailored for chatting. <cite class="ltx_cite ltx_citemacro_citet">Han et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib41" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib79" title="">2023</a>)</cite> include GPT-4 in their evaluation and found that its performance qualitatively matches that of humans in some scenarios.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Mathematical Reasoning</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Mathematical reasoning necessitates models to grasp and manipulate mathematical concepts across diverse scenarios.
For example, the problem may request model to perform arithmetic operations and manipulating abstract symbols to attain an accurate numerical outcome.
Notable examples include GSM8K <cite class="ltx_cite ltx_citemacro_citep">(Cobbe et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib21" title="">2021</a>)</cite> and MATH <cite class="ltx_cite ltx_citemacro_citep">(Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib42" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS2.p2">
<p class="ltx_p" id="S2.SS1.SSS2.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Stolfo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib134" title="">2023</a>)</cite> found that instruction-tuned LLM have a remarkable improvement in both sensitivity and robustness on mathematical problem compared to non-instruction-tuned models.
<cite class="ltx_cite ltx_citemacro_citet">Yuan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib173" title="">2023</a>)</cite> compare the arithemtic capability of 13 models on each operation types and found that GPT-4 is the only model that have excellent performance in every of them.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Commonsense Reasoning</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">Commonsense reasoning entails the capacity to grasp and apply fundamental knowledge about the world.
It’s essential for machines to reach a level of comprehension and interaction comparable to human cognition.
Moreover, commonsense cognition is pivotal in various reasoning processes such as causal detection, spatial and temporal understanding, among others.
Typically, commonsense reasoning tasks are structured as multiple-choice or true/false problem, which contain questions that require model to apply commonsense knowledge to answer.
For instance, the problem may ask "Where do you put your grapes just before checking out?", and the model should select the correct answer, which is "grocery cart."
The CommonsenseQA <cite class="ltx_cite ltx_citemacro_citep">(Talmor et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib138" title="">2018</a>)</cite> consist questions with complex semantics that require prior knowledge to answer.
Similarly, OpenBookQA <cite class="ltx_cite ltx_citemacro_citep">(Mihaylov et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib92" title="">2018</a>)</cite> contains elementary-level questions designed to assess understanding of basic scientific facts and their application in novel scenarios.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS3.p2">
<p class="ltx_p" id="S2.SS1.SSS3.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Bang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib8" title="">2023</a>)</cite> shows that ChatGPT has commonsense reasoning capability over several commonsence benchmark over general knowledge <cite class="ltx_cite ltx_citemacro_citep">(Talmor et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib138" title="">2018</a>)</cite> and physical concepts <cite class="ltx_cite ltx_citemacro_citep">(Bisk et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib12" title="">2020</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib150" title="">2018</a>)</cite>.
<cite class="ltx_cite ltx_citemacro_citet">Bian et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib11" title="">2024</a>)</cite> shows that instruction tuning models have superior performance on several commonsense QA dataset including CommonsenseQA <cite class="ltx_cite ltx_citemacro_citep">(Talmor et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib138" title="">2018</a>)</cite> and OpenBookQA <cite class="ltx_cite ltx_citemacro_citep">(Mihaylov et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib92" title="">2018</a>)</cite>, which illustrates that commonsense ability can be improved by with human alignment.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Multi-hop Reasoning</h4>
<div class="ltx_para" id="S2.SS1.SSS4.p1">
<p class="ltx_p" id="S2.SS1.SSS4.p1.1">The multi-hop reasoning tasks necessitate models to engage in sequential reasoning steps to derive answers.
It serves as a prominent assessment for LLMs, evaluating their capability to analyze questions and solve them through a step-by-step decomposition process akin to human-level ability.
The process can be viewed as an amalgamation of diverse reasoning ability, as each step may necessitate the application of one or more of the reasoning tasks discussed earlier.
For instance, the question might be, ’Was the director of ’Interstellar’ born in Paris?’
In this case, the models must first identify the director of the movie and then ascertain their birthplace.
StrategyQA <cite class="ltx_cite ltx_citemacro_citep">(Geva et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib37" title="">2021</a>)</cite> requires models to generate several implicit reasoning steps to devise a strategy leading to a final decision for the question.
HotpotQA <cite class="ltx_cite ltx_citemacro_citep">(Yang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib166" title="">2018</a>)</cite> is requires finding and reasoning over multiple supporting documents to formulate responses. Its questions are diverse and not confined by any pre-existing knowledge bases.
HoVer <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib53" title="">2020</a>)</cite> requires models to gather facts from multiple Wikipedia articles which are related to a claim and determine if these facts substantiate the claim.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS4.p2">
<p class="ltx_p" id="S2.SS1.SSS4.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib184" title="">2023b</a>)</cite> discovered that ChatGPT fails to deliver reliable and accurate answers on HotpotQA. Their further analysis indicates that this failure can stem from various factors, with factual correctness being the most critical. Addressing this issue, they underscore the significance of knowledge memorization and recall for LLMs.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.5 </span>Structured Data Reasoning</h4>
<div class="ltx_para" id="S2.SS1.SSS5.p1">
<p class="ltx_p" id="S2.SS1.SSS5.p1.1">The aforementioned reasoning tasks have primarily concentrated on scenarios involving purely plain text data.
In contrast, structured data, characterized by specific formats like tables, knowledge graphs, and databases, presents greater challenges for machine comprehension and reasoning.
To perform structured data reasoning, models must be able to understand the format of the data, analyze the information it contains, and generate answers to questions related to that data.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS5.p2">
<p class="ltx_p" id="S2.SS1.SSS5.p2.1">HybridQA <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib18" title="">2020</a>)</cite> integrates questions aligned with Wikipedia tables and multiple free-form corpora linked with entities from the table. The model is required to aggregate both tabular and textual information to generate answers.
MetaQA <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib180" title="">2018</a>)</cite> comprises question-answer pairs within the movie domain and offers a knowledge graph (KG) to facilitate information retrieval. The models are tasked with conducting multi-hop reasoning on the KG and accommodating potential mismatches between KG entities and the question in order to derive answers.
Spider Realistic <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib26" title="">2020</a>)</cite> presents a SQL-based QA dataset, necessitating models to engage in text-to-SQL generation. Specifically, models must accurately identify textual references to columns and values and map them to the provided database schema.</p>
</div>
<div class="ltx_para" id="S2.SS1.SSS5.p3">
<p class="ltx_p" id="S2.SS1.SSS5.p3.1"><cite class="ltx_cite ltx_citemacro_citep">Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib36" title="">2023</a></cite> conducted a comprehensive investigation into the text-to-SQL task across multiple LLMs, employing various prompt engineering methods.
Furthermore, they performed fine-tuning experiments on open-source models.
However, their findings revealed that even after fine-tuning, the performance of these models still lags behind proprietary models with zero-shot evaluation.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Societal Impact</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">LLMs have become crucial elements in modern society, significantly influencing various fields.
With their remarkable abilities in text generation and comprehension, LLMs are reshaping our interactions with information.
Therefore, it is essential to understand the implications of LLMs.
By exploring these dimensions, we aim to comprehend the broader societal impacts of LLMs.
Our goal is to simplify complex concepts into accessible insights, improving our ability to evaluate LLMs effectively.
This discussion explores the societal impacts of LLMs, focusing on two critical aspects: Safety and Trustworthiness.
Through exploring these dimensions, we aim to understand the broader societal implications of LLMs.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Safety</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">In this section, we explore essential safety mechanisms required to protect users when interacting with LLMs. Ensuring that these models generate only safe content is crucial, <cite class="ltx_cite ltx_citemacro_citet">Oviedo-Trespalacios et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib99" title="">2023</a>)</cite> found that ChatGPT sometimes made incorrect or harmful statements, emphasizing the need for expert verification. We address safety concerns by categorizing them into three main areas:
This section explores essential concerns related to the safety of LLMs, including <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS1.p1.1.1">Content Safety, Security, and Ethical Consideration</span>.</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Content Safety</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px1.p1.1">As LLMs and generative AI become more prevalent, the associated content safety risks also escalate.
Benchmarks offer critical insights into these risks.
ToxicChat <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib77" title="">2023</a>)</cite>, based on real user queries from an open-source chatbot, emphasizes the unique challenges of detecting toxicity in user-AI conversations.
The Open AI Moderation Dataset <cite class="ltx_cite ltx_citemacro_cite">Markov et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib88" title="">2023</a>)</cite> provides a comprehensive approach to identifying undesired content in real-world applications.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.Px1.p2">
<p class="ltx_p" id="S2.SS2.SSS1.Px1.p2.1">AEGISSAFETYDATASET <cite class="ltx_cite ltx_citemacro_cite">Ghosh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib39" title="">2024</a>)</cite>, with around 26,000 human-LLM interaction instances annotated by humans, deepens the understanding of content safety issues.
The AI Safety Benchmark v0.5 <cite class="ltx_cite ltx_citemacro_cite">Vidgen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib146" title="">2024</a>)</cite>, created by the MLCommons AI Safety Working Group, focuses on evaluating LLM safety.
SALAD-Bench <cite class="ltx_cite ltx_citemacro_cite">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib67" title="">2024a</a>)</cite>, designed to estimate LLMs, includes evaluations of attack and defense methods.
SafetyBench <cite class="ltx_cite ltx_citemacro_citep">(Scherrer et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib118" title="">2023</a>)</cite>, a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple-choice questions spanning seven distinct categories of safety concerns.
CValues <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib160" title="">2023b</a>)</cite>, the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria.
KCDD <cite class="ltx_cite ltx_citemacro_citep">(Kim et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib60" title="">2024a</a>)</cite> contains 22,249 dialogues generated by crowd workers, designed to simulate offline scenarios. This dataset categorizes dialogues into four criminal classes that align with international legal standards.
BeaverTails <cite class="ltx_cite ltx_citemacro_citep">(Ji et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib50" title="">2023</a>)</cite> introduces a novel "QA moderation" strategy to test models’ safety alignment, offering a fresh perspective distinct from conventional content moderation approaches.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.Px1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.Px1.p3.1">Additionally, it is crucial to ensure that LLMs do not produce adult content accessible by minors <cite class="ltx_cite ltx_citemacro_citep">(Cifuentes et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib20" title="">2022</a>; Karamizadeh et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib55" title="">2023</a>)</cite>, mitigate any harmful content that could affect children, guarantee that outputs do not encourage illegal activities <cite class="ltx_cite ltx_citemacro_citep">(Nayerifard et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib97" title="">2023</a>; Casino et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib16" title="">2022</a>)</cite>, and avoid the generation of content that could incite violence.
In this section, benchmarks and datasets play a vital role in evaluating the safety alignment of LLMs.
By providing annotated data that highlights harmful or inappropriate content, these resources enable researchers to develop and refine algorithms for content moderation and safety enforcement.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Security</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p1.1">This section reviews a collection of papers that focus on the dual aspects of enhancing data privacy practices and strengthening the resilience of LLMs against adversarial threats.
<cite class="ltx_cite ltx_citemacro_citet">Staab et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib133" title="">2023</a>)</cite> discusses the ability of LLMs to infer personal attributes such as location, income, and gender from seemingly innocuous text inputs, using a dataset derived from actual Reddit profiles to demonstrate significant privacy risks. The discussion extends with <cite class="ltx_cite ltx_citemacro_citet">Kim et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib61" title="">2024b</a>)</cite> introducing ProPILE, a probing tool that enables data subjects to detect potential PII leakage in services based on LLMs.
<cite class="ltx_cite ltx_citemacro_citet">Das et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib24" title="">2024</a>)</cite> examines these vulnerabilities in depth, highlighting the urgent need for improved security protocols and the exploration of effective defenses, while <cite class="ltx_cite ltx_citemacro_citet">Yan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib162" title="">2024a</a>)</cite> focuses on clarifying the data privacy concerns associated with LLMs.
Moreover, <cite class="ltx_cite ltx_citemacro_citet">Carlini et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib15" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Yao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib169" title="">2024</a>)</cite> emphasize the significant privacy risks posed by LLMs, particularly through their tendency to memorize and reproduce parts of their training data verbatim.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.Px2.p2">
<p class="ltx_p" id="S2.SS2.SSS1.Px2.p2.1">On the resilience against adversarial attacks, <cite class="ltx_cite ltx_citemacro_citet">Yip et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib170" title="">2024</a>)</cite> introduces a framework that quantifies the resilience of applications against prompt inject attacks using innovative techniques for robust and interoperable evaluations.
<cite class="ltx_cite ltx_citemacro_citet">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib80" title="">2024b</a>); Jin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib54" title="">2024</a>)</cite> both proposes for the use of gradient-based method to enhance the evaluation of adversarial resilience in LLM. These methodologies emphasize a critical shift towards more sophisticated and reliable assessments of adversarial threat landscapes in LLMs.
RigorLLM <cite class="ltx_cite ltx_citemacro_cite">Yuan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib174" title="">2024</a>)</cite>, a framework employing techniques like energy-based data generation and minimax optimization to enhance the moderation of harmful content and improve resilience against complex adversarial attacks.
InjecAgent <cite class="ltx_cite ltx_citemacro_cite">Zhan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib175" title="">2024</a>)</cite>, a benchmark specifically designed to assess the vulnerability of tool-integrated LLM agents to indirect prompt injection attacks, demonstrating significant susceptibility in commonly used LLM agents.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS1.Px3">
<h5 class="ltx_title ltx_title_paragraph">Ethical Consideration</h5>
<div class="ltx_para" id="S2.SS2.SSS1.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p1.1">Ethical evaluation in sensitive areas such as medical ethics <cite class="ltx_cite ltx_citemacro_cite">Balas et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib6" title="">2024</a>)</cite> and moral decision-making <cite class="ltx_cite ltx_citemacro_cite">Scherrer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib118" title="">2023</a>)</cite> is challenging.
These studies are important for enhancing the ethical functioning of LLMs, ensuring their responsible application in real-world settings.
In the following sections, we will introduce some existing studies that address ethical considerations in LLM evaluation and their approaches to improving ethical standards.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.Px3.p2">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Sorensen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib130" title="">2024</a>)</cite> emphasizes value pluralism in decision-making. They introduce ValuePrism, a dataset of 218k values, rights, and duties connected to 31k human-written situations, used to build Value Kaleidoscope (Kaleido), a model that explains and assesses human values. Kaleido’s outputs are preferred over GPT-4, showing more accuracy and broader coverage.
<cite class="ltx_cite ltx_citemacro_citet">Duan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib31" title="">2024</a>)</cite> explores LLMs’ ethical values using Moral Foundation Theory, proposing DeNEVIL, a prompt generation algorithm that reveals value inclinations. They create MoralPrompt, a dataset of 2,397 prompts, and benchmark LLMs’ intrinsic values, finding many models misaligned. They develop VILMO, an alignment method enhancing value compliance, applicable to both black-box and open-source models.
In the MoralChoice datasets <cite class="ltx_cite ltx_citemacro_cite">Scherrer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib118" title="">2023</a>)</cite>, one example of a moral dilemma involves a scenario where the respondent is a driver approaching a pedestrian crossing the street. The possible responses are: (1) Slowing down and waiting for the pedestrian, which reflects a responsible and ethical decision. (2) Accelerating and hitting the pedestrian, which represents an irresponsible and unethical choice. This dilemma illustrates the types of decision-making scenarios used to evaluate ethical behavior in realistic settings.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.Px3.p3">
<p class="ltx_p" id="S2.SS2.SSS1.Px3.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Scherrer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib118" title="">2023</a>)</cite> introduces a novel statistical method to examine the moral beliefs of LLMs and quantifies how likely LLMs are to make decisions in various moral scenarios, analyzing their responses across 680 high-ambiguity and 687 low-ambiguity dilemmas. The findings indicate that LLMs generally align with common sense in straightforward situations but exhibit notable uncertainty in more ambiguous contexts. This research provides insights into LLMs’ decision-making tendencies and their ability to mirror human moral judgments in ethical situations.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Truthfulness</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">Evaluating the reliability of LLMs necessitates ensuring the truthfulness of their outputs.
<cite class="ltx_cite ltx_citemacro_citet">Turpin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib143" title="">2023</a>)</cite> demonstrate that Chain-of-Thought (CoT) explanations can systematically misrepresent the true reasoning behind a model’s predictions.
<cite class="ltx_cite ltx_citemacro_citet">Khan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib59" title="">2024</a>)</cite> points out that as LLMs grow more complex, possibly surpassing human experts, the evaluation dynamic might shift, raising the question of whether simpler models can effectively assess more advanced ones. This scenario underscores the ongoing importance of truthfulness in LLM outputs, reflecting the evolving challenges in model evaluation.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.p2.1">As trustworthiness becomes a key priority, researchers have implemented various evaluation strategies to ensure model reliability.
This section details strategies to reinforce the trustworthiness of LLM outputs.
Besides the widely known TruthfulQA benchmark <cite class="ltx_cite ltx_citemacro_cite">Lin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib76" title="">2022</a>)</cite> , we also focus on the following topics: <span class="ltx_text ltx_font_italic" id="S2.SS2.SSS2.p2.1.1">Hallucination, Bias Mitigation</span>.</p>
</div>
<section class="ltx_paragraph" id="S2.SS2.SSS2.Px1">
<h5 class="ltx_title ltx_title_paragraph">Hallucination</h5>
<div class="ltx_para" id="S2.SS2.SSS2.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS2.Px1.p1.1">Hallucinations in LLMs, where models generate factually incorrect or fabricated content, pose significant challenges to their trustworthiness and reliability.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.Px1.p2">
<p class="ltx_p" id="S2.SS2.SSS2.Px1.p2.1">Techniques such as HaluEval 2.0 <cite class="ltx_cite ltx_citemacro_cite">Jiang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib52" title="">2024</a>)</cite>
and HalluCode <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib78" title="">2024a</a>)</cite> benchmarks have been developed for effective hallucination detection.
Other methods include FEWL <cite class="ltx_cite ltx_citemacro_cite">Wei et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib153" title="">2024</a>)</cite>, which measures hallucinations without gold-standard answers by leveraging multiple LLM responses,
and TofuEval <cite class="ltx_cite ltx_citemacro_cite">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib139" title="">2024</a>)</cite>, which evaluates hallucinations in dialogue summarization with detailed error taxonomy.
Self-Alignment for Factuality <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib178" title="">2024b</a>)</cite> uses self-evaluation to improve factual accuracy within LLMs.
The LLM-free multi-dimensional benchmark AMBER <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib147" title="">2024a</a>)</cite> allows for the evaluation of both generative and discriminative tasks, including various types of hallucinations, through a low-cost and efficient evaluation pipeline. This benchmark facilitates a comprehensive evaluation and detailed analysis of mainstream MLLMs like GPT-4V, also providing guidelines for mitigating hallucinations.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.Px1.p3">
<p class="ltx_p" id="S2.SS2.SSS2.Px1.p3.1"><cite class="ltx_cite ltx_citemacro_citet">Feldman et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib35" title="">2023</a>)</cite> helps recognize and flag instances when LLMs operate outside their domain knowledge, ensuring that users receive accurate information.
This method significantly reduces hallucinations when context accompanies question prompts, achieving a high effectiveness in eliminating hallucinations through tag evaluation.
<cite class="ltx_cite ltx_citemacro_citet">Yang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib165" title="">2023</a>)</cite> introduces a self-check approach for detecting factual errors in LLMs during critical tasks, using reverse validation in a zero-resource setting. The PHD benchmark, designed for detecting hallucinations at the passage level and annotated by humans, enhances the evaluation of detection methods and surpasses existing approaches in efficiency and accuracy.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS2.Px2">
<h5 class="ltx_title ltx_title_paragraph">Bias Mitigation</h5>
<div class="ltx_para" id="S2.SS2.SSS2.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.Px2.p1.1">A range of studies address the issue of bias in the evaluation and operation of LLMs, emphasizing the need to diminish these biases to improve both quality and reliability.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.Px2.p2">
<p class="ltx_p" id="S2.SS2.SSS2.Px2.p2.1">Here are some general bias benchmarks.
BBQ <cite class="ltx_cite ltx_citemacro_cite">Parrish et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib102" title="">2021</a>)</cite> is a dataset of question sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts.
BIAS <cite class="ltx_cite ltx_citemacro_cite">Vermetten et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib145" title="">2022</a>)</cite> is a novel behavior-based benchmark designed to detect structural bias per dimension and across dimension-based on 39 statistical tests.
RecLLM <cite class="ltx_cite ltx_citemacro_cite">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib176" title="">2023</a>)</cite> investigates fairness in LLM-based recommendations, presenting the FaiRLLM benchmark to evaluate biases towards sensitive user attributes.
MERS <cite class="ltx_cite ltx_citemacro_cite">Wu and Aji (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib156" title="">2023</a>)</cite> introduced assesses machine-generated text on multiple dimensions, including factual accuracy and linguistic quality, to specifically target and reduce biases that favor incorrect factual content in LLM evaluations.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.Px2.p3">
<p class="ltx_p" id="S2.SS2.SSS2.Px2.p3.1">Below are specific bias benchmarks relevant to distinct sectors.
In the financial sector, <cite class="ltx_cite ltx_citemacro_citet">Daniel et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib23" title="">2008</a>)</cite> tackles the "look-ahead benchmark bias" in the evaluation of investment managers, which identifies significant discrepancies in performance metrics due to timing differences in benchmark composition. This finding stresses the need for precise benchmarking methods to avoid overstated performance assessments.
<cite class="ltx_cite ltx_citemacro_citet">Hort et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib44" title="">2021</a>)</cite> uses a model behavior mutation approach for benchmarking ML bias mitigation methods. Although the results indicate that many methods struggle to effectively balance fairness and accuracy, they underline the need for more robust strategies in bias mitigation.
<cite class="ltx_cite ltx_citemacro_citet">Wessel et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib154" title="">2023</a>)</cite> introduces the Media Bias Identification Benchmark (MBIB), a comprehensive framework that integrates various types of media biases, enhancing the effectiveness of detection techniques and promoting a more unified and effective approach to bias evaluation in media content.</p>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Domain Knowledge</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">As LLMs demonstrate their capabilities in reasoning and safety, experts have begun to explore the knowledge of LLMs in various domains. They utilize LLMs to complete specific tasks, making these models useful assistants. In this section, we delve into five domains: Finance, Legislation, Psychology, Medicine, and Education, introducing the applications, evaluation methods, and discussing the direction and limitations of LLMs in each domain.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Finance</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">The application of LLMs in Finance field developed relatively earlier. A few models were even designed specifically for financial use, such as FinBERT <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib82" title="">2021b</a>)</cite>, XuanYuan 2.0 <cite class="ltx_cite ltx_citemacro_cite">Zhang and Yang (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib179" title="">2023</a>)</cite>, and BloombergGPT <cite class="ltx_cite ltx_citemacro_cite">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib157" title="">2023</a>)</cite>. BloombergGPT is a 50 billion parameter language model that is trained on a wide range of financial data. From the validation process of BloombergGPT, we can understand the evaluation methods of financial LLMs. <cite class="ltx_cite ltx_citemacro_citet">Wu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib157" title="">2023</a>)</cite> evaluated BloombergGPT on two broad categories of tasks: finance-specific and general purpose. Regarding the finance-specific tasks, FPB <cite class="ltx_cite ltx_citemacro_cite">Malo et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib86" title="">2014</a>)</cite>, FiQA SA <cite class="ltx_cite ltx_citemacro_cite">Maia et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib85" title="">2018</a>)</cite>, Headline <cite class="ltx_cite ltx_citemacro_cite">Sinha and Khandait (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib127" title="">2021</a>)</cite>, NER <cite class="ltx_cite ltx_citemacro_cite">Alvarado et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib4" title="">2015</a>)</cite>, and ConvFinQA <cite class="ltx_cite ltx_citemacro_cite">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib19" title="">2022</a>)</cite> were used. They also used social media and news as aspect-specific sentiment analysis dataset, and compared BloombergGPT response with financial experts’ annotation. Regarding the general purpose tasks, standard LLM benchmarks were utilized for evaluation, such as BIG-bench Hard <cite class="ltx_cite ltx_citemacro_cite">Suzgun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib137" title="">2022</a>)</cite>, and several datasets about Knowledge Assessments, Reading Comprehension, and Linguistic Tasks. Conditionally, <cite class="ltx_cite ltx_citemacro_citet">Xie et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib158" title="">2023</a>)</cite> proposed PIXIU, a framework including the financial LLM based on fine-tuning LLaMA, a instruction data with 136K data samples to support the finetuning, and an evaluation benchmark with 5 tasks and 9 datasets, giving LLMs in financial area a benchmark to assess their ability. When mentioning LLMs for financial use, <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib69" title="">2023b</a>)</cite> argued that two major challenges are the production of disinformation and the manifestation of biases, such as racial, gender, and religious biases, in LLMs. Also, the primary challenge in evaluation was incorporating domain knowledge from financial experts to validate the model’s performance based on financial NLP tasks <cite class="ltx_cite ltx_citemacro_cite">Lee et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib66" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Legislation</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">LLMs’ ability in legislation area has also attracted attention because GPT-4 scored approximately 297 points on the uniform bar examination, passing the threshold for all jurisdiction <cite class="ltx_cite ltx_citemacro_cite">Katz et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib58" title="">2024</a>)</cite>. Various tasks such as statutory reasoning, term interpretation, and legal rule classification were performed by LLMs, and their performance were also evaluated. <cite class="ltx_cite ltx_citemacro_citet">Blair-Stanek et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib13" title="">2023</a>)</cite> evaluated the performance of GPT-3 in statutory reasoning with SARA dataset <cite class="ltx_cite ltx_citemacro_cite">Holzenberger et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib43" title="">2020</a>)</cite>. they found that GPT-3 only reached 78% accuracy in zero-shot condition, showing that GPT-3 couldn’t handle basic legal work because statutes in the dataset were far less complex than real statutes. <cite class="ltx_cite ltx_citemacro_citet">Engel and Mcadams (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib33" title="">2024</a>)</cite> asked Chat 3.5 Turbo whether the statutory term “vehicle” includes a list of candidate objects to assessment LLMs’ understanding of statutory meaning. They found that Chat 3.5 Turbo give the similar result to 2,800 English speakers’ response <cite class="ltx_cite ltx_citemacro_cite">Tobia (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib142" title="">2020</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Liga and Robaldo (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib74" title="">2023</a>)</cite> found that GPT-3 is capable to recognize the difference between obligation rules, permission rules and constitutive rules with LegalDocML <cite class="ltx_cite ltx_citemacro_cite">Palmirani and Vitali (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib100" title="">2011</a>)</cite> and LegalRuleML <cite class="ltx_cite ltx_citemacro_cite">Athan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib5" title="">2013</a>)</cite> dataset. Whether LLMs possess sufficient capability to be applied in the professional legal field, The investigation indicates that pre-trained LLMs are not yet ready for fully automatic deployment for case judgement summarization because inconsistent or hallucinated information has been found in the generated abstractive summaries <cite class="ltx_cite ltx_citemacro_cite">Deroy et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib28" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.3 </span>Psychology</h4>
<div class="ltx_para" id="S2.SS3.SSS3.p1">
<p class="ltx_p" id="S2.SS3.SSS3.p1.1">Human language data is important and valuable in every subdomain in psychology. Because LLMs have the capability to understand and utilize multiple language, emotion detection and psychological measurement can be done by LLMs. Plenty of researches evaluated whether LLMs could complete these tasks with enough quality.<cite class="ltx_cite ltx_citemacro_citet">Rathje et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib114" title="">2023</a>)</cite> tested whether different versions of GPT (3.5 Turbo, 4, and 4 Turbo) can detect sentiment, discrete emotions, offensiveness, and moral foundations in text across 12 languages. They found that LLMs outperformed existing English-language dictionary analysis at detecting psychological constructs as judged by manual annotators. <cite class="ltx_cite ltx_citemacro_citet">Lu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib83" title="">2024</a>)</cite> evaluated GPT-4V’s performance in 5 crucial abilities for affective computing tasks. They used DISFA dataset <cite class="ltx_cite ltx_citemacro_cite">Mavadati et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib89" title="">2013</a>)</cite> to assess GPT-4V’s ability to action unit detection, RAF-DB dataset <cite class="ltx_cite ltx_citemacro_cite">Shan and Deng (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib123" title="">2018</a>)</cite> for facial expression and compound emotion recognition <cite class="ltx_cite ltx_citemacro_cite">Du et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib30" title="">2014</a>)</cite>, CASME2 dataset <cite class="ltx_cite ltx_citemacro_cite">Yan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib164" title="">2014</a>)</cite> for Micro-expression Recognition <cite class="ltx_cite ltx_citemacro_cite">Zhao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib181" title="">2023</a>)</cite>, and iMiGUE dataset <cite class="ltx_cite ltx_citemacro_cite">Liu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib81" title="">2021a</a>)</cite> for Micro-gesture Recognition. The results showed that GPT-4V could give satisfactory answers to action unit, compound emotion and Micro-gesture test samples, but failed to answer facial expression and Micro-expression test samples correctly. Regarding psychological measurement, <cite class="ltx_cite ltx_citemacro_citet">Demszky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib25" title="">2023</a>)</cite> proposed 2 methods to evaluate the effects of features on human thought and behaviour: 1) <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS3.p1.1.1">Expert evaluation</span> means trained research assistants and LLMs score the same texts for particular psychological construct, and then compute agreement between their scores. 2) <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS3.p1.1.2">Impact evaluation</span> means assessing the effect before and after the manipulation. For instance, <cite class="ltx_cite ltx_citemacro_citet">Karinshak et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib56" title="">2023</a>)</cite> used impact evaluation to measure participants’ attitude to GPT-3-generated pro-vaccination messages. <cite class="ltx_cite ltx_citemacro_citet">Demszky et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib25" title="">2023</a>)</cite> additionally proposed that in assessing the capability of LLMs for psychological tasks, initial assessment could be conducted using expert evaluation for a manipulation check or a measure of construct validity. Subsequently, text aligning with expert evaluations might be utilized in an impact evaluation study that attempts to measure the intended effects on third-party participants, similar to assessing predictive or external validity.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.4 </span>Medicine</h4>
<div class="ltx_para" id="S2.SS3.SSS4.p1">
<p class="ltx_p" id="S2.SS3.SSS4.p1.1">As ChatGPT was able to pass the United States Medical Licensing Exam (USMLE) <cite class="ltx_cite ltx_citemacro_cite">Kung et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib64" title="">2023</a>)</cite> without additionally training, LLMs were noticed in medical area. Previous researches focused on exploring LLMs’ potential in clinical work and research <cite class="ltx_cite ltx_citemacro_cite">Thirunavukarasu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib141" title="">2023</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Agrawal et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib3" title="">2022</a>)</cite> introduced dataset from manual reannotation of the CASI dataset <cite class="ltx_cite ltx_citemacro_cite">Moon et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib94" title="">2014</a>)</cite> for benchmarking few-shot clinical information extraction, and showed that GPT-3 outperform existing baseline of this task. <cite class="ltx_cite ltx_citemacro_citet">Sharma and Thakur (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib124" title="">2023</a>)</cite> demonstrated ChatGPT can help researchers design new drugs and optimize the pharmacokinetics and pharmacodynamics of new drugs. <cite class="ltx_cite ltx_citemacro_citet">Benoit (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib9" title="">2023</a>)</cite> showed when presented with 45 simplified standardized vignettes <cite class="ltx_cite ltx_citemacro_cite">Semigran et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib120" title="">2015</a>)</cite>, ChatGPT identified illnesses with 75.6% first-pass diagnostic accuracy and 57.8% triage accuracy, which performed similarly to physicians’ 72.1% on the same set of 45 vignettes. However, when writing academic clinical paper, current LLMs cannot meet ICMJE authorship criteria because they cannot understand the role of authors or take responsibility for the paper <cite class="ltx_cite ltx_citemacro_cite">Zielinski et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib187" title="">2023</a>)</cite>. Also, <cite class="ltx_cite ltx_citemacro_citet">Kumar (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib63" title="">2023</a>)</cite> assess the ChatGPT’s utility for academic writing in biomedical domain, showing that although the content of the response were systematic, precise and original, it lacked quality and depth of academic writing. In summary, plenty of deployment of LLM applications in medical area is not currently feasible and need to have deeper evaluation. clinicians and researchers will remain responsible for delivering optimal knowledge and care <cite class="ltx_cite ltx_citemacro_cite">Thirunavukarasu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib141" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.5 </span>Education</h4>
<div class="ltx_para" id="S2.SS3.SSS5.p1">
<p class="ltx_p" id="S2.SS3.SSS5.p1.1">The conversational and knowledgeable features of LLMs make the applications of LLMs in education possible. Current evaluation methods of LLMs in education field can be generally divided into two categories: 1) <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS5.p1.1.1">Human annotation</span> means that experts directly score the material generated by LLMs or annotate unlabeled data from external datasets or online websites to create an evaluation dataset. <cite class="ltx_cite ltx_citemacro_citet">Abdelghani et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib1" title="">2023</a>)</cite> used GPT-3 for generating linguistic and semantic cues that can help children formulate divergent questions. They have 2 experts to evaluate the quality of the linguistic and semantic cues generated. <cite class="ltx_cite ltx_citemacro_citet">Jia et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib51" title="">2021</a>)</cite> had fluent English speakers to annotated data from a peer-assessment platform, Expertiza and make sure enough inter-annotator agreement to test the accuracy of the BERT model for evaluating peer assessments. <cite class="ltx_cite ltx_citemacro_citet">Menick et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib90" title="">2022</a>)</cite>. evaluated their Self-Supported Question Answering model by asking paid contractors to assess model samples from Natural Questions <cite class="ltx_cite ltx_citemacro_cite">Kwiatkowski et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib65" title="">2019</a>)</cite> and ELI5 <cite class="ltx_cite ltx_citemacro_cite">Fan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib34" title="">2019</a>)</cite> datasets. 2) <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS5.p1.1.2">Metrics and models</span> means that traditional metrics or trained model are utilized to assess the material generated by LLMs automatically. <cite class="ltx_cite ltx_citemacro_citet">Dijkstra et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib29" title="">2022</a>)</cite> proposed EduQuiz, an end-to-end quiz generator based
on a GPT-3 model, able to generate a complete multiple-choice question, with the correct and distractor answers. They used BLEU-4 <cite class="ltx_cite ltx_citemacro_cite">Papineni et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib101" title="">2002</a>)</cite>, ROUGE-L <cite class="ltx_cite ltx_citemacro_cite">Lin (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib75" title="">2004</a>)</cite>, and METEOR <cite class="ltx_cite ltx_citemacro_cite">Banerjee and Lavie (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib7" title="">2005</a>)</cite> metrics to compared prediction and ground truth instances. <cite class="ltx_cite ltx_citemacro_citet">Raina and Gales (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib112" title="">2022</a>)</cite> use the RACE++ dataset <cite class="ltx_cite ltx_citemacro_cite">Liang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib73" title="">2019</a>)</cite> to train a deep learning
model to explicitly class a multiple-choice question in the complexity levels of easy, medium and hard, which could make the process of assessing multiple-choice question generation automatic. After the overall review, <cite class="ltx_cite ltx_citemacro_citet">Kasneci et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib57" title="">2023</a>)</cite> concluded integrating LLMs into the educational area offers considerable benefits, such as enhancing student learning experiences and assisting teachers, but this integration must adhere to strict requirements concerning privacy, security, environmental sustainability, regulation, and ethics. Additionally, it should be accompanied by continuous human oversight, guidance, and the application of critical thinking.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Agent Evaluation</h2>
<figure class="ltx_figure" id="S3.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S3.F3.1">{forest}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S3.F3.2">for tree=
grow=east,
reversed=true,
anchor=base west,
parent anchor=east,
child anchor=west,
base=left,
font=,
rectangle,
draw,
rounded corners,align=left,
inner xsep=4pt,
inner ysep=1pt,
,
where level=1font=,fill=pink!50,
where level=2font=,fill=green!10,
where level=3font=,fill=gray!20,
[Agent Evaluation
<br class="ltx_break"/>(Sec. <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.F3" title="Figure 3 ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">3</span></a>),fill=yellow!20,font=[Planning
<br class="ltx_break"/>(Sec. <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS1" title="3.1 Planning ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">3.1</span></a>)
[<cite class="ltx_cite ltx_citemacro_citet">Song et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib128" title="">2023a</a>)</cite><span class="ltx_text" id="S3.F3.2.1">, </span><cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib49" title="">2022b</a>)</cite><span class="ltx_text" id="S3.F3.2.2">, </span><cite class="ltx_cite ltx_citemacro_citet">Yao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib168" title="">2023b</a>)</cite><span class="ltx_text" id="S3.F3.2.3">, </span><cite class="ltx_cite ltx_citemacro_citet">Shinn et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib125" title="">2023</a>)</cite>,fill=gray!20]
]
[Application Scenarios
<br class="ltx_break"/>(Sec. <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS2" title="3.2 Application Scenarios ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">3.2</span></a>)
[Web Grounding
[<cite class="ltx_cite ltx_citemacro_citet">Nakano et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib96" title="">2022</a>)</cite><span class="ltx_text" id="S3.F3.2.4">, </span><cite class="ltx_cite ltx_citemacro_citet">Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib107" title="">2023a</a>)</cite><span class="ltx_text" id="S3.F3.2.5">, </span><cite class="ltx_cite ltx_citemacro_citet">Yao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib167" title="">2023a</a>)</cite>
]
]
[Code Generation
[<cite class="ltx_cite ltx_citemacro_citet">Liang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib71" title="">2023</a>)</cite><span class="ltx_text" id="S3.F3.2.6">,
</span><cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib177" title="">2024a</a>)</cite>
]
]
[Database Queries
[<cite class="ltx_cite ltx_citemacro_citet">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib45" title="">2023</a>)</cite>]
]
[API Calls
[<cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib68" title="">2023a</a>)</cite><span class="ltx_text" id="S3.F3.2.7">,
</span><cite class="ltx_cite ltx_citemacro_citet">Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib108" title="">2023b</a>)</cite><span class="ltx_text" id="S3.F3.2.8">,
</span><cite class="ltx_cite ltx_citemacro_citet">Yan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib163" title="">2024b</a>)</cite>
]
]
[Tool Creation
[<cite class="ltx_cite ltx_citemacro_citet">Cai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib14" title="">2024</a>)</cite><span class="ltx_text" id="S3.F3.2.9">,
</span><cite class="ltx_cite ltx_citemacro_citet">Qian et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib106" title="">2023</a>)</cite>
]
]
[Robotic Navigation
[<cite class="ltx_cite ltx_citemacro_citet">Shah et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib122" title="">2022</a>)</cite><span class="ltx_text" id="S3.F3.2.10">, </span><cite class="ltx_cite ltx_citemacro_citet">Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib185" title="">2023a</a>)</cite><span class="ltx_text" id="S3.F3.2.11">, </span><cite class="ltx_cite ltx_citemacro_citet">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib182" title="">2023a</a>)</cite>
]
]
[Robotic Manipulation
[<cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib48" title="">2023</a>)</cite><span class="ltx_text" id="S3.F3.2.12">, </span><cite class="ltx_cite ltx_citemacro_citet">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib172" title="">2023</a>)</cite>
]
]
]
[Benchmark
<br class="ltx_break"/>(Sec. <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#S3.SS3" title="3.3 Benchmark ‣ 3 Agent Evaluation ‣ A Survey of Useful LLM Evaluation"><span class="ltx_text ltx_ref_tag">3.3</span></a>)
[<cite class="ltx_cite ltx_citemacro_citet">Ruan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib116" title="">2023</a>)</cite><span class="ltx_text" id="S3.F3.2.13">, </span><cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib68" title="">2023a</a>)</cite><span class="ltx_text" id="S3.F3.2.14">, </span><cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib140" title="">2023</a>)</cite>,fill=gray!20]
]
]</p>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The overview of agent evaluation.</figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Building upon LLM’s core abilities, there has been a growing research area that employs LLMs as central controllers to construct autonomous agents to obtain human-like decision-making capabilities <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib148" title="">2024b</a>)</cite>. 
<br class="ltx_break"/>In this section, we’ll first discuss the methods used to assess LLM agents’ capabilities of planning. And also introduce the evaluation based on various application scenarios.
Each subsection will provide detailed insights into the applications of LLMs, the methodologies used for evaluation, and the datasets employed.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Planning</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Planning by an agent involves the strategic formulation and execution of actions or steps to achieve specific goals or outcomes within a given environment, typically using algorithms or models to predict and decide the best course of action.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Facing the challenge of executing complex tasks that require decomposition into simpler subtasks, robot planning empowers robots to autonomously identify and execute actions towards achieving specific goals, taking into account their surroundings and objectives. In this context, several innovative approaches, such as <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib47" title="">2022a</a>); Singh et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib126" title="">2023</a>); Song et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib128" title="">2023a</a>)</cite> harness the extensive commonsense knowledge available through LLMs, enabling these models to efficiently segment tasks into manageable subtasks. The Inner Monologue <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib49" title="">2022b</a>)</cite> system utilizes LLMs for dynamic planning in robotic tasks by integrating continuous natural language feedback. Similarly, SayPlan <cite class="ltx_cite ltx_citemacro_cite">Rana et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib113" title="">2023</a>)</cite> enhances task planning capabilities of LLMs by grounding them with 3D Scene Graphs to facilitate extensive environmental interactions. These methods are evaluated across virtual environments, embodied agents, and physical robots. Moreover, several works like DEPS <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib152" title="">2023b</a>)</cite>, AdaPlanner <cite class="ltx_cite ltx_citemacro_cite">Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib135" title="">2023</a>)</cite>, and Robots That Ask For Help <cite class="ltx_cite ltx_citemacro_cite">Ren et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib115" title="">2023</a>)</cite>, introduce dynamic elements of interactive re-planning, adaptive strategies, and the ability to seek assistance when faced with uncertainties. These developments are pivotal for the practical application and effectiveness of robotics in real-world settings, illustrating a significant stride towards more adaptable and intelligent robotic systems. They are evaluated in increasingly complex situations that closely mirror real-life conditions.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">An LLM-based agent employs LLMs to analyze and generate human-like text, aiding in decision-making and strategic planning by processing vast amounts of information quickly and accurately. React <cite class="ltx_cite ltx_citemacro_cite">Yao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib168" title="">2023b</a>)</cite> presents a paradigm that synergistically blends reasoning and action within language models, enhancing performance and interpretability across various decision-making tasks, as evidenced by benchmarks in ALFWorld and WebShop. Reflexion <cite class="ltx_cite ltx_citemacro_cite">Shinn et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib125" title="">2023</a>)</cite> introduces a groundbreaking framework that employs verbal feedback for reinforcement learning, enabling language agents to refine their skills through self-reflection without updating model weights. This method is evaluated across diverse decision-making, reasoning, and programming tasks, demonstrating marked enhancements over traditional approaches in environments such as AlfWorld, HotPotQA, and HumanEval. SelfCheck <cite class="ltx_cite ltx_citemacro_cite">Miao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib91" title="">2023</a>)</cite> offers a zero-shot mechanism that empowers LLMs to autonomously verify their multi-step reasoning in math problem-solving, which significantly boosts accuracy on benchmarks including GSM8K, MathQA, and MATH by filtering out low-confidence solutions.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Application Scenarios</h3>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1 </span>Web Grounding</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">In this section, we focus on LLMs performing tasks in web environments. We categorize the evaluation methods based on tasks.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph">Search Engine</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p1.1">WebGPT <cite class="ltx_cite ltx_citemacro_citet">Nakano et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib96" title="">2022</a>)</cite> developed a text-based web-browsing environment, enabling interaction with a fine-tuned language model to generate more faithful outputs. Evaluation of WebGPT models is conducted through three main approaches: comparison with answers authored by human demonstrators on a held-out set of questions, comparison with the highest-voted answers from the ELI5 dataset, and evaluation using the TruthfulQA dataset.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.Px1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.Px1.p2.1">WebCPM <cite class="ltx_cite ltx_citemacro_citet">Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib107" title="">2023a</a>)</cite> employs tool learning to enable models to answer long-form questions through web searches. Its evaluation encompasses four sub-tasks: action prediction, search query generation, supporting fact extraction, and information synthesis, with each task independently assessed using Micro-F1 and Macro-F1 for action prediction and Rouge-L for other three tasks including text generation. In holistic evaluation, eight annotators manually compare the model-generated answers based on human preference.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph">Onlineshopping</h5>
<div class="ltx_para" id="S3.SS2.SSS1.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS1.Px2.p1.1">WebShop <cite class="ltx_cite ltx_citemacro_cite">Yao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib167" title="">2023a</a>)</cite> introduces a benchmark for assessing LLM-based agents’ abilities in product search and retrieval. Their dataset, comprising 12,087 instructions, is divided into 10,587 for training, 1,000 for development, and 500 for testing, with human shopping paths recorded for each instance. Evaluation metrics include task score and success rate, revealing that humans outperform LLMs across all measures.</p>
</div>
</section>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2 </span>Code Generation</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">To enable nuanced control in robots for complex real-world tasks, the Code as Policies <cite class="ltx_cite ltx_citemacro_cite">Liang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib71" title="">2023</a>)</cite> paradigm uses LLMs to generate policy code for spatial reasoning and adapting to new instructions. The code quality is assessed with HumanEval and RoboCodeGen. RoboCodeGen, a benchmark with 37 function generation tasks, focuses on spatial and geometric reasoning and control, supports third-party libraries like NumPy, lacks documentation strings and type hints, and permits undefined functions for hierarchical code generation. The evaluation metric is the pass rate of generated code that passes manually written unit tests.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">The CODEAGENTBENCH benchmark <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib177" title="">2024a</a>)</cite> is designed to evaluate LLMs in real-world repo-level code generation tasks. It provides comprehensive input information, such as documentation, code dependencies, and runtime environment details, challenging LLMs to produce accurate and well-integrated code solutions.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3 </span>Database Queries</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">Integrating external databases or knowledge bases allows agents to access specific domain information, resulting in more realistic actions. For example, ChatDB <cite class="ltx_cite ltx_citemacro_cite">Hu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib45" title="">2023</a>)</cite> uses SQL statements to query databases, enabling logical actions by the agents. They created a dataset of 70 records from fruit shop management logs for evaluation. The experiment clearly demonstrates that ChatDB outperforms ChatGPT with significantly higher accuracy.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4 </span>API Calls</h4>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1">LLM agents can also enhance their capabilities by calling APIs. API-Bank, as introduced by <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib68" title="">2023a</a>)</cite>, provides a specialized benchmark to evaluate tool-augmented LLM performance. This benchmark includes 53 standard API tools, a detailed workflow for tool-augmented LLMs, and a dataset with 264 annotated dialogues. Evaluation metrics involve accuracy of API calls and ROUGE-L for post-call responses, with task planning efficacy measured by the successful completion of planned tasks through model-driven API calls.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS4.p2">
<p class="ltx_p" id="S3.SS2.SSS4.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib108" title="">2023b</a>)</cite> undertake a scholarly inquiry into the utilization of tool learning within contemporary Language Models (LLMs), delving into both their effectiveness and limitations. They evaluate 18 representative tools across six tasks using existing datasets and extend their study to 12 additional tasks, such as slide-making, AI painting, and 3D model construction. They augment user queries generated by ChatGPT and manually assess the success rates of these operations.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS4.p3">
<p class="ltx_p" id="S3.SS2.SSS4.p3.1">The Berkeley Function-Calling Leaderboard (BFCL) <cite class="ltx_cite ltx_citemacro_citet">Yan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib163" title="">2024b</a>)</cite> evaluates LLMs on function processing, syntax tree analysis, and function execution across various scenarios. It features an interactive comparison tool and a dataset covering fields like Mathematics, Sports, and Finance. Evaluations include Simple, Multiple, and Parallel Function tests. BFCL aids the integration of LLMs into platforms like Langchain and AutoGPT, providing detailed analyses on cost and latency for models like GPT-4.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.5 </span>Tool Creation</h4>
<div class="ltx_para" id="S3.SS2.SSS5.p1">
<p class="ltx_p" id="S3.SS2.SSS5.p1.1">The usage of tools is contingent upon the accessibility of external tools <cite class="ltx_cite ltx_citemacro_citet">Schick et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib119" title="">2023</a>)</cite>. Recently, efforts have been made to employ LLM as a tool creator in order to generate tools that can be utilized for diverse requests(<cite class="ltx_cite ltx_citemacro_citet">Ruan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib116" title="">2023</a>)</cite>).
LATM <cite class="ltx_cite ltx_citemacro_cite">Cai et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib14" title="">2024</a>)</cite> utilizes GPT-4 to develop tools, demonstrating that more cost-effective models can achieve comparable performance to larger models in these applications. They employ six datasets from various domains: logic reasoning, object tracking, Dyck language, word sequencing, the Chinese remainder theorem, and meeting scheduling. The first five datasets are sourced from BigBench <cite class="ltx_cite ltx_citemacro_cite">Srivastava et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib132" title="">2023</a>)</cite>, while the meeting scheduling task is specifically designed to showcase the model’s real-world utility.
CREATOR <cite class="ltx_cite ltx_citemacro_cite">Qian et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib106" title="">2023</a>)</cite> evaluates LLMs’ ability to create tools using the Creation Challenge dataset, which includes 2,000 novel and challenging problems that existing tools or code packages cannot adequately solve. Evaluations demonstrate that ChatGPT’s tool-making performance improves with additional hints, achieving up to 75.5% accuracy, highlighting the importance of tool creation in enhancing LLM problem-solving capabilities.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.6 </span>Robotic Navigation</h4>
<div class="ltx_para" id="S3.SS2.SSS6.p1">
<p class="ltx_p" id="S3.SS2.SSS6.p1.1">Navigation by an embodied agent involves the autonomous movement and decision-making of a robotic or virtual entity within a physical or simulated environment, using sensors and algorithms to perceive surroundings, plan routes, and accomplish navigational tasks.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS6.p2">
<p class="ltx_p" id="S3.SS2.SSS6.p2.1">LM-Nav <cite class="ltx_cite ltx_citemacro_cite">Shah et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib122" title="">2022</a>)</cite> proposed a system for robotic navigation that utilizes LLM, VLM, visual navigation model (VNM), and robotic navigation—enabling a robot to navigate complex environments using natural language instructions without needing specific training data annotated with language descriptions. They benchmark on 20 queries, in environments of varying difficulty, corresponding to a total combined length of over 6 km. LFG <cite class="ltx_cite ltx_citemacro_cite">Shah et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib121" title="">2023</a>)</cite> leverages language models as heuristics to enhance planning algorithms, guiding robots through unfamiliar environments using semantic cues from natural language descriptions. They evaluate navigational performance on ObjectNav.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS6.p3">
<p class="ltx_p" id="S3.SS2.SSS6.p3.1">NavGPT <cite class="ltx_cite ltx_citemacro_cite">Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib185" title="">2023a</a>)</cite> utilizes LLMs to perform explicit reasoning and planning. This approach incorporates textual descriptions of visual observations, navigation history, and potential future paths to enhance navigation tasks. Following this, the NaviLLM model <cite class="ltx_cite ltx_citemacro_cite">Zheng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib182" title="">2023a</a>)</cite> emerges as a versatile solution for embodied navigation. It adeptly tailors LLMs to manage a wide spectrum of embodied navigation challenges by employing schema-based instructions that transform disparate tasks into unified generative modeling problems. The performance of these models is rigorously assessed using vision-language navigation (VLN) benchmarks, such as R2R, Reverie, CVDN, and SOON.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.7 </span>Robotic Manipulation</h4>
<div class="ltx_para" id="S3.SS2.SSS7.p1">
<p class="ltx_p" id="S3.SS2.SSS7.p1.1">Manipulation involves the use of embodied agent to interact with and manipulate physical objects in their environment, enabling tasks ranging from simple pick-and-place operations to complex assembly processes.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS7.p2">
<p class="ltx_p" id="S3.SS2.SSS7.p2.1">VoxPoser <cite class="ltx_cite ltx_citemacro_cite">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib48" title="">2023</a>)</cite> presents an innovative approach where the key novelty is the use of LLMs not just for understanding natural language instructions, but crucially, for generating code that interacts with VLMs to create detailed 3D value maps. These maps guide robotic actions, bridging the gap between abstract instructions and physical execution. They directly evaluate the result on the success rate of robot manipulation tasks. L2R <cite class="ltx_cite ltx_citemacro_cite">Yu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib172" title="">2023</a>)</cite> presents a method for translating language instructions into reward functions using LLMs that robots can optimize to execute specific tasks, demonstrating this approach with a variety of complex locomotion and manipulation tasks in simulated environments.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Benchmark</h3>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:867.2pt;height:431.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(192.3pt,-95.7pt) scale(1.79710027065224,1.79710027065224) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T2.1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T2.1.1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.1.1.1" style="width:85.4pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1.1.1.1">Benchmark</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t" id="S3.T2.1.1.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.2.1.1" style="width:369.9pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.2.1.1.1">Description</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="S3.T2.1.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.2.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.2.1.1.1.1" style="width:85.4pt;">APIBench <cite class="ltx_cite ltx_citemacro_citep">(Patil et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib103" title="">2023</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.2.1.2.1">
<span class="ltx_p" id="S3.T2.1.1.2.1.2.1.1" style="width:369.9pt;">An evaluation system with 73 API tools, 314 annotated tool-use dialogues with 753 API calls, and a training set containing 1,888 tool-use dialogues from 2,138 APIs across 1,000 domains</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T2.1.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.3.2.1.1">
<span class="ltx_p" id="S3.T2.1.1.3.2.1.1.1" style="width:85.4pt;">ToolEval <cite class="ltx_cite ltx_citemacro_citep">(Qin et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib109" title="">2023c</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.3.2.2.1">
<span class="ltx_p" id="S3.T2.1.1.3.2.2.1.1" style="width:369.9pt;">constructed automatically using ChatGPT, includes a collection of 16,464 real-world RESTful APIs across 49 categories, with diverse instructions and solution paths generated for both single-tool and multi-tool scenarios.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T2.1.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.4.3.1.1">
<span class="ltx_p" id="S3.T2.1.1.4.3.1.1.1" style="width:85.4pt;">ToolAlpaca <cite class="ltx_cite ltx_citemacro_citep">(Tang et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib140" title="">2023</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.4.3.2.1">
<span class="ltx_p" id="S3.T2.1.1.4.3.2.1.1" style="width:369.9pt;">containing 3,938 instances from over 400 real-world tool APIs across 50 categories</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T2.1.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.5.4.1.1">
<span class="ltx_p" id="S3.T2.1.1.5.4.1.1.1" style="width:85.4pt;">RestBench <cite class="ltx_cite ltx_citemacro_citep">(Song et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib129" title="">2023b</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.5.4.2.1">
<span class="ltx_p" id="S3.T2.1.1.5.4.2.1.1" style="width:369.9pt;">human-annotated dataset comprising two real-world scenarios (TMDB movie database and Spotify music player) with 54 and 40 commonly used APIs respectively, annotated with 10 instruction-solution pairs for development and 157 pairs (100 for TMDB, 57 for Spotify) for testing</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="S3.T2.1.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.6.5.1.1">
<span class="ltx_p" id="S3.T2.1.1.6.5.1.1.1" style="width:85.4pt;">WebArena <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib186" title="">2023b</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.1.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.6.5.2.1">
<span class="ltx_p" id="S3.T2.1.1.6.5.2.1.1" style="width:369.9pt;">A realistic and reproducible web environment featuring four fully operational web applications (e-commerce, discussion forums, collaborative development, and content management) with 812 long-horizon tasks</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r" id="S3.T2.1.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.7.6.1.1">
<span class="ltx_p" id="S3.T2.1.1.7.6.1.1.1" style="width:85.4pt;">MIND2WEB <cite class="ltx_cite ltx_citemacro_citep">(Deng et al., <a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib27" title="">2023</a>)</cite></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b" id="S3.T2.1.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.7.6.2.1">
<span class="ltx_p" id="S3.T2.1.1.7.6.2.1.1" style="width:369.9pt;">over 2,000 tasks from 137 real-world websites across 31 domains with crowdsourced action sequences, enabling the creation of agents that handle diverse, complex web interactions</span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Benchmarks for Agent Evaluation</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The evaluation LLMs’ capability on tool manipulation primarily revolves around assessing the efficacy of a single tool, gauging its impact on downstream tasks using established benchmarks, as discussed previously. However, an increasing number of researchers are shifting their focus towards scenarios that involve the combined use of multiple tools to evaluate the performance of LLMs trained with tool learning. This approach ensures a more comprehensive and diverse appraisal of the model’s abilities and constraints across various tool sets.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">APIBench <cite class="ltx_cite ltx_citemacro_cite">Patil et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib103" title="">2023</a>)</cite> assembles a comprehensive API corpus from major hubs like HuggingFace, TorchHub, and TensorHub, including all API calls from TorchHub and TensorHub and the top 20 most downloaded models from each HuggingFace task category. Using Self-Instruct <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib151" title="">2023a</a>)</cite>, they create 10 synthetic user prompts per API to evaluate LLMs for functional correctness and hallucination issues.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">ToolBench, developed by <cite class="ltx_cite ltx_citemacro_citet">Xu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib161" title="">2023c</a>)</cite>, evaluates LLMs’ generalization and advanced reasoning skills across various tool-based tasks. It integrates existing and newly collected datasets, featuring eight tasks with about 100 test cases each.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">Based on ToolBench, ToolLLM <cite class="ltx_cite ltx_citemacro_cite">Qin et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib109" title="">2023c</a>)</cite> introduces ToolEval, an automatic evaluator resembling a leaderboard. ToolEval uses two metrics: pass rate, which measures the proportion of successfully completed instructions within limited attempts, and win rate, which compares performance against ChatGPT. This evaluation method combines automatic and manual assessments while using ChatGPT-generated solutions as a benchmark, reducing potential human biases and unfairness.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">ToolAlpaca <cite class="ltx_cite ltx_citemacro_cite">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib140" title="">2023</a>)</cite> expands the evaluation framework to encompass real-world scenarios. Using a training set of 426 tool uses, the study evaluates ten new tools across 100 evaluation instances. Following the ReAct style <cite class="ltx_cite ltx_citemacro_cite">Yao et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib168" title="">2023b</a>)</cite>, tool usage is integrated during text generation, with human reviewers assessing program accuracy and overall correctness.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">RestBench <cite class="ltx_cite ltx_citemacro_cite">Song et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib129" title="">2023b</a>)</cite> explores real-world user instructions using APIs, focusing on TMDB movie database and Spotify music player scenarios. It filters 54 and 40 commonly used APIs respectively, constructing OpenAPI specifications. Integrating RestGPT, which links LLMs with RESTful APIs, it follows standard web service protocols. RestBench evaluates performance with human-annotated instructions and gold solution paths, demonstrating RestGPT’s effectiveness in complex tasks and advancing towards Artificial General Intelligence (AGI).</p>
</div>
<div class="ltx_para" id="S3.SS3.p7">
<p class="ltx_p" id="S3.SS3.p7.1">WebArena (<cite class="ltx_cite ltx_citemacro_citet">Zhou et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib186" title="">2023b</a>)</cite>) offers an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Its purpose is to evaluate agents in an end-to-end fashion and determine the accuracy of their completed tasks.</p>
</div>
<div class="ltx_para" id="S3.SS3.p8">
<p class="ltx_p" id="S3.SS3.p8.1">MIND2WEB (<cite class="ltx_cite ltx_citemacro_citet">Deng et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib27" title="">2023</a>)</cite>), is the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. MIND2WEB boasts a collection of over 2,000 tasks curated from 137 websites that span 31 different domains, replacing the oversimplified simulation environments commonly found in other datasets with a realm of real-world websites.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Future Directions</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The rapid advancements in the capabilities and application areas of LLMs have enabled them to replace other tools in a short time, significantly enhancing people’s lives. However, the progress in evaluation methodologies has not kept pace with the expansion of LLM capabilities, often making it challenging to find benchmarks that fully match current tasks. There is substantial room for improvement in current evaluation methods to assess LLMs’ performance in various tasks more accurately and provide a basis for decision-making. Consequently, we propose five future directions for developing evaluation methods. We expect these improvements will make LLMs a more "useful" presence in the eyes of the public.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Dynamic Evaluation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Current benchmarks are mostly static and do not change once they are created. However, unchanging benchmarks can present two problems when used for evaluation. Firstly, factual knowledge in the real world changes over time. For example, the presidency may change every four years, necessitating that datasets for evaluating the factual knowledge of LLMs also be updated over time and ideally updated automatically to ensure that the information provided by LLMs is accurate and contemporary.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Secondly, as LLM models expand, data from the datasets might leak and become part of the training data for LLMs, at which point these datasets no longer function as effective evaluative tools. Therefore, the evaluation questions within the datasets must be capable of being automatically replaced and updated. For example, the framework proposed by <cite class="ltx_cite ltx_citemacro_cite">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib149" title="">2024c</a>)</cite> can manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extends existing benchmarks. Such advancements would ensure that benchmarks can consistently measure the capabilities of LLMs as they progress.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>LLMs as Evaluators</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Many datasets currently require human annotators to label each question’s answer, a process that is both time-consuming and prone to errors. Therefore, employing LLMs as evaluators represents a promising direction for development. LLMs can simulate a scorer by reading text and providing ratings, allowing us to avoid designing new benchmarks for every task. Instead, we can leverage the broad capabilities of LLMs to act as scorers across various tasks. <cite class="ltx_cite ltx_citemacro_citet">Li et al. (<a class="ltx_ref" href="https://arxiv.org/html/2406.00936v1#bib.bib70" title="">2024b</a>)</cite> has reviewed the current methods of using LLMs as scorers and has also identified potential issues, such as a preference for content generated by the same model or specific biases in evaluation order. In the future, we can gradually address the biases inherent in LLMs as evaluators. In that case, we can enhance the rapid development of LLM applications while enabling them to self-assess, thus eliminating the need for additional dataset design.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Root Cause Analysis</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">The evaluation methods we mentioned earlier primarily rely on assessing LLMs’ outputs. For instance, we pose questions to LLMs and evaluate them based on the accuracy of their responses. This evaluation approach allows us to quickly gauge the extent of a model’s capabilities in various aspects and understand what it can help us accomplish. However, by solely examining the model’s output, we cannot identify the <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">root cause</span> of why the model produces a particular response. When the model answers correctly, we cannot ascertain whether it genuinely possesses the corresponding ability or if it has simply encountered similar questions before and memorized the answers. Similarly, when the model’s response does not meet expectations, it is also challenging to determine why the model made an error. Therefore, we propose that future evaluation methods should include analyzing the <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.2">root cause</span> of model predictions. This will enable us to better analyze LLMs, facilitating the development of more useful LLMs in the future.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Fine-grained LLM Agent Evaluation</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Existing benchmarks mostly rely on the final completion status of tasks, lacking fine-grained step-wise evaluations. Additionally, while current research focuses more on agents’ capabilities in executing tasks within limited environments such as online-shopping, environmental feedback is often rule-based, simplistic, and distant from real-world scenarios. A potential future direction is to leverage high-intelligence models like LLM to design more realistic evaluation environments.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Robot Benchmark Development</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">Recent research in robotics primarily emphasizes the use of simulation environments to facilitate the transition to real-world applications. These environments are pivotal in enhancing the generalization capabilities of robots across various conditions. There is an increasing need to develop large-scale benchmarks, comparable to ImageNet in the field of computer vision, to rigorously assess these generalization abilities. Moreover, to accurately simulate real-world scenarios, it is essential to integrate specific tasks that mirror actual conditions. Additionally, the concept of a digital twin represents another promising avenue for evaluating robots in both simulated and real-world settings. Given the substantial disparities that still exist in computer vision when testing out-of-domain data, employing digital twins and similar methodologies could significantly reduce the sim-2-real gap, thereby enabling a more focused approach on evaluate models capabilities.</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">Furthermore, detailed evaluations of other aspects, such as the sim-to-real gap, robustness against adversarial perturbations, human-robot collaboration, and multi-robot coordination, remain critical for deploying robots effectively in real-world scenarios. Lastly, as deep learning continues to demonstrate success with extensive data training, evaluating robot foundational models like RT-2 and PaLM-E will also be essential for advancing our understanding and application of robotics in complex environments.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Because of the inexplicability of LLMs, we need various evaluation methods to understand their capabilities, and this is the driving force behind the progress of LLMs. This study introduced the two-stage framework: from core ability to agent to evaluate the usability of LLMs. We reviewed applications, benchmarks, and evaluation methods in each section, aiming to elucidate the advantages and limitations of current LLM development. Lastly, we proposed several directions for the advancement of LLMs evaluation methods aimed at making future evaluations of LLMs more flexible, automated, and capable of identifying the root causes of issues. We look forward to future research making LLMs a more useful tool for aiding human society.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdelghani et al. (2023)</span>
<span class="ltx_bibblock">
Rania Abdelghani, Yen-Hsiang Wang, Xingdi Yuan, Tong Wang, Pauline Lucas, Hélène Sauzéon, and Pierre-Yves Oudeyer. 2023.

</span>
<span class="ltx_bibblock">Gpt-3-driven pedagogical agents to train children’s curious question-asking skills.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">International Journal of Artificial Intelligence in Education</em>, pages 1–36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al. (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2303.08774</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al. (2022)</span>
<span class="ltx_bibblock">
Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. 2022.

</span>
<span class="ltx_bibblock">Large language models are few-shot clinical information extractors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2205.12689</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alvarado et al. (2015)</span>
<span class="ltx_bibblock">
Julio Cesar Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. 2015.

</span>
<span class="ltx_bibblock">Domain adaption of named entity recognition to support credit risk assessment.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the Australasian Language Technology Association Workshop 2015</em>, pages 84–90.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Athan et al. (2013)</span>
<span class="ltx_bibblock">
Tara Athan, Harold Boley, Guido Governatori, Monica Palmirani, Adrian Paschke, and Adam Wyner. 2013.

</span>
<span class="ltx_bibblock">Oasis legalruleml.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">proceedings of the fourteenth international conference on artificial intelligence and law</em>, pages 3–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Balas et al. (2024)</span>
<span class="ltx_bibblock">
Michael Balas, Jordan Joseph Wadden, Philip C Hébert, Eric Mathison, Marika D Warren, Victoria Seavilleklein, Daniel Wyzynski, Alison Callahan, Sean A Crawford, Parnian Arjmand, et al. 2024.

</span>
<span class="ltx_bibblock">Exploring the potential utility of ai large language models for medical ethics: an expert panel evaluation of gpt-4.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Journal of Medical Ethics</em>, 50(2):90–96.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee and Lavie (2005)</span>
<span class="ltx_bibblock">
Satanjeev Banerjee and Alon Lavie. 2005.

</span>
<span class="ltx_bibblock">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</em>, pages 65–72.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bang et al. (2023)</span>
<span class="ltx_bibblock">
Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu, and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2302.04023" title="">A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benoit (2023)</span>
<span class="ltx_bibblock">
James RA Benoit. 2023.

</span>
<span class="ltx_bibblock">Chatgpt for clinical vignette generation, revision, and evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">MedRxiv</em>, pages 2023–02.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bhagavatula et al. (2019)</span>
<span class="ltx_bibblock">
Chandra Bhagavatula, Ronan Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Wen-tau Yih, and Yejin Choi. 2019.

</span>
<span class="ltx_bibblock">Abductive commonsense reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:1908.05739</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bian et al. (2024)</span>
<span class="ltx_bibblock">
Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, Ben He, Shanshan Jiang, and Bin Dong. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.16421" title="">Chatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bisk et al. (2020)</span>
<span class="ltx_bibblock">
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020.

</span>
<span class="ltx_bibblock">Piqa: Reasoning about physical commonsense in natural language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, volume 34, pages 7432–7439.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blair-Stanek et al. (2023)</span>
<span class="ltx_bibblock">
Andrew Blair-Stanek, Nils Holzenberger, and Benjamin Van Durme. 2023.

</span>
<span class="ltx_bibblock">Can gpt-3 perform statutory reasoning?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law</em>, pages 22–31.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cai et al. (2024)</span>
<span class="ltx_bibblock">
Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.17126" title="">Large language models as tool makers</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carlini et al. (2023)</span>
<span class="ltx_bibblock">
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2202.07646" title="">Quantifying memorization across neural language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Casino et al. (2022)</span>
<span class="ltx_bibblock">
Fran Casino, Thomas K Dasaklis, Georgios P Spathoulas, Marios Anagnostopoulos, Amrita Ghosal, Istvan Borocz, Agusti Solanas, Mauro Conti, and Constantinos Patsakis. 2022.

</span>
<span class="ltx_bibblock">Research trends, challenges, and emerging topics in digital forensics: A review of reviews.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">IEEE Access</em>, 10:25464–25493.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al. (2023)</span>
<span class="ltx_bibblock">
Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023.

</span>
<span class="ltx_bibblock">A survey on evaluation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">ACM Transactions on Intelligent Systems and Technology</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2020)</span>
<span class="ltx_bibblock">
Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Wang. 2020.

</span>
<span class="ltx_bibblock">Hybridqa: A dataset of multi-hop question answering over tabular and textual data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2004.07347</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2022)</span>
<span class="ltx_bibblock">
Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022.

</span>
<span class="ltx_bibblock">Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2210.03849</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cifuentes et al. (2022)</span>
<span class="ltx_bibblock">
Jenny Cifuentes, Ana Lucila Sandoval Orozco, and Luis Javier Garcia Villalba. 2022.

</span>
<span class="ltx_bibblock">A survey of artificial intelligence strategies for automatic detection of sexually explicit videos.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Multimedia Tools and Applications</em>, 81(3):3205–3222.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cobbe et al. (2021)</span>
<span class="ltx_bibblock">
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021.

</span>
<span class="ltx_bibblock">Training verifiers to solve math word problems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2110.14168</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dalvi et al. (2021)</span>
<span class="ltx_bibblock">
Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021.

</span>
<span class="ltx_bibblock">Explaining answers with entailment trees.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2104.08661</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Daniel et al. (2008)</span>
<span class="ltx_bibblock">
Gilles Daniel, Didier Sornette, and Peter Wohrmann. 2008.

</span>
<span class="ltx_bibblock">Look-ahead benchmark bias in portfolio performance evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:0810.1922</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das et al. (2024)</span>
<span class="ltx_bibblock">
Badhan Chandra Das, M Hadi Amini, and Yanzhao Wu. 2024.

</span>
<span class="ltx_bibblock">Security and privacy challenges of large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2402.00888</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Demszky et al. (2023)</span>
<span class="ltx_bibblock">
Dorottya Demszky, Diyi Yang, David S Yeager, Christopher J Bryan, Margarett Clapper, Susannah Chandhok, Johannes C Eichstaedt, Cameron Hecht, Jeremy Jamieson, Meghann Johnson, et al. 2023.

</span>
<span class="ltx_bibblock">Using large language models in psychology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Nature Reviews Psychology</em>, 2(11):688–701.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2020)</span>
<span class="ltx_bibblock">
Xiang Deng, Ahmed Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, and Matthew Richardson. 2020.

</span>
<span class="ltx_bibblock">Structure-grounded pretraining for text-to-sql.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2010.12773</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al. (2023)</span>
<span class="ltx_bibblock">
Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.06070" title="">Mind2web: Towards a generalist agent for the web</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deroy et al. (2023)</span>
<span class="ltx_bibblock">
Aniket Deroy, Kripabandhu Ghosh, and Saptarshi Ghosh. 2023.

</span>
<span class="ltx_bibblock">How ready are pre-trained abstractive models and llms for legal case judgement summarization?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2306.01248</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dijkstra et al. (2022)</span>
<span class="ltx_bibblock">
Ramon Dijkstra, Zülküf Genç, Subhradeep Kayal, Jaap Kamps, et al. 2022.

</span>
<span class="ltx_bibblock">Reading comprehension quiz generation using generative pre-trained transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">iTextbooks@ AIED</em>, pages 4–17.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Du et al. (2014)</span>
<span class="ltx_bibblock">
Shichuan Du, Yong Tao, and Aleix M Martinez. 2014.

</span>
<span class="ltx_bibblock">Compound facial expressions of emotion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the national academy of sciences</em>, 111(15):E1454–E1462.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Duan et al. (2024)</span>
<span class="ltx_bibblock">
Shitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, and Ning Gu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.11053" title="">Denevil: Towards deciphering and navigating the ethical values of large language models via instruction learning</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois et al. (2024)</span>
<span class="ltx_bibblock">
Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. 2024.

</span>
<span class="ltx_bibblock">Alpacafarm: A simulation framework for methods that learn from human feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Engel and Mcadams (2024)</span>
<span class="ltx_bibblock">
Christoph Engel and Richard H Mcadams. 2024.

</span>
<span class="ltx_bibblock">Asking gpt for the ordinary meaning of statutory terms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">MPI Collective Goods Discussion Paper</em>, (2024/5).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al. (2019)</span>
<span class="ltx_bibblock">
Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019.

</span>
<span class="ltx_bibblock">Eli5: Long form question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:1907.09190</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feldman et al. (2023)</span>
<span class="ltx_bibblock">
Philip Feldman, James R. Foulds, and Shimei Pan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.06085" title="">Trapping llm hallucinations using tagged context prompts</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. 2023.

</span>
<span class="ltx_bibblock">Text-to-sql empowered by large language models: A benchmark evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2308.15363</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geva et al. (2021)</span>
<span class="ltx_bibblock">
Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021.

</span>
<span class="ltx_bibblock">Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Transactions of the Association for Computational Linguistics</em>, 9:346–361.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghallab et al. (2004)</span>
<span class="ltx_bibblock">
Malik Ghallab, Dana Nau, and Paolo Traverso. 2004.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Automated Planning: theory and practice</em>.

</span>
<span class="ltx_bibblock">Elsevier.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ghosh et al. (2024)</span>
<span class="ltx_bibblock">
Shaona Ghosh, Prasoon Varshney, Erick Galinkin, and Christopher Parisien. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2404.05993" title="">Aegis: Online adaptive ai content safety moderation with ensemble of llm experts</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al. (2023)</span>
<span class="ltx_bibblock">
Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al. 2023.

</span>
<span class="ltx_bibblock">Evaluating large language models: A comprehensive survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2310.19736</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. (2023)</span>
<span class="ltx_bibblock">
Simon J. Han, Keith Ransom, Andrew Perfors, and Charles Kemp. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.06548" title="">Inductive reasoning in humans and large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. (2021)</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.

</span>
<span class="ltx_bibblock">Measuring mathematical problem solving with the math dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2103.03874</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Holzenberger et al. (2020)</span>
<span class="ltx_bibblock">
Nils Holzenberger, Andrew Blair-Stanek, and Benjamin Van Durme. 2020.

</span>
<span class="ltx_bibblock">A dataset for statutory reasoning in tax law entailment and question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2005.05257</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hort et al. (2021)</span>
<span class="ltx_bibblock">
Max Hort, Jie M Zhang, Federica Sarro, and Mark Harman. 2021.

</span>
<span class="ltx_bibblock">Fairea: A model behaviour mutation approach to benchmarking bias mitigation methods.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Proceedings of the 29th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering</em>, pages 994–1006.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2023)</span>
<span class="ltx_bibblock">
Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.03901" title="">Chatdb: Augmenting llms with databases as their symbolic memory</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang and Chang (2023)</span>
<span class="ltx_bibblock">
Jie Huang and Kevin Chen-Chuan Chang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2212.10403" title="">Towards reasoning in large language models: A survey</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2022a)</span>
<span class="ltx_bibblock">
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022a.

</span>
<span class="ltx_bibblock">Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">International Conference on Machine Learning</em>, pages 9118–9147. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2023)</span>
<span class="ltx_bibblock">
Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. 2023.

</span>
<span class="ltx_bibblock">Voxposer: Composable 3d value maps for robotic manipulation with language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2307.05973</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2022b)</span>
<span class="ltx_bibblock">
Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. 2022b.

</span>
<span class="ltx_bibblock">Inner monologue: Embodied reasoning through planning with language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2207.05608</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al. (2023)</span>
<span class="ltx_bibblock">
Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.04657" title="">Beavertails: Towards improved safety alignment of llm via a human-preference dataset</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jia et al. (2021)</span>
<span class="ltx_bibblock">
Qinjin Jia, Jialin Cui, Yunkai Xiao, Chengyuan Liu, Parvez Rashid, and Edward F Gehringer. 2021.

</span>
<span class="ltx_bibblock">All-in-one: Multi-task learning bert models for evaluating peer assessments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">arXiv preprint arXiv:2110.03895</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Chaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan, Ji Zhang, and Shikun Zhang. 2024.

</span>
<span class="ltx_bibblock">Hal-eval: A universal and fine-grained hallucination evaluation framework for large vision language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">arXiv preprint arXiv:2402.15721</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2020)</span>
<span class="ltx_bibblock">
Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2011.03088" title="">Hover: A dataset for many-hop fact extraction and claim verification</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. (2024)</span>
<span class="ltx_bibblock">
Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong Zhang, Yongfeng Zhang, et al. 2024.

</span>
<span class="ltx_bibblock">Attackeval: How to evaluate the effectiveness of jailbreak attacking on large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2401.09002</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karamizadeh et al. (2023)</span>
<span class="ltx_bibblock">
Sasan Karamizadeh, Saman Shojae Chaeikar, and Alireza Jolfaei. 2023.

</span>
<span class="ltx_bibblock">Adult content image recognition by boltzmann machine limited and deep learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Evolutionary Intelligence</em>, 16(4):1185–1194.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karinshak et al. (2023)</span>
<span class="ltx_bibblock">
Elise Karinshak, Sunny Xun Liu, Joon Sung Park, and Jeffrey T Hancock. 2023.

</span>
<span class="ltx_bibblock">Working with ai to persuade: Examining a large language model’s ability to generate pro-vaccination messages.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the ACM on Human-Computer Interaction</em>, 7(CSCW1):1–29.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasneci et al. (2023)</span>
<span class="ltx_bibblock">
Enkelejda Kasneci, Kathrin Seßler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, et al. 2023.

</span>
<span class="ltx_bibblock">Chatgpt for good? on opportunities and challenges of large language models for education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Learning and individual differences</em>, 103:102274.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Katz et al. (2024)</span>
<span class="ltx_bibblock">
Daniel Martin Katz, Michael James Bommarito, Shang Gao, and Pablo Arredondo. 2024.

</span>
<span class="ltx_bibblock">Gpt-4 passes the bar exam.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Philosophical Transactions of the Royal Society A</em>, 382(2270):20230254.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. (2024)</span>
<span class="ltx_bibblock">
Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R Bowman, Tim Rocktäschel, and Ethan Perez. 2024.

</span>
<span class="ltx_bibblock">Debating with more persuasive llms leads to more truthful answers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:2402.06782</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2024a)</span>
<span class="ltx_bibblock">
Minju Kim, Heuiyeen Yeen, and Myoung-Wan Koo. 2024a.

</span>
<span class="ltx_bibblock">Towards context-based violence detection: A korean crime dialogue dataset.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Findings of the Association for Computational Linguistics: EACL 2024</em>, pages 603–623.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al. (2024b)</span>
<span class="ltx_bibblock">
Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong Joon Oh. 2024b.

</span>
<span class="ltx_bibblock">Propile: Probing privacy leakage in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kosinski (2023)</span>
<span class="ltx_bibblock">
Michal Kosinski. 2023.

</span>
<span class="ltx_bibblock">Theory of mind may have spontaneously emerged in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2302.02083</em>, 4:169.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar (2023)</span>
<span class="ltx_bibblock">
Arun HS Kumar. 2023.

</span>
<span class="ltx_bibblock">Analysis of chatgpt tool to assess the potential of its utility for academic writing in biomedical domain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Biology, Engineering, Medicine and Science Reports</em>, 9(1):24–30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kung et al. (2023)</span>
<span class="ltx_bibblock">
Tiffany H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepaño, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, et al. 2023.

</span>
<span class="ltx_bibblock">Performance of chatgpt on usmle: potential for ai-assisted medical education using large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">PLoS digital health</em>, 2(2):e0000198.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwiatkowski et al. (2019)</span>
<span class="ltx_bibblock">
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019.

</span>
<span class="ltx_bibblock">Natural questions: a benchmark for question answering research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Transactions of the Association for Computational Linguistics</em>, 7:453–466.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. (2024)</span>
<span class="ltx_bibblock">
Jean Lee, Nicholas Stevens, Soyeon Caren Han, and Minseok Song. 2024.

</span>
<span class="ltx_bibblock">A survey of large language models in finance (finllms).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">arXiv preprint arXiv:2402.02315</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024a)</span>
<span class="ltx_bibblock">
Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, and Jing Shao. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2402.05044" title="">Salad-bench: A hierarchical and comprehensive safety benchmark for large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023a)</span>
<span class="ltx_bibblock">
Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.08244" title="">Api-bank: A comprehensive benchmark for tool-augmented llms</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023b)</span>
<span class="ltx_bibblock">
Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023b.

</span>
<span class="ltx_bibblock">Large language models in finance: A survey.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">Proceedings of the Fourth ACM International Conference on AI in Finance</em>, pages 374–382.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2024b)</span>
<span class="ltx_bibblock">
Zhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, and Chongyang Tao. 2024b.

</span>
<span class="ltx_bibblock">Leveraging large language models for nlg evaluation: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2401.07103</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2023)</span>
<span class="ltx_bibblock">
Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2023.

</span>
<span class="ltx_bibblock">Code as policies: Language model programs for embodied control.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">2023 IEEE International Conference on Robotics and Automation (ICRA)</em>, pages 9493–9500. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2022)</span>
<span class="ltx_bibblock">
Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. 2022.

</span>
<span class="ltx_bibblock">Holistic evaluation of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">arXiv preprint arXiv:2211.09110</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al. (2019)</span>
<span class="ltx_bibblock">
Yichan Liang, Jianheng Li, and Jian Yin. 2019.

</span>
<span class="ltx_bibblock">A new multi-choice reading comprehension dataset for curriculum learning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Asian Conference on Machine Learning</em>, pages 742–757. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liga and Robaldo (2023)</span>
<span class="ltx_bibblock">
Davide Liga and Livio Robaldo. 2023.

</span>
<span class="ltx_bibblock">Fine-tuning gpt-3 for legal rule classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Computer Law &amp; Security Review</em>, 51:105864.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock">Rouge: A package for automatic evaluation of summaries.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Text summarization branches out</em>, pages 74–81.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2022)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2109.07958" title="">Truthfulqa: Measuring how models mimic human falsehoods</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2023)</span>
<span class="ltx_bibblock">
Zi Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.17389" title="">Toxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024a)</span>
<span class="ltx_bibblock">
Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, and Li Zhang. 2024a.

</span>
<span class="ltx_bibblock">Exploring and evaluating hallucinations in llm-powered code generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">arXiv preprint arXiv:2404.00971</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2023)</span>
<span class="ltx_bibblock">
Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.03439" title="">Evaluating the logical reasoning ability of chatgpt and gpt-4</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024b)</span>
<span class="ltx_bibblock">
Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, and Chaowei Xiao. 2024b.

</span>
<span class="ltx_bibblock">Automatic and universal prompt injection attacks against large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">arXiv preprint arXiv:2403.04957</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021a)</span>
<span class="ltx_bibblock">
Xin Liu, Henglin Shi, Haoyu Chen, Zitong Yu, Xiaobai Li, and Guoying Zhao. 2021a.

</span>
<span class="ltx_bibblock">imigue: An identity-free video dataset for micro-gesture understanding and emotion analysis.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, pages 10631–10642.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2021b)</span>
<span class="ltx_bibblock">
Zhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li, and Jun Zhao. 2021b.

</span>
<span class="ltx_bibblock">Finbert: A pre-trained financial language representation model for financial text mining.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">Proceedings of the twenty-ninth international conference on international joint conferences on artificial intelligence</em>, pages 4513–4519.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2024)</span>
<span class="ltx_bibblock">
Hao Lu, Xuesong Niu, Jiyao Wang, Yin Wang, Qingyong Hu, Jiaqi Tang, Yuting Zhang, Kaishen Yuan, Bin Huang, Zitong Yu, et al. 2024.

</span>
<span class="ltx_bibblock">Gpt as psychologist? preliminary evaluations for gpt-4v on visual affective computing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">arXiv preprint arXiv:2403.05916</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mahowald et al. (2023)</span>
<span class="ltx_bibblock">
Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. 2023.

</span>
<span class="ltx_bibblock">Dissociating language and thought in large language models: a cognitive perspective.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">arXiv preprint arXiv:2301.06627</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maia et al. (2018)</span>
<span class="ltx_bibblock">
Macedo Maia, Siegfried Handschuh, André Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018.

</span>
<span class="ltx_bibblock">Www’18 open challenge: financial opinion mining and question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">Companion proceedings of the the web conference 2018</em>, pages 1941–1942.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malo et al. (2014)</span>
<span class="ltx_bibblock">
Pekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. 2014.

</span>
<span class="ltx_bibblock">Good debt or bad debt: Detecting semantic orientations in economic texts.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">Journal of the Association for Information Science and Technology</em>, 65(4):782–796.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mann et al. (2020)</span>
<span class="ltx_bibblock">
Ben Mann, N Ryder, M Subbiah, J Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, S Agarwal, et al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">arXiv preprint arXiv:2005.14165</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Markov et al. (2023)</span>
<span class="ltx_bibblock">
Todor Markov, Chong Zhang, Sandhini Agarwal, Tyna Eloundou, Teddy Lee, Steven Adler, Angela Jiang, and Lilian Weng. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2208.03274" title="">A holistic approach to undesired content detection in the real world</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mavadati et al. (2013)</span>
<span class="ltx_bibblock">
S Mohammad Mavadati, Mohammad H Mahoor, Kevin Bartlett, Philip Trinh, and Jeffrey F Cohn. 2013.

</span>
<span class="ltx_bibblock">Disfa: A spontaneous facial action intensity database.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">IEEE Transactions on Affective Computing</em>, 4(2):151–160.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Menick et al. (2022)</span>
<span class="ltx_bibblock">
Jacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingam, Geoffrey Irving, et al. 2022.

</span>
<span class="ltx_bibblock">Teaching language models to support answers with verified quotes. arxiv.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miao et al. (2023)</span>
<span class="ltx_bibblock">
Ning Miao, Yee Whye Teh, and Tom Rainforth. 2023.

</span>
<span class="ltx_bibblock">Selfcheck: Using llms to zero-shot check their own step-by-step reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">arXiv preprint arXiv:2308.00436</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mihaylov et al. (2018)</span>
<span class="ltx_bibblock">
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.

</span>
<span class="ltx_bibblock">Can a suit of armor conduct electricity? a new dataset for open book question answering.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">EMNLP</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Min et al. (2023)</span>
<span class="ltx_bibblock">
Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023.

</span>
<span class="ltx_bibblock">Recent advances in natural language processing via large pre-trained language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">ACM Computing Surveys</em>, 56(2):1–40.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moon et al. (2014)</span>
<span class="ltx_bibblock">
Sungrim Moon, Serguei Pakhomov, Nathan Liu, James O Ryan, and Genevieve B Melton. 2014.

</span>
<span class="ltx_bibblock">A sense inventory for clinical abbreviations and acronyms created using clinical notes and medical dictionary resources.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Journal of the American Medical Informatics Association</em>, 21(2):299–307.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Muthukrishnan et al. (2020)</span>
<span class="ltx_bibblock">
Nikesh Muthukrishnan, Farhad Maleki, Katie Ovens, Caroline Reinhold, Behzad Forghani, Reza Forghani, et al. 2020.

</span>
<span class="ltx_bibblock">Brief history of artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">Neuroimaging Clinics of North America</em>, 30(4):393–399.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakano et al. (2022)</span>
<span class="ltx_bibblock">
Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2112.09332" title="">Webgpt: Browser-assisted question-answering with human feedback</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nayerifard et al. (2023)</span>
<span class="ltx_bibblock">
Tahereh Nayerifard, Haleh Amintoosi, Abbas Ghaemi Bafghi, and Ali Dehghantanha. 2023.

</span>
<span class="ltx_bibblock">Machine learning in digital forensics: a systematic literature review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">arXiv preprint arXiv:2306.04965</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nolfi (2023)</span>
<span class="ltx_bibblock">
Stefano Nolfi. 2023.

</span>
<span class="ltx_bibblock">On the unexpected abilities of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">arXiv preprint arXiv:2308.09720</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oviedo-Trespalacios et al. (2023)</span>
<span class="ltx_bibblock">
Oscar Oviedo-Trespalacios, Amy E Peden, Thomas Cole-Hunter, Arianna Costantini, Milad Haghani, JE Rod, Sage Kelly, Helma Torkamaan, Amina Tariq, James David Albert Newton, et al. 2023.

</span>
<span class="ltx_bibblock">The risks of using chatgpt to obtain common safety-related information and advice.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">Safety science</em>, 167:106244.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Palmirani and Vitali (2011)</span>
<span class="ltx_bibblock">
Monica Palmirani and Fabio Vitali. 2011.

</span>
<span class="ltx_bibblock">Akoma-ntoso for legal documents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">Legislative XML for the Semantic Web: Principles, Models, Standards for Document Management</em>, pages 75–100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>, pages 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Parrish et al. (2021)</span>
<span class="ltx_bibblock">
Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R Bowman. 2021.

</span>
<span class="ltx_bibblock">Bbq: A hand-built bias benchmark for question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">arXiv preprint arXiv:2110.08193</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patil et al. (2023)</span>
<span class="ltx_bibblock">
Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.15334" title="">Gorilla: Large language model connected with massive apis</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petroni et al. (2019)</span>
<span class="ltx_bibblock">
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019.

</span>
<span class="ltx_bibblock">Language models as knowledge bases?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">arXiv preprint arXiv:1909.01066</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pinar Saygin et al. (2000)</span>
<span class="ltx_bibblock">
Ayse Pinar Saygin, Ilyas Cicekli, and Varol Akman. 2000.

</span>
<span class="ltx_bibblock">Turing test: 50 years later.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">Minds and machines</em>, 10(4):463–518.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qian et al. (2023)</span>
<span class="ltx_bibblock">
Cheng Qian, Chi Han, Yi R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.14318" title="">Creator: Tool creation for disentangling abstract and concrete reasoning of large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2023a)</span>
<span class="ltx_bibblock">
Yujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.06849" title="">Webcpm: Interactive web search for chinese long-form question answering</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2023b)</span>
<span class="ltx_bibblock">
Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.08354" title="">Tool learning with foundation models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al. (2023c)</span>
<span class="ltx_bibblock">
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023c.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.16789" title="">Toolllm: Facilitating large language models to master 16000+ real-world apis</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2018)</span>
<span class="ltx_bibblock">
Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018.

</span>
<span class="ltx_bibblock">Improving language understanding by generative pre-training.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">OpenAI blog</em>, 1(8):9.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raina and Gales (2022)</span>
<span class="ltx_bibblock">
Vatsal Raina and Mark Gales. 2022.

</span>
<span class="ltx_bibblock">Multiple-choice question generation: Towards an automated assessment framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">arXiv preprint arXiv:2209.11830</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rana et al. (2023)</span>
<span class="ltx_bibblock">
Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=wMpOMO0Ss7a" title="">Sayplan: Grounding large language models using 3d scene graphs for scalable task planning</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">7th Annual Conference on Robot Learning</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rathje et al. (2023)</span>
<span class="ltx_bibblock">
Steve Rathje, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay J Van Bavel. 2023.

</span>
<span class="ltx_bibblock">Gpt is an effective tool for multilingual psychological text analysis.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al. (2023)</span>
<span class="ltx_bibblock">
Allen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et al. 2023.

</span>
<span class="ltx_bibblock">Robots that ask for help: Uncertainty alignment for large language model planners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">arXiv preprint arXiv:2307.01928</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ruan et al. (2023)</span>
<span class="ltx_bibblock">
Jingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2308.03427" title="">Tptu: Large language model-based ai agents for task planning and tool usage</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Salehi and Burgueño (2018)</span>
<span class="ltx_bibblock">
Hadi Salehi and Rigoberto Burgueño. 2018.

</span>
<span class="ltx_bibblock">Emerging artificial intelligence methods in structural engineering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">Engineering structures</em>, 171:170–189.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scherrer et al. (2023)</span>
<span class="ltx_bibblock">
Nino Scherrer, Claudia Shi, Amir Feder, and David M. Blei. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.14324" title="">Evaluating the moral beliefs encoded in llms</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al. (2023)</span>
<span class="ltx_bibblock">
Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2302.04761" title="">Toolformer: Language models can teach themselves to use tools</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Semigran et al. (2015)</span>
<span class="ltx_bibblock">
Hannah L Semigran, Jeffrey A Linder, Courtney Gidengil, and Ateev Mehrotra. 2015.

</span>
<span class="ltx_bibblock">Evaluation of symptom checkers for self diagnosis and triage: audit study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">bmj</em>, 351.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al. (2023)</span>
<span class="ltx_bibblock">
Dhruv Shah, Michael Robert Equi, Błażej Osiński, Fei Xia, Brian Ichter, and Sergey Levine. 2023.

</span>
<span class="ltx_bibblock">Navigation with large language models: Semantic guesswork as a heuristic for planning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">Conference on Robot Learning</em>, pages 2683–2699. PMLR.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shah et al. (2022)</span>
<span class="ltx_bibblock">
Dhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=UW5A3SweAH" title="">LM-nav: Robotic navigation with large pre-trained models of language, vision, and action</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">6th Annual Conference on Robot Learning</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shan and Deng (2018)</span>
<span class="ltx_bibblock">
Li Shan and Weihong Deng. 2018.

</span>
<span class="ltx_bibblock">Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">IEEE Transactions on Image Processing</em>, 28(1):356–370.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sharma and Thakur (2023)</span>
<span class="ltx_bibblock">
Gaurav Sharma and Abhishek Thakur. 2023.

</span>
<span class="ltx_bibblock">Chatgpt in drug discovery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shinn et al. (2023)</span>
<span class="ltx_bibblock">
Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2303.11366" title="">Reflexion: Language agents with verbal reinforcement learning</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. (2023)</span>
<span class="ltx_bibblock">
Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023.

</span>
<span class="ltx_bibblock">Progprompt: Generating situated robot task plans using large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">2023 IEEE International Conference on Robotics and Automation (ICRA)</em>, pages 11523–11530. IEEE.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sinha and Khandait (2021)</span>
<span class="ltx_bibblock">
Ankur Sinha and Tanmay Khandait. 2021.

</span>
<span class="ltx_bibblock">Impact of news on the commodity market: Dataset and results.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">Advances in Information and Communication: Proceedings of the 2021 Future of Information and Communication Conference (FICC), Volume 2</em>, pages 589–601. Springer.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2023a)</span>
<span class="ltx_bibblock">
Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. 2023a.

</span>
<span class="ltx_bibblock">Llm-planner: Few-shot grounded planning for embodied agents with large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al. (2023b)</span>
<span class="ltx_bibblock">
Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, Ye Tian, and Sujian Li. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.06624" title="">Restgpt: Connecting large language models with real-world restful apis</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sorensen et al. (2024)</span>
<span class="ltx_bibblock">
Taylor Sorensen, Liwei Jiang, Jena D Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, et al. 2024.

</span>
<span class="ltx_bibblock">Value kaleidoscope: Engaging ai with pluralistic human values, rights, and duties.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 38, pages 19937–19947.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. (2022)</span>
<span class="ltx_bibblock">
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, et al. 2022.

</span>
<span class="ltx_bibblock">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">arXiv preprint arXiv:2206.04615</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivastava et al. (2023)</span>
<span class="ltx_bibblock">
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlmüller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karakaş, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bojanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill Yuchen Lin, Blake Howald, Bryan
Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, César Ferri Ramírez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher D. Manning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Moseguí González, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang,
Ethan A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Martínez-Plumed, Francesca Happé, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra Winata, Gerard de Melo, Germán Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Schütze, Hiromu Yakura, Hongming Zhang, Hugh Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fernández Fisac, James B. Simon, James Koppel, James Zheng, James Zou, Jan Kocoń, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan
Waweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliveros Colón, Luke Metz, Lütfi Kerem Şenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramírez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew L. Leavitt, Matthias Hagen, Mátyás Schubert, Medina Orduna
Baitemirova, Melody Arnaud, Melvin McElrath, Michael A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Michał Swędrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma T, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel
Habacker, Ramon Risco, Raphaël Millière, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen
Prasad, Steven T. Piantadosi, Stuart M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2206.04615" title="">Beyond the imitation game: Quantifying and extrapolating the capabilities of language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Staab et al. (2023)</span>
<span class="ltx_bibblock">
Robin Staab, Mark Vero, Mislav Balunović, and Martin Vechev. 2023.

</span>
<span class="ltx_bibblock">Beyond memorization: Violating privacy via inference with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">arXiv preprint arXiv:2310.07298</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stolfo et al. (2023)</span>
<span class="ltx_bibblock">
Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schölkopf, and Mrinmaya Sachan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2210.12023" title="">A causal framework to quantify the robustness of mathematical reasoning with language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023)</span>
<span class="ltx_bibblock">
Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/b5c8c1c117618267944b2617add0a766-Paper-Conference.pdf" title="">Adaplanner: Adaptive planning from feedback with language models</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">Advances in Neural Information Processing Systems</em>, volume 36, pages 58202–58245. Curran Associates, Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2024)</span>
<span class="ltx_bibblock">
Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang, Pheng Ann Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Ji-Rong Wen, Xipeng Qiu, Yike Guo, Hui Xiong, Qun Liu, and Zhenguo Li. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2312.11562" title="">A survey of reasoning with foundation models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suzgun et al. (2022)</span>
<span class="ltx_bibblock">
Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022.

</span>
<span class="ltx_bibblock">Challenging big-bench tasks and whether chain-of-thought can solve them.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">arXiv preprint arXiv:2210.09261</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor et al. (2018)</span>
<span class="ltx_bibblock">
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018.

</span>
<span class="ltx_bibblock">Commonsenseqa: A question answering challenge targeting commonsense knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">arXiv preprint arXiv:1811.00937</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2024)</span>
<span class="ltx_bibblock">
Liyan Tang, Igor Shalyminov, Amy Wing mei Wong, Jon Burnsky, Jake W. Vincent, Yu’an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, Lijia Sun, Yi Zhang, Saab Mansour, and Kathleen McKeown. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2402.13249" title="">Tofueval: Evaluating hallucinations of llms on topic-focused dialogue summarization</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2023)</span>
<span class="ltx_bibblock">
Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.05301" title="">Toolalpaca: Generalized tool learning for language models with 3000 simulated cases</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thirunavukarasu et al. (2023)</span>
<span class="ltx_bibblock">
Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. 2023.

</span>
<span class="ltx_bibblock">Large language models in medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">Nature medicine</em>, 29(8):1930–1940.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tobia (2020)</span>
<span class="ltx_bibblock">
Kevin P Tobia. 2020.

</span>
<span class="ltx_bibblock">Testing ordinary meaning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">Harv. L. Rev.</em>, 134:726.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Turpin et al. (2023)</span>
<span class="ltx_bibblock">
Miles Turpin, Julian Michael, Ethan Perez, and Samuel R. Bowman. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.04388" title="">Language models don’t always say what they think: Unfaithful explanations in chain-of-thought prompting</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani et al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">Advances in neural information processing systems</em>, 30.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vermetten et al. (2022)</span>
<span class="ltx_bibblock">
Diederick Vermetten, Bas van Stein, Fabio Caraffini, Leandro L Minku, and Anna V Kononova. 2022.

</span>
<span class="ltx_bibblock">Bias: A toolbox for benchmarking structural bias in the continuous domain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib145.1.1">IEEE Transactions on Evolutionary Computation</em>, 26(6):1380–1393.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vidgen et al. (2024)</span>
<span class="ltx_bibblock">
Bertie Vidgen, Adarsh Agrawal, Ahmed M. Ahmed, Victor Akinwande, Namir Al-Nuaimi, Najla Alfaraj, Elie Alhajjar, Lora Aroyo, Trupti Bavalatti, Borhane Blili-Hamelin, Kurt Bollacker, Rishi Bomassani, Marisa Ferrara Boston, Siméon Campos, Kal Chakra, Canyu Chen, Cody Coleman, Zacharie Delpierre Coudert, Leon Derczynski, Debojyoti Dutta, Ian Eisenberg, James Ezick, Heather Frase, Brian Fuller, Ram Gandikota, Agasthya Gangavarapu, Ananya Gangavarapu, James Gealy, Rajat Ghosh, James Goel, Usman Gohar, Sujata Goswami, Scott A. Hale, Wiebke Hutiri, Joseph Marvin Imperial, Surgan Jandial, Nick Judd, Felix Juefei-Xu, Foutse Khomh, Bhavya Kailkhura, Hannah Rose Kirk, Kevin Klyman, Chris Knotz, Michael Kuchnik, Shachi H. Kumar, Chris Lengerich, Bo Li, Zeyi Liao, Eileen Peters Long, Victor Lu, Yifan Mai, Priyanka Mary Mammen, Kelvin Manyeki, Sean McGregor, Virendra Mehta, Shafee Mohammed, Emanuel Moss, Lama Nachman, Dinesh Jinenhally Naganna, Amin Nikanjam, Besmira Nushi, Luis Oala, Iftach Orr, Alicia Parrish, Cigdem
Patlak, William Pietri, Forough Poursabzi-Sangdeh, Eleonora Presani, Fabrizio Puletti, Paul Röttger, Saurav Sahay, Tim Santos, Nino Scherrer, Alice Schoenauer Sebag, Patrick Schramowski, Abolfazl Shahbazi, Vin Sharma, Xudong Shen, Vamsi Sistla, Leonard Tang, Davide Testuggine, Vithursan Thangarasa, Elizabeth Anne Watkins, Rebecca Weiss, Chris Welty, Tyler Wilbers, Adina Williams, Carole-Jean Wu, Poonam Yadav, Xianjun Yang, Yi Zeng, Wenhui Zhang, Fedor Zhdanov, Jiacheng Zhu, Percy Liang, Peter Mattson, and Joaquin Vanschoren. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2404.12241" title="">Introducing v0.5 of the ai safety benchmark from mlcommons</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024a)</span>
<span class="ltx_bibblock">
Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji Zhang, and Jitao Sang. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2311.07397" title="">Amber: An llm-free multi-dimensional benchmark for mllms hallucination evaluation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024b)</span>
<span class="ltx_bibblock">
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. 2024b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/s11704-024-40231-1" title="">A survey on large language model based autonomous agents</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">Frontiers of Computer Science</em>, 18(6).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024c)</span>
<span class="ltx_bibblock">
Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei, and Xuanjing Huang. 2024c.

</span>
<span class="ltx_bibblock">Benchmark self-evolving: A multi-agent framework for dynamic llm evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">arXiv preprint arXiv:2402.11443</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2018)</span>
<span class="ltx_bibblock">
Su Wang, Greg Durrett, and Katrin Erk. 2018.

</span>
<span class="ltx_bibblock">Modeling semantic plausibility by injecting world knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">arXiv preprint arXiv:1804.00619</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023a)</span>
<span class="ltx_bibblock">
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2212.10560" title="">Self-instruct: Aligning language models with self-generated instructions</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2023b)</span>
<span class="ltx_bibblock">
Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. 2023b.

</span>
<span class="ltx_bibblock">Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">arXiv preprint arXiv:2302.01560</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2024)</span>
<span class="ltx_bibblock">
Jiaheng Wei, Yuanshun Yao, Jean-Francois Ton, Hongyi Guo, Andrew Estornell, and Yang Liu. 2024.

</span>
<span class="ltx_bibblock">Measuring and reducing llm hallucination without gold-standard answers via expertise-weighting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">arXiv preprint arXiv:2402.10412</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wessel et al. (2023)</span>
<span class="ltx_bibblock">
Martin Wessel, Tomás Horych, Terry Ruas, Akiko Aizawa, Bela Gipp, and Timo Spinde. 2023.

</span>
<span class="ltx_bibblock">Introducing mbib-the first media bias identification benchmark task and dataset collection.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>, pages 2765–2774.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weston et al. (2015)</span>
<span class="ltx_bibblock">
Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merriënboer, Armand Joulin, and Tomas Mikolov. 2015.

</span>
<span class="ltx_bibblock">Towards ai-complete question answering: A set of prerequisite toy tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">arXiv preprint arXiv:1502.05698</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu and Aji (2023)</span>
<span class="ltx_bibblock">
Minghao Wu and Alham Fikri Aji. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.03025" title="">Style over substance: Evaluation biases for large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al. (2023)</span>
<span class="ltx_bibblock">
Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023.

</span>
<span class="ltx_bibblock">Bloomberggpt: A large language model for finance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">arXiv preprint arXiv:2303.17564</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al. (2023)</span>
<span class="ltx_bibblock">
Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023.

</span>
<span class="ltx_bibblock">Pixiu: A large language model, instruction data and evaluation benchmark for finance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">arXiv preprint arXiv:2306.05443</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023a)</span>
<span class="ltx_bibblock">
Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2306.09841" title="">Are large language models really good logical reasoners? a comprehensive evaluation and beyond</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023b)</span>
<span class="ltx_bibblock">
Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji Zhang, Chao Peng, Fei Huang, and Jingren Zhou. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.09705" title="">Cvalues: Measuring the values of chinese large language models from safety to responsibility</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023c)</span>
<span class="ltx_bibblock">
Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023c.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2305.16504" title="">On the tool manipulation capability of open-source large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2024a)</span>
<span class="ltx_bibblock">
Biwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, and Xiuzhen Cheng. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2403.05156" title="">On protecting the data privacy of large language models (llms): A survey</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2024b)</span>
<span class="ltx_bibblock">
Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. 2024b.

</span>
<span class="ltx_bibblock">Berkeley function calling leaderboard.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al. (2014)</span>
<span class="ltx_bibblock">
Wen-Jing Yan, Xiaobai Li, Su-Jing Wang, Guoying Zhao, Yong-Jin Liu, Yu-Hsin Chen, and Xiaolan Fu. 2014.

</span>
<span class="ltx_bibblock">Casme ii: An improved spontaneous micro-expression database and the baseline evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">PloS one</em>, 9(1):e86041.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2023)</span>
<span class="ltx_bibblock">
Shiping Yang, Renliang Sun, and Xiaojun Wan. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2310.06498" title="">A new benchmark and reverse validation method for passage-level hallucination detection</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W Cohen, Ruslan Salakhutdinov, and Christopher D Manning. 2018.

</span>
<span class="ltx_bibblock">Hotpotqa: A dataset for diverse, explainable multi-hop question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib166.1.1">arXiv preprint arXiv:1809.09600</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2023a)</span>
<span class="ltx_bibblock">
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2207.01206" title="">Webshop: Towards scalable real-world web interaction with grounded language agents</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2023b)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2210.03629" title="">React: Synergizing reasoning and acting in language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al. (2024)</span>
<span class="ltx_bibblock">
Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.hcc.2024.100211" title="">A survey on large language model (llm) security and privacy: The good, the bad, and the ugly</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib169.1.1">High-Confidence Computing</em>, page 100211.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yip et al. (2024)</span>
<span class="ltx_bibblock">
Daniel Wankit Yip, Aysan Esmradi, and Chun Fai Chan. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2401.00991" title="">A novel evaluation framework for assessing resilience against prompt injection attacks in large language models</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Young et al. (2022)</span>
<span class="ltx_bibblock">
Nathan Young, Qiming Bao, Joshua Bensemann, and Michael Witbrock. 2022.

</span>
<span class="ltx_bibblock">Abductionrules: Training transformers to explain unexpected inputs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib171.1.1">arXiv preprint arXiv:2203.12186</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al. (2023)</span>
<span class="ltx_bibblock">
Wenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Arenas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, Yuval Tassa, and Fei Xia. 2023.

</span>
<span class="ltx_bibblock">Language to rewards for robotic skill synthesis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib172.1.1">Arxiv preprint arXiv:2306.08647</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2023)</span>
<span class="ltx_bibblock">
Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.02015" title="">How well do large language models perform in arithmetic tasks?</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al. (2024)</span>
<span class="ltx_bibblock">
Zhuowen Yuan, Zidi Xiong, Yi Zeng, Ning Yu, Ruoxi Jia, Dawn Song, and Bo Li. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2403.13031" title="">Rigorllm: Resilient guardrails for large language models against undesired content</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhan et al. (2024)</span>
<span class="ltx_bibblock">
Qiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. 2024.

</span>
<span class="ltx_bibblock">Injecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib175.1.1">arXiv preprint arXiv:2403.02691</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2023)</span>
<span class="ltx_bibblock">
Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3604915.3608860" title="">Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib176.1.1">Proceedings of the 17th ACM Conference on Recommender Systems</em>, RecSys ’23. ACM.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024a)</span>
<span class="ltx_bibblock">
Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. 2024a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2401.07339" title="">Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024b)</span>
<span class="ltx_bibblock">
Xiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. 2024b.

</span>
<span class="ltx_bibblock">Self-alignment for factuality: Mitigating hallucinations in llms via self-evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">arXiv preprint arXiv:2402.09267</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang and Yang (2023)</span>
<span class="ltx_bibblock">
Xuanyu Zhang and Qing Yang. 2023.

</span>
<span class="ltx_bibblock">Xuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">Proceedings of the 32nd ACM International Conference on Information and Knowledge Management</em>, pages 4435–4439.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2018)</span>
<span class="ltx_bibblock">
Yuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander J Smola, and Le Song. 2018.

</span>
<span class="ltx_bibblock">Variational reasoning for question answering with knowledge graph.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib180.1.1">AAAI</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al. (2023)</span>
<span class="ltx_bibblock">
Guoying Zhao, Xiaobai Li, Yante Li, and Matti Pietikäinen. 2023.

</span>
<span class="ltx_bibblock">Facial micro-expressions: an overview.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib181.1.1">Proceedings of the IEEE</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023a)</span>
<span class="ltx_bibblock">
Duo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, and Liwei Wang. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2312.02010" title="">Towards learning a generalist model for embodied navigation</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2024)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">Advances in Neural Information Processing Systems</em>, 36.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al. (2023b)</span>
<span class="ltx_bibblock">
Shen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2304.10513" title="">Why does chatgpt fall short in providing truthful answers?</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023a)</span>
<span class="ltx_bibblock">
Gengze Zhou, Yicong Hong, and Qi Wu. 2023a.

</span>
<span class="ltx_bibblock">Navgpt: Explicit reasoning in vision-and-language navigation with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib185.1.1">arXiv preprint arXiv:2305.16986</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. (2023b)</span>
<span class="ltx_bibblock">
Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://arxiv.org/abs/2307.13854" title="">Webarena: A realistic web environment for building autonomous agents</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zielinski et al. (2023)</span>
<span class="ltx_bibblock">
Chris Zielinski, Margaret Winker, Rakesh Aggarwal, Lorraine Ferris, Markus Heinemann, Jose Florencio Lapeña Jr, Sanjay Pai, Edsel Ing, Leslie Citrome, et al. 2023.

</span>
<span class="ltx_bibblock">Wame recommendations on chatgpt and chatbots in relation to scholarly publications.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon Jun  3 02:19:28 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
