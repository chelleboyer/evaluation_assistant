<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods</title>
<!--Generated on Tue Dec 10 05:49:16 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Large Language Models,  Evaluation,  LLMs-as-Judges" lang="en" name="keywords"/>
<base href="/html/2412.05579v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S1" title="In LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S2" title="In LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>PRELIMINARIES</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S2.SS1" title="In 2. PRELIMINARIES ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Evaluation Function <math alttext="E" class="ltx_Math" display="inline"><semantics><mi>E</mi><annotation-xml encoding="MathML-Content"><ci>𝐸</ci></annotation-xml><annotation encoding="application/x-tex">E</annotation><annotation encoding="application/x-llamapun">italic_E</annotation></semantics></math></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S2.SS2" title="In 2. PRELIMINARIES ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Evaluation Input</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S2.SS2.SSS1" title="In 2.2. Evaluation Input ‣ 2. PRELIMINARIES ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Evaluation Type <math alttext="\mathcal{T}" class="ltx_Math" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">𝒯</mi><annotation-xml encoding="MathML-Content"><ci>𝒯</ci></annotation-xml><annotation encoding="application/x-tex">\mathcal{T}</annotation><annotation encoding="application/x-llamapun">caligraphic_T</annotation></semantics></math></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S2.SS2.SSS2" title="In 2.2. Evaluation Input ‣ 2. PRELIMINARIES ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Evaluation Criteria <math alttext="\mathcal{C}" class="ltx_Math" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">𝒞</mi><annotation-xml encoding="MathML-Content"><ci>𝒞</ci></annotation-xml><annotation encoding="application/x-tex">\mathcal{C}</annotation><annotation encoding="application/x-llamapun">caligraphic_C</annotation></semantics></math>.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S2.SS2.SSS3" title="In 2.2. Evaluation Input ‣ 2. PRELIMINARIES ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.3 </span>Evaluation References <math alttext="\mathcal{R}" class="ltx_Math" display="inline"><semantics><mi class="ltx_font_mathcaligraphic">ℛ</mi><annotation-xml encoding="MathML-Content"><ci>ℛ</ci></annotation-xml><annotation encoding="application/x-tex">\mathcal{R}</annotation><annotation encoding="application/x-llamapun">caligraphic_R</annotation></semantics></math>.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S2.SS3" title="In 2. PRELIMINARIES ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Evaluation Output</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3" title="In LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Functionality</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS1" title="In 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Performance Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS1.SSS1" title="In 3.1. Performance Evaluation ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Responses Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS1.SSS2" title="In 3.1. Performance Evaluation ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>Model Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS2" title="In 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Model Enhancement</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS2.SSS1" title="In 3.2. Model Enhancement ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span>Reward Modeling During Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS2.SSS2" title="In 3.2. Model Enhancement ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span>Acting as Verifier During Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS2.SSS3" title="In 3.2. Model Enhancement ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span>Feedback for Refinement</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS3" title="In 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Data Construction</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS3.SSS1" title="In 3.3. Data Construction ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Data Annotation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS3.SSS2" title="In 3.3. Data Construction ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Data Synthesize</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4" title="In LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS1" title="In 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Single-LLM System</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS1.SSS1" title="In 4.1. Single-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Prompt-based</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS1.SSS2" title="In 4.1. Single-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Tuning-based</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS1.SSS3" title="In 4.1. Single-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Post-processing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS2" title="In 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Multi-LLM System</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS2.SSS1" title="In 4.2. Multi-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Communication</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS2.SSS2" title="In 4.2. Multi-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Aggregation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS3" title="In 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Human-AI Collaboration System</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5" title="In LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Application</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS1" title="In 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>General</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS2" title="In 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Multimodal</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS3" title="In 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Medical</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS4" title="In 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Legal</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS5" title="In 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Financial</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS6" title="In 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.6 </span>Education</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS7" title="In 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.7 </span>Information Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS8" title="In 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8 </span>Others</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS8.SSS1" title="In 5.8. Others ‣ 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8.1 </span>Soft Engineering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS8.SSS2" title="In 5.8. Others ‣ 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8.2 </span>Biology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS8.SSS3" title="In 5.8. Others ‣ 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.8.3 </span>Social Science</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6" title="In LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Meta-evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1" title="In 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Benchmarks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS1" title="In 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Code Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS2" title="In 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Machine Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS3" title="In 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>Text Summarization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS4" title="In 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.4 </span>Dialogue Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS5" title="In 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.5 </span>Automatic Story Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS6" title="In 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.6 </span>Values Alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS7" title="In 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.7 </span>Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS8" title="In 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.8 </span>Search</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS9" title="In 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.9 </span>Comprehensive Data</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS2" title="In 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Metric</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS2.SSS1" title="In 6.2. Metric ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Accuracy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS2.SSS2" title="In 6.2. Metric ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Pearson Correlation Coefficient</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS2.SSS3" title="In 6.2. Metric ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.3 </span>Spearman’s Rank Correlation Coefficient</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS2.SSS4" title="In 6.2. Metric ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.4 </span>Kendall’s Tau</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS2.SSS5" title="In 6.2. Metric ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.5 </span>Cohen’s Kappa</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS2.SSS6" title="In 6.2. Metric ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.6 </span>Intraclass Correlation Coefficient (ICC)</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7" title="In LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Limitation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1" title="In 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Biases</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS1" title="In 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.1 </span>Presentation-Related Biases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS2" title="In 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.2 </span>Social-Related Biases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS3" title="In 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.3 </span>Content-Related Biases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS4" title="In 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.4 </span>Cognitive-Related Biases</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS2" title="In 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Adversarial Attacks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS2.SSS1" title="In 7.2. Adversarial Attacks ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.1 </span>Adversarial Attacks on LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS2.SSS2" title="In 7.2. Adversarial Attacks ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.2 </span>Adversarial Attacks on LLMs-as-judges</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS3" title="In 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Inherent Weaknesses</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS3.SSS1" title="In 7.3. Inherent Weaknesses ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.1 </span>Knowledge Recency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS3.SSS2" title="In 7.3. Inherent Weaknesses ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.2 </span>Hallucination</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS3.SSS3" title="In 7.3. Inherent Weaknesses ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.3 </span>Domain-Specific Knowledge Gaps</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8" title="In LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Future Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS1" title="In 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>More Efficient LLMs-as-Judges</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS1.SSS1" title="In 8.1. More Efficient LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1.1 </span>Automated Construction of Evaluation Criteria and Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS1.SSS2" title="In 8.1. More Efficient LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1.2 </span>Scalable Evaluation Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS1.SSS3" title="In 8.1. More Efficient LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1.3 </span>Accelerating Evaluation Processes</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS2" title="In 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>More Effective LLMs-as-Judges</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS2.SSS1" title="In 8.2. More Effective LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2.1 </span>Integration of Reasoning and Judge Capabilities</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS2.SSS2" title="In 8.2. More Effective LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2.2 </span>Establishing a Collective Judgment Mechanism</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS2.SSS3" title="In 8.2. More Effective LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2.3 </span>Enhancing Domain Knowledge</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS2.SSS4" title="In 8.2. More Effective LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2.4 </span>Cross-Domain and Cross-Language Transferability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS2.SSS5" title="In 8.2. More Effective LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2.5 </span>Multimodal Integration Evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS3" title="In 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span>More Reliable LLMs-as-Judges</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS3.SSS1" title="In 8.3. More Reliable LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3.1 </span>Enhancing Interpretability and Transparency</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS3.SSS2" title="In 8.3. More Reliable LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3.2 </span>Mitigating Bias and Ensuring Fairness</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS3.SSS3" title="In 8.3. More Reliable LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3.3 </span>Enhancing Robustness</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S9" title="In LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Haitao Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:liht22@mails.tsinghua.edu.cn">liht22@mails.tsinghua.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qian Dong
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:dq22@mails.tsinghua.edu.cn">dq22@mails.tsinghua.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Junjie Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:chenjj826@gmail.com">chenjj826@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Huixue Su
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:suhuixue@ruc.edu.cn">suhuixue@ruc.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Gaoling School of Artificial Intelligence, Renmin University of China</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yujia Zhou
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:suhuixue@ruc.edu.cn">suhuixue@ruc.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qingyao Ai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:aiqy@tsinghua.edu.cn">aiqy@tsinghua.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University</span><span class="ltx_text ltx_affiliation_city" id="id17.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id18.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ziyi Ye
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yeziyi1998@gmail.com">yeziyi1998@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id19.1.id1">Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University</span><span class="ltx_text ltx_affiliation_city" id="id20.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id21.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiqun Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yiqunliu@tsinghua.edu.cn">yiqunliu@tsinghua.edu.cn</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id22.1.id1">Department of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua University</span><span class="ltx_text ltx_affiliation_city" id="id23.2.id2">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id24.3.id3">China</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id25.id1">The rapid advancement of Large Language Models (LLMs) has driven their expanding application across various fields.
One of the most promising applications is their role as evaluators based on natural language responses, referred to as “LLMs-as-judges”.
This framework has attracted growing attention from both academia and industry due to their excellent effectiveness, ability to generalize across tasks, and interpretability in the form of natural language.
This paper presents a comprehensive survey of the LLMs-as-judges paradigm from five key perspectives: <span class="ltx_text ltx_font_bold" id="id25.id1.1">Functionality</span>, <span class="ltx_text ltx_font_bold" id="id25.id1.2">Methodology</span>, <span class="ltx_text ltx_font_bold" id="id25.id1.3">Applications</span>, <span class="ltx_text ltx_font_bold" id="id25.id1.4">Meta-evaluation</span>, and <span class="ltx_text ltx_font_bold" id="id25.id1.5">Limitations</span>.
We begin by providing a systematic definition of LLMs-as-Judges and introduce their functionality (Why use LLM judges?).
Then we address methodology to construct an evaluation system with LLMs (How to use LLM judges?).
Additionally, we investigate the potential domains for their application (Where to use LLM judges?) and discuss methods for evaluating them in various contexts (How to evaluate LLM judges?).
Finally, we provide a detailed analysis of the limitations of LLM judges and discuss potential future directions.</p>
<p class="ltx_p" id="id26.id2">Through a structured and comprehensive analysis, we aim aims to provide insights on the development and application of LLMs-as-judges in both research and practice. We will continue to maintain the relevant resource list at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/CSHaitao/Awesome-LLMs-as-Judges" title="">https://github.com/CSHaitao/Awesome-LLMs-as-Judges</a>.</p>
</div>
<div class="ltx_keywords">Large Language Models, Evaluation, LLMs-as-Judges
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Studies on evaluation methods have long been a key force in guiding the development of modern Artificial Intelligence (AI) <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib24" title="">2024</a>)</cite>.
AI researchers have continuously sought to measure and validate the intelligence of AI models through various tasks <cite class="ltx_cite ltx_citemacro_citep">(Chang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib24" title="">2024</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib76" title="">2023</a>)</cite>.
In the mid-20th century, AI evaluation primarily centered on assessing algorithm performance in specific tasks, such as logical reasoning and numerical computation <cite class="ltx_cite ltx_citemacro_citep">(Nilsson, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib165" title="">2014</a>)</cite>.
Traditional machine learning tasks like classification and regression often use programmable and statistical metrics, including accuracy, precision, and recall.
With the emergence of deep learning, the complexity of AI systems grew rapidly, prompting a shift in evaluation standards <cite class="ltx_cite ltx_citemacro_citep">(LeCun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib120" title="">2015</a>)</cite>.
The evaluation of AI has expanded from pre-defined, programmable machine metrics to more flexible, robust evaluators for solving complex, realistic tasks.
A typical example is the Turing Test <cite class="ltx_cite ltx_citemacro_citep">(French, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib68" title="">2000</a>; Turing, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib223" title="">2009</a>)</cite>, which determines whether an AI model can exhibit human-like intelligent behavior through dialogue with humans.
The Turing Test provides a fundamental guideline in the evaluation of AI models, especially on AI models’ intelligence in flexible and realistic environments.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Recently, the emergence of Large Language Models (LLMs) and generative AI serves as a new milestone in the evolution of AI evaluation.
LLMs exhibit remarkable generalization and adaptability, showcasing strong transfer capabilities across previously unseen tasks and diverse domains <cite class="ltx_cite ltx_citemacro_citep">(Achiam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib2" title="">2023</a>; Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib10" title="">2023</a>)</cite>. However, their powerful capabilities also present new challenges for evaluation.
Due to the highly generative and open-ended nature of their outputs, standardized metrics are often insufficient for a comprehensive evaluation.
For example, in natural language generation (NLG) tasks, traditional metrics like BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib174" title="">2002</a>)</cite> and ROUGE <cite class="ltx_cite ltx_citemacro_citep">(Lin, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib140" title="">2004</a>)</cite> often fail to capture key aspects such as text fluency, logical coherence, and creativity. Moreover, modern AI evaluation extends beyond task performance and must account for the ability to address complex, dynamic problems in real-world scenarios, including robustness, fairness, and interpretability.
Human annotations, frequently regarded as the “ground truth,” can offer comprehensive insights and valuable feedback. By gathering responses from experts or users, researchers can gain a deeper understanding of a model’s performance, practicality, and potential risks. However, collecting them are typically time-consuming and resource-intensive, making it challenging to scale up for large-scale evaluation.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this context, a new paradigm has emerged to replace humans and statistical metrics with LLMs in evaluation, referred to as LLMs-as-judges <cite class="ltx_cite ltx_citemacro_citep">(Ashktorab et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib6" title="">2024</a>; Tseng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib222" title="">2024</a>; Bavaresco et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib15" title="">2024</a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib15" title="">2024</a>)</cite>.
Compared to traditional evaluation methods, LLMs-as-judges show significant strengths.
First, LLM judges can adjust their evaluation criteria based on the specific task context, rather than relying on a fixed set of metrics, making the evaluation process more flexible and refined.
Second, LLM judges can generate interpretive evaluations, offering more comprehensive feedback on model performance and enabling researchers to gain deeper insights into the evaluater’s strengths and weaknesses.
Finally, LLM judges offer a scalable and reproducible alternative to human evaluation, significantly reducing the costs and time associated with human involvement.
</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Despite its great potential and significant advantages, LLMs-as-judges also face several critical challenges.
For example, the evaluation results of LLMs are often influenced by the prompt template, which can lead to biased or inconsistent assessments <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib261" title="">2023a</a>)</cite>.
Considering that LLMs are trained on extensive text corpus, they may also inherit various implicit biases, impacting the fairness and reliability of their assessments <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite>.
Moreover, distinct tasks and domains require specific evaluation criteria, making it difficult for LLMs to adapt their standards dynamically to specific contexts.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Considering the vast potential of this field,
this survey aims to systematically review and analyze the current state and key challenges of the LLMs-as-judges.
As shown in Figures <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S1.F1" title="Figure 1 ‣ 1. Introduction ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S1.F2" title="Figure 2 ‣ 1. Introduction ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">2</span></a>, we discuss existing research across five key perspectives: 1) <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">Functionality</span>: Why use LLM judges, 2) <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">Methodology</span>: How to use LLM judges, 3) <span class="ltx_text ltx_font_bold" id="S1.p5.1.3">Application</span>: Where to use LLM judges, 4) <span class="ltx_text ltx_font_bold" id="S1.p5.1.4">Meta-evaluation</span>: How to evaluate LLM judges and 5) <span class="ltx_text ltx_font_bold" id="S1.p5.1.5">Limitation</span>: Existing problems of LLM judges. We explore the key challenges confronting LLMs-as-judges and hope to provide a clearer guideline for their future development.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In summary, the main contributions of this paper are as follows:</p>
</div>
<div class="ltx_para" id="S1.p7">
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">Comprehensive and Timely Survey</span>: We present the extensive survey on the emerging paradigm of LLMs-as-judges, systematically reviewing the current state of research and developments in this field. By examining LLMs as performance evaluators based on their generated natural language, we highlight the unique role of LLMs in shaping the future of AI evaluation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i2.p1.1.1">Systematic Analysis Across Five Key Perspectives</span>: We organize our survey around five critical aspects: Functionality, Methodology, Application, Meta-evaluation, and Limitation. This structured approach allows for a nuanced understanding of how and why LLMs are utilized as evaluators, their practical implementations, and reliability concerns.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.i3.p1.1.1">Current Challenges and Future Research Directions</span>: We discuss the existing challenges for adopting LLMs-as-judges, highlighting potential research opportunities and directions while offering a forward-looking perspective on the future development of this paradigm, encouraging researchers to delve deeper into this exciting area. We also provide an open-source repository at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/CSHaitao/Awesome-LLMs-as-Judges" title="">https://github.com/CSHaitao/Awesome-LLMs-as-Judges</a>, with the goal of fostering a collaborative community and advancing best practices in this area.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">The organization of this paper is as follows. In Section (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S2" title="2. PRELIMINARIES ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">2</span></a>), we provide the formal definition of LLMs-as-judges. Then, Section (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3" title="3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3</span></a>) reviews existing work from the perspective of “Why use LLM judges”. Following that, Section (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4" title="4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4</span></a>) covers “How to use LLM judges”, summarizing the current technical developments in LLMs-as-judges. Section (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5" title="5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">5</span></a>) discusses “Where to use LLM judges”, focusing on their application domains. In Section (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6" title="6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6</span></a>), we review the metrics and benchmarks used for evaluating LLMs-as-judges. Section (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7" title="7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7</span></a>) discusses the limitations and challenges of LLM judges. We discuss major future work in Sections (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8" title="8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">8</span></a>) and (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S9" title="9. Conclusion ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">9</span></a>) to conclude the paper.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.F1.1" style="width:433.6pt;height:1141.6pt;vertical-align:-1137.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-163.4pt,1.5pt) scale(0.570202345974976,0.570202345974976) ;"><span class="ltx_ERROR undefined" id="S1.F1.1.1">{forest}</span>
<p class="ltx_p" id="S1.F1.1.2">forked edges,
for tree=
grow=east,
reversed=true,
anchor=base west,
parent anchor=east,
child anchor=west,
base=left,
font=<span class="ltx_text" id="S1.F1.1.2.1" style="font-size:90%;">,
rectangle,
draw=hidden-draw,
rounded corners,
align=left,
minimum width=4em,
edge+=darkgray, line width=1pt,
s sep=3pt,
inner xsep=2pt,
inner ysep=3pt,
ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,
</span>,
where level=1text width=6.4em,font=<span class="ltx_text" id="S1.F1.1.2.2" style="font-size:70%;">,</span>,
where level=2text width=6.4em,font=<span class="ltx_text" id="S1.F1.1.2.3" style="font-size:70%;">,</span>,
where level=3text width=6.4em,font=<span class="ltx_text" id="S1.F1.1.2.4" style="font-size:70%;">,</span>,
where level=4text width=6.4em,font=<span class="ltx_text" id="S1.F1.1.2.5" style="font-size:70%;">,</span>,
[LLMs-as-Judges, ver
[FUNCTIONALITY(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3" title="3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3</span></a>)
[Performance 
<br class="ltx_break"/>Evaluation (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS1" title="3.1. Performance Evaluation ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.1</span></a>)
[Responses 
<br class="ltx_break"/>Evaluation(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS1.SSS1" title="3.1.1. Responses Evaluation ‣ 3.1. Performance Evaluation ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>)
[LLM-Eval <cite class="ltx_cite ltx_citemacro_citep">(Lin and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib142" title="">2023</a>)</cite>, Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib229" title="">2024c</a>)</cite>, Zhou et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib298" title="">2024a</a>)</cite>, ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib189" title="">2023</a>)</cite>, SELF-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib4" title="">2023</a>)</cite>, Lei et al. <cite class="ltx_cite ltx_citemacro_citep">(Lei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib125" title="">2024</a>)</cite>, leaf, text width=41em]]
[Model 
<br class="ltx_break"/>Evaluation (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS1.SSS2" title="3.1.2. Model Evaluation ‣ 3.1. Performance Evaluation ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>)
[Auto-Arena <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib287" title="">2024b</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib154" title="">2024</a>)</cite>, LMExam <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib11" title="">2024</a>)</cite>, KIEval <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib274" title="">2024</a>)</cite>, leaf, text width=41em]]]
[Model 
<br class="ltx_break"/>Enhancement (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS2" title="3.2. Model Enhancement ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.2</span></a>)
[Reward Modeling 
<br class="ltx_break"/>During Training 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS2.SSS1" title="3.2.1. Reward Modeling During Training ‣ 3.2. Model Enhancement ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.2.1</span></a>)
[SRLMs <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib275" title="">2024</a>)</cite>, OAIF <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib75" title="">2024</a>)</cite>, RLAIF <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib122" title="">2023</a>)</cite>, RELC <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib22" title="">2024b</a>)</cite>, CREAM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib240" title="">2024d</a>)</cite>, CGPO <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib258" title="">2024a</a>)</cite>, Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib29" title="">2023c</a>)</cite>, leaf, text width=41em]]
[Acting as Verifier 
<br class="ltx_break"/>During Inference 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS2.SSS2" title="3.2.2. Acting as Verifier During Inference ‣ 3.2. Model Enhancement ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>)
[Best-of-N sampling <cite class="ltx_cite ltx_citemacro_citep">(Jinnai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib103" title="">2024</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib210" title="">2024</a>)</cite>, ToT <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib266" title="">2024</a>)</cite>, GoT <cite class="ltx_cite ltx_citemacro_citep">(Besta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib16" title="">2024</a>)</cite>, Lightman et al. <cite class="ltx_cite ltx_citemacro_citep">(Lightman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib138" title="">2023</a>)</cite>, SE-GBS <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib251" title="">2024a</a>)</cite>, REPS <cite class="ltx_cite ltx_citemacro_citep">(Kawabata and Sugawara, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib106" title="">2024</a>)</cite>, Musolesi et al. <cite class="ltx_cite ltx_citemacro_citep">(Musolesi, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib161" title="">2024</a>)</cite>, leaf, text width=41em]]
[Feedback for 
<br class="ltx_break"/>Refinement (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS2.SSS3" title="3.2.3. Feedback for Refinement ‣ 3.2. Model Enhancement ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.2.3</span></a>)
[SELF-REFINE <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib156" title="">2024</a>)</cite>, SELF-DEBUGGING <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib33" title="">2023a</a>)</cite>, REFINER <cite class="ltx_cite ltx_citemacro_citep">(Paul et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib177" title="">2023</a>)</cite>, Xu et al. <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib262" title="">2023c</a>)</cite>, Self-Correct <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib94" title="">2023</a>; Tyen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib224" title="">2023</a>)</cite>, Valmeekam et al. <cite class="ltx_cite ltx_citemacro_citep">(Valmeekam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib225" title="">2023</a>)</cite>, leaf, text width=41em]]]
[Data 
<br class="ltx_break"/>Construction (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS3" title="3.3. Data Construction ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.3</span></a>)
[Data 
<br class="ltx_break"/>Annotation (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS3.SSS1" title="3.3.1. Data Annotation ‣ 3.3. Data Construction ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>)
[He et al. <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib86" title="">2024a</a>)</cite>, Gilardi et al. <cite class="ltx_cite ltx_citemacro_citep">(Gilardi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib72" title="">2023</a>)</cite>, Törnberg et al. <cite class="ltx_cite ltx_citemacro_citep">(Törnberg, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib218" title="">2023</a>)</cite>, FullAnno <cite class="ltx_cite ltx_citemacro_citep">(Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib80" title="">2024</a>)</cite>, Latif et al. <cite class="ltx_cite ltx_citemacro_citep">(Latif et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib118" title="">2023</a>)</cite>, AnnoLLM <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib84" title="">2023a</a>)</cite>, LLMAAA <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib283" title="">2023a</a>)</cite>, leaf, text width=41em]]
[Data 
<br class="ltx_break"/>Synthesize (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS3.SSS2" title="3.3.2. Data Synthesize ‣ 3.3. Data Construction ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>)
[SELFEE <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib269" title="">2023a</a>)</cite>, SynPO <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib53" title="">2024a</a>)</cite>, Arif et al. <cite class="ltx_cite ltx_citemacro_citep">(Arif et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib3" title="">2024</a>)</cite>, SELF-INSTRUCT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib236" title="">2022</a>)</cite>, Evol-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib255" title="">2023d</a>; Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib279" title="">2024</a>)</cite>, STaR <cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib278" title="">2024</a>)</cite>, Mendoncca et al. <cite class="ltx_cite ltx_citemacro_citep">(Mendonça et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib158" title="">2024</a>)</cite>, 
<br class="ltx_break"/>ReSTEM <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib200" title="">2023</a>)</cite>, Kim et al. <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib109" title="">2024a</a>)</cite>, leaf, text width=41em]]]]
[
METHODOLOGY (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4" title="4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4</span></a>)
[
Single-LLM (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS1" title="4.1. Single-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.1</span></a>)
[
Prompt-based (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS1.SSS1" title="4.1.1. Prompt-based ‣ 4.1. Single-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>)
[
In-Context Learning
[
GPTScore <cite class="ltx_cite ltx_citemacro_citep">(Fu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib69" title="">2023</a>)</cite>, LLM-EVAL <cite class="ltx_cite ltx_citemacro_citep">(Lin and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib142" title="">2023</a>)</cite>, TALEC <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib281" title="">2024c</a>)</cite>, Jain et al. <cite class="ltx_cite ltx_citemacro_citep">(Jain et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib95" title="">2023</a>)</cite>, ALLURE <cite class="ltx_cite ltx_citemacro_citep">(Hasanbeig et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib82" title="">2023</a>)</cite>, Song et al. <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib205" title="">2024b</a>)</cite>, leaf, text width=33em
]
]
[
Step-by-step
[
Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib243" title="">2022</a>; Kotonya et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib114" title="">2023</a>)</cite>, G-EVAL <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib146" title="">2023a</a>)</cite>, ICE-Score <cite class="ltx_cite ltx_citemacro_citep">(Zhuo, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib305" title="">2023</a>)</cite>, ProtocoLLM <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib272" title="">2024</a>)</cite>, Chiang et al. <cite class="ltx_cite ltx_citemacro_citep">(Chiang and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib37" title="">2023</a>)</cite>, 
<br class="ltx_break"/>FineSurE <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib204" title="">2024a</a>)</cite>, leaf, text width=33em
]
]
[
Definition 
<br class="ltx_break"/>Augmentation
[
AUTOCALIBRATE <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib147" title="">2023d</a>)</cite>, PORTIA <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib135" title="">2023d</a>)</cite>, SALC <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib77" title="">2024</a>)</cite>, LLM-as-a-personalized-judge <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib55" title="">2024b</a>)</cite>, BiasAlert <cite class="ltx_cite ltx_citemacro_citep">(Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib62" title="">2024</a>)</cite>, 
<br class="ltx_break"/>Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib34" title="">2024c</a>)</cite>, leaf, text width=33em
]
]
[
Multi-turn 
<br class="ltx_break"/>Optimization
[
ACTIVE-CRITIC <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib257" title="">2024b</a>)</cite>, AUTOCALIBRATE <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib147" title="">2023d</a>)</cite>, Auto-Arena <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib287" title="">2024b</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib154" title="">2024</a>)</cite>, LMExam <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib11" title="">2024</a>)</cite>, KIEval <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib274" title="">2024</a>)</cite>, leaf, text width=33em
]
]]
[
Tuning-based (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS1.SSS2" title="4.1.2. Tuning-based ‣ 4.1. Single-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>)
[
Score-based Tuning
[
Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib29" title="">2023c</a>)</cite>, AttrScore <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib277" title="">2023b</a>)</cite>, PHUDGE <cite class="ltx_cite ltx_citemacro_citep">(Deshwal and Chawla, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib48" title="">2024</a>)</cite>, ECT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib230" title="">2023e</a>)</cite>, SELF-J <cite class="ltx_cite ltx_citemacro_citep">(Ye and Ng, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib267" title="">2024</a>)</cite>, SorryBench <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib250" title="">2024b</a>)</cite>, TIGERScore <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib100" title="">2023b</a>)</cite>, 
<br class="ltx_break"/>FENCE <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib253" title="">2024d</a>)</cite>, ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib189" title="">2023</a>)</cite>, leaf, text width=33em
]
]
[
Preference-based 
<br class="ltx_break"/>Learning
[
Meta-Rewarding <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib246" title="">2024b</a>)</cite>, Con-J <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib271" title="">2024a</a>)</cite>, JudgeLM <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib302" title="">2023</a>)</cite>, INSTRUCTSCORE <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib259" title="">2023e</a>)</cite>, AUTO-J <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib131" title="">2023c</a>)</cite>, Shepherd <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib233" title="">2023c</a>)</cite>, 
<br class="ltx_break"/>X-EVAL <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib143" title="">2023c</a>)</cite>, Themis <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib89" title="">2024a</a>)</cite>, CritiqueLLM <cite class="ltx_cite ltx_citemacro_citep">(Ke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib107" title="">2024</a>)</cite>, FedEval-LLM <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib85" title="">2024b</a>)</cite>, PandaLM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib237" title="">2023d</a>)</cite>, Self-Taught <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib232" title="">2024e</a>)</cite>, 
<br class="ltx_break"/>FLAMe <cite class="ltx_cite ltx_citemacro_citep">(Vu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib227" title="">2024</a>)</cite>, Self-Rationalization <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib221" title="">2024a</a>)</cite>, CompassJudger-1 <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib21" title="">2024a</a>)</cite>, Zhou et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib295" title="">2024c</a>)</cite>, HALU-J <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib228" title="">2024b</a>)</cite>, 
<br class="ltx_break"/>PROMETHEUS <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib110" title="">2023</a>)</cite>, PROMETHEUS 2 <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib111" title="">2024b</a>)</cite>, PROMETHEUS-VISION <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib123" title="">2024a</a>)</cite>, LLaVA-Critic <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib254" title="">2024</a>)</cite>, leaf, text width=33em
]
]
]
[
Post-processing(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS1.SSS3" title="4.1.3. Post-processing ‣ 4.1. Single-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.1.3</span></a>)
[
Probability 
<br class="ltx_break"/>Calibration
[
Daynauth et al. <cite class="ltx_cite ltx_citemacro_citep">(Daynauth and Mars, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib46" title="">2024</a>)</cite>, ProbDiff <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib248" title="">2024b</a>)</cite>, PoE <cite class="ltx_cite ltx_citemacro_citep">(Liusie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib151" title="">2024</a>)</cite>, CRISPR <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib265" title="">2024</a>)</cite>, leaf, text width=33em
]
]
[
Text Reprocessing
[
Sottana et al. <cite class="ltx_cite ltx_citemacro_citep">(Sottana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib207" title="">2023</a>)</cite>, AUTO-J <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib131" title="">2023c</a>)</cite>, Yan et al. <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib263" title="">2024a</a>)</cite>, Tessler et al. <cite class="ltx_cite ltx_citemacro_citep">(Tessler et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib215" title="">2024</a>)</cite>, REVISEVAL <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib282" title="">2024b</a>)</cite>Ren et al. <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib187" title="">2023</a>)</cite>, 
<br class="ltx_break"/>Open-LLM-Leaderboard <cite class="ltx_cite ltx_citemacro_citep">(Myrzakhan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib162" title="">2024</a>)</cite>, leaf, text width=33em
]
]
]]
[
Multi-LLM (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS2" title="4.2. Multi-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.2</span></a>)
[
Communication(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS2.SSS1" title="4.2.1. Communication ‣ 4.2. Multi-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>)
[
Cooperation
[
WideDeep <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib286" title="">2023b</a>)</cite>, Xu et al. <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib262" title="">2023c</a>)</cite>, ABSEval <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib137" title="">2024b</a>)</cite>, leaf, text width=33em
]
]
[
Competition
[
Owens et al. <cite class="ltx_cite ltx_citemacro_citep">(Owens et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib169" title="">2024</a>)</cite>, Auto-Arena <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib287" title="">2024b</a>)</cite>, Bandi et al. <cite class="ltx_cite ltx_citemacro_citep">(Bandi and Harrasse, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib13" title="">2024</a>)</cite>, Moniri et al. <cite class="ltx_cite ltx_citemacro_citep">(Moniri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib159" title="">2024</a>)</cite>, ChatEval <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib23" title="">2023</a>)</cite>, PRD <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib133" title="">2023b</a>)</cite>, leaf, text width=33em
]
]
]
[
Aggregation (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS2.SSS2" title="4.2.2. Aggregation ‣ 4.2. Multi-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>)
[
Badshah et al. <cite class="ltx_cite ltx_citemacro_citep">(Badshah and Sajjad, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib9" title="">2024</a>)</cite>, PoLL <cite class="ltx_cite ltx_citemacro_citep">(Verga et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib226" title="">2024</a>)</cite>, Language-Model-as-an-Examiner <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib11" title="">2024</a>)</cite>, MULTI-NEWS+ <cite class="ltx_cite ltx_citemacro_citep">(Choi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib39" title="">2024</a>)</cite>, PiCO <cite class="ltx_cite ltx_citemacro_citep">(Ning et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib166" title="">2024</a>)</cite>, PRE <cite class="ltx_cite ltx_citemacro_citep">(Chu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib40" title="">2024</a>)</cite>, Chen et al.  <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib28" title="">2024e</a>)</cite>, 
<br class="ltx_break"/>Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib285" title="">2024a</a>)</cite>, 
AIME <cite class="ltx_cite ltx_citemacro_citep">(Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib176" title="">2024</a>)</cite>, HD-EVAL <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib148" title="">2024a</a>)</cite>, Gao et al. <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib71" title="">2024</a>)</cite>, GED <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib91" title="">2024c</a>)</cite>, Fusion-Eval <cite class="ltx_cite ltx_citemacro_citep">(Shu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib199" title="">2024</a>)</cite>, Jung et al. <cite class="ltx_cite ltx_citemacro_citep">(Jung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib104" title="">2024</a>)</cite>, CascadedEval <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib92" title="">2024a</a>)</cite>, leaf, text width=41em
]
]]
[
Human-AI 
<br class="ltx_break"/>Collaboration (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS3" title="4.3. Human-AI Collaboration System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.3</span></a>)
[
COEVAL<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib132" title="">2023a</a>)</cite>, EvalGen<cite class="ltx_cite ltx_citemacro_citep">(Shankar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib193" title="">2024</a>)</cite>, EvaluLLM<cite class="ltx_cite ltx_citemacro_citep">(Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib171" title="">2024a</a>)</cite>, LLM TAs<cite class="ltx_cite ltx_citemacro_citep">(Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib36" title="">2024</a>)</cite>, Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib231" title="">2023b</a>)</cite>, leaf, text width=49em
]
]
]
[APPLICATION (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5" title="5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">5</span></a>)
[
General (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS1" title="5.1. General ‣ 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">5.1</span></a>)
[
Dialogue Generation <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib134" title="">2017</a>)</cite>, Summarization <cite class="ltx_cite ltx_citemacro_citep">(Narayan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib163" title="">2018</a>)</cite>, Translation <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib65" title="">2024</a>)</cite>, Fusion-Eval <cite class="ltx_cite ltx_citemacro_citep">(Shu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib199" title="">2024</a>)</cite>, leaf, text width=49em
]
]
[
Multimodal (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS2" title="5.2. Multimodal ‣ 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">5.2</span></a>)
[
LLaVA-Critic <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib254" title="">2024</a>)</cite>, Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib25" title="">2024b</a>)</cite>, Latif et al. <cite class="ltx_cite ltx_citemacro_citep">(Latif et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib118" title="">2023</a>)</cite>, self-reward <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib300" title="">2024b</a>; Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib47" title="">2024</a>)</cite>,CODA-LM <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib30" title="">2024d</a>)</cite>, leaf, text width=49em
]
]
[
Medical (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS3" title="5.3. Medical ‣ 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">5.3</span></a>)
[
Xie et al. <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib252" title="">2024c</a>)</cite>, Brake et al. <cite class="ltx_cite ltx_citemacro_citep">(Brake and Schaaf, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib18" title="">2024</a>)</cite>, Krolik et al. <cite class="ltx_cite ltx_citemacro_citep">(Krolik et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib115" title="">2024</a>)</cite>, Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib127" title="">2024c</a>)</cite>, Medical Reasoning <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib96" title="">2024</a>)</cite>, leaf, text width=49em
]
]
[
Legal (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS4" title="5.4. Legal ‣ 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">5.4</span></a>)
[
Yue et al. <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib276" title="">2023a</a>)</cite>,Ryu et al. <cite class="ltx_cite ltx_citemacro_citep">(Ryu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib188" title="">2023</a>)</cite>, Raju et al. <cite class="ltx_cite ltx_citemacro_citep">(Raju et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib186" title="">2024</a>)</cite>, Ma et al. <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib155" title="">2024</a>)</cite>, leaf, text width=49em
]
]
[
Financial (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS5" title="5.5. Financial ‣ 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">5.5</span></a>)
[
FinMA <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib249" title="">2023</a>)</cite>, Babaei et al. <cite class="ltx_cite ltx_citemacro_citep">(Babaei and Giudici, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib8" title="">2024</a>)</cite>, Son et al. <cite class="ltx_cite ltx_citemacro_citep">(Son et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib202" title="">2024a</a>)</cite>, leaf, text width=49em
]
]
[
Education (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS6" title="5.6. Education ‣ 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">5.6</span></a>)
[
LLM TA <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib36" title="">2024</a>)</cite>, Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib229" title="">2024c</a>)</cite>, Song et al. <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib206" title="">2024c</a>)</cite>, Zhou et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib298" title="">2024a</a>)</cite>, Xia et al. <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib247" title="">2024a</a>)</cite>, Debatrix <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib136" title="">2024a</a>)</cite>, leaf, text width=49em
]
]
[
Information Retrieval 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS7" title="5.7. Information Retrieval ‣ 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">5.7</span></a>)
[
Rahmani et al. <cite class="ltx_cite ltx_citemacro_citep">(Rahmani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib184" title="">2024</a>)</cite>, JudgeRank <cite class="ltx_cite ltx_citemacro_citep">(Niu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib167" title="">2024</a>)</cite>, Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib285" title="">2024a</a>)</cite>, Soboroff et al.  <cite class="ltx_cite ltx_citemacro_citep">(Soboroff, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib201" title="">2024</a>)</cite>, ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib189" title="">2023</a>)</cite>, leaf, text width=49em
]
]
[
Others (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S5.SS8" title="5.8. Others ‣ 5. Application ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">5.8</span></a>)
[
Code <cite class="ltx_cite ltx_citemacro_citep">(Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib176" title="">2024</a>; Weyssow et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib244" title="">2024</a>)</cite>, Kumar et al. <cite class="ltx_cite ltx_citemacro_citep">(Kumar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib116" title="">2024</a>)</cite>, Hijazi et al. <cite class="ltx_cite ltx_citemacro_citep">(Hijazi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib87" title="">2024</a>)</cite>, Tessler et al. <cite class="ltx_cite ltx_citemacro_citep">(Tessler et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib215" title="">2024</a>)</cite>, Sotopia <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib299" title="">2023b</a>)</cite>, leaf, text width=49em
]
]]
[META-EVALUATION 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6" title="6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6</span></a>)
[Benchmarks (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1" title="6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.1</span></a>)
[
Code Generation 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS1" title="6.1.1. Code Generation ‣ 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.1.1</span></a>)
[
HumanEval <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib31" title="">2021</a>)</cite>, SWEBench <cite class="ltx_cite ltx_citemacro_citep">(Jimenez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib102" title="">2023</a>)</cite>, DevAI <cite class="ltx_cite ltx_citemacro_citep">(Zhuge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib304" title="">2024</a>)</cite>, CrossCodeEval <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib51" title="">2024</a>)</cite>, CodeUltraFeedback <cite class="ltx_cite ltx_citemacro_citep">(Weyssow et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib244" title="">2024</a>)</cite>, leaf, text width=41em
]
]
[
Machine Translation 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS2" title="6.1.2. Machine Translation ‣ 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.1.2</span></a>)
[
Freitag et al. <cite class="ltx_cite ltx_citemacro_citep">(Freitag et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib67" title="">2021b</a>)</cite>, Literary Translation Comparisons <cite class="ltx_cite ltx_citemacro_citep">(Karpinska and Iyyer, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib105" title="">2023</a>)</cite>, MQM <cite class="ltx_cite ltx_citemacro_citep">(Freitag et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib66" title="">2021a</a>)</cite>, leaf, text width=41em
]
]
[
Text Summarization 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS3" title="6.1.3. Text Summarization ‣ 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.1.3</span></a>)
[
SummEval <cite class="ltx_cite ltx_citemacro_citep">(Fabbri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib60" title="">2021</a>)</cite>, FRANK <cite class="ltx_cite ltx_citemacro_citep">(Pagnoni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib170" title="">2021</a>)</cite>, OpinsummEval <cite class="ltx_cite ltx_citemacro_citep">(Shen and Wan, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib195" title="">2023</a>)</cite>, leaf, text width=41em
]
]
[
Dialogue Generation 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS4" title="6.1.4. Dialogue Generation ‣ 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.1.4</span></a>)
[
Topical-Chat <cite class="ltx_cite ltx_citemacro_citep">(Gopalakrishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib73" title="">2023</a>)</cite>, PERSONA-CHAT <cite class="ltx_cite ltx_citemacro_citep">(Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib284" title="">2018</a>)</cite>, Mehri et al. <cite class="ltx_cite ltx_citemacro_citep">(Mehri and Eskenazi, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib157" title="">2020</a>)</cite>, DSTC10 Track 5 <cite class="ltx_cite ltx_citemacro_citep">(Yoshino et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib273" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib280" title="">2021</a>)</cite>, leaf, text width=41em
]
]
[
Automatic Story 
<br class="ltx_break"/>Generation (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS5" title="6.1.5. Automatic Story Generation ‣ 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.1.5</span></a>)
[
HANNA <cite class="ltx_cite ltx_citemacro_citep">(Chhun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib35" title="">2022</a>)</cite>, MANS <cite class="ltx_cite ltx_citemacro_citep">(Guan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib74" title="">2021</a>)</cite>, OpenMEVA <cite class="ltx_cite ltx_citemacro_citep">(Guan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib74" title="">2021</a>)</cite>, StoryER <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib27" title="">2023b</a>)</cite>, PERSER <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib230" title="">2023e</a>)</cite>, leaf, text width=41em
]
]
[
Values Alignment 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS6" title="6.1.6. Values Alignment ‣ 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.1.6</span></a>)
[
PKU-SafeRLHF <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib97" title="">2024</a>)</cite>, HHH <cite class="ltx_cite ltx_citemacro_citep">(Askell et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib7" title="">2021</a>)</cite>, CVALUES <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib256" title="">2023b</a>)</cite>, leaf, text width=41em
]
]
[
Recommendation 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS7" title="6.1.7. Recommendation ‣ 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.1.7</span></a>)
[
MovieLens <cite class="ltx_cite ltx_citemacro_citep">(Harper and Konstan, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib81" title="">2015</a>)</cite>, Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib285" title="">2024a</a>)</cite>, Yelp <cite class="ltx_cite ltx_citemacro_citep">(Asghar, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib5" title="">2016</a>)</cite>, leaf, text width=41em
]
]
[
Search (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS8" title="6.1.8. Search ‣ 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.1.8</span></a>)
[
TREC Deep Learning Track <cite class="ltx_cite ltx_citemacro_citep">(Lawrie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib119" title="">2024</a>)</cite>, MS MARCO v2 collection <cite class="ltx_cite ltx_citemacro_citep">(Bajaj et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib12" title="">2016</a>)</cite>, LeCaRDv2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib130" title="">2024d</a>)</cite>, leaf, text width=41em
]
]
[
Comprehensive Data 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1.SSS9" title="6.1.9. Comprehensive Data ‣ 6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.1.9</span></a>)
[
HelpSteer <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib239" title="">2023a</a>)</cite>, HelpSteer2 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib238" title="">2024a</a>)</cite>, UltraFeedback <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib45" title="">2024</a>)</cite>, UltraChat <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib50" title="">2023</a>)</cite>, ShareGPT <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib38" title="">2023</a>)</cite>, TruthfulQA <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib141" title="">2021</a>)</cite>, AlpacaEval <cite class="ltx_cite ltx_citemacro_citep">(Dubois et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib57" title="">2024</a>)</cite>, 
<br class="ltx_break"/>Chatbot Arena <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>)</cite>, MT-Bench <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>)</cite>, 
WildBench <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib139" title="">2024</a>)</cite>, FLASK <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib270" title="">2023b</a>)</cite>, RewardBench <cite class="ltx_cite ltx_citemacro_citep">(Lambert et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib117" title="">2024</a>)</cite>, RM-Bench <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib149" title="">2024b</a>)</cite>, 
JudgeBench <cite class="ltx_cite ltx_citemacro_citep">(Tan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib214" title="">2024</a>)</cite>, 
<br class="ltx_break"/>MLLM-as-a-Judge <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib25" title="">2024b</a>)</cite>, MM-Eval <cite class="ltx_cite ltx_citemacro_citep">(Son et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib203" title="">2024b</a>)</cite>, leaf, text width=41em
]
]]
[Metric (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS2" title="6.2. Metric ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.2</span></a>)
[Accuracy, Pearson <cite class="ltx_cite ltx_citemacro_citep">(Cohen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib42" title="">2009</a>)</cite>, Spearman <cite class="ltx_cite ltx_citemacro_citep">(Sedgwick, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib191" title="">2014</a>)</cite>, Kendall’s Tau <cite class="ltx_cite ltx_citemacro_citep">(Sen, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib192" title="">1968</a>)</cite>, Cohen’s Kappa <cite class="ltx_cite ltx_citemacro_citep">(Warrens, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib241" title="">2015</a>)</cite>, ICC <cite class="ltx_cite ltx_citemacro_citep">(Bartko, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib14" title="">1966</a>)</cite>, leaf, text width=49em ]]]]</p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Taxonomy of LLMs-as-judges in functionality, methodology, application, meta-evaluation.</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S1.F2.1" style="width:433.6pt;height:399.6pt;vertical-align:-396.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-205.6pt,1.7pt) scale(0.513261857661393,0.513261857661393) ;"><span class="ltx_ERROR undefined" id="S1.F2.1.1">{forest}</span>
<p class="ltx_p" id="S1.F2.1.2">forked edges,
for tree=
grow=east,
reversed=true,
anchor=base west,
parent anchor=east,
child anchor=west,
base=left,
font=<span class="ltx_text" id="S1.F2.1.2.1" style="font-size:90%;">,
rectangle,
draw=hidden-draw,
rounded corners,
align=left,
minimum width=4em,
edge+=darkgray, line width=1pt,
s sep=3pt,
inner xsep=2pt,
inner ysep=3pt,
ver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,
</span>,
where level=1text width=6.4em,font=<span class="ltx_text" id="S1.F2.1.2.2" style="font-size:70%;">,</span>,
where level=2text width=6.4em,font=<span class="ltx_text" id="S1.F2.1.2.3" style="font-size:70%;">,</span>,
where level=3text width=6.4em,font=<span class="ltx_text" id="S1.F2.1.2.4" style="font-size:70%;">,</span>,
where level=4text width=6.4em,font=<span class="ltx_text" id="S1.F2.1.2.5" style="font-size:70%;">,</span>,
[LLMs-as-judges, ver
[LIMITATION (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7" title="7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7</span></a>)
[Biases (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1" title="7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1</span></a>)
[Presentation-Related 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS1" title="7.1.1. Presentation-Related Biases ‣ 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1.1</span></a>)
[Position bias <cite class="ltx_cite ltx_citemacro_citep">(Blunch, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib17" title="">1984</a>; Raghubir and Valenzuela, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib183" title="">2006</a>; Ko et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib112" title="">2020</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib235" title="">2018</a>; LLMS, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib152" title="">2025</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib26" title="">2024a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib231" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib131" title="">2023c</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib292" title="">2023b</a>; Raina et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib185" title="">2024</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib88" title="">2024</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib135" title="">2023d</a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib133" title="">b</a>; Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib108" title="">2024</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib297" title="">2023a</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib128" title="">2024a</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib198" title="">2024a</a>; Stureborg et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib209" title="">2024</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib289" title="">2024a</a>)</cite>, Verbosity bias <cite class="ltx_cite ltx_citemacro_citep">(Nasrabadi, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib164" title="">2024</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib271" title="">a</a>)</cite>, leaf, text width=41em] ]
[Social-Related (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS2" title="7.1.2. Social-Related Biases ‣ 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1.2</span></a>)
[Authority bias <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib26" title="">2024a</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib290" title="">2023a</a>)</cite>, Bandwagon-effect bias <cite class="ltx_cite ltx_citemacro_citep">(Koo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib113" title="">2023</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite>, Compassion-fade bias <cite class="ltx_cite ltx_citemacro_citep">(Koo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib113" title="">2023</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite>, Diversity bias <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib26" title="">2024a</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite>, leaf, text width=41em] ]
[Content-Related 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS3" title="7.1.3. Content-Related Biases ‣ 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1.3</span></a>)
[Sentiment bias <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite>, Token Bias <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib99" title="">2024</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib128" title="">2024a</a>; Pezeshkpour and Hruschka, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib179" title="">2023</a>; Raina et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib185" title="">2024</a>)</cite>, Contextual Bias <cite class="ltx_cite ltx_citemacro_citep">(Poulain et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib180" title="">2024</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib301" title="">2024d</a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib297" title="">2023a</a>; Fei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib63" title="">2023</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib291" title="">2021</a>; Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib79" title="">2022</a>)</cite>, leaf, text width=41em] ]
[Cognitive-Related 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS4" title="7.1.4. Cognitive-Related Biases ‣ 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1.4</span></a>)
[Overconfidence bias <cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib108" title="">2024</a>; Jung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib104" title="">2024</a>)</cite>, Self-enhancement bias <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib146" title="">2023a</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib133" title="">2023b</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib146" title="">2023a</a>; Brown, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib20" title="">1986</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>; Badshah and Sajjad, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib9" title="">2024</a>)</cite>, Refinement-aware bias <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib260" title="">2024c</a>)</cite>
<br class="ltx_break"/>Distraction bias <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>; Koo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib113" title="">2023</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib196" title="">2023</a>)</cite>, Fallacy-oversight bias <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib26" title="">2024a</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite>, leaf, text width=41em] ] ]
[Adversarial Attacks 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS2" title="7.2. Adversarial Attacks ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.2</span></a>)
[Adversarial Attacks 
<br class="ltx_break"/>on LLMs (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS2.SSS1" title="7.2.1. Adversarial Attacks on LLMs ‣ 7.2. Adversarial Attacks ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.2.1</span></a>)
[Text-Level Manipulations <cite class="ltx_cite ltx_citemacro_citep">(Ebrahimi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib58" title="">2017</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib101" title="">2023a</a>; Branch et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib19" title="">2022</a>; Perez and Ribeiro, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib178" title="">2022</a>)</cite>, Structural and Semantic Distortions <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib261" title="">2023a</a>)</cite>, Optimization-Based Attacks <cite class="ltx_cite ltx_citemacro_citep">(Sun, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib211" title="">2020</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib212" title="">2020</a>; Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib121" title="">2022</a>)</cite>, leaf, text width=41em] ]
[Adversarial Attacks 
<br class="ltx_break"/>on LLMs-as-Judges 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS2.SSS2" title="7.2.2. Adversarial Attacks on LLMs-as-judges ‣ 7.2. Adversarial Attacks ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.2.2</span></a>)
[Zheng et al. <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib294" title="">2024</a>)</cite>, Doddapaneni et al. <cite class="ltx_cite ltx_citemacro_citep">(Doddapaneni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib52" title="">2024</a>)</cite>, MT-Bench <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>)</cite>, Raina et al. <cite class="ltx_cite ltx_citemacro_citep">(Raina et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib185" title="">2024</a>)</cite>, Shi et al. <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib197" title="">2024b</a>)</cite>, leaf, text width=41em] ] ]
[Inherent Weaknesses 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS3" title="7.3. Inherent Weaknesses ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.3</span></a>)
[Knowledge Recency 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS3.SSS1" title="7.3.1. Knowledge Recency ‣ 7.3. Inherent Weaknesses ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.3.1</span></a>)
[Zhao et al.  <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib288" title="">2023b</a>)</cite>, Luo et al. <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib153" title="">2023</a>)</cite>, Gao et al. <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib70" title="">2023</a>)</cite>, Lewis et al. <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib126" title="">2020</a>)</cite>, Wu et al. <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib245" title="">2024a</a>)</cite>, Dierickx et al.  <cite class="ltx_cite ltx_citemacro_citep">(Dierickx et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib49" title="">2024</a>)</cite>, leaf, text width=41em]]
[Hallucination (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS3.SSS2" title="7.3.2. Hallucination ‣ 7.3. Inherent Weaknesses ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.3.2</span></a>)
[Dierickx et al. <cite class="ltx_cite ltx_citemacro_citep">(Dierickx et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib49" title="">2024</a>)</cite>, Ji et al. <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib98" title="">2023</a>)</cite>, Tonmoy et al. <cite class="ltx_cite ltx_citemacro_citep">(Tonmoy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib217" title="">2024</a>)</cite>, leaf, text width=41em]]
[Domain-Specific 
<br class="ltx_break"/>Knowledge Gaps 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS3.SSS3" title="7.3.3. Domain-Specific Knowledge Gaps ‣ 7.3. Inherent Weaknesses ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.3.3</span></a>)
[Feng et al. <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib64" title="">2023</a>)</cite>, Pan et al. <cite class="ltx_cite ltx_citemacro_citep">(Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib172" title="">2024b</a>)</cite>, Gao et al. <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib70" title="">2023</a>)</cite>, Szymanski et al. <cite class="ltx_cite ltx_citemacro_citep">(Szymanski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib213" title="">2024</a>)</cite>, Dorner et al. <cite class="ltx_cite ltx_citemacro_citep">(Dorner et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib56" title="">2024</a>)</cite>, leaf, text width=41em]] ]]
[
FUTURE WORK 
<br class="ltx_break"/>(§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8" title="8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">8</span></a>)
[
More Efficient (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS1" title="8.1. More Efficient LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">8.1</span></a>)
[
Automated Construction of Evaluation Criteria and Tasks <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib11" title="">2024</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib287" title="">2024b</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib274" title="">2024</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib281" title="">2024c</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib234" title="">2024f</a>)</cite>, Scalable Evaluation Systems <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib258" title="">2024a</a>)</cite>, Accelerating Evaluation Processes <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib124" title="">2024b</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib150" title="">2024c</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib32" title="">2024f</a>)</cite>, leaf, text width=49em
]
]
[
More Effective (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS2" title="8.2. More Effective LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">8.2</span></a>)
[
Integration of Reasoning and Judge Capabilities <cite class="ltx_cite ltx_citemacro_citep">(Zhuo, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib305" title="">2023</a>; Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib272" title="">2024</a>; Stephan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib208" title="">2024</a>)</cite>, Establishing a Collective Judgment Mechanism <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib23" title="">2023</a>; Chu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib40" title="">2024</a>)</cite>, Enhancing Domain Knowledge <cite class="ltx_cite ltx_citemacro_citep">(Raju et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib186" title="">2024</a>)</cite>, 
<br class="ltx_break"/>Cross-Domain and Cross-Language Transferability <cite class="ltx_cite ltx_citemacro_citep">(Son et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib203" title="">2024b</a>; Hada et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib78" title="">2023</a>; Watts et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib242" title="">2024</a>)</cite>, Multimodal Integration Evaluation <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib25" title="">2024b</a>)</cite>, leaf, text width=49em
]
]
[
More Reliable (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S8.SS3" title="8.3. More Reliable LLMs-as-Judges ‣ 8. Future Work ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">8.3</span></a>)
[
Enhancing Interpretability and Transparency <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib148" title="">2024a</a>)</cite>, 
Mitigating Bias and Ensuring Fairness <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib128" title="">2024a</a>)</cite>, 
Enhancing Robustness <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib197" title="">2024b</a>; Elangovan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib59" title="">2024</a>)</cite>, leaf, text width=49em
]]]]</p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Taxonomy of LLMs-as-judges in limitation and future work.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>PRELIMINARIES</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">In this section, we will provide a formal definition of LLMs-as-judges, aiming to encompass all current evaluation paradigms and methods, thereby offering readers a clear and thorough understanding. Figure <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S2.F3" title="Figure 3 ‣ 2.2.1. Evaluation Type 𝒯 ‣ 2.2. Evaluation Input ‣ 2. PRELIMINARIES ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3</span></a> presents an overview of the LLMs-as-judges system.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.9">The LLMs-as-judges paradigm is a flexible and powerful evaluation framework where LLMs are employed as evaluative tools, responsible for assessing the quality, relevance, and effectiveness of generated outputs based on defined evaluation criteria. This framework leverages the extensive knowledge and deep contextual understanding of LLMs, enabling it to flexibly adapt to various tasks in NLP and machine learning.
We formalize the input-output structure of the LLMs-as-Judges paradigm, unifying various evaluation scenarios into a unified perspective.
Specifically, the evaluation process can be defined as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="(\mathcal{Y},\mathcal{E},\mathcal{F})=E(\mathcal{T},\mathcal{C},\mathcal{X},%
\mathcal{R})" class="ltx_Math" display="block" id="S2.E1.m1.7"><semantics id="S2.E1.m1.7a"><mrow id="S2.E1.m1.7.8" xref="S2.E1.m1.7.8.cmml"><mrow id="S2.E1.m1.7.8.2.2" xref="S2.E1.m1.7.8.2.1.cmml"><mo id="S2.E1.m1.7.8.2.2.1" stretchy="false" xref="S2.E1.m1.7.8.2.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml">𝒴</mi><mo id="S2.E1.m1.7.8.2.2.2" xref="S2.E1.m1.7.8.2.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.2.2" xref="S2.E1.m1.2.2.cmml">ℰ</mi><mo id="S2.E1.m1.7.8.2.2.3" xref="S2.E1.m1.7.8.2.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml">ℱ</mi><mo id="S2.E1.m1.7.8.2.2.4" stretchy="false" xref="S2.E1.m1.7.8.2.1.cmml">)</mo></mrow><mo id="S2.E1.m1.7.8.1" xref="S2.E1.m1.7.8.1.cmml">=</mo><mrow id="S2.E1.m1.7.8.3" xref="S2.E1.m1.7.8.3.cmml"><mi id="S2.E1.m1.7.8.3.2" xref="S2.E1.m1.7.8.3.2.cmml">E</mi><mo id="S2.E1.m1.7.8.3.1" xref="S2.E1.m1.7.8.3.1.cmml">⁢</mo><mrow id="S2.E1.m1.7.8.3.3.2" xref="S2.E1.m1.7.8.3.3.1.cmml"><mo id="S2.E1.m1.7.8.3.3.2.1" stretchy="false" xref="S2.E1.m1.7.8.3.3.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.4.4" xref="S2.E1.m1.4.4.cmml">𝒯</mi><mo id="S2.E1.m1.7.8.3.3.2.2" xref="S2.E1.m1.7.8.3.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.5.5" xref="S2.E1.m1.5.5.cmml">𝒞</mi><mo id="S2.E1.m1.7.8.3.3.2.3" xref="S2.E1.m1.7.8.3.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.6.6" xref="S2.E1.m1.6.6.cmml">𝒳</mi><mo id="S2.E1.m1.7.8.3.3.2.4" xref="S2.E1.m1.7.8.3.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S2.E1.m1.7.7" xref="S2.E1.m1.7.7.cmml">ℛ</mi><mo id="S2.E1.m1.7.8.3.3.2.5" stretchy="false" xref="S2.E1.m1.7.8.3.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.7b"><apply id="S2.E1.m1.7.8.cmml" xref="S2.E1.m1.7.8"><eq id="S2.E1.m1.7.8.1.cmml" xref="S2.E1.m1.7.8.1"></eq><vector id="S2.E1.m1.7.8.2.1.cmml" xref="S2.E1.m1.7.8.2.2"><ci id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.1">𝒴</ci><ci id="S2.E1.m1.2.2.cmml" xref="S2.E1.m1.2.2">ℰ</ci><ci id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3">ℱ</ci></vector><apply id="S2.E1.m1.7.8.3.cmml" xref="S2.E1.m1.7.8.3"><times id="S2.E1.m1.7.8.3.1.cmml" xref="S2.E1.m1.7.8.3.1"></times><ci id="S2.E1.m1.7.8.3.2.cmml" xref="S2.E1.m1.7.8.3.2">𝐸</ci><vector id="S2.E1.m1.7.8.3.3.1.cmml" xref="S2.E1.m1.7.8.3.3.2"><ci id="S2.E1.m1.4.4.cmml" xref="S2.E1.m1.4.4">𝒯</ci><ci id="S2.E1.m1.5.5.cmml" xref="S2.E1.m1.5.5">𝒞</ci><ci id="S2.E1.m1.6.6.cmml" xref="S2.E1.m1.6.6">𝒳</ci><ci id="S2.E1.m1.7.7.cmml" xref="S2.E1.m1.7.7">ℛ</ci></vector></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.7c">(\mathcal{Y},\mathcal{E},\mathcal{F})=E(\mathcal{T},\mathcal{C},\mathcal{X},%
\mathcal{R})</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.7d">( caligraphic_Y , caligraphic_E , caligraphic_F ) = italic_E ( caligraphic_T , caligraphic_C , caligraphic_X , caligraphic_R )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.p2.8">where <math alttext="E" class="ltx_Math" display="inline" id="S2.p2.1.m1.1"><semantics id="S2.p2.1.m1.1a"><mi id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><ci id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">E</annotation><annotation encoding="application/x-llamapun" id="S2.p2.1.m1.1d">italic_E</annotation></semantics></math> is the evaluation function, taking the evaluation type <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S2.p2.2.m2.1"><semantics id="S2.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><ci id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.2.m2.1d">caligraphic_T</annotation></semantics></math>, evaluation criteria <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S2.p2.3.m3.1"><semantics id="S2.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.3.m3.1d">caligraphic_C</annotation></semantics></math>, evaluation item <math alttext="\mathcal{X}" class="ltx_Math" display="inline" id="S2.p2.4.m4.1"><semantics id="S2.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml">𝒳</mi><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><ci id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1">𝒳</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">\mathcal{X}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.4.m4.1d">caligraphic_X</annotation></semantics></math> and optional references <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S2.p2.5.m5.1"><semantics id="S2.p2.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><ci id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.5.m5.1d">caligraphic_R</annotation></semantics></math> as input. Based on these inputs, the LLM can produces three outputs: evaluation result <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S2.p2.6.m6.1"><semantics id="S2.p2.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.1b"><ci id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.6.m6.1d">caligraphic_Y</annotation></semantics></math>, explanation <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S2.p2.7.m7.1"><semantics id="S2.p2.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><ci id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.7.m7.1d">caligraphic_E</annotation></semantics></math> and feedback <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S2.p2.8.m8.1"><semantics id="S2.p2.8.m8.1a"><mi class="ltx_font_mathcaligraphic" id="S2.p2.8.m8.1.1" xref="S2.p2.8.m8.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S2.p2.8.m8.1b"><ci id="S2.p2.8.m8.1.1.cmml" xref="S2.p2.8.m8.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m8.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S2.p2.8.m8.1d">caligraphic_F</annotation></semantics></math>.
Different input-output configurations correspond to distinct methods and objectives. This unified formulation brings together diverse evaluation paradigms, offering a structured framework for categorizing and understanding various approaches within LLMs-as-judges.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Evaluation Function <math alttext="E" class="ltx_Math" display="inline" id="S2.SS1.1.m1.1"><semantics id="S2.SS1.1.m1.1b"><mi id="S2.SS1.1.m1.1.1" xref="S2.SS1.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.1.m1.1c"><ci id="S2.SS1.1.m1.1.1.cmml" xref="S2.SS1.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.1.m1.1d">E</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.1.m1.1e">italic_E</annotation></semantics></math>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The evaluation function <math alttext="E" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mi id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">E</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><ci id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">𝐸</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">E</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">italic_E</annotation></semantics></math> in the context of LLMs-as-judges can be categorized into three primary configurations: Single-LLM systems, Multi-LLM systems, and Hybrid systems that combine LLMs with human evaluators. Each of these configurations serves distinct purposes, offers different advantages, and faces unique challenges.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i1.p1.1.1">Single-LLM Evaluation System <cite class="ltx_cite ltx_citemacro_citep">(Lin and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib142" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib281" title="">2024c</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib146" title="">2023a</a>)</cite>: </span> A single LLM evaluation system relies on a single model to perform the evaluation tasks. It is simple to deploy and scale, making it efficient for tasks that don’t require specialized evaluation. However, its flexibility is limited, as it may struggle with tasks that demand specialized knowledge or reasoning over complex inputs. Additionally, if not properly trained, a single model may introduce biases, leading to inaccurate evaluations.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i2.p1.1.1">Multi-LLM Evaluation Systems <cite class="ltx_cite ltx_citemacro_citep">(Chu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib40" title="">2024</a>; Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib23" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib133" title="">2023b</a>)</cite>: </span> A Multi-LLM evaluation system combines multiple models that work together to perform evaluation tasks. These models may interact through various mechanisms, such as collaboration, or competition, to refine their outputs and achieve more accurate results. By leveraging the strengths of different models, a multi-model system can cover a broader range of evaluation criteria and provide a more comprehensive assessment. However, this comes at a higher computational cost and requires more resources, making deployment and maintenance more challenging, particularly for large-scale tasks.
Moreover, while cooperation between models often enhances evaluation results, the methods through which these models achieve consensus or resolve differences remain key areas of ongoing exploration.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I1.i3.p1.1.1">Human-AI Collaboration System <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib132" title="">2023a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib231" title="">2023b</a>; Shankar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib193" title="">2024</a>)</cite>: </span> In this system, LLMs work alongside human evaluators, combining the efficiency of automated evaluation with the nuanced judgment of human expertise. This configuration allows human evaluators to mitigate potential biases in the LLM’s output and provide subjective insights into complex evaluation tasks. While this system offers greater reliability and depth, it comes with challenges in coordinating between the models and humans, ensuring consistent evaluation standards, and integrating feedback. Additionally, the inclusion of human evaluators increases both the cost and time required for the evaluation process, making it less scalable than purely model-based systems.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Evaluation Input</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.4">In the LLMs-as-judges paradigm, in addition to the evaluation item <math alttext="\mathcal{X}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">𝒳</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">𝒳</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\mathcal{X}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">caligraphic_X</annotation></semantics></math>, LLM judges typically receive three other types of inputs: Evaluation Type <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">caligraphic_T</annotation></semantics></math> , Evaluation Criteria <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.1d">caligraphic_C</annotation></semantics></math>, and Evaluation References <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.1"><semantics id="S2.SS2.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><ci id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.4.m4.1d">caligraphic_R</annotation></semantics></math>. The following provides a detailed explanation:</p>
</div>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1. </span>Evaluation Type <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.1.m1.1"><semantics id="S2.SS2.SSS1.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.1.m1.1.1" xref="S2.SS2.SSS1.1.m1.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.1.m1.1c"><ci id="S2.SS2.SSS1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.1.m1.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.1.m1.1d">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.1.m1.1e">caligraphic_T</annotation></semantics></math>
</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">The Evaluation Type <math alttext="\mathcal{T}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p1.1.m1.1"><semantics id="S2.SS2.SSS1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS1.p1.1.m1.1.1" xref="S2.SS2.SSS1.p1.1.m1.1.1.cmml">𝒯</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p1.1.m1.1b"><ci id="S2.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p1.1.m1.1.1">𝒯</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p1.1.m1.1c">\mathcal{T}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p1.1.m1.1d">caligraphic_T</annotation></semantics></math> defines the specific evaluation mode, determining how the evaluation will be conducted. It typically includes three approaches: pointwise, pairwise, and listwise evaluation.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p2">
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i1.p1.1.1">Pointwise Evaluation <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib230" title="">2023e</a>; Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib110" title="">2023</a>; Ye and Ng, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib267" title="">2024</a>)</cite>:</span> This method evaluates each candidate item individually based on the specified criteria.
For example, in a text summarization task, the LLM might evaluate each generated summary separately, assigning a score based on factors like informativeness, coherence, and conciseness. Although pointwise evaluation is simple and easy to apply, it may fail to capture the relative quality differences between candidates and can be influenced by biases arising from evaluating items in isolation.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i2.p1.1.1">Pairwise Evaluation <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib189" title="">2023</a>; Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib21" title="">2024a</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib85" title="">2024b</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib90" title="">2024b</a>)</cite>:</span> This method involves directly comparing two candidate items to determine which one performs better according to the specified criteria. It is commonly used in preference-based tasks. For example, given two summaries of a news article, the LLM may be asked to decide which summary is more coherent or informative. Pairwise evaluation closely mirrors human decision-making processes by focusing on relative preferences rather than assigning absolute scores. This approach is especially effective when the differences between outputs are subtle and difficult to quantify.</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I2.i3.p1.1.1">Listwise Evaluation <cite class="ltx_cite ltx_citemacro_citep">(Zhuang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib303" title="">2024</a>; Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib264" title="">2024b</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib88" title="">2024</a>; Niu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib167" title="">2024</a>)</cite>:</span>
This method is designed to collectively evaluate the entire list of candidate items, evaluating and ranking them based on the specific criteria. It is often applied in ranking tasks, such as document retrieval in search engines, where the objective is to determine the relevance of the documents in relation to a user query. Listwise evaluation takes into account the interactions between multiple candidates, making it well-suited for applications that require holistic analysis.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.SSS1.p3">
<p class="ltx_p" id="S2.SS2.SSS1.p3.6">In general, these three evaluation modes are not entirely independent. pointwise scores can be aggregated to create pairwise comparisons or used to construct a ranked list. Similarly, pairwise preferences can be organized into a complete ranking list for listwise analysis.
However, these transformations are not always reliable within the LLMs-as-judges framework <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib150" title="">2024c</a>)</cite>. For example, in pointwise evaluation, output <math alttext="A" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p3.1.m1.1"><semantics id="S2.SS2.SSS1.p3.1.m1.1a"><mi id="S2.SS2.SSS1.p3.1.m1.1.1" xref="S2.SS2.SSS1.p3.1.m1.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p3.1.m1.1b"><ci id="S2.SS2.SSS1.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS1.p3.1.m1.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p3.1.m1.1c">A</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p3.1.m1.1d">italic_A</annotation></semantics></math> may receive a score of 5, while output <math alttext="B" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p3.2.m2.1"><semantics id="S2.SS2.SSS1.p3.2.m2.1a"><mi id="S2.SS2.SSS1.p3.2.m2.1.1" xref="S2.SS2.SSS1.p3.2.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p3.2.m2.1b"><ci id="S2.SS2.SSS1.p3.2.m2.1.1.cmml" xref="S2.SS2.SSS1.p3.2.m2.1.1">𝐵</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p3.2.m2.1c">B</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p3.2.m2.1d">italic_B</annotation></semantics></math> receives a score of 4, yet, a direct pairwise comparison might not consistently yield <math alttext="A&gt;B" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p3.3.m3.1"><semantics id="S2.SS2.SSS1.p3.3.m3.1a"><mrow id="S2.SS2.SSS1.p3.3.m3.1.1" xref="S2.SS2.SSS1.p3.3.m3.1.1.cmml"><mi id="S2.SS2.SSS1.p3.3.m3.1.1.2" xref="S2.SS2.SSS1.p3.3.m3.1.1.2.cmml">A</mi><mo id="S2.SS2.SSS1.p3.3.m3.1.1.1" xref="S2.SS2.SSS1.p3.3.m3.1.1.1.cmml">&gt;</mo><mi id="S2.SS2.SSS1.p3.3.m3.1.1.3" xref="S2.SS2.SSS1.p3.3.m3.1.1.3.cmml">B</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p3.3.m3.1b"><apply id="S2.SS2.SSS1.p3.3.m3.1.1.cmml" xref="S2.SS2.SSS1.p3.3.m3.1.1"><gt id="S2.SS2.SSS1.p3.3.m3.1.1.1.cmml" xref="S2.SS2.SSS1.p3.3.m3.1.1.1"></gt><ci id="S2.SS2.SSS1.p3.3.m3.1.1.2.cmml" xref="S2.SS2.SSS1.p3.3.m3.1.1.2">𝐴</ci><ci id="S2.SS2.SSS1.p3.3.m3.1.1.3.cmml" xref="S2.SS2.SSS1.p3.3.m3.1.1.3">𝐵</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p3.3.m3.1c">A&gt;B</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p3.3.m3.1d">italic_A &gt; italic_B</annotation></semantics></math> due to potential bias. Additionally, LLM judges do not always satisfy transitivity in their judgments. For instance, given pairwise preferences where <math alttext="z_{i}&gt;z_{j}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p3.4.m4.1"><semantics id="S2.SS2.SSS1.p3.4.m4.1a"><mrow id="S2.SS2.SSS1.p3.4.m4.1.1" xref="S2.SS2.SSS1.p3.4.m4.1.1.cmml"><msub id="S2.SS2.SSS1.p3.4.m4.1.1.2" xref="S2.SS2.SSS1.p3.4.m4.1.1.2.cmml"><mi id="S2.SS2.SSS1.p3.4.m4.1.1.2.2" xref="S2.SS2.SSS1.p3.4.m4.1.1.2.2.cmml">z</mi><mi id="S2.SS2.SSS1.p3.4.m4.1.1.2.3" xref="S2.SS2.SSS1.p3.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS2.SSS1.p3.4.m4.1.1.1" xref="S2.SS2.SSS1.p3.4.m4.1.1.1.cmml">&gt;</mo><msub id="S2.SS2.SSS1.p3.4.m4.1.1.3" xref="S2.SS2.SSS1.p3.4.m4.1.1.3.cmml"><mi id="S2.SS2.SSS1.p3.4.m4.1.1.3.2" xref="S2.SS2.SSS1.p3.4.m4.1.1.3.2.cmml">z</mi><mi id="S2.SS2.SSS1.p3.4.m4.1.1.3.3" xref="S2.SS2.SSS1.p3.4.m4.1.1.3.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p3.4.m4.1b"><apply id="S2.SS2.SSS1.p3.4.m4.1.1.cmml" xref="S2.SS2.SSS1.p3.4.m4.1.1"><gt id="S2.SS2.SSS1.p3.4.m4.1.1.1.cmml" xref="S2.SS2.SSS1.p3.4.m4.1.1.1"></gt><apply id="S2.SS2.SSS1.p3.4.m4.1.1.2.cmml" xref="S2.SS2.SSS1.p3.4.m4.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p3.4.m4.1.1.2.1.cmml" xref="S2.SS2.SSS1.p3.4.m4.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS1.p3.4.m4.1.1.2.2.cmml" xref="S2.SS2.SSS1.p3.4.m4.1.1.2.2">𝑧</ci><ci id="S2.SS2.SSS1.p3.4.m4.1.1.2.3.cmml" xref="S2.SS2.SSS1.p3.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S2.SS2.SSS1.p3.4.m4.1.1.3.cmml" xref="S2.SS2.SSS1.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p3.4.m4.1.1.3.1.cmml" xref="S2.SS2.SSS1.p3.4.m4.1.1.3">subscript</csymbol><ci id="S2.SS2.SSS1.p3.4.m4.1.1.3.2.cmml" xref="S2.SS2.SSS1.p3.4.m4.1.1.3.2">𝑧</ci><ci id="S2.SS2.SSS1.p3.4.m4.1.1.3.3.cmml" xref="S2.SS2.SSS1.p3.4.m4.1.1.3.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p3.4.m4.1c">z_{i}&gt;z_{j}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p3.4.m4.1d">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT &gt; italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="z_{j}&gt;z_{k}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p3.5.m5.1"><semantics id="S2.SS2.SSS1.p3.5.m5.1a"><mrow id="S2.SS2.SSS1.p3.5.m5.1.1" xref="S2.SS2.SSS1.p3.5.m5.1.1.cmml"><msub id="S2.SS2.SSS1.p3.5.m5.1.1.2" xref="S2.SS2.SSS1.p3.5.m5.1.1.2.cmml"><mi id="S2.SS2.SSS1.p3.5.m5.1.1.2.2" xref="S2.SS2.SSS1.p3.5.m5.1.1.2.2.cmml">z</mi><mi id="S2.SS2.SSS1.p3.5.m5.1.1.2.3" xref="S2.SS2.SSS1.p3.5.m5.1.1.2.3.cmml">j</mi></msub><mo id="S2.SS2.SSS1.p3.5.m5.1.1.1" xref="S2.SS2.SSS1.p3.5.m5.1.1.1.cmml">&gt;</mo><msub id="S2.SS2.SSS1.p3.5.m5.1.1.3" xref="S2.SS2.SSS1.p3.5.m5.1.1.3.cmml"><mi id="S2.SS2.SSS1.p3.5.m5.1.1.3.2" xref="S2.SS2.SSS1.p3.5.m5.1.1.3.2.cmml">z</mi><mi id="S2.SS2.SSS1.p3.5.m5.1.1.3.3" xref="S2.SS2.SSS1.p3.5.m5.1.1.3.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p3.5.m5.1b"><apply id="S2.SS2.SSS1.p3.5.m5.1.1.cmml" xref="S2.SS2.SSS1.p3.5.m5.1.1"><gt id="S2.SS2.SSS1.p3.5.m5.1.1.1.cmml" xref="S2.SS2.SSS1.p3.5.m5.1.1.1"></gt><apply id="S2.SS2.SSS1.p3.5.m5.1.1.2.cmml" xref="S2.SS2.SSS1.p3.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p3.5.m5.1.1.2.1.cmml" xref="S2.SS2.SSS1.p3.5.m5.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS1.p3.5.m5.1.1.2.2.cmml" xref="S2.SS2.SSS1.p3.5.m5.1.1.2.2">𝑧</ci><ci id="S2.SS2.SSS1.p3.5.m5.1.1.2.3.cmml" xref="S2.SS2.SSS1.p3.5.m5.1.1.2.3">𝑗</ci></apply><apply id="S2.SS2.SSS1.p3.5.m5.1.1.3.cmml" xref="S2.SS2.SSS1.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p3.5.m5.1.1.3.1.cmml" xref="S2.SS2.SSS1.p3.5.m5.1.1.3">subscript</csymbol><ci id="S2.SS2.SSS1.p3.5.m5.1.1.3.2.cmml" xref="S2.SS2.SSS1.p3.5.m5.1.1.3.2">𝑧</ci><ci id="S2.SS2.SSS1.p3.5.m5.1.1.3.3.cmml" xref="S2.SS2.SSS1.p3.5.m5.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p3.5.m5.1c">z_{j}&gt;z_{k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p3.5.m5.1d">italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT &gt; italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> , the LLM may not necessarily yield <math alttext="z_{i}&gt;z_{k}" class="ltx_Math" display="inline" id="S2.SS2.SSS1.p3.6.m6.1"><semantics id="S2.SS2.SSS1.p3.6.m6.1a"><mrow id="S2.SS2.SSS1.p3.6.m6.1.1" xref="S2.SS2.SSS1.p3.6.m6.1.1.cmml"><msub id="S2.SS2.SSS1.p3.6.m6.1.1.2" xref="S2.SS2.SSS1.p3.6.m6.1.1.2.cmml"><mi id="S2.SS2.SSS1.p3.6.m6.1.1.2.2" xref="S2.SS2.SSS1.p3.6.m6.1.1.2.2.cmml">z</mi><mi id="S2.SS2.SSS1.p3.6.m6.1.1.2.3" xref="S2.SS2.SSS1.p3.6.m6.1.1.2.3.cmml">i</mi></msub><mo id="S2.SS2.SSS1.p3.6.m6.1.1.1" xref="S2.SS2.SSS1.p3.6.m6.1.1.1.cmml">&gt;</mo><msub id="S2.SS2.SSS1.p3.6.m6.1.1.3" xref="S2.SS2.SSS1.p3.6.m6.1.1.3.cmml"><mi id="S2.SS2.SSS1.p3.6.m6.1.1.3.2" xref="S2.SS2.SSS1.p3.6.m6.1.1.3.2.cmml">z</mi><mi id="S2.SS2.SSS1.p3.6.m6.1.1.3.3" xref="S2.SS2.SSS1.p3.6.m6.1.1.3.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS1.p3.6.m6.1b"><apply id="S2.SS2.SSS1.p3.6.m6.1.1.cmml" xref="S2.SS2.SSS1.p3.6.m6.1.1"><gt id="S2.SS2.SSS1.p3.6.m6.1.1.1.cmml" xref="S2.SS2.SSS1.p3.6.m6.1.1.1"></gt><apply id="S2.SS2.SSS1.p3.6.m6.1.1.2.cmml" xref="S2.SS2.SSS1.p3.6.m6.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p3.6.m6.1.1.2.1.cmml" xref="S2.SS2.SSS1.p3.6.m6.1.1.2">subscript</csymbol><ci id="S2.SS2.SSS1.p3.6.m6.1.1.2.2.cmml" xref="S2.SS2.SSS1.p3.6.m6.1.1.2.2">𝑧</ci><ci id="S2.SS2.SSS1.p3.6.m6.1.1.2.3.cmml" xref="S2.SS2.SSS1.p3.6.m6.1.1.2.3">𝑖</ci></apply><apply id="S2.SS2.SSS1.p3.6.m6.1.1.3.cmml" xref="S2.SS2.SSS1.p3.6.m6.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.SSS1.p3.6.m6.1.1.3.1.cmml" xref="S2.SS2.SSS1.p3.6.m6.1.1.3">subscript</csymbol><ci id="S2.SS2.SSS1.p3.6.m6.1.1.3.2.cmml" xref="S2.SS2.SSS1.p3.6.m6.1.1.3.2">𝑧</ci><ci id="S2.SS2.SSS1.p3.6.m6.1.1.3.3.cmml" xref="S2.SS2.SSS1.p3.6.m6.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS1.p3.6.m6.1c">z_{i}&gt;z_{k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS1.p3.6.m6.1d">italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT &gt; italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. These inconsistencies contribute to concerns about the reliability and trustworthiness of the LLM-as-Judge framework, which we will discuss in detail in Section (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7" title="7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7</span></a>).</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="590" id="S2.F3.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Overview of the LLMs-as-judges system.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2. </span>Evaluation Criteria <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.1.m1.1"><semantics id="S2.SS2.SSS2.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS2.1.m1.1.1" xref="S2.SS2.SSS2.1.m1.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.1.m1.1c"><ci id="S2.SS2.SSS2.1.m1.1.1.cmml" xref="S2.SS2.SSS2.1.m1.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.1.m1.1d">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.1.m1.1e">caligraphic_C</annotation></semantics></math>.</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">The evaluation criteria <math alttext="\mathcal{C}" class="ltx_Math" display="inline" id="S2.SS2.SSS2.p1.1.m1.1"><semantics id="S2.SS2.SSS2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS2.p1.1.m1.1.1" xref="S2.SS2.SSS2.p1.1.m1.1.1.cmml">𝒞</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS2.p1.1.m1.1b"><ci id="S2.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS2.p1.1.m1.1.1">𝒞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS2.p1.1.m1.1c">\mathcal{C}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS2.p1.1.m1.1d">caligraphic_C</annotation></semantics></math> define the specific standards that determine which aspects of the output should be assessed. These criteria are designed to cover a broad range of quality attributes and can be tailored based on the nature of the task. Typically, the criteria encompass the following aspects:</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p2">
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i1.p1.1.1">Linguistic Quality <cite class="ltx_cite ltx_citemacro_citep">(Fabbri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib60" title="">2021</a>; Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib284" title="">2018</a>; Chhun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib35" title="">2022</a>)</cite>:</span> This category evaluates the language-related features of the output, such as fluency, grammatical accuracy, coherence, and Conciseness. Linguistic quality is crucial in tasks like text generation, machine translation, and summarization, where clarity and readability are essential.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i2.p1.1.1">Content Accuracy <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib31" title="">2021</a>; Jimenez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib102" title="">2023</a>; Tan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib214" title="">2024</a>)</cite>:</span> This dimension focuses on the correctness and relevance of the content. It includes evaluating aspects such as factual accuracy, ensuring that the output does not contain misleading or incorrect information. Content accuracy is particularly crucial in tasks such as code generation and fact-checking.</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i3.p1">
<p class="ltx_p" id="S2.I3.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I3.i3.p1.1.1">Task-Specific Metrics <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib97" title="">2024</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib139" title="">2024</a>; Tan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib214" title="">2024</a>)</cite>:</span> In addition to general quality metrics, many tasks require evaluation based on standards specific to their respective domains. These standards may include metrics such as informativeness (assessing whether the output provides comprehensive and valuable information) or completeness (ensuring all key aspects of the input are covered). Other criteria may include diversity, well-structured content, and logical clarity.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS2.SSS2.p3">
<p class="ltx_p" id="S2.SS2.SSS2.p3.1">In addition to providing clear evaluation criteria, offering several examples can also be beneficial for the assessment. By incorporating well-structured examples, LLMs can better align its output with user expectations, especially when handling complex tasks or ambiguous queries.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.3. </span>Evaluation References <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S2.SS2.SSS3.1.m1.1"><semantics id="S2.SS2.SSS3.1.m1.1b"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS3.1.m1.1.1" xref="S2.SS2.SSS3.1.m1.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.1.m1.1c"><ci id="S2.SS2.SSS3.1.m1.1.1.cmml" xref="S2.SS2.SSS3.1.m1.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.1.m1.1d">\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.1.m1.1e">caligraphic_R</annotation></semantics></math>.</h4>
<div class="ltx_para" id="S2.SS2.SSS3.p1">
<p class="ltx_p" id="S2.SS2.SSS3.p1.1">Evaluation References <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S2.SS2.SSS3.p1.1.m1.1"><semantics id="S2.SS2.SSS3.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.SSS3.p1.1.m1.1.1" xref="S2.SS2.SSS3.p1.1.m1.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS3.p1.1.m1.1b"><ci id="S2.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS3.p1.1.m1.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS3.p1.1.m1.1c">\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS3.p1.1.m1.1d">caligraphic_R</annotation></semantics></math> are optional. Depending on the availability of evaluation reference, the evaluation process can be broadly divided into reference-based and reference-free scenarios.</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS3.p2">
<ul class="ltx_itemize" id="S2.I4">
<li class="ltx_item" id="S2.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i1.p1">
<p class="ltx_p" id="S2.I4.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I4.i1.p1.1.1">Reference-Based Evaluation <cite class="ltx_cite ltx_citemacro_citep">(Freitag et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib67" title="">2021b</a>; Karpinska and Iyyer, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib105" title="">2023</a>)</cite>: </span>
The reference-based evaluation leverages reference data to determine whether the performance meets the expected standards. It is commonly applied in tasks where the quality of the output can be objectively judged by its similarity to established reference. In Natural Language Generation (NLG) tasks, this method is widely used to evaluate the resemblance between generated content and reference content. For example, in machine translation or text summarization, an LLM can compare the generated translations or summaries against high-quality references. The key strength of this approach is its well-defined benchmarking process; however, its effectiveness may be constrained by the quality and variety of the reference data.</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I4.i2.p1">
<p class="ltx_p" id="S2.I4.i2.p1.2"><span class="ltx_text ltx_font_bold" id="S2.I4.i2.p1.2.1">Reference-Free Evaluation <cite class="ltx_cite ltx_citemacro_citep">(Shen and Wan, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib195" title="">2023</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>; He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib83" title="">2023b</a>)</cite>: </span>
The reference-free evaluation does not rely on a specific reference <math alttext="\mathcal{R}" class="ltx_Math" display="inline" id="S2.I4.i2.p1.1.m1.1"><semantics id="S2.I4.i2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.I4.i2.p1.1.m1.1.1" xref="S2.I4.i2.p1.1.m1.1.1.cmml">ℛ</mi><annotation-xml encoding="MathML-Content" id="S2.I4.i2.p1.1.m1.1b"><ci id="S2.I4.i2.p1.1.m1.1.1.cmml" xref="S2.I4.i2.p1.1.m1.1.1">ℛ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I4.i2.p1.1.m1.1c">\mathcal{R}</annotation><annotation encoding="application/x-llamapun" id="S2.I4.i2.p1.1.m1.1d">caligraphic_R</annotation></semantics></math>, instead, it evaluates <math alttext="\mathcal{X}" class="ltx_Math" display="inline" id="S2.I4.i2.p1.2.m2.1"><semantics id="S2.I4.i2.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.I4.i2.p1.2.m2.1.1" xref="S2.I4.i2.p1.2.m2.1.1.cmml">𝒳</mi><annotation-xml encoding="MathML-Content" id="S2.I4.i2.p1.2.m2.1b"><ci id="S2.I4.i2.p1.2.m2.1.1.cmml" xref="S2.I4.i2.p1.2.m2.1.1">𝒳</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I4.i2.p1.2.m2.1c">\mathcal{X}</annotation><annotation encoding="application/x-llamapun" id="S2.I4.i2.p1.2.m2.1d">caligraphic_X</annotation></semantics></math> based on intrinsic quality standards or its alignment with the source context. For example, when assessing language fluency or content coherence, an LLM can autonomously generate evaluation results using internal grammatical and semantic rules. This method is widely used in fields like sentiment analysis and dialogue generation. The main advantage of this approach is its independence from specific references, providing greater flexibility for open-ended tasks. However, its drawback lies in the difficulty of obtaining satisfactory evaluations in domains where the LLM lacks relevant knowledge.</p>
</div>
</li>
</ul>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Evaluation Output</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.3">In the LLMs-as-judges paradigm, the LLM typically generates three types of outputs: the evaluation result <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><ci id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">caligraphic_Y</annotation></semantics></math>, the explanation <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S2.SS3.p1.2.m2.1"><semantics id="S2.SS3.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><ci id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.2.m2.1d">caligraphic_E</annotation></semantics></math>, and the feedback <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S2.SS3.p1.3.m3.1"><semantics id="S2.SS3.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p1.3.m3.1.1" xref="S2.SS3.p1.3.m3.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.3.m3.1b"><ci id="S2.SS3.p1.3.m3.1.1.cmml" xref="S2.SS3.p1.3.m3.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.3.m3.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.3.m3.1d">caligraphic_F</annotation></semantics></math>. Below are detailed descriptions.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<ul class="ltx_itemize" id="S2.I5">
<li class="ltx_item" id="S2.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i1.p1">
<p class="ltx_p" id="S2.I5.i1.p1.4"><span class="ltx_text ltx_font_bold" id="S2.I5.i1.p1.1.1">Evaluation Result <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S2.I5.i1.p1.1.1.m1.1"><semantics id="S2.I5.i1.p1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.I5.i1.p1.1.1.m1.1.1" xref="S2.I5.i1.p1.1.1.m1.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S2.I5.i1.p1.1.1.m1.1b"><ci id="S2.I5.i1.p1.1.1.m1.1.1.cmml" xref="S2.I5.i1.p1.1.1.m1.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I5.i1.p1.1.1.m1.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.I5.i1.p1.1.1.m1.1d">caligraphic_Y</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib287" title="">2024b</a>; Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib189" title="">2023</a>)</cite>: </span> The evaluation result <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S2.I5.i1.p1.2.m1.1"><semantics id="S2.I5.i1.p1.2.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.I5.i1.p1.2.m1.1.1" xref="S2.I5.i1.p1.2.m1.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S2.I5.i1.p1.2.m1.1b"><ci id="S2.I5.i1.p1.2.m1.1.1.cmml" xref="S2.I5.i1.p1.2.m1.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I5.i1.p1.2.m1.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.I5.i1.p1.2.m1.1d">caligraphic_Y</annotation></semantics></math> is the primary output, which can take the form of a numerical score, a ranking, a categorical label, or a qualitative assessment. It reflects the quality, relevance, or performance of the candidate items according to the specified evaluation criteria. For example, in a machine translation task, <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S2.I5.i1.p1.3.m2.1"><semantics id="S2.I5.i1.p1.3.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.I5.i1.p1.3.m2.1.1" xref="S2.I5.i1.p1.3.m2.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S2.I5.i1.p1.3.m2.1b"><ci id="S2.I5.i1.p1.3.m2.1.1.cmml" xref="S2.I5.i1.p1.3.m2.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I5.i1.p1.3.m2.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.I5.i1.p1.3.m2.1d">caligraphic_Y</annotation></semantics></math> could be a score indicating translation quality, while in a dialogue generation task, it might be a rating of coherence and appropriateness on a scale from 1 to 5. The evaluation result <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S2.I5.i1.p1.4.m3.1"><semantics id="S2.I5.i1.p1.4.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.I5.i1.p1.4.m3.1.1" xref="S2.I5.i1.p1.4.m3.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S2.I5.i1.p1.4.m3.1b"><ci id="S2.I5.i1.p1.4.m3.1.1.cmml" xref="S2.I5.i1.p1.4.m3.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I5.i1.p1.4.m3.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.I5.i1.p1.4.m3.1d">caligraphic_Y</annotation></semantics></math> provides a clear measure of performance, enabling researchers to effectively compare different models or outputs.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i2.p1">
<p class="ltx_p" id="S2.I5.i2.p1.2"><span class="ltx_text ltx_font_bold" id="S2.I5.i2.p1.1.1">Explanation <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S2.I5.i2.p1.1.1.m1.1"><semantics id="S2.I5.i2.p1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.I5.i2.p1.1.1.m1.1.1" xref="S2.I5.i2.p1.1.1.m1.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S2.I5.i2.p1.1.1.m1.1b"><ci id="S2.I5.i2.p1.1.1.m1.1.1.cmml" xref="S2.I5.i2.p1.1.1.m1.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I5.i2.p1.1.1.m1.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S2.I5.i2.p1.1.1.m1.1d">caligraphic_E</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib271" title="">2024a</a>; Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib253" title="">2024d</a>)</cite>: </span> The explanation <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S2.I5.i2.p1.2.m1.1"><semantics id="S2.I5.i2.p1.2.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.I5.i2.p1.2.m1.1.1" xref="S2.I5.i2.p1.2.m1.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S2.I5.i2.p1.2.m1.1b"><ci id="S2.I5.i2.p1.2.m1.1.1.cmml" xref="S2.I5.i2.p1.2.m1.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I5.i2.p1.2.m1.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S2.I5.i2.p1.2.m1.1d">caligraphic_E</annotation></semantics></math> provides detailed reasoning and justifications for the evaluation result. It offers insights into why certain result received higher or lower scores, highlighting specific features of the candidate item that influenced the evaluation. For example, in a summarization task, the LLM judges might explain that the score was lowered due to missing critical information or the presence of redundant content. The explanation component enhances transparency, allowing users to understand the decision-making process of the LLM and gain deeper insights into the strengths and weaknesses of the evaluated content.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I5.i3.p1">
<p class="ltx_p" id="S2.I5.i3.p1.2"><span class="ltx_text ltx_font_bold" id="S2.I5.i3.p1.1.1">Feedback <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S2.I5.i3.p1.1.1.m1.1"><semantics id="S2.I5.i3.p1.1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.I5.i3.p1.1.1.m1.1.1" xref="S2.I5.i3.p1.1.1.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S2.I5.i3.p1.1.1.m1.1b"><ci id="S2.I5.i3.p1.1.1.m1.1.1.cmml" xref="S2.I5.i3.p1.1.1.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I5.i3.p1.1.1.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S2.I5.i3.p1.1.1.m1.1d">caligraphic_F</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib156" title="">2024</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib33" title="">2023a</a>)</cite>: </span> The feedback <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S2.I5.i3.p1.2.m1.1"><semantics id="S2.I5.i3.p1.2.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.I5.i3.p1.2.m1.1.1" xref="S2.I5.i3.p1.2.m1.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S2.I5.i3.p1.2.m1.1b"><ci id="S2.I5.i3.p1.2.m1.1.1.cmml" xref="S2.I5.i3.p1.2.m1.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I5.i3.p1.2.m1.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S2.I5.i3.p1.2.m1.1d">caligraphic_F</annotation></semantics></math> consists of actionable suggestions or recommendations aimed at improving the evaluated output. Unlike the evaluation result, which merely indicates performance, the feedback component is designed to guide the refinement of the content. For instance, in a creative writing task, feedback might include recommendations for enhancing the narrative flow or improving clarity. This component is especially valuable for the iterative development of the evaluated item, as it provides concrete pointers that help both the LLM and content creators enhance the quality of the generated outputs.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.6">Depending on the intended purpose and specific requirements of the evaluation, the LLM judges can generate various combinations of the three outputs ( <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S2.SS3.p3.1.m1.1"><semantics id="S2.SS3.p3.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><ci id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.1.m1.1d">caligraphic_Y</annotation></semantics></math>, <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S2.SS3.p3.2.m2.1"><semantics id="S2.SS3.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p3.2.m2.1.1" xref="S2.SS3.p3.2.m2.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.2.m2.1b"><ci id="S2.SS3.p3.2.m2.1.1.cmml" xref="S2.SS3.p3.2.m2.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.2.m2.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.2.m2.1d">caligraphic_E</annotation></semantics></math>, <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S2.SS3.p3.3.m3.1"><semantics id="S2.SS3.p3.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p3.3.m3.1.1" xref="S2.SS3.p3.3.m3.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.3.m3.1b"><ci id="S2.SS3.p3.3.m3.1.1.cmml" xref="S2.SS3.p3.3.m3.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.3.m3.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.3.m3.1d">caligraphic_F</annotation></semantics></math> ) for a given task.
In most cases, providing explanation <math alttext="\mathcal{E}" class="ltx_Math" display="inline" id="S2.SS3.p3.4.m4.1"><semantics id="S2.SS3.p3.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p3.4.m4.1.1" xref="S2.SS3.p3.4.m4.1.1.cmml">ℰ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.4.m4.1b"><ci id="S2.SS3.p3.4.m4.1.1.cmml" xref="S2.SS3.p3.4.m4.1.1">ℰ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.4.m4.1c">\mathcal{E}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.4.m4.1d">caligraphic_E</annotation></semantics></math> not only helps users better understand and trust the evaluation results but also leads to more human-aligned and accurate evaluation result <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S2.SS3.p3.5.m5.1"><semantics id="S2.SS3.p3.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p3.5.m5.1.1" xref="S2.SS3.p3.5.m5.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.5.m5.1b"><ci id="S2.SS3.p3.5.m5.1.1.cmml" xref="S2.SS3.p3.5.m5.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.5.m5.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.5.m5.1d">caligraphic_Y</annotation></semantics></math>. Moreover, generating feedback <math alttext="\mathcal{F}" class="ltx_Math" display="inline" id="S2.SS3.p3.6.m6.1"><semantics id="S2.SS3.p3.6.m6.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS3.p3.6.m6.1.1" xref="S2.SS3.p3.6.m6.1.1.cmml">ℱ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.6.m6.1b"><ci id="S2.SS3.p3.6.m6.1.1.cmml" xref="S2.SS3.p3.6.m6.1.1">ℱ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.6.m6.1c">\mathcal{F}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p3.6.m6.1d">caligraphic_F</annotation></semantics></math> generally demands a higher level of model capability, as it requires not only assessing the quality of the input but also providing concrete, actionable recommendations for improvement.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Functionality</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">As an emerging evaluation paradigm, LLMs-as-judges play a significant role across various scenarios. Based on their functionality, we categorize the application of LLM evaluators into three main directions: <span class="ltx_text ltx_font_bold" id="S3.p1.1.1">Performance Evaluation</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS1" title="3.1. Performance Evaluation ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.1</span></a>), <span class="ltx_text ltx_font_bold" id="S3.p1.1.2">Model Enhancement</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS2" title="3.2. Model Enhancement ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.2</span></a>), and <span class="ltx_text ltx_font_bold" id="S3.p1.1.3">Data Construction</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS3" title="3.3. Data Construction ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.3</span></a>). In this section, we will delve into these functionalities, explore their potential, and discuss specific implementation methods.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="387" id="S3.F4.g1" src="x2.png" width="831"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Overview of the Functionality of LLMs-as-judges.</figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Performance Evaluation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Performance evaluation represents the most fundamental application objective of an LLM judges, serving as the cornerstone for understanding and optimizing their other function. It can be broadly divided into two components: <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Responses Evaluation</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS1.SSS1" title="3.1.1. Responses Evaluation ‣ 3.1. Performance Evaluation ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.1.1</span></a>) and <span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.2">Model Evaluation</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS1.SSS2" title="3.1.2. Model Evaluation ‣ 3.1. Performance Evaluation ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.1.2</span></a>).
Response Evaluation focuses on aspects such as the quality, relevance, and coherence, and fluency of the responses for a given task. In contrast, model evaluation takes a holistic approach, assessing the overall capabilities of LLMs. Although these two aspects are interconnected, they focus on different levels of analysis, providing multidimensional insights into performance.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Responses Evaluation</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">The purpose of evaluating responses is to identify better answers within the context of a specific question or task, which can enhance overall decision-making.
These responses can originate from either AI models or humans.
Evaluation criteria typically consider general attributes such as accuracy, relevance, coherence, and fluency. However, in practical applications, the evaluation of responses often requires customized metrics tailored to specific tasks. For instance, in the education domain, the focus may be more on the inspirational and educational value of the answers.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p2">
<p class="ltx_p" id="S3.SS1.SSS1.p2.1">LLM judges have also been widely applied in the assessment of text response <cite class="ltx_cite ltx_citemacro_citep">(Lin and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib142" title="">2023</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib229" title="">2024c</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib298" title="">2024a</a>)</cite>. Lin et al. <cite class="ltx_cite ltx_citemacro_citep">(Lin and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib142" title="">2023</a>)</cite> propose LLM-Eval, a unified framework employing a single-prompt strategy to evaluate the performance of open-domain dialogue systems across multiple dimensions, including content, grammar, relevance, and appropriateness.
Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib229" title="">2024c</a>)</cite> proposed an article scoring and feedback system tailored to different genres, such as essays, narratives, and question-answering articles. Using BERT and ChatGPT models, they enabled automated scoring and detailed feedback, showcasing the potential of LLMs in article evaluation.
Moreover, Zhou et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib298" title="">2024a</a>)</cite> conduct a detailed evaluation of whether LLMs can serve as reliable tools for automated paper review. Their findings indicate that current LLMs are still not sufficiently reliable for such tasks, particularly in scenarios requiring logical reasoning or a deep knowledge base.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS1.p3">
<p class="ltx_p" id="S3.SS1.SSS1.p3.1">Furthermore, the evaluation of a single response is not limited to assessing the quality of the final answer but can also extend to analyzing the response process <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib189" title="">2023</a>; Asai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib4" title="">2023</a>; Lei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib125" title="">2024</a>)</cite>. For instance, this can include evaluating whether retrieval is necessary at a given step, the relevance of the retrieved documents, and the interpretability of the response.
For example, ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib189" title="">2023</a>)</cite> uses LLM judges to evaluate RAG systems across three dimensions: Contextual Relevance, Answer Faithfulness, and Answer Relevance. Similarly, Asai et al.  <cite class="ltx_cite ltx_citemacro_citep">(Asai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib4" title="">2023</a>)</cite> proposed SELF-RAG, which employs reflective token to determine whether retrieval is required and to self-assess the quality of generated outputs. Lei et al. <cite class="ltx_cite ltx_citemacro_citep">(Lei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib125" title="">2024</a>)</cite> introduced LLMs to evaluate the quality of generated explanations, demonstrating the effectiveness of LLMs in understanding and generating explanations for recommendation tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>Model Evaluation</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">Model evaluation typically begins with assessing individual responses and then extends to analyzing overall capabilities. This wider perspective aims to analyze the model’s performance across various tasks or domains, such as coding ability, instruction-following proficiency, reasoning, and other specialized skills relevant to its intended applications.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p2">
<p class="ltx_p" id="S3.SS1.SSS2.p2.1">A common and straightforward approach is to represent model performance using average performance on static benchmarks <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib139" title="">2024</a>; Tan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib214" title="">2024</a>)</cite>. LLM judges assess the model’s performance using a set of carefully designed metrics, which results in a performance ranking. This method is widely adopted due to its simplicity and comparability. For example, task sets can be designed to evaluate the model’s knowledge coverage, reasoning depth, and language generation quality <cite class="ltx_cite ltx_citemacro_citep">(Son et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib203" title="">2024b</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib149" title="">2024b</a>; Lambert et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib117" title="">2024</a>)</cite>, or real-world scenarios can be simulated to assess the model’s ability to handle complex situations <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib145" title="">2023e</a>; Trivedi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib220" title="">2024b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS2.p3">
<p class="ltx_p" id="S3.SS1.SSS2.p3.1">As the demand for evaluation increases, the evaluation process has gradually shifted from traditional static testing to more dynamic, interactive assessments <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib11" title="">2024</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib287" title="">2024b</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib274" title="">2024</a>)</cite>.
LLMs-as-judges has pioneered this approach, similar to Chatbot Arena <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>)</cite>, a crowdsourced platform that collects anonymous votes on LLM performance and ranks them using Elo scores.
Auto-Arena <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib287" title="">2024b</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib154" title="">2024</a>)</cite> and LMExam <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib11" title="">2024</a>)</cite> assess model capabilities by using LLMs as both question setters and evaluators. These frameworks innovatively combine diverse question generation, multi-turn question-answering evaluation, and a decentralized model-to-model evaluation mechanism, providing more detailed and granular performance assessments.
Additionally, KIEval <cite class="ltx_cite ltx_citemacro_citep">(Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib274" title="">2024</a>)</cite> introduces an LLM-driven “interactor” role, which evaluates the knowledge mastery and generation abilities of LLMs through dynamic multi-turn conversations. These dynamic evaluation methods effectively address data leakage and evaluation bias issues common in traditional benchmark tests.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Model Enhancement</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In addition to Performance Evaluation, LLMs-as-judges is also widely used for Model Enhancement. From training to inference, LLMs-as-judges plays a key role in improving model performance. Its application in model enhancement offers a novel optimization pathway for artificial intelligence, fostering the refinement and personalization of intelligent systems across a broader spectrum of real-world applications.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span>Reward Modeling During Training</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">A primary application of LLMs-as-judges is in reward modeling during training, particularly in reinforcement learning with feedback <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib275" title="">2024</a>; Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib75" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib240" title="">2024d</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib258" title="">2024a</a>; Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib22" title="">2024b</a>)</cite>. LLM judges assign scores to model outputs by evaluating them against human-defined criteria, guiding optimization toward desired behaviors. This ensures alignment with human values, improving the quality and relevance of the generated outputs and improving the effectiveness of LLMs in real-world tasks.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1">A series of works, such as SRLMs <cite class="ltx_cite ltx_citemacro_citep">(Yuan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib275" title="">2024</a>)</cite>, OAIF <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib75" title="">2024</a>)</cite>, and RLAIF <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib122" title="">2023</a>)</cite>, have enabled LLMs to become their own reward models. This overcomes the traditional RLHF dependency on fixed reward models, allowing the model to iteratively reward and self-optimize, fostering self-evolution through continuous self-assessment.
RELC <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib22" title="">2024b</a>)</cite> tackles the challenge of sparse rewards in traditional RL by introducing a Critic Language Model (Critic LM) to evaluate intermediate generation steps. This dense feedback at each step helps mitigate reward sparsity, offering more detailed guidance to the model during training.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1">However, using the same LLM for both policy generation and reward modeling can pose challenges in ensuring the accuracy of the rewards. This dual role setup may lead to accumulated biases and preference data noise, which can undermine the training effectiveness. To address this issue, CREAM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib240" title="">2024d</a>)</cite> introduces cross-iteration consistency constraints to regulate the training process and prevent the model from learning unreliable preference data. This significantly enhances reward consistency and alignment performance. In addition, CGPO <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib258" title="">2024a</a>)</cite> groups tasks by category (such as dialogue, mathematical reasoning, safety, etc.) and uses “Mixed Judges” to assign a specific reward model to each task group. This ensures that the reward signals are closely aligned with the task objectives, thereby preventing conflicts between different goals.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span>Acting as Verifier During Inference</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">During inference, LLM judges serve as verifier, responsible for selecting the optimal response from multiple candidates <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib266" title="">2024</a>; Besta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib16" title="">2024</a>; Lightman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib138" title="">2023</a>; Musolesi, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib161" title="">2024</a>; Besta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib16" title="">2024</a>)</cite>. By comparing the outputs based on various metrics, such as factual accuracy and reasoning consistency, they are able to identify the best fit for the given task or context, thereby optimizing the inference process or improving the quality of the generated results.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1">One of the simplest applications is Best-of-N sampling <cite class="ltx_cite ltx_citemacro_citep">(Jinnai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib103" title="">2024</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib210" title="">2024</a>)</cite>, where the model is sampled N times, and the best result is selected to improve model performance. Similarly, Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib236" title="">2022</a>)</cite> introduced a promising sampling method called self-consistency, where n samples are drawn from the judge model, and the average score is output. These sampling methods enhance inference stability by selecting the best result from multiple evaluations.
Further optimization strategies include the Tree of Thoughts (ToT) <cite class="ltx_cite ltx_citemacro_citep">(Yao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib266" title="">2024</a>)</cite> method, which models the problem-solving process as a tree structure. This allows the model to explore multiple solution paths and optimize path selection through self-assessment mechanisms.
The Graph of Thoughts (GoT) <cite class="ltx_cite ltx_citemacro_citep">(Besta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib16" title="">2024</a>)</cite> method extends this concept by introducing directed graphs, where the non-linear interactions between nodes improve the efficiency and precision of multi-step reasoning. In both methods, LLM judges play a crucial role in guiding the model to select the most promising paths, thereby enhancing the quality and accuracy of reasoning.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1">Similarly, Lightman et al. <cite class="ltx_cite ltx_citemacro_citep">(Lightman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib138" title="">2023</a>)</cite> discuss how step-by-step validation can enhance the performance of LLMs in multi-step reasoning tasks, particularly in the domain of mathematics. SE-GBS <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib251" title="">2024a</a>)</cite> integrates self-assessment into the multi-step reasoning decoding process, generating scores that reflect logical correctness and further ensuring the accuracy and consistency of the reasoning chain. The REPS <cite class="ltx_cite ltx_citemacro_citep">(Kawabata and Sugawara, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib106" title="">2024</a>)</cite> improves the accuracy and reliability of reasoning validation models by comparing reasoning paths pairwise, verifying their logical consistency and factual basis.
Also, Musolesi et al.  <cite class="ltx_cite ltx_citemacro_citep">(Musolesi, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib161" title="">2024</a>)</cite> proposed Creative Beam Search, with the LLM acting as a judge to simulate the human creative selection process, thereby enhancing the diversity and creativity of the generated results.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span>Feedback for Refinement</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">After receiving the initial response, LLM judges provide actionable feedback to iteratively improve output quality. By analyzing the response based on specific task criteria, such as accuracy, coherence, or creativity, the LLM can identify weaknesses in the output and offer suggestions for improvement. This iterative refinement process plays a crucial role in applications that require adaptability <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib156" title="">2024</a>; Paul et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib177" title="">2023</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib33" title="">2023a</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib262" title="">2023c</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib94" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1">SELF-REFINE <cite class="ltx_cite ltx_citemacro_citep">(Madaan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib156" title="">2024</a>)</cite> enables LLMs to iteratively improve output quality through feedback generated by the model itself, without requiring additional training or supervision data. On the other hand, SELF-DEBUGGING <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib33" title="">2023a</a>)</cite> demonstrates a practical application of self-correction in code generation by identifying and rectifying errors through self-explanation and feedback. This approach has significantly enhanced the performance of LLMs across various code generation tasks.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1">In addition to refining response quality, LLMs judges are also widely used to enhance reasoning abilities. For example, REFINER <cite class="ltx_cite ltx_citemacro_citep">(Paul et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib177" title="">2023</a>)</cite> optimizes the reasoning performance of LLMs through interactions between a generator model and a critic model. In this framework, the generator model is responsible for producing intermediate reasoning steps, while the critic model analyzes these steps and provides detailed feedback, such as identifying calculation errors or logical inconsistencies. Xu et al. <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib262" title="">2023c</a>)</cite> propose a multi-agent collaboration strategy to enhance the reasoning abilities of LLMs by simulating the academic peer review process. The framework is divided into three stages: generation, review, and revision. Agents provide feedback and attach confidence scores to refine the initial answers, with the final result determined through majority voting.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p4">
<p class="ltx_p" id="S3.SS2.SSS3.p4.1">While the feedback and correction mechanisms of LLMs judges are continually evolving, the limitations of self-feedback in improving quality should not be overlooked. Research on Self-Correct <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib94" title="">2023</a>; Tyen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib224" title="">2023</a>)</cite> shows that, the intrinsic self-correction capabilities of LLMs often fall short of effectively improving reasoning quality.
Valmeekam et al. <cite class="ltx_cite ltx_citemacro_citep">(Valmeekam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib225" title="">2023</a>)</cite> also raise concerns about the effectiveness of LLMs as self-validation tools in the absence of reliable external validators. Future research can focus on improving the accuracy of feedback provided by these LLM judges and incorporating external validation mechanisms to optimize their performance in complex reasoning tasks.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Data Construction</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Data collection is a crucial stage in the development of machine learning systems, especially those driven by the rapid advancements in deep learning. The quality of the data directly determines the performance of the trained models.
The LLMs-as-judges has significantly transformed the landscape of data collection, substantially reducing reliance on human effort.
In this section, we will explore the pivotal role of LLMs-as-judges in data collection from two key perspectives: <span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.1">Data Annotation</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS3.SSS1" title="3.3.1. Data Annotation ‣ 3.3. Data Construction ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.3.1</span></a>) and <span class="ltx_text ltx_font_bold" id="S3.SS3.p1.1.2">Data Synthesize</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S3.SS3.SSS2" title="3.3.2. Data Synthesize ‣ 3.3. Data Construction ‣ 3. Functionality ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">3.3.2</span></a>).</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1. </span>Data Annotation</h4>
<div class="ltx_para" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">Data Annotation involves leveraging LLM judges to label large, unlabeled datasets efficiently <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib86" title="">2024a</a>; Gilardi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib72" title="">2023</a>; Törnberg, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib218" title="">2023</a>; Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib80" title="">2024</a>)</cite>. By utilizing the advanced natural language understanding and reasoning capabilities of LLMs, the annotation process can be automated to a significant extent, enabling the generation of high-quality labels with reduced human intervention.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1">LLMs have demonstrated remarkable potential in text annotation tasks, consistently outperforming traditional methods and human annotators in various settings.
He et al. <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib86" title="">2024a</a>)</cite> evaluated the performance of GPT-4 in crowdsourced data annotation workflows, particularly in text annotation tasks. Their comparative study revealed that, even with best practices, the highest accuracy achievable by MTurk workers was 81.5%, whereas GPT-4 achieved an accuracy of 83.6%.
Similarly, Gilardi et al. <cite class="ltx_cite ltx_citemacro_citep">(Gilardi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib72" title="">2023</a>)</cite> analyzed 6,183 tweets and news articles, demonstrating that ChatGPT outperformed crowdsourced workers in tasks such as stance detection, topic detection, and framing.
Törnberg et al. <cite class="ltx_cite ltx_citemacro_citep">(Törnberg, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib218" title="">2023</a>)</cite> further investigated the classification of Twitter users’ political leanings based on their tweet content. Their findings revealed that ChatGPT-4 not only surpassed human classifiers in accuracy and reliability but also exhibited bias levels that were comparable to or lower than those of human classifiers.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p3">
<p class="ltx_p" id="S3.SS3.SSS1.p3.1">As technology advances, more and more research is exploring their application in multimodal data annotation. For example, the FullAnno <cite class="ltx_cite ltx_citemacro_citep">(Hao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib80" title="">2024</a>)</cite> uses the GPT-4V model to generate image annotations, significantly improving the quality of image descriptions through a multi-stage annotation process.
Furthermore, Latif et al. <cite class="ltx_cite ltx_citemacro_citep">(Latif et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib118" title="">2023</a>)</cite> explored the application of LLMs in speech emotion annotation, demonstrating that, with data augmentation, LLM-annotated samples can significantly enhance the performance of speech emotion recognition models. By integrating text, audio features, and gender information, the effectiveness of LLM-based annotations was further improved, highlighting their potential in advancing multimodal annotation tasks.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS1.p4">
<p class="ltx_p" id="S3.SS3.SSS1.p4.1">As LLMs perform excellently in annotation tasks, researchers are actively exploring methods to further improve annotation quality and address potential challenges. For example, AnnoLLM <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib84" title="">2023a</a>)</cite> introducedthe “explain-then-annotate” method, which enhances both the accuracy and transparency of annotations by prompting the LLM to justify its label assignments.
Additionally, the LLMAAA <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib283" title="">2023a</a>)</cite> framework incorporates an active learning strategy to efficiently select high-information samples for annotation, thereby mitigating the effects of noisy labels and reducing the reliance on costly human annotation. These approach not only enhance the performance of task-specific models but also offer new perspectives on the efficient application of LLMs in annotation workflows.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2. </span>Data Synthesize</h4>
<div class="ltx_para" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">The goal of Data Synthesis is to create entirely new data, either from scratch or based on seed data, while ensuring it is similar in distribution to real data. Data Synthesis enables the generation of diverse data samples, enhancing a model’s generalization ability to unseen examples while reducing reliance on sensitive real-world data <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib269" title="">2023a</a>; Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib53" title="">2024a</a>; Arif et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib3" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib236" title="">2022</a>; Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib109" title="">2024a</a>; Mendonça et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib158" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">In recent years, advancements in LLMs have led to significant improvements in both the quality and efficiency of data synthesis methods. In this domain, methods like SELFEE <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib269" title="">2023a</a>)</cite> and SynPO <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib53" title="">2024a</a>)</cite> have effectively enhanced the alignment capabilities of LLMs by leveraging small amounts of labeled data and iteratively generating preference-aligned data. Arif et al. <cite class="ltx_cite ltx_citemacro_citep">(Arif et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib3" title="">2024</a>)</cite> also introduce a multi-agent workflow for generating optimized preference datasets.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p3">
<p class="ltx_p" id="S3.SS3.SSS2.p3.1">SELF-INSTRUCT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib236" title="">2022</a>)</cite> and Evol-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib255" title="">2023d</a>; Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib279" title="">2024</a>)</cite> represent innovative approaches to improving model alignment and performance through self-generated instruction data. SELF-INSTRUCT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib236" title="">2022</a>)</cite> requires minimal human annotation, instead relying on self-generated instruction data to align pre-trained models. Evol-Instruct <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib255" title="">2023d</a>; Zeng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib279" title="">2024</a>)</cite> further enhances LLM performance by automatically generating instruction data, significantly boosting model capabilities.</p>
</div>
<div class="ltx_para" id="S3.SS3.SSS2.p4">
<p class="ltx_p" id="S3.SS3.SSS2.p4.1">STaR <cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib278" title="">2024</a>)</cite> and ReSTEM <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib200" title="">2023</a>)</cite> are research efforts aimed at enhancing reasoning capabilities through synthetic data. STaR <cite class="ltx_cite ltx_citemacro_citep">(Zelikman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib278" title="">2024</a>)</cite> employs a self-guided iterative process to improve model performance on complex reasoning tasks, offering an effective solution for tackling increasingly sophisticated reasoning challenges in the future. ReSTEM <cite class="ltx_cite ltx_citemacro_citep">(Singh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib200" title="">2023</a>)</cite>, on the other hand, utilizes a self-training approach based on the expectation-maximization framework to enhance the problem-solving capabilities of large language models, particularly in areas such as solving mathematical problems and generating code.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Methodology</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The use of LLM judges requires careful methodological considerations to ensure the accuracy and consistency of judgments.
Researchers have developed various approaches according to the complexity and specific requirements of different judgment tasks, each offering unique advantages.
In this section, we categorize these methodologies into three broad approaches: <span class="ltx_text ltx_font_bold" id="S4.p1.1.1">Single-LLM System</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS1" title="4.1. Single-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.1</span></a>): evaluation by a single-LLM, <span class="ltx_text ltx_font_bold" id="S4.p1.1.2">Multi-LLM System</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS2" title="4.2. Multi-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.2</span></a>): evaluation by cooperation among multi-LLMs, and <span class="ltx_text ltx_font_bold" id="S4.p1.1.3">Human-AI Collaboration</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS3" title="4.3. Human-AI Collaboration System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.3</span></a>): evaluation by cooperation of LLMs and Human.
Figure  <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.F5" title="Figure 5 ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">5</span></a> presents an overview of methodology.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="467" id="S4.F5.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Overview of the Methodology of LLMs-as-judges.</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Single-LLM System</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Single-LLM System relies on a single model to perform judgment tasks, with its effectiveness largely determined by the LLM’s capabilities and the strategies used to process input data. This approach can generally be divided into three fundamental components: <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.1">Prompt Engineering</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS1.SSS1" title="4.1.1. Prompt-based ‣ 4.1. Single-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.1.1</span></a>), <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.2">Tuning</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS1.SSS2" title="4.1.2. Tuning-based ‣ 4.1. Single-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>), and <span class="ltx_text ltx_font_bold" id="S4.SS1.p1.1.3">Post-processing</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S4.SS1.SSS3" title="4.1.3. Post-processing ‣ 4.1. Single-LLM System ‣ 4. Methodology ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">4.1.3</span></a>) of model outputs.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Prompt-based</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">Prompt engineering <cite class="ltx_cite ltx_citemacro_citep">(Sahoo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib190" title="">2024</a>)</cite> involves crafting clear and structured input prompts tailored to elicit accurate and contextually appropriate responses from LLM judges. This approach is crucial for ensuring that LLMs grasp the complexities of specific tasks and provide relevant, consistent, and goal-aligned judgments.
In many cases, well-designed prompts significantly reduce the need for extensive model training.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.1.1">In-Context Learning.</span> In-Context Learning (ICL) is a distinctive capability of LLMs that allows them to dynamically adapt to evaluation tasks using carefully curated examples or explanations within the prompt <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib54" title="">2022</a>)</cite>. Several recent methods have demonstrated the power of ICL in LLM-as-judges, showcasing how it enhances the flexibility and performance of LLMs in diverse settings. For example, GPTScore <cite class="ltx_cite ltx_citemacro_citep">(Fu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib69" title="">2023</a>)</cite> leverages the few-shot learning capability of generative pre-trained models to evaluate generated text. By using relevant examples to customize prompts, it provides a flexible, training-free approach to assess multiple aspects of text quality.
Similarly, LLM-EVAL <cite class="ltx_cite ltx_citemacro_citep">(Lin and Chen, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib142" title="">2023</a>)</cite> incorporates carefully crafted examples into prompts, proposing a unified, multi-dimensional automatic evaluation method for open-domain dialogue.
Another notable example is TALEC <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib281" title="">2024c</a>)</cite>, a model-based evaluation method that leverages in-context learning to enable users to set custom evaluation criteria for LLMs in specific domains. Through careful prompt engineering, users can iteratively adjust the examples to refine the evaluation process as needed.
In addition, Jain et al. <cite class="ltx_cite ltx_citemacro_citep">(Jain et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib95" title="">2023</a>)</cite> proposed the In-Context Learning-based Evaluator (ICE) for multi-dimensional text evaluation. ICE leverages LLMs and a small number of in-context examples to evaluate generated text summaries, achieving competitive results.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1">While ICL can enable effective evaluation, it is not without challenges. One major issue is that the model’s responses may be influenced by the selection of prompt examples, potentially leading to bias <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib291" title="">2021</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib297" title="">2023a</a>; Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib79" title="">2022</a>; Fei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib63" title="">2023</a>)</cite>.
To address this issue, Hasanbeig et al. proposed ALLURE <cite class="ltx_cite ltx_citemacro_citep">(Hasanbeig et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib82" title="">2023</a>)</cite>, a comprehensive protocol designed to mitigate bias in ICL for LLMs during text evaluation. ALLURE <cite class="ltx_cite ltx_citemacro_citep">(Hasanbeig et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib82" title="">2023</a>)</cite> improves evaluator accuracy by iteratively incorporating discrepancies between its assessments and annotated data into the learning context. Moreover, after uncovering the existence of symbol bias within LLM evaluators when using ICL, Song et al. <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib205" title="">2024b</a>)</cite> proposed two effective mitigation strategy prompt templates, Many-Shot with Reference (MSwR) and Many-Shot without Reference (MSoR), to bolster the reliability and precision of LLM-based assessments.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p4.1.1">Step-by-step.</span>
Step-by-step involves breaking down complex evaluation tasks into fine-grained components, leveraging the reasoning capabilities of LLMs to simplify the evaluation process. The most straightforward example of which is perhaps Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib243" title="">2022</a>; Kotonya et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib114" title="">2023</a>)</cite>. Building on that, frameworks like G-EVAL <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib146" title="">2023a</a>)</cite> have been proposed to assess the quality of NLG outputs. G-EVAL <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib146" title="">2023a</a>)</cite> combines CoT with a form-filling paradigm, allowing the LLM to assess outputs in a structured manner.
Similarly, ICE-Score <cite class="ltx_cite ltx_citemacro_citep">(Zhuo, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib305" title="">2023</a>)</cite> introduces a step-by-step framework for evaluating code, in which the LLM is instructed with task definitions, evaluation criteria, and detailed evaluation steps. By breaking the task down into clear steps, ICE-Score <cite class="ltx_cite ltx_citemacro_citep">(Zhuo, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib305" title="">2023</a>)</cite> improves the quality and consistency of code evaluation. Also, ProtocoLLM <cite class="ltx_cite ltx_citemacro_citep">(Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib272" title="">2024</a>)</cite> employs a similar step-by-step approach to evaluate the specialized capabilities of LLMs in generating scientific protocols.
Portia <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib135" title="">2023d</a>)</cite> achieves better evaluation results in a lightweight yet effective manner. It divides the answer into multiple parts, aligns similar content between candidate answers, and then merges them back into a single prompt for evaluation by the LLM.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p5">
<p class="ltx_p" id="S4.SS1.SSS1.p5.1">Some studies break down evaluations into two steps: “explanation-rating.” This approach suggests that providing an explanation enhances the reliability of the rating. Chiang et al. <cite class="ltx_cite ltx_citemacro_citep">(Chiang and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib37" title="">2023</a>)</cite> offer empirical guidelines to improve the quality of LLM evaluations, demonstrating that combining rating with explanation (rate-explain) or explanation with rating (explain-rate) leads to higher correlations with human ratings.
Another effective strategy is to decompose complex evaluation standards into specific, discrete criteria, allowing the LLM to assess each aspect independently. FineSurE <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib204" title="">2024a</a>)</cite> is an advanced example of this method, offering a framework for the fine-grained evaluation of text summarization quality. It breaks down the evaluation into multiple dimensions, such as faithfulness, completeness, and conciseness. Through detailed analysis, including fact-checking and key fact alignment, FineSurE <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib204" title="">2024a</a>)</cite> outperforms traditional methods in terms of evaluation accuracy.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p6">
<p class="ltx_p" id="S4.SS1.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p6.1.1">Definition Augmentation.</span>
The Enhanced Definition approach involves refining prompts to inject improved evaluation criteria, establish assessment principles, or incorporate external knowledge into the LLM judge’s decision-making process.
Some studies focus on enriching and clarifying the prompts to ensure that the evaluation criteria are both comprehensive and well-defined.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p7">
<p class="ltx_p" id="S4.SS1.SSS1.p7.1">For example, Liu et al. propose AUTOCALIBRATE <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib147" title="">2023d</a>)</cite>, a multi-stage, gradient-free approach. This method involves the drafting, revision, and application of calibrated criteria, and it automatically calibrates and aligns an LLM-based evaluator to match human preferences for NLG quality assessment.
Furthermore, SALC <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib77" title="">2024</a>)</cite> enables LLMs to autonomously generate context-aware evaluation criteria for self-assessment, overcoming the limitations of static, human-defined metrics.
On the other hand, the LLM-as-a-Personalized-Judge approach <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib55" title="">2024b</a>)</cite> introduces a novel perspective by incorporating diverse evaluative roles and principles. This allows LLMs to adapt to complex, varied evaluation scenarios, resulting in more nuanced and context-sensitive assessments.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p8">
<p class="ltx_p" id="S4.SS1.SSS1.p8.1">Another key aspect of Definition Augmentation is the retrieval of external knowledge, which helps reduce hallucinations and provides more factual support. For instance, BiasAlert <cite class="ltx_cite ltx_citemacro_citep">(Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib62" title="">2024</a>)</cite>, a tool designed to detect social bias in LLM-generated open-text outputs. It integrates external human knowledge with the LLM judge’s inherent reasoning capabilities to reliably identify and mitigate bias, outperforming GPT4-as-A-Judge across various scenarios.
Moreover, Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib34" title="">2024c</a>)</cite> found that within retrieval-augmented generation (RAG) frameworks, LLM judges do not exhibit a significant self-preference effect during evaluation.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p9">
<p class="ltx_p" id="S4.SS1.SSS1.p9.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p9.1.1">Multi-turn Optimization.</span>
Multi-turn optimization involves iterative interactions between the evaluator and the evaluated entity, refining evaluation results through diverse forms of feedback, thus fostering deeper analysis and a progressive improvement in evaluation quality <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib296" title="">2024e</a>)</cite>.
Unlike traditional methods that rely on predefined criteria, Xu et al. proposed ACTIVE-CRITIC <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib257" title="">2024b</a>)</cite>, enabling LLMs to infer evaluation criteria from data and dynamically optimize prompts through multiple rounds of interaction. Moreover,
Some studies <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib287" title="">2024b</a>; Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib154" title="">2024</a>; Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib11" title="">2024</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib274" title="">2024</a>)</cite> leverage LLMs as question designers to engage in dynamic interactions with the evaluated entities, adjusting the questions and task design in real time. This allows for flexible modification of the evaluation content based on the performance of the evaluated entity, thereby enabling more comprehensive assessments.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Tuning-based</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">Tuning involves training a pre-existing LLM on a specialized dataset to adapt it to specific judgment tasks. It’s especially useful when the judgment domain involves highly specialized knowledge or nuanced decision-making <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib93" title="">2024b</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p2.1.1">Score-based Tuning.</span> Score-based tuning involves using data with scores to train models and enhance their ability to predict judgment scores based on specific evaluation criteria <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib29" title="">2023c</a>; Deshwal and Chawla, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib48" title="">2024</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib230" title="">2023e</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<p class="ltx_p" id="S4.SS1.SSS2.p3.1">Many studies have explored the enhancement of LLM-as-judges by fine-tuning them on human-labeled datasets.
For instance, PHUDGE <cite class="ltx_cite ltx_citemacro_citep">(Deshwal and Chawla, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib48" title="">2024</a>)</cite>, fine-tuned from the Phi-3 model, achieves state-of-the-art performance in terms of latency and throughput when automatically evaluating the quality of outputs from LLMs. This fine-tuning process equips the model with the necessary judgment skills, enabling it to assess various types of content in a structured and accurate manner.
Additionally, ECT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib230" title="">2023e</a>)</cite> introduces a novel method for transferring scoring capabilities from LLMs to lighter models. This allows the lighter models to function as effective reward models for sequence generation tasks, enhancing sequence generation models through reinforcement learning and reranking approaches.
AttrScore <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib277" title="">2023b</a>)</cite> is another framework for evaluating attribution and identifying specific types of attribution errors, using a curated test set from a generative search engine and simulated examples from existing benchmarks.
The above research highlights that LLMs can better align their decision-making process with humans through fine-tuning with human-constructed datasets.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p4">
<p class="ltx_p" id="S4.SS1.SSS2.p4.1">In addition to human-labeled data, some studies have also attempted to fine-tune models using synthetic datasets like SorryBench <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib250" title="">2024b</a>)</cite> generated for evaluation tasks. These datasets are often created through rule-based methods or by generating artificial evaluation examples, which also give rise to some metrics like TIGERScore <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib100" title="">2023b</a>)</cite>.
SELF-J <cite class="ltx_cite ltx_citemacro_citep">(Ye and Ng, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib267" title="">2024</a>)</cite> is a self-training framework for developing judge models to evaluate LLMs’ adherence to human instructions without human-annotated quality scores. SELF-J <cite class="ltx_cite ltx_citemacro_citep">(Ye and Ng, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib267" title="">2024</a>)</cite> proposes selective instruction following, allowing systems to decline low-quality instructions.
FENCE <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib253" title="">2024d</a>)</cite> is another factuality evaluator designed to provide claim-level feedback to language model generators. It details a data augmentation approach that enriches public datasets with textual critiques and diverse source documents from various tools, thereby enhancing factuality without introducing lesser-known facts.
Utilizing synthetic training data to fine-tune lightweight language model judges and employing prediction-powered inference (PPI) for statistical confidence to mitigate potential prediction errors, ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib189" title="">2023</a>)</cite> can automatically assess RAG systems.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p5">
<p class="ltx_p" id="S4.SS1.SSS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p5.1.1">Preference-based Learning.</span>
Preference-based learning focuses on training LLMs to make inferences and learn based on preferences, enabling the development of more adaptive and customizable evaluation capabilities.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p6">
<p class="ltx_p" id="S4.SS1.SSS2.p6.1">Initially, researchers leverage these data in conjunction with advanced techniques like Direct Preference Optimization (DPO) <cite class="ltx_cite ltx_citemacro_citep">(Rafailov et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib182" title="">2024</a>)</cite> to train LLMs for more nuanced evaluative capabilities. In this method, the model is trained to predict which of two outputs is preferred according to human-like values, rather than learning a scalar reward signal. Such self-improving approach is well reflected in Meta-Rewarding <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib246" title="">2024b</a>)</cite>.
Con-J <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib271" title="">2024a</a>)</cite> trains a generative judge by using the DPO loss on contrastive judgments and the SFT loss on positive judgments to align LLMs with human values.
In terms of evaluating other LLMs effectively in open-ended scenarios, JudgeLM <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib302" title="">2023</a>)</cite> addresses key biases in the fine-tuning process with a high-quality preference dataset.
Another typical method is PandaLM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib237" title="">2023d</a>)</cite>, which is trained on a reliable human-annotated preference dataset, focusing extends beyond just the objective correctness of responses, and addresses vital subjective factors.
Moreover, Self-Taught <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib232" title="">2024e</a>)</cite> is another approach to train LLMs as effective evaluators without relying on human-annotated preference judgments, using synthetic training data only. Through an iterative self-improvement scheme, LLM judges are able to produce reasoning traces and final judgments.
Not quite the same, FedEval-LLM <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib85" title="">2024b</a>)</cite> fine-tunes many personalized LLMs without relying on labeled datasets to provide domain-specific evaluation, mitigating biases associated with single referees. It is designed to assess the performance of LLMs on downstream tasks, at the same time, ensuring privacy preservation.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p7">
<p class="ltx_p" id="S4.SS1.SSS2.p7.1">As research has progressed, newer methods have emerged that combine both score-based and preference-based data to refine model evaluation capabilities, not to mention some novel metrics like INSTRUCTSCORE <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib259" title="">2023e</a>)</cite>.
FLAMe <cite class="ltx_cite ltx_citemacro_citep">(Vu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib227" title="">2024</a>)</cite> is an example of such an approach. It’s a family of Foundational Large Autorater Models which significantly improves generalization to a wide variety of held-out tasks using both pointwise and pairwise methods during training.
As generative judge model, AUTO-J <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib131" title="">2023c</a>)</cite> addresses challenges in generality, flexibility, and interpretability by training on a diverse dataset containing scoring and preference.
To critique and refine the outputs of large language models, Shepherd <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib233" title="">2023c</a>)</cite> leverages a high-quality feedback dataset to identify errors and suggest improvements across various domains.
In the domain of NLG, X-EVAL <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib143" title="">2023c</a>)</cite> consists of a vanilla instruction tuning stage and an enhanced instruction tuning stage that exploits connections between fine-grained evaluation aspects.
Notably, Themis <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib89" title="">2024a</a>)</cite> also achieved outstanding results acting as a reference-free NLG evaluation language model designed for flexibility and interpretability.
Similarly, CritiqueLLM <cite class="ltx_cite ltx_citemacro_citep">(Ke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib107" title="">2024</a>)</cite> provides effective and explainable evaluations of LLM outputs, and uses a dialogue-based prompting method to generate high-quality referenced and reference-free evaluation data.
Self-Rationalization <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib221" title="">2024a</a>)</cite> enhances LLM performance by iteratively fine-tuning the judge via DPO, which allows LLMs to learn from their own reasoning.
Based on pointwise and pairwise dataset, CompassJudger-1 <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib21" title="">2024a</a>)</cite> acts as an open-source, versatile LLM for efficient and accurate evaluation of other LLMs.
Likewise, Zhou et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib295" title="">2024c</a>)</cite> introduces a systematic framework for bias reduction, employing calibration for closed-source models and contrastive training for open-source models.
Apart from that, HALU-J <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib228" title="">2024b</a>)</cite> is designed to enhance hallucination detection in LLMs by selecting pertinent evidence and providing detailed critiques.
PROMETHEUS <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib110" title="">2023</a>)</cite> and PROMETHEUS 2 <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib111" title="">2024b</a>)</cite> are open-source LLMs specialized for fine-grained evaluation that can generalize to diverse, real-world scoring rubrics beyond a single-dimensional preference, supporting both direct assessment and pairwise ranking, and can evaluate based on custom criteria. What’s more, the following PROMETHEUS-VISION <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib123" title="">2024a</a>)</cite> fills the gap in the visual field.
As for various multimodal tasks, LLaVA-Critic <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib254" title="">2024</a>)</cite> demonstrates its effectiveness in providing reliable evaluation scores and generating reward signals for preference learning, highlighting the potential of open-source LMMs in self-critique and evaluation.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>An Overview of Fine-Tuning Methods in Single-LLM Evaluation (Sorted in ascending alphabetical order).</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T1.1" style="width:558.0pt;height:413.9pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-219.2pt,162.4pt) scale(0.56,0.56) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T1.1.1.1.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="3" id="S4.T1.1.1.1.1.2">Data Construction</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="2" id="S4.T1.1.1.1.1.3">Tuning Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.1.1.4" rowspan="2"><span class="ltx_text" id="S4.T1.1.1.1.1.4.1">Base LLM</span></th>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.1">Annotator</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.2">Domain</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.3">Scale</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.4">Evaluation Type</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T1.1.1.2.2.5">Technique</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.1">ARES <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib189" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.2">Human &amp; LLM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.3">RAG System</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.5">Pairwise</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.6">PPI</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.1.3.1.7">DeBERTa-v3-Large</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.4.2" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.1"><span class="ltx_text" id="S4.T1.1.1.4.2.1.1" style="background-color:#E6E6E6;">AttrScore <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib277" title="">2023b</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.2"><span class="ltx_text" id="S4.T1.1.1.4.2.2.1" style="background-color:#E6E6E6;">Human</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.3"><span class="ltx_text" id="S4.T1.1.1.4.2.3.1" style="background-color:#E6E6E6;">Various</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.4"><span class="ltx_text" id="S4.T1.1.1.4.2.4.1" style="background-color:#E6E6E6;">63.8K</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.5"><span class="ltx_text" id="S4.T1.1.1.4.2.5.1" style="background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.6"><span class="ltx_text" id="S4.T1.1.1.4.2.6.1" style="background-color:#E6E6E6;">SFT</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.4.2.7"><span class="ltx_text" id="S4.T1.1.1.4.2.7.1" style="background-color:#E6E6E6;">Multiple LLMs</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.5.3">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.1">AUTO-J <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib131" title="">2023c</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.2">Human &amp; GPT-4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.3">Various</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.4">4396</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.5">Pointwise &amp; Pairwise</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.6">SFT</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.5.3.7">Llama2-13B-Chat</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.6.4">
<td class="ltx_td" id="S4.T1.1.1.6.4.1"></td>
<td class="ltx_td" id="S4.T1.1.1.6.4.2"></td>
<td class="ltx_td" id="S4.T1.1.1.6.4.3"></td>
<td class="ltx_td" id="S4.T1.1.1.6.4.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.6.4.5"><span class="ltx_text" id="S4.T1.1.1.6.4.5.1" style="background-color:#E6E6E6;">Pointwise, Pairwise,</span></td>
<td class="ltx_td" id="S4.T1.1.1.6.4.6"></td>
<td class="ltx_td" id="S4.T1.1.1.6.4.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.7.5" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.1"><span class="ltx_text" id="S4.T1.1.1.7.5.1.1" style="background-color:#E6E6E6;">CompassJudger-1 <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib21" title="">2024a</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.2"><span class="ltx_text" id="S4.T1.1.1.7.5.2.1" style="background-color:#E6E6E6;">Human &amp; LLM</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.3"><span class="ltx_text" id="S4.T1.1.1.7.5.3.1" style="background-color:#E6E6E6;">Various</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.4"><span class="ltx_text" id="S4.T1.1.1.7.5.4.1" style="background-color:#E6E6E6;">900K</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.5"><span class="ltx_text" id="S4.T1.1.1.7.5.5.1" style="background-color:#E6E6E6;">&amp; Generative</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.6"><span class="ltx_text" id="S4.T1.1.1.7.5.6.1" style="background-color:#E6E6E6;">SFT</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.7.5.7"><span class="ltx_text" id="S4.T1.1.1.7.5.7.1" style="background-color:#E6E6E6;">Qwen2.5 Series</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.8.6">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.1">Con-J <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib271" title="">2024a</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.2">Human &amp; ChatGPT</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.3">Creation, Math, &amp; Code</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.4">220K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.5">Pairwise</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.6">SFT &amp; DPO</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.8.6.7">Qwen2-7B-Instruct</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.9.7" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.7.1"><span class="ltx_text" id="S4.T1.1.1.9.7.1.1" style="background-color:#E6E6E6;">CritiqueLLM <cite class="ltx_cite ltx_citemacro_citep">(Ke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib107" title="">2024</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.7.2"><span class="ltx_text" id="S4.T1.1.1.9.7.2.1" style="background-color:#E6E6E6;">Human &amp; GPT-4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.7.3"><span class="ltx_text" id="S4.T1.1.1.9.7.3.1" style="background-color:#E6E6E6;">Various</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.7.4"><span class="ltx_text" id="S4.T1.1.1.9.7.4.1" style="background-color:#E6E6E6;">7722</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.7.5"><span class="ltx_text" id="S4.T1.1.1.9.7.5.1" style="background-color:#E6E6E6;">Pointwise &amp; Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.7.6"><span class="ltx_text" id="S4.T1.1.1.9.7.6.1" style="background-color:#E6E6E6;">SFT</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.9.7.7"><span class="ltx_text" id="S4.T1.1.1.9.7.7.1" style="background-color:#E6E6E6;">ChatGLM3-6B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.10.8">
<td class="ltx_td" id="S4.T1.1.1.10.8.1"></td>
<td class="ltx_td" id="S4.T1.1.1.10.8.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.10.8.3">Machine Translation,</td>
<td class="ltx_td" id="S4.T1.1.1.10.8.4"></td>
<td class="ltx_td" id="S4.T1.1.1.10.8.5"></td>
<td class="ltx_td" id="S4.T1.1.1.10.8.6"></td>
<td class="ltx_td" id="S4.T1.1.1.10.8.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.11.9">
<td class="ltx_td" id="S4.T1.1.1.11.9.1"></td>
<td class="ltx_td" id="S4.T1.1.1.11.9.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.11.9.3">Text Style Transfer,</td>
<td class="ltx_td" id="S4.T1.1.1.11.9.4"></td>
<td class="ltx_td" id="S4.T1.1.1.11.9.5"></td>
<td class="ltx_td" id="S4.T1.1.1.11.9.6"></td>
<td class="ltx_td" id="S4.T1.1.1.11.9.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.12.10">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.10.1"><span class="ltx_text" id="S4.T1.1.1.12.10.1.1">ECT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib230" title="">2023e</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.10.2"><span class="ltx_text" id="S4.T1.1.1.12.10.2.1">ChatGPT</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.10.3">&amp; Summarization</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.10.4"><span class="ltx_text" id="S4.T1.1.1.12.10.4.1">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.10.5"><span class="ltx_text" id="S4.T1.1.1.12.10.5.1">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.10.6"><span class="ltx_text" id="S4.T1.1.1.12.10.6.1">SFT &amp; RLHF</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.12.10.7"><span class="ltx_text" id="S4.T1.1.1.12.10.7.1">RoBERTa</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.13.11">
<td class="ltx_td" id="S4.T1.1.1.13.11.1"></td>
<td class="ltx_td" id="S4.T1.1.1.13.11.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.11.3"><span class="ltx_text" id="S4.T1.1.1.13.11.3.1" style="background-color:#E6E6E6;">Instruct-tuning</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.13.11.4"><span class="ltx_text" id="S4.T1.1.1.13.11.4.1" style="background-color:#E6E6E6;">5K, 10K,</span></td>
<td class="ltx_td" id="S4.T1.1.1.13.11.5"></td>
<td class="ltx_td" id="S4.T1.1.1.13.11.6"></td>
<td class="ltx_td" id="S4.T1.1.1.13.11.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.14.12" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.12.1"><span class="ltx_text" id="S4.T1.1.1.14.12.1.1" style="background-color:#E6E6E6;">FedEval-LLM <cite class="ltx_cite ltx_citemacro_citep">(He et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib85" title="">2024b</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.12.2"><span class="ltx_text" id="S4.T1.1.1.14.12.2.1" style="background-color:#E6E6E6;">Human</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.12.3"><span class="ltx_text" id="S4.T1.1.1.14.12.3.1" style="background-color:#E6E6E6;">&amp; Summary</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.12.4"><span class="ltx_text" id="S4.T1.1.1.14.12.4.1" style="background-color:#E6E6E6;">per client</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.12.5"><span class="ltx_text" id="S4.T1.1.1.14.12.5.1" style="background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.12.6"><span class="ltx_text" id="S4.T1.1.1.14.12.6.1" style="background-color:#E6E6E6;">LoRA</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.14.12.7"><span class="ltx_text" id="S4.T1.1.1.14.12.7.1" style="background-color:#E6E6E6;">Llama-7B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.15.13">
<td class="ltx_td" id="S4.T1.1.1.15.13.1"></td>
<td class="ltx_td" id="S4.T1.1.1.15.13.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.15.13.3">Summarization,</td>
<td class="ltx_td" id="S4.T1.1.1.15.13.4"></td>
<td class="ltx_td" id="S4.T1.1.1.15.13.5"></td>
<td class="ltx_td" id="S4.T1.1.1.15.13.6"></td>
<td class="ltx_td" id="S4.T1.1.1.15.13.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.16.14">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.16.14.1"><span class="ltx_text" id="S4.T1.1.1.16.14.1.1">FENCE <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib253" title="">2024d</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.16.14.2"><span class="ltx_text" id="S4.T1.1.1.16.14.2.1">Human &amp; LLM</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.16.14.3">QA, &amp; Dialogue</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.16.14.4"><span class="ltx_text" id="S4.T1.1.1.16.14.4.1">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.16.14.5"><span class="ltx_text" id="S4.T1.1.1.16.14.5.1">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.16.14.6"><span class="ltx_text" id="S4.T1.1.1.16.14.6.1">SFT &amp; DPO</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.16.14.7"><span class="ltx_text" id="S4.T1.1.1.16.14.7.1">Llama3-8B-Chat</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.17.15">
<td class="ltx_td" id="S4.T1.1.1.17.15.1"></td>
<td class="ltx_td" id="S4.T1.1.1.17.15.2"></td>
<td class="ltx_td" id="S4.T1.1.1.17.15.3"></td>
<td class="ltx_td" id="S4.T1.1.1.17.15.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.17.15.5"><span class="ltx_text" id="S4.T1.1.1.17.15.5.1" style="background-color:#E6E6E6;">Pointwise, Pairwise,</span></td>
<td class="ltx_td" id="S4.T1.1.1.17.15.6"></td>
<td class="ltx_td" id="S4.T1.1.1.17.15.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.18.16">
<td class="ltx_td" id="S4.T1.1.1.18.16.1"></td>
<td class="ltx_td" id="S4.T1.1.1.18.16.2"></td>
<td class="ltx_td" id="S4.T1.1.1.18.16.3"></td>
<td class="ltx_td" id="S4.T1.1.1.18.16.4"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.18.16.5"><span class="ltx_text" id="S4.T1.1.1.18.16.5.1" style="background-color:#E6E6E6;">Classification,</span></td>
<td class="ltx_td" id="S4.T1.1.1.18.16.6"></td>
<td class="ltx_td" id="S4.T1.1.1.18.16.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.19.17" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.19.17.1"><span class="ltx_text" id="S4.T1.1.1.19.17.1.1" style="background-color:#E6E6E6;">FLAMe <cite class="ltx_cite ltx_citemacro_citep">(Vu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib227" title="">2024</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.19.17.2"><span class="ltx_text" id="S4.T1.1.1.19.17.2.1" style="background-color:#E6E6E6;">Human</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.19.17.3"><span class="ltx_text" id="S4.T1.1.1.19.17.3.1" style="background-color:#E6E6E6;">Various</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.19.17.4"><span class="ltx_text" id="S4.T1.1.1.19.17.4.1" style="background-color:#E6E6E6;">5.3M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.19.17.5"><span class="ltx_text" id="S4.T1.1.1.19.17.5.1" style="background-color:#E6E6E6;">&amp; Open-ended generation</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.19.17.6"><span class="ltx_text" id="S4.T1.1.1.19.17.6.1" style="background-color:#E6E6E6;">RLHF</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.19.17.7"><span class="ltx_text" id="S4.T1.1.1.19.17.7.1" style="background-color:#E6E6E6;">PaLM-2-24B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.20.18">
<td class="ltx_td" id="S4.T1.1.1.20.18.1"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.20.18.2">GPT-4-Turbo</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.20.18.3">Multiple-Evidence</td>
<td class="ltx_td" id="S4.T1.1.1.20.18.4"></td>
<td class="ltx_td" id="S4.T1.1.1.20.18.5"></td>
<td class="ltx_td" id="S4.T1.1.1.20.18.6"></td>
<td class="ltx_td" id="S4.T1.1.1.20.18.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.21.19">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.21.19.1"><span class="ltx_text" id="S4.T1.1.1.21.19.1.1">HALU-J <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib228" title="">2024b</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.21.19.2">&amp; GPT-3.5-Turbo</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.21.19.3">Hallucination Detection</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.21.19.4"><span class="ltx_text" id="S4.T1.1.1.21.19.4.1">2663</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.21.19.5"><span class="ltx_text" id="S4.T1.1.1.21.19.5.1">Pointwise &amp; Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.21.19.6"><span class="ltx_text" id="S4.T1.1.1.21.19.6.1">SFT &amp; DPO</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.21.19.7"><span class="ltx_text" id="S4.T1.1.1.21.19.7.1">Mistral-7B-Instruct</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.22.20" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.22.20.1"><span class="ltx_text" id="S4.T1.1.1.22.20.1.1" style="background-color:#E6E6E6;">HelpSteer2 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib238" title="">2024a</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.22.20.2"><span class="ltx_text" id="S4.T1.1.1.22.20.2.1" style="background-color:#E6E6E6;">Human</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.22.20.3"><span class="ltx_text" id="S4.T1.1.1.22.20.3.1" style="background-color:#E6E6E6;">Various</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.22.20.4"><span class="ltx_text" id="S4.T1.1.1.22.20.4.1" style="background-color:#E6E6E6;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.22.20.5"><span class="ltx_text" id="S4.T1.1.1.22.20.5.1" style="background-color:#E6E6E6;">Pointwise &amp; Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.22.20.6"><span class="ltx_text" id="S4.T1.1.1.22.20.6.1" style="background-color:#E6E6E6;">PPI &amp; RLHF</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.22.20.7"><span class="ltx_text" id="S4.T1.1.1.22.20.7.1" style="background-color:#E6E6E6;">Llama3.1-70B-Instruct</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.23.21">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.23.21.1">INSTRUCTSCORE <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib259" title="">2023e</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.23.21.2">GPT-4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.23.21.3">Various</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.23.21.4">40K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.23.21.5">Pointwise &amp; Pairwise</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.23.21.6">SFT</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.23.21.7">Llama-7B</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.24.22" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.24.22.1"><span class="ltx_text" id="S4.T1.1.1.24.22.1.1" style="background-color:#E6E6E6;">JudgeLM <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib302" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.24.22.2"><span class="ltx_text" id="S4.T1.1.1.24.22.2.1" style="background-color:#E6E6E6;">GPT-4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.24.22.3"><span class="ltx_text" id="S4.T1.1.1.24.22.3.1" style="background-color:#E6E6E6;">Open-ended Tasks</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.24.22.4"><span class="ltx_text" id="S4.T1.1.1.24.22.4.1" style="background-color:#E6E6E6;">100K</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.24.22.5"><span class="ltx_text" id="S4.T1.1.1.24.22.5.1" style="background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.24.22.6"><span class="ltx_text" id="S4.T1.1.1.24.22.6.1" style="background-color:#E6E6E6;">SFT</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.24.22.7"><span class="ltx_text" id="S4.T1.1.1.24.22.7.1" style="background-color:#E6E6E6;">Vicuna Series</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.25.23">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.25.23.1">LLaVA-Critic <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib254" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.25.23.2">GPT-4o</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.25.23.3">Various</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.25.23.4">113K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.25.23.5">Pointwise &amp; Pairwise</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.25.23.6">DPO</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.25.23.7">LLaVA-OneVision(OV) 7B &amp; 72B</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.26.24" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.26.24.1"><span class="ltx_text" id="S4.T1.1.1.26.24.1.1" style="background-color:#E6E6E6;">Meta-Rewarding <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib246" title="">2024b</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.26.24.2"><span class="ltx_text" id="S4.T1.1.1.26.24.2.1" style="background-color:#E6E6E6;">Llama3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.26.24.3"><span class="ltx_text" id="S4.T1.1.1.26.24.3.1" style="background-color:#E6E6E6;">Various</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.26.24.4"><span class="ltx_text" id="S4.T1.1.1.26.24.4.1" style="background-color:#E6E6E6;">20K</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.26.24.5"><span class="ltx_text" id="S4.T1.1.1.26.24.5.1" style="background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.26.24.6"><span class="ltx_text" id="S4.T1.1.1.26.24.6.1" style="background-color:#E6E6E6;">DPO</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.26.24.7"><span class="ltx_text" id="S4.T1.1.1.26.24.7.1" style="background-color:#E6E6E6;">Llama3-8B-Instruct</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.27.25">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.27.25.1">OffsetBias <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib175" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.27.25.2">Human &amp; LLM</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.27.25.3">Bias Detection</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.27.25.4">268K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.27.25.5">Pairwise</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.27.25.6">RLHF</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.27.25.7">Llama3-8B-Instruct</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.28.26" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.28.26.1"><span class="ltx_text" id="S4.T1.1.1.28.26.1.1" style="background-color:#E6E6E6;">PandaLM <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib237" title="">2023d</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.28.26.2"><span class="ltx_text" id="S4.T1.1.1.28.26.2.1" style="background-color:#E6E6E6;">Human</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.28.26.3"><span class="ltx_text" id="S4.T1.1.1.28.26.3.1" style="background-color:#E6E6E6;">Various</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.28.26.4"><span class="ltx_text" id="S4.T1.1.1.28.26.4.1" style="background-color:#E6E6E6;">300K</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.28.26.5"><span class="ltx_text" id="S4.T1.1.1.28.26.5.1" style="background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.28.26.6"><span class="ltx_text" id="S4.T1.1.1.28.26.6.1" style="background-color:#E6E6E6;">SFT</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.28.26.7"><span class="ltx_text" id="S4.T1.1.1.28.26.7.1" style="background-color:#E6E6E6;">Llama-7B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.29.27">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.29.27.1">PHUDGE <cite class="ltx_cite ltx_citemacro_citep">(Deshwal and Chawla, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib48" title="">2024</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.29.27.2">Human &amp; GPT-4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.29.27.3">NLG</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.29.27.4">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.29.27.5">Pointwise &amp; Pairwise</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.29.27.6">LoRA</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.29.27.7">Phi-3</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.30.28" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.30.28.1"><span class="ltx_text" id="S4.T1.1.1.30.28.1.1" style="background-color:#E6E6E6;">PROMETHEUS <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib110" title="">2023</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.30.28.2"><span class="ltx_text" id="S4.T1.1.1.30.28.2.1" style="background-color:#E6E6E6;">Human</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.30.28.3"><span class="ltx_text" id="S4.T1.1.1.30.28.3.1" style="background-color:#E6E6E6;">Various</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.30.28.4"><span class="ltx_text" id="S4.T1.1.1.30.28.4.1" style="background-color:#E6E6E6;">100K</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.30.28.5"><span class="ltx_text" id="S4.T1.1.1.30.28.5.1" style="background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.30.28.6"><span class="ltx_text" id="S4.T1.1.1.30.28.6.1" style="background-color:#E6E6E6;">SFT</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.30.28.7"><span class="ltx_text" id="S4.T1.1.1.30.28.7.1" style="background-color:#E6E6E6;">Llama2-Chat-7B &amp; 13B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.31.29">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.31.29.1">PROMETHEUS2 <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib111" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.31.29.2">Human</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.31.29.3">Various</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.31.29.4">300K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.31.29.5">Pointwise &amp; Paiwise</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.31.29.6">SFT</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.31.29.7">Mistral-7B &amp; Mistral-8x7B</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.32.30" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.32.30.1"><span class="ltx_text" id="S4.T1.1.1.32.30.1.1" style="background-color:#E6E6E6;">PROMETHEUS-VISION <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib123" title="">2024a</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.32.30.2"><span class="ltx_text" id="S4.T1.1.1.32.30.2.1" style="background-color:#E6E6E6;">GPT-4V</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.32.30.3"><span class="ltx_text" id="S4.T1.1.1.32.30.3.1" style="background-color:#E6E6E6;">Various</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.32.30.4"><span class="ltx_text" id="S4.T1.1.1.32.30.4.1" style="background-color:#E6E6E6;">15K</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.32.30.5"><span class="ltx_text" id="S4.T1.1.1.32.30.5.1" style="background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.32.30.6"><span class="ltx_text" id="S4.T1.1.1.32.30.6.1" style="background-color:#E6E6E6;">SFT</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.32.30.7"><span class="ltx_text" id="S4.T1.1.1.32.30.7.1" style="background-color:#E6E6E6;">Llava-1.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.33.31">
<td class="ltx_td" id="S4.T1.1.1.33.31.1"></td>
<td class="ltx_td" id="S4.T1.1.1.33.31.2"></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.33.31.3">Common, Coding,</td>
<td class="ltx_td" id="S4.T1.1.1.33.31.4"></td>
<td class="ltx_td" id="S4.T1.1.1.33.31.5"></td>
<td class="ltx_td" id="S4.T1.1.1.33.31.6"></td>
<td class="ltx_td" id="S4.T1.1.1.33.31.7"></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.34.32">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.34.32.1"><span class="ltx_text" id="S4.T1.1.1.34.32.1.1">SELF-J <cite class="ltx_cite ltx_citemacro_citep">(Ye and Ng, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib267" title="">2024</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.34.32.2"><span class="ltx_text" id="S4.T1.1.1.34.32.2.1">Human &amp; GPT-4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.34.32.3">&amp; Academic</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.34.32.4"><span class="ltx_text" id="S4.T1.1.1.34.32.4.1">5.7M</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.34.32.5"><span class="ltx_text" id="S4.T1.1.1.34.32.5.1">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.34.32.6"><span class="ltx_text" id="S4.T1.1.1.34.32.6.1">LoRA</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.34.32.7"><span class="ltx_text" id="S4.T1.1.1.34.32.7.1">Llama2-13B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.35.33" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.35.33.1"><span class="ltx_text" id="S4.T1.1.1.35.33.1.1" style="background-color:#E6E6E6;">Self-Rationalization <cite class="ltx_cite ltx_citemacro_citep">(Trivedi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib221" title="">2024a</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.35.33.2"><span class="ltx_text" id="S4.T1.1.1.35.33.2.1" style="background-color:#E6E6E6;">LLM</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.35.33.3"><span class="ltx_text" id="S4.T1.1.1.35.33.3.1" style="background-color:#E6E6E6;">Various</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.35.33.4"><span class="ltx_text" id="S4.T1.1.1.35.33.4.1" style="background-color:#E6E6E6;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.35.33.5"><span class="ltx_text" id="S4.T1.1.1.35.33.5.1" style="background-color:#E6E6E6;">Pointwise &amp; Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.35.33.6"><span class="ltx_text" id="S4.T1.1.1.35.33.6.1" style="background-color:#E6E6E6;">SFT &amp; DPO</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.35.33.7"><span class="ltx_text" id="S4.T1.1.1.35.33.7.1" style="background-color:#E6E6E6;">Llama3.1-8B-Instruct</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.36.34">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.36.34.1">Self-Taught <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib232" title="">2024e</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.36.34.2">LLM</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.36.34.3">Various</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.36.34.4">20K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.36.34.5">Pairwise</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.36.34.6">-</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.36.34.7">Llama3-70B-Instruct</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.37.35" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.37.35.1"><span class="ltx_text" id="S4.T1.1.1.37.35.1.1" style="background-color:#E6E6E6;">Shepherd <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib233" title="">2023c</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.37.35.2"><span class="ltx_text" id="S4.T1.1.1.37.35.2.1" style="background-color:#E6E6E6;">Human</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.37.35.3"><span class="ltx_text" id="S4.T1.1.1.37.35.3.1" style="background-color:#E6E6E6;">Various</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.37.35.4"><span class="ltx_text" id="S4.T1.1.1.37.35.4.1" style="background-color:#E6E6E6;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.37.35.5"><span class="ltx_text" id="S4.T1.1.1.37.35.5.1" style="background-color:#E6E6E6;">Pointwise &amp; Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.37.35.6"><span class="ltx_text" id="S4.T1.1.1.37.35.6.1" style="background-color:#E6E6E6;">SFT</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.37.35.7"><span class="ltx_text" id="S4.T1.1.1.37.35.7.1" style="background-color:#E6E6E6;">Llama-7B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.38.36">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.38.36.1">SorryBench <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib250" title="">2024b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.38.36.2">Human &amp; GPT-4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.38.36.3">Unsafe Topics</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.38.36.4">2.7K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.38.36.5">Pointwise</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.38.36.6">SFT</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.38.36.7">Multiple LLMs</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.39.37" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.39.37.1"><span class="ltx_text" id="S4.T1.1.1.39.37.1.1" style="background-color:#E6E6E6;">Themis <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib89" title="">2024a</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.39.37.2"><span class="ltx_text" id="S4.T1.1.1.39.37.2.1" style="background-color:#E6E6E6;">Human &amp; GPT-4</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.39.37.3"><span class="ltx_text" id="S4.T1.1.1.39.37.3.1" style="background-color:#E6E6E6;">NLG</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.39.37.4"><span class="ltx_text" id="S4.T1.1.1.39.37.4.1" style="background-color:#E6E6E6;">67K</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.39.37.5"><span class="ltx_text" id="S4.T1.1.1.39.37.5.1" style="background-color:#E6E6E6;">Pointwise &amp; Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.39.37.6"><span class="ltx_text" id="S4.T1.1.1.39.37.6.1" style="background-color:#E6E6E6;">SFT &amp; DPO</span></td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.39.37.7"><span class="ltx_text" id="S4.T1.1.1.39.37.7.1" style="background-color:#E6E6E6;">Llama3-8B</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.40.38">
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.40.38.1">TIGERScore <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib100" title="">2023b</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.40.38.2">Human &amp; GPT-4</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.40.38.3">Text Generation</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.40.38.4">42K</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.40.38.5">Pointwise</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.40.38.6">SFT</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.1.40.38.7">Llama2-7B &amp; 13B</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.1.41.39" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.41.39.1"><span class="ltx_text" id="S4.T1.1.1.41.39.1.1" style="background-color:#E6E6E6;">X-EVAL <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib143" title="">2023c</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.41.39.2"><span class="ltx_text" id="S4.T1.1.1.41.39.2.1" style="background-color:#E6E6E6;">Human</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.41.39.3"><span class="ltx_text" id="S4.T1.1.1.41.39.3.1" style="background-color:#E6E6E6;">NLG</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.41.39.4"><span class="ltx_text" id="S4.T1.1.1.41.39.4.1" style="background-color:#E6E6E6;">55,602</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.41.39.5"><span class="ltx_text" id="S4.T1.1.1.41.39.5.1" style="background-color:#E6E6E6;">Pointwise &amp; Pairwise</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.41.39.6"><span class="ltx_text" id="S4.T1.1.1.41.39.6.1" style="background-color:#E6E6E6;">SFT</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T1.1.1.41.39.7"><span class="ltx_text" id="S4.T1.1.1.41.39.7.1" style="background-color:#E6E6E6;">Flan-T5</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Post-processing</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">Post-processing involves further refining evaluation results to extract more precise and reliable outcomes. This step typically includes analyzing the initial outputs to identify patterns, inconsistencies, or areas requiring improvement, followed by targeted adjustments and in-depth analysis. By addressing these issues, post-processing ensures that the evaluation results are not only accurate but also aligned with the specific objectives and standards of the task.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p2.1.1">Probability Calibration.</span>
During the post-hoc process of the model output, some studies use rigorous mathematical derivations to quantify the differences, thereby optimizing them.
For instance, Daynauth et al. <cite class="ltx_cite ltx_citemacro_citep">(Daynauth and Mars, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib46" title="">2024</a>)</cite> investigates the discrepancy between human preferences and automated evaluations in language model assessments, particularly employs Bayesian statistics and a t-test to quantify bias towards higher token counts, and develops a recalibration procedure to adjust the GPTScorers.
Apart from that, ProbDiff <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib248" title="">2024b</a>)</cite> is another novel self-evaluation method for LLMs that assesses model efficacy by computing the probability discrepancy between initial responses and their revised versions.
Moreover, Liusie et al. <cite class="ltx_cite ltx_citemacro_citep">(Liusie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib151" title="">2024</a>)</cite> introduces a Product of Experts (PoE) framework for efficient comparative assessment using LLMs, which yield an expression that can be maximized with respect to the underlying set of candidates. This paper proposes two experts, a soft Bradley-Terry expert and a Gaussian expert that has closed-form solutions.
Unlike from frameworks above, CRISPR <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib265" title="">2024</a>)</cite> is a novel bias mitigation method for LLMs executing instruction-based tasks, which identifies and prunes bias neurons with probability calibration, reducing bad performance without compromising pre-existing knowledge.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p3">
<p class="ltx_p" id="S4.SS1.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS3.p3.1.1">Text Reprocessing.</span>
In LLMs-as-judges, text reprocessing methods are essential for enhancing the accuracy and reliability of evaluation outcomes. Specifically, text processing can improve the evaluation process by integrating multiple evaluation results or outcomes from several rounds of assessment.
For example, Sottana et al. <cite class="ltx_cite ltx_citemacro_citep">(Sottana et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib207" title="">2023</a>)</cite> employs a multi-round evaluation process. Each round involves scoring model outputs based on specific criteria, with the human and GPT-4 evaluations ranking model performances from best to worst and averaging these rankings to mitigate subjectivity.
For the single-response evaluation, AUTO-J <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib131" title="">2023c</a>)</cite> employs a "divide-and-conquer" strategy. Critiques that either adhere to or deviate from the scenario-specific criteria are consolidated to form a comprehensive evaluation judgment and then generate the final assessment.
Consistent with former aforementioned studies, Yan et al. <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib263" title="">2024a</a>)</cite> introduces a post-processing method to consolidate the relevance labels generated by LLMs. It demonstrates that this approach effectively combines both the ranking and labeling abilities of LLMs through post-processing.
Furthermore, REVISEVAL <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib282" title="">2024b</a>)</cite> is a novel evaluation paradigm that enhances the reliability of LLM Judges by generating response-adapted references through text revision capabilities of LLMs.
Apart from that, Tessler et al. <cite class="ltx_cite ltx_citemacro_citep">(Tessler et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib215" title="">2024</a>)</cite> explores the use of AI as a mediator in democratic deliberation, aiming to help diverse groups find common ground on complex social and political issues. With the goal of maximizing group approval, the researchers developed the "Habermas Machine", which iteratively generate group statements based on individual opinions.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p4">
<p class="ltx_p" id="S4.SS1.SSS3.p4.1">Another category of text reprocessing methods involves task transformation, primarily focusing on the conversion between open-ended and multiple-choice question (MCQ) formats. Ren et al. <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib187" title="">2023</a>)</cite> explores the use of self-evaluation to enhance the selective generation capabilities of LLMs. Specifically, the authors reformulate open-ended generation tasks into token-level prediction tasks, reduce sequence-level scores to token-level scores to improve quality calibration.
Conversely, Myrzakhan et al. <cite class="ltx_cite ltx_citemacro_citep">(Myrzakhan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib162" title="">2024</a>)</cite> introduces the Open-LLM-Leaderboard, a new benchmark for evaluating LLMs using open-style questions, which eliminates selection bias and random guessing issues associated with multiple-choice questions. It presents a method to identify suitable open-style questions and validate the correctness of LLM open-style responses against human-annotated ground-truths.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Multi-LLM System</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Multi-LLM Evaluation harnesses the collective intelligence of multiple LLMs to bolster the robustness and reliability of evaluations. By either facilitating inter-model communication or independently aggregating their outputs, these systems can effectively mitigate biases, leverage complementary strengths across different models, refine decision-making precision, and foster a more nuanced understanding of complex judgments.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Communication</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">Communication means the dynamic flow of information between LLMs, which is pivotal for sparking insights and sharing rationales during the judgment process.
Recent research has shown that communication among LLMs can enable emergent abilities through their interactions <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib262" title="">2023c</a>)</cite>, leading to a cohesive decision-making process and better judgment performance.
The Multi-LLM system can benefit from LLM interactions in two ways: cooperation and competition.
</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p2.1.1">Cooperation.</span>
Multi-LLMs can work together to achieve a common goal with information and rationales sharing through interactions to enhance the overall evaluation process.
For example, <cite class="ltx_cite ltx_citemacro_citet">Zhang et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib286" title="">2023b</a>)</cite> proposed an architecture named WideDeep to aggregate information at the LLM’s neuro-level.
In addition, <cite class="ltx_cite ltx_citemacro_citet">Xu et al<span class="ltx_text">.</span> (<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib262" title="">2023c</a>)</cite> introduced a multi-agent collaboration strategy that mimics the academic peer review process to enhance complex reasoning in LLMs.
The approach involves agents creating solutions, reviewing each other’s work, and revising their initial submissions based on feedback.
Similarly, ABSEval <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib137" title="">2024b</a>)</cite> utilizes four agents for answer synthesize, critique, execution, and commonsense, to build the overall workflow.
Although the cooperation can complement each other’s strengths between LLMs to a certain degree, this method still includes the risk of groupthink, where similar models reinforce each other’s biases rather than providing diverse insights.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.SSS1.p3.1.1">Competition.</span> Multi-LLMs systems can also benefit from competitive or adversarial communication, i.e., LLMs argue or debate to evaluate each other’s outputs <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib287" title="">2024b</a>; Moniri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib159" title="">2024</a>; Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib23" title="">2023</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib133" title="">2023b</a>)</cite>.
Such multi-LLMs systems could be categorized into centralized and decentralized structures <cite class="ltx_cite ltx_citemacro_citep">(Owens et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib169" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1">In the centralized structure, a single central LLM acts as the orchestrator of the conversation, highlighting the efficiency of a unified decision-making process.
Auto-Arena <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib287" title="">2024b</a>)</cite> is such a novel framework that automates the evaluation of LLMs through agent peer battles and committee discussions, aiming to provide timely and reliable assessments. In detail, the framework conducts multi-round debates between LLM candidates, and uses an LLM judge committee to decide the winner.
Inspired by courtroom dynamics, <cite class="ltx_cite ltx_citemacro_citet">Bandi and Harrasse (<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib13" title="">2024</a>)</cite> propose two architectures, MORE and SAMRE, which utilize multiple advocates and iterative debates to dynamically assess LLM outputs.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p5">
<p class="ltx_p" id="S4.SS2.SSS1.p5.1">In contrast, the decentralized structure emphasizes a collective intelligence where all models engage in direct communication, promoting a resilient and distributed decision-making structure.
In the domain of LLM debates, Moniri et al. <cite class="ltx_cite ltx_citemacro_citep">(Moniri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib159" title="">2024</a>)</cite> introduced a unique automated benchmarking framework, employing another LLM as the judge to assess not only the models’ domain knowledge but also their abilities in problem definition and inconsistency recognition.
ChatEval <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib23" title="">2023</a>)</cite> is another multi-agent debate framework that utilizes multiple LLMs with diverse role prompts and communication strategies on open-ended questions and traditional NLG tasks, significantly improves evaluation performance compared to single-agent methods.
Moreover, PRD <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib133" title="">2023b</a>)</cite> applied peer rank and discussion to address issues like self-enhancement and positional bias in current LLM evaluation methods, leading to better alignment with human judgments and a path for fair model capability ranking.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Aggregation</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">Alternatively, in multi-LLM systems without communication, judgments are independently generated by multiple models, which are subsequently synthesized into a final decision through various aggregation strategies. Techniques such as majority vote, weighted averages, and prioritizing the highest confidence predictions, each play a crucial role. These methods allow each model to assess without interference, and eventually extract and combine the most effective elements from each model’s response.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1">Simple voting methods, such as majority voting, by selecting the most frequent answers, offers a straightforward approach to synthesize evaluations.
For example, Badshah et al. <cite class="ltx_cite ltx_citemacro_citep">(Badshah and Sajjad, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib9" title="">2024</a>)</cite> introduced a reference-guided verdict method for evaluating free-form text using multiple LLMs as judges. Combining these LLMs through majority vote significantly improves the reliability and accuracy of evaluations, particularly for complex tasks.
Furthermore, PoLL <cite class="ltx_cite ltx_citemacro_citep">(Verga et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib226" title="">2024</a>)</cite> demonstrates that using a diverse panel of smaller models as judges through max voting and average pooling is not only an effective method for evaluating LLM performance, but also reduces intra-model bias of a single large model.
Language-Model-as-an-Examiner <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib11" title="">2024</a>)</cite> is another benchmarking framework to evaluate the performance of foundation models on open-ended question answering through voting. In the peer-examination mechanism, the LM serves as a knowledgeable examiner that formulates questions and evaluates responses in a reference-free manner.
What’s more, multi-LLM evaluation could also be used in improving dataset quality. Choi et al. <cite class="ltx_cite ltx_citemacro_citep">(Choi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib39" title="">2024</a>)</cite> provided an enhanced dataset, MULTI-NEWS+, which is the result of a cleansing strategy leveraging CoT and majority voting to identify and exclude irrelevant documents through LLM-based data annotation.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1">Weighted scoring aggregation involves assigning different importance to different model outputs, either by aggregating multiple overall scores for the same response or by combining assessments of different aspects of the response to form a comprehensive evaluation.
On the one hand, through a peer-review mechanism, PiCO <cite class="ltx_cite ltx_citemacro_citep">(Ning et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib166" title="">2024</a>)</cite> allows LLMs to answer unlabeled questions and evaluate each other without human annotations. It formalizes the evaluation as a constrained optimization problem, maximizing the consistency between LLMs’ capabilities and corresponding weights.
Likewise, PRE <cite class="ltx_cite ltx_citemacro_citep">(Chu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib40" title="">2024</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib28" title="">2024e</a>)</cite> can automatically evaluate LLMs through a peer-review process. It selects qualified LLMs as reviewers through a qualification exam and aggregates their ratings using weights which is proportional to their agreement of humans, demonstrating effectiveness and robustness in evaluating text summarization tasks.
In the field of recommendation explanations, Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib285" title="">2024a</a>)</cite> suggests that ensembles like averaging ratings of multiple LLMs can enhance evaluation accuracy and stability.
On the other hand, for example, AIME <cite class="ltx_cite ltx_citemacro_citep">(Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib176" title="">2024</a>)</cite> is an evaluation protocol that utilizes multiple LLMs that each with a specific role independently generate an evaluation on separate criteria and then combine them via concatenation.
Similarly, a paper introduces HD-EVAL <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib148" title="">2024a</a>)</cite>, which iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. By decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria, HD-EVAL demonstrates its superiority.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p4">
<p class="ltx_p" id="S4.SS2.SSS2.p4.1">Apart from weighting methods, there are some other advance mathematical aggregation techniques, such as Bayesian methods and graph-based approaches, offering more robust ways to handle uncertainties and inconsistencies across multiple evaluators.
Notably, a paper introduces two calibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian Dawid-Skene <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib71" title="">2024</a>)</cite>, to address the win rate estimation bias when using many LLMs as evaluators for text generation quality.
In addition to that, GED <cite class="ltx_cite ltx_citemacro_citep">(Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib91" title="">2024c</a>)</cite> addresses inconsistencies in LLM preference evaluations by leveraging multiple weak evaluators to construct preference graphs, and then utilize DAG structure to ensemble and denoise these graphs for better, non-contradictory evaluation results.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p5">
<p class="ltx_p" id="S4.SS2.SSS2.p5.1">LLM-based aggregation is a grand-new perspective like Fusion-Eval <cite class="ltx_cite ltx_citemacro_citep">(Shu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib199" title="">2024</a>)</cite>. It’s a novel framework that integrates various assistant evaluators using LLMs, each of which specializes in assessing distinct aspects of responses, to enhance the correlation of evaluation scores with human judgments for natural language systems.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p6">
<p class="ltx_p" id="S4.SS2.SSS2.p6.1">In addition to the above direct use of multiple model evaluation, the cascade framework employs a tiered approach, where weaker models are used initially for evaluations, and stronger models are engaged only when higher confidence is required, optimizing resource use and enhancing evaluation precision.
Jung et al. <cite class="ltx_cite ltx_citemacro_citep">(Jung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib104" title="">2024</a>)</cite> proposes "Cascaded Selective Evaluation" to ensure high agreement with human judgments while using cheaper models.
Similar to the work above, Huang et al. <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib93" title="">2024b</a>)</cite> proposes CascadedEval, a novel method integrating proprietary models, in order to compensate for the limitations of fine-tuned judge models.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Human-AI Collaboration System</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Human-AI Collaboration Systems bridge the gap between automated LLM judgments and the essential need for human oversight, particularly in high-stakes domains such as law, healthcare, and education. Human evaluators act either as the ultimate deciders, or as intermediaries who verify and refine model outputs. By incorporating human insights, Hybrid systems can ensure the final judgment is more reliable and aligned with ethical considerations, and empower continuous model improvement through feedback loops.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">In many Human-AI Collaboration systems, human evaluators play a vital role during the evaluation process itself, actively collaborating with the LLMs to review and refine the generated outputs.
For example, COEVAL<cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib132" title="">2023a</a>)</cite> introduces a collaborative evaluation pipeline where LLMs generate initial criteria and evaluations for open-ended natural language tasks. These machine-generated outputs are then reviewed and refined by human evaluators to guarantee reliability.
To address a significant positional bias in LLMs when used as evaluators, Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib231" title="">2023b</a>)</cite> proposes a calibration framework with three strategies: Multiple Evidence Calibration, Balanced Position Calibration, and Human-in-the-Loop Calibration.
Similarly, EvalGen<cite class="ltx_cite ltx_citemacro_citep">(Shankar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib193" title="">2024</a>)</cite> integrates human feedback iteratively to refine evaluation criteria, addressing challenges such as “criteria drift”, where the standards of evaluation evolve as humans interact with the model.
These systems allow human evaluators to provide real-time adjustments, enhancing the accuracy and trustworthiness of the evaluation process.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">While in other systems, human involvement takes place after the LLM has completed its evaluations, providing a final layer of verification and adjustment. This method ensures that the LLM’s judgments are thoroughly scrutinized and aligned with human values.
EvaluLLM<cite class="ltx_cite ltx_citemacro_citep">(Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib171" title="">2024a</a>)</cite> allows humans to intervene and refine the evaluation results, thereby enhancing trust in the model’s performance while also controlling for potential biases.
Additionally, Chiang et al.<cite class="ltx_cite ltx_citemacro_citep">(Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib36" title="">2024</a>)</cite> tried LLM TAs as an assignment evaluator in a large university course. After students submit assignments and receive LLM-generated feedback, the teaching team reviews and finalizes the evaluation results. This process illustrates how human oversight after the initial automated evaluation can guarantee fairness and consistentcy with academic standards.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Application</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Due to the convenience and effectiveness of LLM Judges, they have been widely applied as judges across various domains.
These applications not only cover general domains but also specific domains such as multimodal, medical, legal, financial, education, information retrieval and others. In this section, we will provide a detailed introduction to these applications, demonstrating how LLMs achieve precise and efficient evaluations in different domains.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="465" id="S5.F6.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>LLMs-as-judges are widely applied across various domains.</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>General</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">In general domains, LLM Judges are applied to tasks requiring both understanding and generation, such as dialogue generation, open-ended question answering, summarization, and language translation. Each task follows its own set of evaluation criteria to meet its specific requirements.
For instance, in dialogue generation <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib134" title="">2017</a>)</cite>, the criteria emphasize the natural flow, emotional resonance, and contextual relevance of the conversation. In summarization tasks <cite class="ltx_cite ltx_citemacro_citep">(Narayan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib163" title="">2018</a>)</cite>, the evaluation focuses on the coherence, consistency, fluency, and relevance of the text. In translation tasks <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib65" title="">2024</a>)</cite>, the assessment prioritizes the quality, accuracy, fluency, and style.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">As these diverse sub-tasks require specialized evaluation criteria, LLM judges provides refined evaluation methods that go beyond traditional metrics, paving the way for more comprehensive and in-depth assessments. For instance, Shu et al. <cite class="ltx_cite ltx_citemacro_citep">(Shu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib199" title="">2024</a>)</cite> introduced Fusion-Eval, an innovative approach that leverages LLMs to integrate insights from various assistant evaluators. Fusion-Eval evaluated summary quality across four dimensions—coherence, consistency, fluency, and relevance, achieving a system-level Kendall-Tau correlation of 0.962 with human judgments. For dialogue quality, it assessed six aspects: coherence, engagingness, naturalness, groundedness, understandability, and overall quality, attaining a turn-level Spearman correlation of 0.744. Furthermore, Xu et al. <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib257" title="">2024b</a>)</cite> proposed the ACTIVE-CRITIC framework, which enables LLMs to actively infer the target task and relevant evaluation criteria while dynamically optimizing prompts. In the story generation task, this framework achieved superior evaluation performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Multimodal</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">In the multimodal domain, the evaluation objects of LLMs are not limited to textual data but extend to various forms of information such as images, audio, and video. One of the primary challenges in evaluating multimodal tasks lies in the significant heterogeneity among these modalities, including substantial differences in data structures, representation methods, and feature distributions.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">To address this challenge, advanced techniques are often required to help LLMs integrate different forms of information, ensuring that they can provide accurate and meaningful evaluations. For example, Xiong et al. <cite class="ltx_cite ltx_citemacro_citep">(Xiong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib254" title="">2024</a>)</cite> trained an open-source multimodal LLM, LLaVA-Critic, specifically to evaluate model performance in multimodal scenarios. Similarly, Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib25" title="">2024b</a>)</cite> developed a Multimodal LLM-as-a-judge for 14 Vision-Language tasks, providing a unified evaluation framework. In addition, LLMs-as-judges can also be used in audio. For instance, Latif et al. <cite class="ltx_cite ltx_citemacro_citep">(Latif et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib118" title="">2023</a>)</cite> used LLMs for identifying and evaluating emotional cues in speech, achieving remarkable accuracy in the process. Beyond these efforts, some recent studies <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib300" title="">2024b</a>; Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib47" title="">2024</a>)</cite> have also explored the potential of multimodal LLMs to self-evaluate and self-reward, enhancing their performance without the need for external evaluators or human annotations.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">As the application of LLMs-as-judges continues to expand in multimodal domains, there is a growing interest in exploring their use in more specific real-world scenarios, such as autonomous driving. Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib30" title="">2024d</a>)</cite> proposed CODA-LM, a novel vision-language benchmark for self-driving, which provides automatic and systematic evaluation of Large Vision-Language Models (LVLMs) on road corner cases. Interestingly, they found that using the text-only LLM judges resulted in a closer alignment with human preferences than LVLMs.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Medical</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">In the medical field, LLMs-as-judges have demonstrated significant potential, particularly in areas such as diagnostic support, medical text analysis, clinical decision-making, and patient education.
In this domain, high-quality evaluation requires LLM judges to possess precise interpretation capabilities for domain-specific terminology, the ability to comprehensively analyze diverse data types (such as clinical records and medical imaging), and strict compliance with high accuracy standards and ethical guidelines.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">In the realm of <span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">medical text generation</span>, Xie et al. <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib252" title="">2024c</a>)</cite> used LLMs to evaluate the compduikeyi1leteness, conciseness, and attribution of medical texts at a fine-grained level. Similarly, Brake et al. <cite class="ltx_cite ltx_citemacro_citep">(Brake and Schaaf, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib18" title="">2024</a>)</cite> leveraged LLMs, such as Llama2, to assess clinical note consistency, with results indicating agreement levels comparable to human annotators. When it comes to <span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.2">medical question answering</span>, Krolik et al. <cite class="ltx_cite ltx_citemacro_citep">(Krolik et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib115" title="">2024</a>)</cite> explored the use of LLMs to automatically evaluate answer quality. Their focus was on evaluating adherence to medical knowledge and professional standards, completeness of information, accuracy of terminology, clarity of expression, and relevance to the question.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">In the area of <span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.1">mental health counseling</span>, Li et al. <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib127" title="">2024c</a>)</cite> utilized LLMs to automate the evaluation of counseling effectiveness and quality. Key assessments included whether the counseling identified the client’s emotional needs, provided appropriate responses, demonstrated empathy, managed negative emotions, and met the overall goals of mental health support. Beyond these above applications, LLMs’ judging capabilities have also been applied to assist in improving performance in specialized <span class="ltx_text ltx_font_bold" id="S5.SS3.p3.1.2">medical reasoning tasks</span>. For instance, many studies <cite class="ltx_cite ltx_citemacro_citep">(Jeong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib96" title="">2024</a>)</cite> employed LLMs to evaluate and filter medical information, thereby supporting enhanced medical reasoning.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Legal</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Due to the powerful evaluation capabilities of LLMs, LLMs-as-judges have been widely applied in the legal domain, covering multiple key scenarios, including evaluating the performance of law LLMs and relevance judgment in legal case retrieval. In the legal domain, the application of LLMs requires a deep understanding of the legal framework of specific jurisdictions, complex legal language, and rigorous logical reasoning abilities <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib129" title="">2024b</a>)</cite>. At the same time, interpretability and transparency of the evaluation results are essential core requirements, as legal practice highly depends on clear logic and verifiable conclusions. Furthermore, the bias and fairness of the model are of significant concern, as any bias in legal evaluations could have a profound impact on judicial fairness. These unique demands set higher standards for LLM judges.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.1">In response to these challenges, recent research has explored various ways in which LLMs can be effectively employed in legal evaluations. Some research used LLMs as judges to assist in <span class="ltx_text ltx_font_bold" id="S5.SS4.p2.1.1">evaluating the performance of Law LLMs</span>. For example, Yue et al. <cite class="ltx_cite ltx_citemacro_citep">(Yue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib276" title="">2023a</a>)</cite> introduced DISC-LawLLM to provide a wide range of legal services and utilized GPT-3.5 as a referee to evaluate the model’s performance. They assessed three key criteria—accuracy, completeness, and clarity—by assigning a rating score from 1 to 5. Similarly, Ryu et al. <cite class="ltx_cite ltx_citemacro_citep">(Ryu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib188" title="">2023</a>)</cite> applied retrieval-based evaluation to assess the performance of LLMs in Korean legal question-answering tasks, which applied RAG not for generation but for evaluation. What’s more, LLMs have also been utilized to construct evaluation sets. Raju et al. <cite class="ltx_cite ltx_citemacro_citep">(Raju et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib186" title="">2024</a>)</cite> explored methods for constructing these domain-specific evaluation sets, which are essential for enabling LLMs-as-judges to perform effective evaluation in legal domain. Beyond performance evaluation, LLMs have also been utilized for <span class="ltx_text ltx_font_bold" id="S5.SS4.p2.1.2">relevance judgment</span> in legal case retrieval. For instance, Ma et al. <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib155" title="">2024</a>)</cite> used LLMs to automate the evaluation of large numbers of retrieved legal documents, improving both the scalability and accuracy of legal case retrieval systems. In conclusion, the application of LLMs-as-judges in law holds significant promise in future.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5. </span>Financial</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">In the financial domain, LLM judges have been extensively explored in scenarios such as investment risk assessment and credit scoring, which presenting unique challenges. For example, the complexity of risk assessment requires LLMs to accurately capture the influence of multifaceted factors, including market volatility, regulatory changes, and geopolitical events. Real-time processing demands further elevate the challenge, requiring LLMs not only to be computationally efficient but also to deliver rapid response times. Additionally, the dynamic nature of high-frequency trading demanding that LLMs swiftly adapt to fluctuating market conditions.</p>
</div>
<div class="ltx_para" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1">In <span class="ltx_text ltx_font_bold" id="S5.SS5.p2.1.1">investment risk assessment</span>, LLMs have proven effective due to their ability to process large amounts of data and make informed judgments. For instance, Xie et al. <cite class="ltx_cite ltx_citemacro_citep">(Xie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib249" title="">2023</a>)</cite> developed a financial LLM, FinMA, fine-tuned on LLaMA to evaluate investment risks more effectively. Their model is designed to follow instructions for risk assessment and decision analysis, improving the accuracy and efficiency of financial evaluations.</p>
</div>
<div class="ltx_para" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.1">Another key application in the financial domain is <span class="ltx_text ltx_font_bold" id="S5.SS5.p3.1.1">credit scoring</span>, which predicts the future repayment ability and default risk of individuals or businesses. By analyzing a vast array of data, including credit history, financial status, and other relevant factors, LLMs can help financial institutions make more accurate credit scoring assessments. For example, Babaei et al. <cite class="ltx_cite ltx_citemacro_citep">(Babaei and Giudici, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib8" title="">2024</a>)</cite> demonstrated how LLMs can process unstructured text data, such as customer histories, contract terms, and news reports, to enhance the precision of credit assessments.</p>
</div>
<div class="ltx_para" id="S5.SS5.p4">
<p class="ltx_p" id="S5.SS5.p4.1">Furthermore, as the use of LLMs in finance continues to grow, there is a rising need to <span class="ltx_text ltx_font_bold" id="S5.SS5.p4.1.1">evaluate the performance of these financial LLMs</span>. To address this, Son et al. <cite class="ltx_cite ltx_citemacro_citep">(Son et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib202" title="">2024a</a>)</cite> developed an automated financial evaluation benchmark that leverages LLMs to extract valuable insights from both unstructured and structured data. This framework helps optimize the construction, updating, and compliance checks of financial benchmarks, supporting more efficient and scalable evaluation processes. Based on this, they facilitated the continuous optimization of financial LLMs, driving further advancements in the financial domain.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.6. </span>Education</h3>
<div class="ltx_para" id="S5.SS6.p1">
<p class="ltx_p" id="S5.SS6.p1.1">LLMs-as-judges have found extensive applications in the education domain, covering a wide range of tasks, such as grading student assignments, evaluating essays, assessing mathematical reasoning, and judging debate performance. These applications present several key challenges, including the diversity of student responses and individual differences, as well as the need for multidimensional evaluation. Effective evaluation in education requires LLMs to consider not only correctness but also creativity, clarity, and logical coherence. Additionally, the interpretability and fairness of the evaluation results are crucial, as educational assessments significantly impact students’ development and future opportunities.</p>
</div>
<div class="ltx_para" id="S5.SS6.p2">
<p class="ltx_p" id="S5.SS6.p2.1">In <span class="ltx_text ltx_font_bold" id="S5.SS6.p2.1.1">assignment grading</span>, Chiang et al. <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib36" title="">2024</a>)</cite> introduced
the concept of an LLM Teaching Assistant (LLM TA) in university classrooms, utilizing GPT-4 to
automate the grading of student assignments. By employing prompt engineering to define scoring
criteria and task descriptions, LLM TA generates quantitative scores and detailed feedback. Their
study emphasized the system’s ability to maintain consistency, adhere to grading standards, and
resist adversarial prompts, highlighting its robustness and practicality for classroom use.</p>
</div>
<div class="ltx_para" id="S5.SS6.p3">
<p class="ltx_p" id="S5.SS6.p3.1">In addition to assignment grading, LLMs-as-judges are also being explored for <span class="ltx_text ltx_font_bold" id="S5.SS6.p3.1.1">automated essay scoring.</span> Wang et al. <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib229" title="">2024c</a>)</cite> proposed an advanced intelligent essay scoring system, integrating LLMs such as BERT and ChatGPT to enable automated scoring and feedback generation for essays across various genres. Similarly, Song et al. <cite class="ltx_cite ltx_citemacro_citep">(Song et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib206" title="">2024c</a>)</cite> investigated a framework and methodology for automated essay scoring and revision based on open-source LLMs. Furthermore, Zhou et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib298" title="">2024a</a>)</cite> explored the potential of LLMs in academic paper reviewing tasks, assessing their reliability, effectiveness, and possible biases as reviewer. They found that while LLMs show certain promise in the domain of automated reviewing, they are not yet sufficient to fully replace human reviewers, particularly in areas with high technical complexity or strong innovation.</p>
</div>
<div class="ltx_para" id="S5.SS6.p4">
<p class="ltx_p" id="S5.SS6.p4.1">Another area where LLMs-as-judges are making an impact is in the evaluation of <span class="ltx_text ltx_font_bold" id="S5.SS6.p4.1.1">math reasoning</span>. Unlike traditional mathematical task evaluation, which focuses solely on the correctness of the final results, Xia et al. <cite class="ltx_cite ltx_citemacro_citep">(Xia et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib247" title="">2024a</a>)</cite> argued that additional aspects of the reasoning process should also be assessed, such as logical errors or unnecessary steps. In their work, the authors proposed ReasonEval, a new methodology for evaluating the quality of reasoning steps based on LLMs-as-judges.</p>
</div>
<div class="ltx_para" id="S5.SS6.p5">
<p class="ltx_p" id="S5.SS6.p5.1">LLMs have also been employed in <span class="ltx_text ltx_font_bold" id="S5.SS6.p5.1.1">judging debate performance</span>. Liang et al. <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib136" title="">2024a</a>)</cite> proposed Debatrix, a new method which leverages LLMs to evaluate and analyze debates. The main aspects assessed include the logical consistency of arguments, the effectiveness of rebuttals, the appropriateness of emotional expression, and the coherence of the debate.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.7. </span>Information Retrieval</h3>
<div class="ltx_para" id="S5.SS7.p1">
<p class="ltx_p" id="S5.SS7.p1.1">Information retrieval refers to the process of effectively retrieving, filtering, and ranking relevant information from a large collection of data, matching information resources to users’ needs (queries). However, evaluating these systems presents several challenges, particularly due to the the complexity of real-world data, the diversity of user needs, and personalization. To solve these challenges, LLMs-as-judges have been used across various applications, including relevance judgment, text ranking, recommendation explanations evaluation, and assessing retrieval-augmented generation (RAG) systems.</p>
</div>
<div class="ltx_para" id="S5.SS7.p2">
<p class="ltx_p" id="S5.SS7.p2.1">One key area in information retrieval is the evaluation of <span class="ltx_text ltx_font_bold" id="S5.SS7.p2.1.1">the relevance of retrieved results to user queries</span>, a task that traditionally relies on manual annotations <cite class="ltx_cite ltx_citemacro_citep">(Soboroff, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib201" title="">2024</a>; Rahmani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib184" title="">2024</a>)</cite>. Rahmani et al. <cite class="ltx_cite ltx_citemacro_citep">(Rahmani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib184" title="">2024</a>)</cite> proposed a framework called LLMJudge which leveraged LLMs to assess the relevance of information retrieval system results to user queries, providing a more scalable and efficient evaluation approach.</p>
</div>
<div class="ltx_para" id="S5.SS7.p3">
<p class="ltx_p" id="S5.SS7.p3.1">Another important aspect of information retrieval is the <span class="ltx_text ltx_font_bold" id="S5.SS7.p3.1.1">ranking of search results or recommendation lists</span>. Traditional ranking models often rely on shallow features or direct matching scores, which may not yield optimal results. To address this, Qin et al. <cite class="ltx_cite ltx_citemacro_citep">(Qin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib181" title="">2023</a>)</cite> examined the performance of LLMs in text ranking tasks and proposed a novel method based on pairwise ranking prompting, utilizing LLMs for text ranking. Additionally, Niu et al. <cite class="ltx_cite ltx_citemacro_citep">(Niu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib167" title="">2024</a>)</cite> introduced a framework called JudgeRank, which leveraged LLMs to rerank results in reasoning-intensive tasks. By evaluating the logic, relevance, and quality of candidate results, this approach tried to enhance ranking performance.</p>
</div>
<div class="ltx_para" id="S5.SS7.p4">
<p class="ltx_p" id="S5.SS7.p4.1">In recommendation systems, <span class="ltx_text ltx_font_bold" id="S5.SS7.p4.1.1">explanation evaluation</span> plays a crucial role in helping users understand why a specific product, movie, or piece of content is recommended. Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib285" title="">2024a</a>)</cite> investigated the potential of LLMs as automated evaluators of recommendation explanations, assessing them across multiple dimensions such as quality, clarity, and relevance. This approach provides a more efficient way to evaluate the effectiveness of explanations, which is essential for improving user trust and satisfaction.</p>
</div>
<div class="ltx_para" id="S5.SS7.p5">
<p class="ltx_p" id="S5.SS7.p5.1">Furthermore, with the growing use of <span class="ltx_text ltx_font_bold" id="S5.SS7.p5.1.1">retrieval-augmented generation (RAG) systems</span> in tasks like question answering, fact-checking, and customer support, there is an increasing need to evaluate the quality of these systems. Traditional evaluation methods rely on large manually annotated datasets, which are time-consuming and costly. To address this, Saad et al. <cite class="ltx_cite ltx_citemacro_citep">(Saad-Falcon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib189" title="">2023</a>)</cite> proposed a new automated evaluation framework called ARES, which leveraged LLMs as the core evaluation tool to directly assess retrieval and generated content across multiple dimensions, including relevance, accuracy, coverage, fluency, and coherence.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS8">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.8. </span>Others</h3>
<section class="ltx_subsubsection" id="S5.SS8.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.8.1. </span>Soft Engineering</h4>
<div class="ltx_para" id="S5.SS8.SSS1.p1">
<p class="ltx_p" id="S5.SS8.SSS1.p1.1">The challenges that LLMs-as-judges need to overcome in the software engineering domain include complex code structures and the diversity of evaluation criteria. A numerous of articles <cite class="ltx_cite ltx_citemacro_citep">(Patel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib176" title="">2024</a>; Weyssow et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib244" title="">2024</a>)</cite> used LLMs-as-judges to assess the quality of code generation. Moreover, Kumar et al. <cite class="ltx_cite ltx_citemacro_citep">(Kumar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib116" title="">2024</a>)</cite> employed LLMs to evaluate the quality of Bug Report Summarization.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS8.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.8.2. </span>Biology</h4>
<div class="ltx_para" id="S5.SS8.SSS2.p1">
<p class="ltx_p" id="S5.SS8.SSS2.p1.1">The main evaluation challenges in biological field include the complexity and diversity of the data and the need for specialized biological knowledge <cite class="ltx_cite ltx_citemacro_citep">(O’Donoghue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib168" title="">2023</a>; Hijazi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib87" title="">2024</a>)</cite>. For example, Hijazi et al. <cite class="ltx_cite ltx_citemacro_citep">(Hijazi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib87" title="">2024</a>)</cite> used LLMs to evaluate Query-focused summarization (QFS), which refers to generating concise and accurate summaries from a large set of biomedical literature based on a specified query. In this context, the LLMs are used to assess whether these summaries accurately answer the specified query and whether they cover the correct biological knowledge.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS8.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.8.3. </span>Social Science</h4>
<div class="ltx_para" id="S5.SS8.SSS3.p1">
<p class="ltx_p" id="S5.SS8.SSS3.p1.1">LLMs-as-Judges have also found applications in social sciences. On one hand, they are used in <span class="ltx_text ltx_font_bold" id="S5.SS8.SSS3.p1.1.1">real-world human social contexts</span>. For example, Tessler et al. <cite class="ltx_cite ltx_citemacro_citep">(Tessler et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib215" title="">2024</a>)</cite> used LLMs to participate in democratic discussions, assessing the quality of arguments, identifying fallacies, or providing a balanced view of an issue, thus helping people reach consensus on complex social and political matters. On the other hand, LLMs-as-judges are also used in <span class="ltx_text ltx_font_bold" id="S5.SS8.SSS3.p1.1.2">social scenarios constructed by language agents</span>. Zhou et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib299" title="">2023b</a>)</cite> proposed an interactive evaluation framework called Sotopia, which used LLMs to assess the social intelligence of language agents from multiple dimensions, such as emotional understanding, response adaptability, and other social skills.</p>
</div>
<div class="ltx_para" id="S5.SS8.SSS3.p2">
<p class="ltx_p" id="S5.SS8.SSS3.p2.1">In this section, we have outlined the specific applications of LLMs-as-judges across various domains. In these applications, LLMs leverage their powerful text understanding and generation capabilities to perform effective evaluations and judgments, providing accurate feedback and improvement suggestions. Although LLMs-as-judges have shown tremendous potential in these areas, especially in handling large-scale data and automating assessments, they still face challenges such as <span class="ltx_text ltx_font_bold" id="S5.SS8.SSS3.p2.1.1">the depth of domain-specific knowledge, limitations in reasoning abilities, and the diversity of evaluation criteria</span>. In the future, with continuous improvements in model performance and domain adaptation capabilities,, we believe the application of LLMs-as-judges will become more widespread and precise across various domains.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Meta-evaluation</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Meta-evaluation, the process of assessing the quality of the evaluator itself, is a crucial step in determining the reliability, consistency, and validity of LLM judges. Given the diverse applications of LLMs as evaluators, meta-evaluation methods have also been evolving. Researchers have proposed various datasets and metrics tailored to different tasks and evaluation objectives to assess the reliability and validity of LLM-based evaluations. This chapter will explore state-of-the-art Benchmarks (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS1" title="6.1. Benchmarks ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.1</span></a>) and evaluation Metrics (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S6.SS2" title="6.2. Metric ‣ 6. Meta-evaluation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">6.2</span></a>), categorize existing approaches, and discuss their advantages and limitations.</p>
</div>
<figure class="ltx_table" id="S6.T2">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 2. </span>Statistical information of different benchmarks (Part 1).</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T2.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T2.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S6.T2.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T2.3.1.1.1.1" style="font-size:80%;">Benchmark</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T2.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T2.3.1.1.2.1" style="font-size:80%;">Task</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T2.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T2.3.1.1.3.1" style="font-size:80%;">Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T2.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T2.3.1.1.4.1" style="font-size:80%;">Num</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T2.3.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T2.3.1.1.5.1" style="font-size:80%;">Evaluation Criteria</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T2.3.1.1.6"><span class="ltx_text ltx_font_bold" id="S6.T2.3.1.1.6.1" style="font-size:80%;">Language</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T2.3.2.1" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T2.3.2.1.1"><span class="ltx_text" id="S6.T2.3.2.1.1.1" style="font-size:80%;background-color:#E6E6E6;">HumanEval <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib31" title="">2021</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.3.2.1.2"><span class="ltx_text" id="S6.T2.3.2.1.2.1" style="font-size:80%;background-color:#E6E6E6;">Code</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.3.2.1.3"><span class="ltx_text" id="S6.T2.3.2.1.3.1" style="font-size:80%;background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.3.2.1.4"><span class="ltx_text" id="S6.T2.3.2.1.4.1" style="font-size:80%;background-color:#E6E6E6;">164</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.3.2.1.5"><span class="ltx_text" id="S6.T2.3.2.1.5.1" style="font-size:80%;background-color:#E6E6E6;">Functional correctness</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T2.3.2.1.6"><span class="ltx_text" id="S6.T2.3.2.1.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.3.2">
<td class="ltx_td ltx_align_left" id="S6.T2.3.3.2.1">
<span class="ltx_text" id="S6.T2.3.3.2.1.1" style="font-size:80%;">SWE-bench </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T2.3.3.2.1.2.1" style="font-size:80%;">(</span>Jimenez et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T2.3.3.2.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib102" title="">2023</a><span class="ltx_text" id="S6.T2.3.3.2.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.3.2.2"><span class="ltx_text" id="S6.T2.3.3.2.2.1" style="font-size:80%;">Code</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.3.2.3"><span class="ltx_text" id="S6.T2.3.3.2.3.1" style="font-size:80%;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.3.2.4"><span class="ltx_text" id="S6.T2.3.3.2.4.1" style="font-size:80%;">2,294</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.3.2.5"><span class="ltx_text" id="S6.T2.3.3.2.5.1" style="font-size:80%;">Task solve</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.3.2.6"><span class="ltx_text" id="S6.T2.3.3.2.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.4.3" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T2.3.4.3.1"><span class="ltx_text" id="S6.T2.3.4.3.1.1" style="font-size:80%;background-color:#E6E6E6;">DevAI <cite class="ltx_cite ltx_citemacro_citep">(Zhuge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib304" title="">2024</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.3.2"><span class="ltx_text" id="S6.T2.3.4.3.2.1" style="font-size:80%;background-color:#E6E6E6;">Code</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.3.3"><span class="ltx_text" id="S6.T2.3.4.3.3.1" style="font-size:80%;background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.3.4"><span class="ltx_text" id="S6.T2.3.4.3.4.1" style="font-size:80%;background-color:#E6E6E6;">365</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.3.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.4.3.5.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T2.3.4.3.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.4.3.5.1.1.1"><span class="ltx_text" id="S6.T2.3.4.3.5.1.1.1.1" style="font-size:80%;">Disagreement, Task solve,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.4.3.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.4.3.5.1.2.1"><span class="ltx_text" id="S6.T2.3.4.3.5.1.2.1.1" style="font-size:80%;">Requirements met</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.4.3.6"><span class="ltx_text" id="S6.T2.3.4.3.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.5.4">
<td class="ltx_td ltx_align_left" id="S6.T2.3.5.4.1">
<span class="ltx_text" id="S6.T2.3.5.4.1.1" style="font-size:80%;">CrossCodeEval </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T2.3.5.4.1.2.1" style="font-size:80%;">(</span>Ding et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T2.3.5.4.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib51" title="">2024</a><span class="ltx_text" id="S6.T2.3.5.4.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.5.4.2"><span class="ltx_text" id="S6.T2.3.5.4.2.1" style="font-size:80%;">Code</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.5.4.3"><span class="ltx_text" id="S6.T2.3.5.4.3.1" style="font-size:80%;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.5.4.4"><span class="ltx_text" id="S6.T2.3.5.4.4.1" style="font-size:80%;">1,002</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.5.4.5"><span class="ltx_text" id="S6.T2.3.5.4.5.1" style="font-size:80%;">Code match, Identifier match</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.5.4.6"><span class="ltx_text" id="S6.T2.3.5.4.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.6.5" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T2.3.6.5.1"><span class="ltx_text" id="S6.T2.3.6.5.1.1" style="font-size:80%;background-color:#E6E6E6;">CodeUltraFeedback <cite class="ltx_cite ltx_citemacro_citep">(Weyssow et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib244" title="">2024</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.6.5.2"><span class="ltx_text" id="S6.T2.3.6.5.2.1" style="font-size:80%;background-color:#E6E6E6;">Code</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.6.5.3">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.6.5.3.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T2.3.6.5.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.6.5.3.1.1.1"><span class="ltx_text" id="S6.T2.3.6.5.3.1.1.1.1" style="font-size:80%;">Pointwise</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.6.5.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.6.5.3.1.2.1"><span class="ltx_text" id="S6.T2.3.6.5.3.1.2.1.1" style="font-size:80%;">Pairwise</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.6.5.4"><span class="ltx_text" id="S6.T2.3.6.5.4.1" style="font-size:80%;background-color:#E6E6E6;">10k</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.6.5.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.6.5.5.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T2.3.6.5.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.6.5.5.1.1.1"><span class="ltx_text" id="S6.T2.3.6.5.5.1.1.1.1" style="font-size:80%;">Code explanation,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.6.5.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.6.5.5.1.2.1"><span class="ltx_text" id="S6.T2.3.6.5.5.1.2.1.1" style="font-size:80%;">Code complexity and efficiency,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.6.5.5.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.6.5.5.1.3.1"><span class="ltx_text" id="S6.T2.3.6.5.5.1.3.1.1" style="font-size:80%;">Code readability, Coding style</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.6.5.6"><span class="ltx_text" id="S6.T2.3.6.5.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.7.6">
<td class="ltx_td ltx_align_left" id="S6.T2.3.7.6.1">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.7.6.1.1">
<tr class="ltx_tr" id="S6.T2.3.7.6.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T2.3.7.6.1.1.1.1"><span class="ltx_text" id="S6.T2.3.7.6.1.1.1.1.1" style="font-size:80%;">Literary Translation</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.7.6.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T2.3.7.6.1.1.2.1">
<span class="ltx_text" id="S6.T2.3.7.6.1.1.2.1.1" style="font-size:80%;">Comparisons </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T2.3.7.6.1.1.2.1.2.1" style="font-size:80%;">(</span>Karpinska and Iyyer<span class="ltx_text" id="S6.T2.3.7.6.1.1.2.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib105" title="">2023</a><span class="ltx_text" id="S6.T2.3.7.6.1.1.2.1.4.3" style="font-size:80%;">)</span></cite>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.7.6.2"><span class="ltx_text" id="S6.T2.3.7.6.2.1" style="font-size:80%;">Translation</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.7.6.3"><span class="ltx_text" id="S6.T2.3.7.6.3.1" style="font-size:80%;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.7.6.4"><span class="ltx_text" id="S6.T2.3.7.6.4.1" style="font-size:80%;">720</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.7.6.5"><span class="ltx_text" id="S6.T2.3.7.6.5.1" style="font-size:80%;">Translation quality and errors</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.7.6.6"><span class="ltx_text" id="S6.T2.3.7.6.6.1" style="font-size:80%;">Multilingual</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.8.7" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T2.3.8.7.1"><span class="ltx_text" id="S6.T2.3.8.7.1.1" style="font-size:80%;background-color:#E6E6E6;">MQM <cite class="ltx_cite ltx_citemacro_citep">(Freitag et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib66" title="">2021a</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.8.7.2"><span class="ltx_text" id="S6.T2.3.8.7.2.1" style="font-size:80%;background-color:#E6E6E6;">Translation</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.8.7.3"><span class="ltx_text" id="S6.T2.3.8.7.3.1" style="font-size:80%;background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.8.7.4"><span class="ltx_text" id="S6.T2.3.8.7.4.1" style="font-size:80%;background-color:#E6E6E6;">100k</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.8.7.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.8.7.5.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T2.3.8.7.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.8.7.5.1.1.1"><span class="ltx_text" id="S6.T2.3.8.7.5.1.1.1.1" style="font-size:80%;">Accuracy, Fluency,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.8.7.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.8.7.5.1.2.1"><span class="ltx_text" id="S6.T2.3.8.7.5.1.2.1.1" style="font-size:80%;">Terminology, Style, Locale</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.8.7.6"><span class="ltx_text" id="S6.T2.3.8.7.6.1" style="font-size:80%;background-color:#E6E6E6;">Multilingual</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.9.8">
<td class="ltx_td ltx_align_left" id="S6.T2.3.9.8.1">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.9.8.1.1">
<tr class="ltx_tr" id="S6.T2.3.9.8.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T2.3.9.8.1.1.1.1"><span class="ltx_text" id="S6.T2.3.9.8.1.1.1.1.1" style="font-size:80%;">WMT Metrics</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.9.8.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_left" id="S6.T2.3.9.8.1.1.2.1">
<span class="ltx_text" id="S6.T2.3.9.8.1.1.2.1.1" style="font-size:80%;">Shared Task </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T2.3.9.8.1.1.2.1.2.1" style="font-size:80%;">(</span>Freitag et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T2.3.9.8.1.1.2.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib67" title="">2021b</a><span class="ltx_text" id="S6.T2.3.9.8.1.1.2.1.4.3" style="font-size:80%;">)</span></cite>
</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.9.8.2"><span class="ltx_text" id="S6.T2.3.9.8.2.1" style="font-size:80%;">Translation</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.9.8.3"><span class="ltx_text" id="S6.T2.3.9.8.3.1" style="font-size:80%;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.9.8.4"><span class="ltx_text" id="S6.T2.3.9.8.4.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.9.8.5"><span class="ltx_text" id="S6.T2.3.9.8.5.1" style="font-size:80%;">Adequacy, Fluency</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.9.8.6"><span class="ltx_text" id="S6.T2.3.9.8.6.1" style="font-size:80%;">Multilingual</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.10.9" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T2.3.10.9.1"><span class="ltx_text" id="S6.T2.3.10.9.1.1" style="font-size:80%;background-color:#E6E6E6;">SummEval <cite class="ltx_cite ltx_citemacro_citep">(Fabbri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib60" title="">2021</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.10.9.2"><span class="ltx_text" id="S6.T2.3.10.9.2.1" style="font-size:80%;background-color:#E6E6E6;">Summary</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.10.9.3"><span class="ltx_text" id="S6.T2.3.10.9.3.1" style="font-size:80%;background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.10.9.4"><span class="ltx_text" id="S6.T2.3.10.9.4.1" style="font-size:80%;background-color:#E6E6E6;">1,600</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.10.9.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.10.9.5.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T2.3.10.9.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.10.9.5.1.1.1"><span class="ltx_text" id="S6.T2.3.10.9.5.1.1.1.1" style="font-size:80%;">Coherence, Consistency,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.10.9.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.10.9.5.1.2.1"><span class="ltx_text" id="S6.T2.3.10.9.5.1.2.1.1" style="font-size:80%;">Fluency, Relevance</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.10.9.6"><span class="ltx_text" id="S6.T2.3.10.9.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.11.10">
<td class="ltx_td ltx_align_left" id="S6.T2.3.11.10.1">
<span class="ltx_text" id="S6.T2.3.11.10.1.1" style="font-size:80%;">Opinsummeval </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T2.3.11.10.1.2.1" style="font-size:80%;">(</span>Shen and Wan<span class="ltx_text" id="S6.T2.3.11.10.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib195" title="">2023</a><span class="ltx_text" id="S6.T2.3.11.10.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.11.10.2"><span class="ltx_text" id="S6.T2.3.11.10.2.1" style="font-size:80%;">Summary</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.11.10.3"><span class="ltx_text" id="S6.T2.3.11.10.3.1" style="font-size:80%;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.11.10.4"><span class="ltx_text" id="S6.T2.3.11.10.4.1" style="font-size:80%;">1,400</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.11.10.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.11.10.5.1">
<tr class="ltx_tr" id="S6.T2.3.11.10.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.11.10.5.1.1.1"><span class="ltx_text" id="S6.T2.3.11.10.5.1.1.1.1" style="font-size:80%;">Aspect relevance,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.11.10.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.11.10.5.1.2.1"><span class="ltx_text" id="S6.T2.3.11.10.5.1.2.1.1" style="font-size:80%;">Self-coherence, Readability</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.11.10.5.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.11.10.5.1.3.1"><span class="ltx_text" id="S6.T2.3.11.10.5.1.3.1.1" style="font-size:80%;">Sentiment consistency,</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.11.10.6"><span class="ltx_text" id="S6.T2.3.11.10.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.12.11" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T2.3.12.11.1"><span class="ltx_text" id="S6.T2.3.12.11.1.1" style="font-size:80%;background-color:#E6E6E6;">Frank  <cite class="ltx_cite ltx_citemacro_citep">(Pagnoni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib170" title="">2021</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.12.11.2"><span class="ltx_text" id="S6.T2.3.12.11.2.1" style="font-size:80%;background-color:#E6E6E6;">Summary</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.12.11.3"><span class="ltx_text" id="S6.T2.3.12.11.3.1" style="font-size:80%;background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.12.11.4"><span class="ltx_text" id="S6.T2.3.12.11.4.1" style="font-size:80%;background-color:#E6E6E6;">2,250</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.12.11.5"><span class="ltx_text" id="S6.T2.3.12.11.5.1" style="font-size:80%;background-color:#E6E6E6;">Factual errors</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.12.11.6"><span class="ltx_text" id="S6.T2.3.12.11.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.13.12">
<td class="ltx_td ltx_align_left" id="S6.T2.3.13.12.1">
<span class="ltx_text" id="S6.T2.3.13.12.1.1" style="font-size:80%;">Topical-Chat </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T2.3.13.12.1.2.1" style="font-size:80%;">(</span>Gopalakrishnan et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T2.3.13.12.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib73" title="">2023</a><span class="ltx_text" id="S6.T2.3.13.12.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.13.12.2"><span class="ltx_text" id="S6.T2.3.13.12.2.1" style="font-size:80%;">Dialogue</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.13.12.3"><span class="ltx_text" id="S6.T2.3.13.12.3.1" style="font-size:80%;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.13.12.4"><span class="ltx_text" id="S6.T2.3.13.12.4.1" style="font-size:80%;">60</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.13.12.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.13.12.5.1">
<tr class="ltx_tr" id="S6.T2.3.13.12.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.13.12.5.1.1.1"><span class="ltx_text" id="S6.T2.3.13.12.5.1.1.1.1" style="font-size:80%;">Understandable, Natural,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.13.12.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.13.12.5.1.2.1"><span class="ltx_text" id="S6.T2.3.13.12.5.1.2.1.1" style="font-size:80%;">Maintains context, Interesting,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.13.12.5.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.13.12.5.1.3.1"><span class="ltx_text" id="S6.T2.3.13.12.5.1.3.1.1" style="font-size:80%;">Uses knowledge, Overall quality</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.13.12.6"><span class="ltx_text" id="S6.T2.3.13.12.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.14.13" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T2.3.14.13.1"><span class="ltx_text" id="S6.T2.3.14.13.1.1" style="font-size:80%;background-color:#E6E6E6;">Personal-Chat <cite class="ltx_cite ltx_citemacro_citep">(Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib284" title="">2018</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.14.13.2"><span class="ltx_text" id="S6.T2.3.14.13.2.1" style="font-size:80%;background-color:#E6E6E6;">Dialogue</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.14.13.3"><span class="ltx_text" id="S6.T2.3.14.13.3.1" style="font-size:80%;background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.14.13.4"><span class="ltx_text" id="S6.T2.3.14.13.4.1" style="font-size:80%;background-color:#E6E6E6;">60</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.14.13.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.14.13.5.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T2.3.14.13.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.14.13.5.1.1.1"><span class="ltx_text" id="S6.T2.3.14.13.5.1.1.1.1" style="font-size:80%;">Understandable, Natural,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.14.13.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.14.13.5.1.2.1"><span class="ltx_text" id="S6.T2.3.14.13.5.1.2.1.1" style="font-size:80%;">Maintains context, Interesting,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.14.13.5.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.14.13.5.1.3.1"><span class="ltx_text" id="S6.T2.3.14.13.5.1.3.1.1" style="font-size:80%;">Uses knowledge, Overall quality</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.14.13.6"><span class="ltx_text" id="S6.T2.3.14.13.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.15.14">
<td class="ltx_td ltx_align_left" id="S6.T2.3.15.14.1">
<span class="ltx_text" id="S6.T2.3.15.14.1.1" style="font-size:80%;">DSTC10 Hidden Set </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T2.3.15.14.1.2.1" style="font-size:80%;">(</span>Zhang et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T2.3.15.14.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib280" title="">2021</a><span class="ltx_text" id="S6.T2.3.15.14.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.15.14.2"><span class="ltx_text" id="S6.T2.3.15.14.2.1" style="font-size:80%;">Dialogue</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.15.14.3"><span class="ltx_text" id="S6.T2.3.15.14.3.1" style="font-size:80%;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.15.14.4"><span class="ltx_text" id="S6.T2.3.15.14.4.1" style="font-size:80%;">9,500</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.15.14.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.15.14.5.1">
<tr class="ltx_tr" id="S6.T2.3.15.14.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.15.14.5.1.1.1"><span class="ltx_text" id="S6.T2.3.15.14.5.1.1.1.1" style="font-size:80%;">Coherence, Appropriateness,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.15.14.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.15.14.5.1.2.1"><span class="ltx_text" id="S6.T2.3.15.14.5.1.2.1.1" style="font-size:80%;">Naturalness, Toxicity control</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.15.14.6"><span class="ltx_text" id="S6.T2.3.15.14.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.16.15" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T2.3.16.15.1"><span class="ltx_text" id="S6.T2.3.16.15.1.1" style="font-size:80%;background-color:#E6E6E6;">HANNA <cite class="ltx_cite ltx_citemacro_citep">(Chhun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib35" title="">2022</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.16.15.2"><span class="ltx_text" id="S6.T2.3.16.15.2.1" style="font-size:80%;background-color:#E6E6E6;">Story</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.16.15.3"><span class="ltx_text" id="S6.T2.3.16.15.3.1" style="font-size:80%;background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.16.15.4"><span class="ltx_text" id="S6.T2.3.16.15.4.1" style="font-size:80%;background-color:#E6E6E6;">1,056</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.16.15.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.16.15.5.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T2.3.16.15.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.16.15.5.1.1.1"><span class="ltx_text" id="S6.T2.3.16.15.5.1.1.1.1" style="font-size:80%;">Relevance, Coherence,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.16.15.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.16.15.5.1.2.1"><span class="ltx_text" id="S6.T2.3.16.15.5.1.2.1.1" style="font-size:80%;">Empathy, Surprise,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.16.15.5.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.16.15.5.1.3.1"><span class="ltx_text" id="S6.T2.3.16.15.5.1.3.1.1" style="font-size:80%;">Engagement, Complexity</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.16.15.6"><span class="ltx_text" id="S6.T2.3.16.15.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.17.16">
<td class="ltx_td ltx_align_left" id="S6.T2.3.17.16.1">
<span class="ltx_text" id="S6.T2.3.17.16.1.1" style="font-size:80%;">MANS </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T2.3.17.16.1.2.1" style="font-size:80%;">(</span>Guan et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T2.3.17.16.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib74" title="">2021</a><span class="ltx_text" id="S6.T2.3.17.16.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.17.16.2"><span class="ltx_text" id="S6.T2.3.17.16.2.1" style="font-size:80%;">Story</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.17.16.3"><span class="ltx_text" id="S6.T2.3.17.16.3.1" style="font-size:80%;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.17.16.4"><span class="ltx_text" id="S6.T2.3.17.16.4.1" style="font-size:80%;">2,000</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.17.16.5"><span class="ltx_text" id="S6.T2.3.17.16.5.1" style="font-size:80%;">Coherence</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.17.16.6"><span class="ltx_text" id="S6.T2.3.17.16.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.18.17" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T2.3.18.17.1"><span class="ltx_text" id="S6.T2.3.18.17.1.1" style="font-size:80%;background-color:#E6E6E6;">StoryER <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib27" title="">2023b</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.18.17.2"><span class="ltx_text" id="S6.T2.3.18.17.2.1" style="font-size:80%;background-color:#E6E6E6;">Story</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.18.17.3"><span class="ltx_text" id="S6.T2.3.18.17.3.1" style="font-size:80%;background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.18.17.4"><span class="ltx_text" id="S6.T2.3.18.17.4.1" style="font-size:80%;background-color:#E6E6E6;">100k</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.18.17.5"><span class="ltx_text" id="S6.T2.3.18.17.5.1" style="font-size:80%;background-color:#E6E6E6;">Upvoted</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.18.17.6"><span class="ltx_text" id="S6.T2.3.18.17.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.19.18">
<td class="ltx_td ltx_align_left" id="S6.T2.3.19.18.1">
<span class="ltx_text" id="S6.T2.3.19.18.1.1" style="font-size:80%;">Per-DOC </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T2.3.19.18.1.2.1" style="font-size:80%;">(</span>Wang et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T2.3.19.18.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib230" title="">2023e</a><span class="ltx_text" id="S6.T2.3.19.18.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.19.18.2"><span class="ltx_text" id="S6.T2.3.19.18.2.1" style="font-size:80%;">Story</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.19.18.3"><span class="ltx_text" id="S6.T2.3.19.18.3.1" style="font-size:80%;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.19.18.4"><span class="ltx_text" id="S6.T2.3.19.18.4.1" style="font-size:80%;">7,000</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.19.18.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.19.18.5.1">
<tr class="ltx_tr" id="S6.T2.3.19.18.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.19.18.5.1.1.1"><span class="ltx_text" id="S6.T2.3.19.18.5.1.1.1.1" style="font-size:80%;">Interestingness, Adaptability,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.19.18.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.19.18.5.1.2.1"><span class="ltx_text" id="S6.T2.3.19.18.5.1.2.1.1" style="font-size:80%;">Character developmentSurprise, Ending</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.19.18.6"><span class="ltx_text" id="S6.T2.3.19.18.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.20.19" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T2.3.20.19.1"><span class="ltx_text" id="S6.T2.3.20.19.1.1" style="font-size:80%;background-color:#E6E6E6;">PKU-SafeRLHF <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib97" title="">2024</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.20.19.2"><span class="ltx_text" id="S6.T2.3.20.19.2.1" style="font-size:80%;background-color:#E6E6E6;">Value</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.20.19.3"><span class="ltx_text" id="S6.T2.3.20.19.3.1" style="font-size:80%;background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.20.19.4"><span class="ltx_text" id="S6.T2.3.20.19.4.1" style="font-size:80%;background-color:#E6E6E6;">83.4K</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.20.19.5"><span class="ltx_text" id="S6.T2.3.20.19.5.1" style="font-size:80%;background-color:#E6E6E6;">Helpfulness, Harmlessness</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.20.19.6"><span class="ltx_text" id="S6.T2.3.20.19.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.21.20">
<td class="ltx_td ltx_align_left" id="S6.T2.3.21.20.1">
<span class="ltx_text" id="S6.T2.3.21.20.1.1" style="font-size:80%;">HHH </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T2.3.21.20.1.2.1" style="font-size:80%;">(</span>Askell et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T2.3.21.20.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib7" title="">2021</a><span class="ltx_text" id="S6.T2.3.21.20.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.21.20.2"><span class="ltx_text" id="S6.T2.3.21.20.2.1" style="font-size:80%;">Value</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.21.20.3"><span class="ltx_text" id="S6.T2.3.21.20.3.1" style="font-size:80%;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.21.20.4"><span class="ltx_text" id="S6.T2.3.21.20.4.1" style="font-size:80%;">221</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.21.20.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.21.20.5.1">
<tr class="ltx_tr" id="S6.T2.3.21.20.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.21.20.5.1.1.1"><span class="ltx_text" id="S6.T2.3.21.20.5.1.1.1.1" style="font-size:80%;">Helpfulness, Honesty,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.21.20.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.21.20.5.1.2.1"><span class="ltx_text" id="S6.T2.3.21.20.5.1.2.1.1" style="font-size:80%;">Harmlessness</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.21.20.6"><span class="ltx_text" id="S6.T2.3.21.20.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.22.21" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T2.3.22.21.1"><span class="ltx_text" id="S6.T2.3.22.21.1.1" style="font-size:80%;background-color:#E6E6E6;">Cvalue <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib256" title="">2023b</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.22.21.2"><span class="ltx_text" id="S6.T2.3.22.21.2.1" style="font-size:80%;background-color:#E6E6E6;">Value</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.22.21.3"><span class="ltx_text" id="S6.T2.3.22.21.3.1" style="font-size:80%;background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.22.21.4"><span class="ltx_text" id="S6.T2.3.22.21.4.1" style="font-size:80%;background-color:#E6E6E6;">145k</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.22.21.5"><span class="ltx_text" id="S6.T2.3.22.21.5.1" style="font-size:80%;background-color:#E6E6E6;">Safety, Responsibility</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.22.21.6"><span class="ltx_text" id="S6.T2.3.22.21.6.1" style="font-size:80%;background-color:#E6E6E6;">Chinese</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.23.22">
<td class="ltx_td ltx_align_left" id="S6.T2.3.23.22.1">
<span class="ltx_text" id="S6.T2.3.23.22.1.1" style="font-size:80%;">Yelp  </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T2.3.23.22.1.2.1" style="font-size:80%;">(</span>Asghar<span class="ltx_text" id="S6.T2.3.23.22.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib5" title="">2016</a><span class="ltx_text" id="S6.T2.3.23.22.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.23.22.2"><span class="ltx_text" id="S6.T2.3.23.22.2.1" style="font-size:80%;">Recom</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.23.22.3"><span class="ltx_text" id="S6.T2.3.23.22.3.1" style="font-size:80%;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.23.22.4"><span class="ltx_text" id="S6.T2.3.23.22.4.1" style="font-size:80%;">8,630k</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.23.22.5"><span class="ltx_text" id="S6.T2.3.23.22.5.1" style="font-size:80%;">User perference</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.23.22.6"><span class="ltx_text" id="S6.T2.3.23.22.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.24.23" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T2.3.24.23.1"><span class="ltx_text" id="S6.T2.3.24.23.1.1" style="font-size:80%;background-color:#E6E6E6;">Movielens_Explanation <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib285" title="">2024a</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.24.23.2"><span class="ltx_text" id="S6.T2.3.24.23.2.1" style="font-size:80%;background-color:#E6E6E6;">Recom</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.24.23.3"><span class="ltx_text" id="S6.T2.3.24.23.3.1" style="font-size:80%;background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.24.23.4"><span class="ltx_text" id="S6.T2.3.24.23.4.1" style="font-size:80%;background-color:#E6E6E6;">2,496</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.24.23.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.24.23.5.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T2.3.24.23.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.24.23.5.1.1.1"><span class="ltx_text" id="S6.T2.3.24.23.5.1.1.1.1" style="font-size:80%;">Persuasiveness, Transparency,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.24.23.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.24.23.5.1.2.1"><span class="ltx_text" id="S6.T2.3.24.23.5.1.2.1.1" style="font-size:80%;">Accuracy, Satisfaction</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.24.23.6"><span class="ltx_text" id="S6.T2.3.24.23.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.25.24">
<td class="ltx_td ltx_align_left" id="S6.T2.3.25.24.1">
<span class="ltx_text" id="S6.T2.3.25.24.1.1" style="font-size:80%;">Trec DL21&amp;22  </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T2.3.25.24.1.2.1" style="font-size:80%;">(</span>Craswell et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T2.3.25.24.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib43" title="">2021</a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib44" title="">2022</a><span class="ltx_text" id="S6.T2.3.25.24.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.25.24.2"><span class="ltx_text" id="S6.T2.3.25.24.2.1" style="font-size:80%;">Search</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.25.24.3"><span class="ltx_text" id="S6.T2.3.25.24.3.1" style="font-size:80%;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.25.24.4">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.25.24.4.1">
<tr class="ltx_tr" id="S6.T2.3.25.24.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.25.24.4.1.1.1"><span class="ltx_text" id="S6.T2.3.25.24.4.1.1.1.1" style="font-size:80%;">1,549/</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.25.24.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.25.24.4.1.2.1"><span class="ltx_text" id="S6.T2.3.25.24.4.1.2.1.1" style="font-size:80%;">2,673</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.25.24.5"><span class="ltx_text" id="S6.T2.3.25.24.5.1" style="font-size:80%;">Relevacne</span></td>
<td class="ltx_td ltx_align_center" id="S6.T2.3.25.24.6"><span class="ltx_text" id="S6.T2.3.25.24.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.26.25" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left ltx_border_b" id="S6.T2.3.26.25.1"><span class="ltx_text" id="S6.T2.3.26.25.1.1" style="font-size:80%;background-color:#E6E6E6;">LeCarDv2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib130" title="">2024d</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T2.3.26.25.2"><span class="ltx_text" id="S6.T2.3.26.25.2.1" style="font-size:80%;background-color:#E6E6E6;">Search</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T2.3.26.25.3"><span class="ltx_text" id="S6.T2.3.26.25.3.1" style="font-size:80%;background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T2.3.26.25.4"><span class="ltx_text" id="S6.T2.3.26.25.4.1" style="font-size:80%;background-color:#E6E6E6;">55,192</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T2.3.26.25.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T2.3.26.25.5.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T2.3.26.25.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.26.25.5.1.1.1"><span class="ltx_text" id="S6.T2.3.26.25.5.1.1.1.1" style="font-size:80%;">Characterization relevance,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.26.25.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.26.25.5.1.2.1"><span class="ltx_text" id="S6.T2.3.26.25.5.1.2.1.1" style="font-size:80%;">Penalty relevance,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T2.3.26.25.5.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T2.3.26.25.5.1.3.1"><span class="ltx_text" id="S6.T2.3.26.25.5.1.3.1.1" style="font-size:80%;">Procedure relevance</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T2.3.26.25.6"><span class="ltx_text" id="S6.T2.3.26.25.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S6.T3">
<figcaption class="ltx_caption" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 3. </span>Statistical information of different benchmarks (Part 2).</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S6.T3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T3.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S6.T3.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T3.3.1.1.1.1" style="font-size:80%;">Benchmark</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T3.3.1.1.2.1" style="font-size:80%;">Domain</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T3.3.1.1.3.1" style="font-size:80%;">Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T3.3.1.1.4.1" style="font-size:80%;">Num</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.3.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T3.3.1.1.5.1" style="font-size:80%;">Evaluation Criteria</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S6.T3.3.1.1.6"><span class="ltx_text ltx_font_bold" id="S6.T3.3.1.1.6.1" style="font-size:80%;">Language</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T3.3.2.1" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T3.3.2.1.1"><span class="ltx_text" id="S6.T3.3.2.1.1.1" style="font-size:80%;background-color:#E6E6E6;">UltraFeedback <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib45" title="">2024</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.3.2.1.2"><span class="ltx_text" id="S6.T3.3.2.1.2.1" style="font-size:80%;background-color:#E6E6E6;">Compre.</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.3.2.1.3"><span class="ltx_text" id="S6.T3.3.2.1.3.1" style="font-size:80%;background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.3.2.1.4"><span class="ltx_text" id="S6.T3.3.2.1.4.1" style="font-size:80%;background-color:#E6E6E6;">64k</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.3.2.1.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.2.1.5.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T3.3.2.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.2.1.5.1.1.1"><span class="ltx_text" id="S6.T3.3.2.1.5.1.1.1.1" style="font-size:80%;">Helpfulness, Honesty,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.2.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.2.1.5.1.2.1"><span class="ltx_text" id="S6.T3.3.2.1.5.1.2.1.1" style="font-size:80%;">Instruction following,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.2.1.5.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.2.1.5.1.3.1"><span class="ltx_text" id="S6.T3.3.2.1.5.1.3.1.1" style="font-size:80%;">Truthfulness</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T3.3.2.1.6"><span class="ltx_text" id="S6.T3.3.2.1.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.3.2">
<td class="ltx_td ltx_align_left" id="S6.T3.3.3.2.1">
<span class="ltx_text" id="S6.T3.3.3.2.1.1" style="font-size:80%;">AlpacaEval </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T3.3.3.2.1.2.1" style="font-size:80%;">(</span>Dubois et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T3.3.3.2.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib57" title="">2024</a><span class="ltx_text" id="S6.T3.3.3.2.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.3.2.2"><span class="ltx_text" id="S6.T3.3.3.2.2.1" style="font-size:80%;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.3.2.3"><span class="ltx_text" id="S6.T3.3.3.2.3.1" style="font-size:80%;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.3.2.4"><span class="ltx_text" id="S6.T3.3.3.2.4.1" style="font-size:80%;">20k</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.3.2.5"><span class="ltx_text" id="S6.T3.3.3.2.5.1" style="font-size:80%;">Instruction-following</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.3.2.6"><span class="ltx_text" id="S6.T3.3.3.2.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.4.3" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T3.3.4.3.1"><span class="ltx_text" id="S6.T3.3.4.3.1.1" style="font-size:80%;background-color:#E6E6E6;">Chatbot Arena <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.4.3.2"><span class="ltx_text" id="S6.T3.3.4.3.2.1" style="font-size:80%;background-color:#E6E6E6;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.4.3.3"><span class="ltx_text" id="S6.T3.3.4.3.3.1" style="font-size:80%;background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.4.3.4"><span class="ltx_text" id="S6.T3.3.4.3.4.1" style="font-size:80%;background-color:#E6E6E6;">33k</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.4.3.5"><span class="ltx_text" id="S6.T3.3.4.3.5.1" style="font-size:80%;background-color:#E6E6E6;">User perference</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.4.3.6"><span class="ltx_text" id="S6.T3.3.4.3.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.5.4">
<td class="ltx_td ltx_align_left" id="S6.T3.3.5.4.1">
<span class="ltx_text" id="S6.T3.3.5.4.1.1" style="font-size:80%;">MTBench </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T3.3.5.4.1.2.1" style="font-size:80%;">(</span>Zheng et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T3.3.5.4.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a><span class="ltx_text" id="S6.T3.3.5.4.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.5.4.2"><span class="ltx_text" id="S6.T3.3.5.4.2.1" style="font-size:80%;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.5.4.3"><span class="ltx_text" id="S6.T3.3.5.4.3.1" style="font-size:80%;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.5.4.4"><span class="ltx_text" id="S6.T3.3.5.4.4.1" style="font-size:80%;">3,000</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.5.4.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.5.4.5.1">
<tr class="ltx_tr" id="S6.T3.3.5.4.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.5.4.5.1.1.1"><span class="ltx_text" id="S6.T3.3.5.4.5.1.1.1.1" style="font-size:80%;">Multi-turn conversational,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.5.4.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.5.4.5.1.2.1"><span class="ltx_text" id="S6.T3.3.5.4.5.1.2.1.1" style="font-size:80%;">Instruction-following</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.5.4.6"><span class="ltx_text" id="S6.T3.3.5.4.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.6.5" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T3.3.6.5.1"><span class="ltx_text" id="S6.T3.3.6.5.1.1" style="font-size:80%;background-color:#E6E6E6;">RewardBench <cite class="ltx_cite ltx_citemacro_citep">(Lambert et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib117" title="">2024</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.6.5.2"><span class="ltx_text" id="S6.T3.3.6.5.2.1" style="font-size:80%;background-color:#E6E6E6;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.6.5.3"><span class="ltx_text" id="S6.T3.3.6.5.3.1" style="font-size:80%;background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.6.5.4"><span class="ltx_text" id="S6.T3.3.6.5.4.1" style="font-size:80%;background-color:#E6E6E6;">2,998</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.6.5.5"><span class="ltx_text" id="S6.T3.3.6.5.5.1" style="font-size:80%;background-color:#E6E6E6;">User perference</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.6.5.6"><span class="ltx_text" id="S6.T3.3.6.5.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.7.6">
<td class="ltx_td ltx_align_left" id="S6.T3.3.7.6.1">
<span class="ltx_text" id="S6.T3.3.7.6.1.1" style="font-size:80%;">JudgerBench </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T3.3.7.6.1.2.1" style="font-size:80%;">(</span>Cao et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T3.3.7.6.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib21" title="">2024a</a><span class="ltx_text" id="S6.T3.3.7.6.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.7.6.2"><span class="ltx_text" id="S6.T3.3.7.6.2.1" style="font-size:80%;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.7.6.3"><span class="ltx_text" id="S6.T3.3.7.6.3.1" style="font-size:80%;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.7.6.4"><span class="ltx_text" id="S6.T3.3.7.6.4.1" style="font-size:80%;">1,900</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.7.6.5"><span class="ltx_text" id="S6.T3.3.7.6.5.1" style="font-size:80%;">Instruction following</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.7.6.6">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.7.6.6.1">
<tr class="ltx_tr" id="S6.T3.3.7.6.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.7.6.6.1.1.1"><span class="ltx_text" id="S6.T3.3.7.6.6.1.1.1.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.7.6.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.7.6.6.1.2.1"><span class="ltx_text" id="S6.T3.3.7.6.6.1.2.1.1" style="font-size:80%;">Chinese</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.8.7" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T3.3.8.7.1"><span class="ltx_text" id="S6.T3.3.8.7.1.1" style="font-size:80%;background-color:#E6E6E6;">RM-Benchh <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib149" title="">2024b</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.8.7.2"><span class="ltx_text" id="S6.T3.3.8.7.2.1" style="font-size:80%;background-color:#E6E6E6;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.8.7.3"><span class="ltx_text" id="S6.T3.3.8.7.3.1" style="font-size:80%;background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.8.7.4"><span class="ltx_text" id="S6.T3.3.8.7.4.1" style="font-size:80%;background-color:#E6E6E6;">1,327</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.8.7.5"><span class="ltx_text" id="S6.T3.3.8.7.5.1" style="font-size:80%;background-color:#E6E6E6;">Instruction following</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.8.7.6"><span class="ltx_text" id="S6.T3.3.8.7.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.9.8">
<td class="ltx_td ltx_align_left" id="S6.T3.3.9.8.1">
<span class="ltx_text" id="S6.T3.3.9.8.1.1" style="font-size:80%;">JUDGEBENCH </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T3.3.9.8.1.2.1" style="font-size:80%;">(</span>Tan et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T3.3.9.8.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib214" title="">2024</a><span class="ltx_text" id="S6.T3.3.9.8.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.9.8.2"><span class="ltx_text" id="S6.T3.3.9.8.2.1" style="font-size:80%;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.9.8.3"><span class="ltx_text" id="S6.T3.3.9.8.3.1" style="font-size:80%;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.9.8.4"><span class="ltx_text" id="S6.T3.3.9.8.4.1" style="font-size:80%;">350</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.9.8.5"><span class="ltx_text" id="S6.T3.3.9.8.5.1" style="font-size:80%;">Factual, Logical correctness</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.9.8.6"><span class="ltx_text" id="S6.T3.3.9.8.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.10.9" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T3.3.10.9.1"><span class="ltx_text" id="S6.T3.3.10.9.1.1" style="font-size:80%;background-color:#E6E6E6;">Infinity-Preference<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text" id="footnote1.1.1.1" style="font-size:125%;">1</span></span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/BAAI/Infinity-Preference" style="font-size:125%;" title="">https://huggingface.co/datasets/BAAI/Infinity-Preference</a></span></span></span></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.10.9.2"><span class="ltx_text" id="S6.T3.3.10.9.2.1" style="font-size:80%;background-color:#E6E6E6;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.10.9.3"><span class="ltx_text" id="S6.T3.3.10.9.3.1" style="font-size:80%;background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.10.9.4"><span class="ltx_text" id="S6.T3.3.10.9.4.1" style="font-size:80%;background-color:#E6E6E6;">59.3k</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.10.9.5"><span class="ltx_text" id="S6.T3.3.10.9.5.1" style="font-size:80%;background-color:#E6E6E6;">User perference</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.10.9.6">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.10.9.6.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T3.3.10.9.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.10.9.6.1.1.1"><span class="ltx_text" id="S6.T3.3.10.9.6.1.1.1.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.10.9.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.10.9.6.1.2.1"><span class="ltx_text" id="S6.T3.3.10.9.6.1.2.1.1" style="font-size:80%;">Chinese</span></td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.11.10">
<td class="ltx_td ltx_align_left" id="S6.T3.3.11.10.1">
<span class="ltx_text" id="S6.T3.3.11.10.1.1" style="font-size:80%;">LLMeval </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T3.3.11.10.1.2.1" style="font-size:80%;">(</span>Zhang et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T3.3.11.10.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib286" title="">2023b</a><span class="ltx_text" id="S6.T3.3.11.10.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.11.10.2"><span class="ltx_text" id="S6.T3.3.11.10.2.1" style="font-size:80%;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.11.10.3">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.11.10.3.1">
<tr class="ltx_tr" id="S6.T3.3.11.10.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.11.10.3.1.1.1"><span class="ltx_text" id="S6.T3.3.11.10.3.1.1.1.1" style="font-size:80%;">Pointwise</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.11.10.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.11.10.3.1.2.1"><span class="ltx_text" id="S6.T3.3.11.10.3.1.2.1.1" style="font-size:80%;">Pairwise</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.11.10.4"><span class="ltx_text" id="S6.T3.3.11.10.4.1" style="font-size:80%;">453</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.11.10.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.11.10.5.1">
<tr class="ltx_tr" id="S6.T3.3.11.10.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.11.10.5.1.1.1"><span class="ltx_text" id="S6.T3.3.11.10.5.1.1.1.1" style="font-size:80%;">Correctness, Fluency, Logic,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.11.10.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.11.10.5.1.2.1"><span class="ltx_text" id="S6.T3.3.11.10.5.1.2.1.1" style="font-size:80%;">Informativeness, Harmlessness</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.11.10.6"><span class="ltx_text" id="S6.T3.3.11.10.6.1" style="font-size:80%;">Chinese</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.12.11" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T3.3.12.11.1"><span class="ltx_text" id="S6.T3.3.12.11.1.1" style="font-size:80%;background-color:#E6E6E6;">WildBench <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib139" title="">2024</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.12.11.2"><span class="ltx_text" id="S6.T3.3.12.11.2.1" style="font-size:80%;background-color:#E6E6E6;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.12.11.3"><span class="ltx_text" id="S6.T3.3.12.11.3.1" style="font-size:80%;background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.12.11.4"><span class="ltx_text" id="S6.T3.3.12.11.4.1" style="font-size:80%;background-color:#E6E6E6;">1,024</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.12.11.5"><span class="ltx_text" id="S6.T3.3.12.11.5.1" style="font-size:80%;background-color:#E6E6E6;">Checklists</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.12.11.6"><span class="ltx_text" id="S6.T3.3.12.11.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.13.12">
<td class="ltx_td ltx_align_left" id="S6.T3.3.13.12.1">
<span class="ltx_text" id="S6.T3.3.13.12.1.1" style="font-size:80%;">Flask </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T3.3.13.12.1.2.1" style="font-size:80%;">(</span>Ye et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T3.3.13.12.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib270" title="">2023b</a><span class="ltx_text" id="S6.T3.3.13.12.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.13.12.2"><span class="ltx_text" id="S6.T3.3.13.12.2.1" style="font-size:80%;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.13.12.3"><span class="ltx_text" id="S6.T3.3.13.12.3.1" style="font-size:80%;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.13.12.4"><span class="ltx_text" id="S6.T3.3.13.12.4.1" style="font-size:80%;">1,740</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.13.12.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.13.12.5.1">
<tr class="ltx_tr" id="S6.T3.3.13.12.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.13.12.5.1.1.1"><span class="ltx_text" id="S6.T3.3.13.12.5.1.1.1.1" style="font-size:80%;">Logical thinking,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.13.12.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.13.12.5.1.2.1"><span class="ltx_text" id="S6.T3.3.13.12.5.1.2.1.1" style="font-size:80%;">Background knowledge,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.13.12.5.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.13.12.5.1.3.1"><span class="ltx_text" id="S6.T3.3.13.12.5.1.3.1.1" style="font-size:80%;">Problem handling, User alignment</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.13.12.6"><span class="ltx_text" id="S6.T3.3.13.12.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.14.13" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T3.3.14.13.1"><span class="ltx_text" id="S6.T3.3.14.13.1.1" style="font-size:80%;background-color:#E6E6E6;">AlignBench <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib144" title="">2023b</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.14.13.2"><span class="ltx_text" id="S6.T3.3.14.13.2.1" style="font-size:80%;background-color:#E6E6E6;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.14.13.3"><span class="ltx_text" id="S6.T3.3.14.13.3.1" style="font-size:80%;background-color:#E6E6E6;">Pointwise</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.14.13.4"><span class="ltx_text" id="S6.T3.3.14.13.4.1" style="font-size:80%;background-color:#E6E6E6;">683</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.14.13.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.14.13.5.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T3.3.14.13.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.14.13.5.1.1.1"><span class="ltx_text" id="S6.T3.3.14.13.5.1.1.1.1" style="font-size:80%;">Task-oriented, Clarity &amp; Fluency,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.14.13.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.14.13.5.1.2.1"><span class="ltx_text" id="S6.T3.3.14.13.5.1.2.1.1" style="font-size:80%;">Complexity &amp; Difficulty,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.14.13.5.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.14.13.5.1.3.1"><span class="ltx_text" id="S6.T3.3.14.13.5.1.3.1.1" style="font-size:80%;">Desensitization</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.14.13.6"><span class="ltx_text" id="S6.T3.3.14.13.6.1" style="font-size:80%;background-color:#E6E6E6;">Chinese</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.15.14">
<td class="ltx_td ltx_align_left" id="S6.T3.3.15.14.1">
<span class="ltx_text" id="S6.T3.3.15.14.1.1" style="font-size:80%;">HELPSTEER </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T3.3.15.14.1.2.1" style="font-size:80%;">(</span>Wang et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T3.3.15.14.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib239" title="">2023a</a><span class="ltx_text" id="S6.T3.3.15.14.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.15.14.2"><span class="ltx_text" id="S6.T3.3.15.14.2.1" style="font-size:80%;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.15.14.3">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.15.14.3.1">
<tr class="ltx_tr" id="S6.T3.3.15.14.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.15.14.3.1.1.1"><span class="ltx_text" id="S6.T3.3.15.14.3.1.1.1.1" style="font-size:80%;">Pointwise</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.15.14.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.15.14.3.1.2.1"><span class="ltx_text" id="S6.T3.3.15.14.3.1.2.1.1" style="font-size:80%;">Pairwise</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.15.14.4"><span class="ltx_text" id="S6.T3.3.15.14.4.1" style="font-size:80%;">37,120</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.15.14.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.15.14.5.1">
<tr class="ltx_tr" id="S6.T3.3.15.14.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.15.14.5.1.1.1"><span class="ltx_text" id="S6.T3.3.15.14.5.1.1.1.1" style="font-size:80%;">Helpfulness, Correctness,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.15.14.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.15.14.5.1.2.1"><span class="ltx_text" id="S6.T3.3.15.14.5.1.2.1.1" style="font-size:80%;">Coherence, Complexity Verbosity</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.15.14.6"><span class="ltx_text" id="S6.T3.3.15.14.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.16.15" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left" id="S6.T3.3.16.15.1"><span class="ltx_text" id="S6.T3.3.16.15.1.1" style="font-size:80%;background-color:#E6E6E6;">HELPSTEER2 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib238" title="">2024a</a>)</cite></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.16.15.2"><span class="ltx_text" id="S6.T3.3.16.15.2.1" style="font-size:80%;background-color:#E6E6E6;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.16.15.3">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.16.15.3.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T3.3.16.15.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.16.15.3.1.1.1"><span class="ltx_text" id="S6.T3.3.16.15.3.1.1.1.1" style="font-size:80%;">Pointwise</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.16.15.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.16.15.3.1.2.1"><span class="ltx_text" id="S6.T3.3.16.15.3.1.2.1.1" style="font-size:80%;">Pairwise</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.16.15.4"><span class="ltx_text" id="S6.T3.3.16.15.4.1" style="font-size:80%;background-color:#E6E6E6;">21,362</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.16.15.5">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.16.15.5.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S6.T3.3.16.15.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.16.15.5.1.1.1"><span class="ltx_text" id="S6.T3.3.16.15.5.1.1.1.1" style="font-size:80%;">Helpfulness, Correctness,</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.16.15.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.16.15.5.1.2.1"><span class="ltx_text" id="S6.T3.3.16.15.5.1.2.1.1" style="font-size:80%;">Coherence, Complexity, Verbosity</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.16.15.6"><span class="ltx_text" id="S6.T3.3.16.15.6.1" style="font-size:80%;background-color:#E6E6E6;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.17.16">
<td class="ltx_td ltx_align_left" id="S6.T3.3.17.16.1">
<span class="ltx_text" id="S6.T3.3.17.16.1.1" style="font-size:80%;">MLLM-as-a-Judge </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="S6.T3.3.17.16.1.2.1" style="font-size:80%;">(</span>Chen et al<span class="ltx_text">.</span><span class="ltx_text" id="S6.T3.3.17.16.1.3.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib25" title="">2024b</a><span class="ltx_text" id="S6.T3.3.17.16.1.4.3" style="font-size:80%;">)</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.17.16.2"><span class="ltx_text" id="S6.T3.3.17.16.2.1" style="font-size:80%;">Compre.</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.17.16.3">
<table class="ltx_tabular ltx_align_middle" id="S6.T3.3.17.16.3.1">
<tr class="ltx_tr" id="S6.T3.3.17.16.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.17.16.3.1.1.1"><span class="ltx_text" id="S6.T3.3.17.16.3.1.1.1.1" style="font-size:80%;">Pointwise</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.17.16.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.17.16.3.1.2.1"><span class="ltx_text" id="S6.T3.3.17.16.3.1.2.1.1" style="font-size:80%;">Pairwise</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.17.16.3.1.3">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.17.16.3.1.3.1"><span class="ltx_text" id="S6.T3.3.17.16.3.1.3.1.1" style="font-size:80%;">Listwise</span></td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.17.16.4"><span class="ltx_text" id="S6.T3.3.17.16.4.1" style="font-size:80%;">17,000</span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.17.16.5"><span class="ltx_text" id="S6.T3.3.17.16.5.1" style="font-size:80%;color:#4A4A4A;">
<span class="ltx_tabular ltx_align_middle" id="S6.T3.3.17.16.5.1.1">
<span class="ltx_tr" id="S6.T3.3.17.16.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.17.16.5.1.1.1.1">Relevance, Accuracy,</span></span>
<span class="ltx_tr" id="S6.T3.3.17.16.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S6.T3.3.17.16.5.1.1.2.1">Creativity, Response granularity</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S6.T3.3.17.16.6"><span class="ltx_text" id="S6.T3.3.17.16.6.1" style="font-size:80%;">English</span></td>
</tr>
<tr class="ltx_tr" id="S6.T3.3.18.17" style="background-color:#E6E6E6;">
<td class="ltx_td ltx_align_left ltx_border_b" id="S6.T3.3.18.17.1"><span class="ltx_text" id="S6.T3.3.18.17.1.1" style="font-size:80%;background-color:#E6E6E6;">MM-EvalMM-Eval <cite class="ltx_cite ltx_citemacro_citep">(Son et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib203" title="">2024b</a>)</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.3.18.17.2"><span class="ltx_text" id="S6.T3.3.18.17.2.1" style="font-size:80%;background-color:#E6E6E6;">Compre.</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.3.18.17.3"><span class="ltx_text" id="S6.T3.3.18.17.3.1" style="font-size:80%;background-color:#E6E6E6;">Pairwise</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.3.18.17.4"><span class="ltx_text" id="S6.T3.3.18.17.4.1" style="font-size:80%;background-color:#E6E6E6;">4,981</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.3.18.17.5"><span class="ltx_text" id="S6.T3.3.18.17.5.1" style="font-size:80%;background-color:#E6E6E6;">Task-oriented</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S6.T3.3.18.17.6"><span class="ltx_text" id="S6.T3.3.18.17.6.1" style="font-size:80%;background-color:#E6E6E6;">Multilingual</span></td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Benchmarks</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">To evaluate LLM-based judges, a common approach is to measure their alignment with human preferences, as human judgments are often considered the gold standard for quality and reliability. Given the diverse range of applications for LLM-based judges, different benchmarks have been created, each tailored to specific evaluation criteria and use cases.
In this section, we present a comprehensive collection of 40 widely-used benchmarks, each designed to capture different aspects of evaluation, such as language understanding, factual accuracy, coherence, creativity, and fairness. To enhance clarity and facilitate comparison, we categorize these benchmarks by application domain.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span>Code Generation</h4>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">Code generation aims to produce executable program code from natural language input. This task typically involves translating user requirements or descriptions into precise code. The applications of code generation are vast, including automated script creation, bug fixing, and the generation of complex programming tasks.Evaluating code generation is highly challenging, and LLMs are increasingly being used as evaluators for assessing code quality.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p2">
<p class="ltx_p" id="S6.SS1.SSS1.p2.1">HumanEval <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib31" title="">2021</a>)</cite> is a widely used benchmark dataset designed to evaluate programming capabilities. It consists of 164 coding tasks, each accompanied by a brief natural language description. The tasks primarily involve algorithmic problems and data structure exercises, with difficulty levels ranging from basic to intermediate.
One notable feature of HumanEval <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib31" title="">2021</a>)</cite> is the inclusion of input-output examples, which facilitate the assessment of functional correctness. However, the dataset’s limited size and scope may not sufficiently capture the diversity of real-world programming challenges.
SWEBench <cite class="ltx_cite ltx_citemacro_citep">(Jimenez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib102" title="">2023</a>)</cite> targets more complex programming tasks that are closer to real-world software development scenarios. It includes 2,294 tasks requiring advanced operations such as reasoning, multi-step problem solving, and API usage. Unlike simpler benchmarks, SWEBench <cite class="ltx_cite ltx_citemacro_citep">(Jimenez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib102" title="">2023</a>)</cite> assesses the model’s ability to handle comprehensive problem-solving and logical reasoning. However, the increased complexity also introduces challenges in establishing consistent evaluation criteria, particularly when it comes to subjective aspects like code style and efficiency. Moreover, DevAI <cite class="ltx_cite ltx_citemacro_citep">(Zhuge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib304" title="">2024</a>)</cite> was introduced to address the limitations of existing benchmarks, which often fail to capture the iterative nature of software development and lack adequate signals for measuring long-term progress. The dataset includes 365 task requirements, focusing on more complex and challenging programming scenarios.
CrossCodeEval <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib51" title="">2024</a>)</cite> focuses on assessing cross-language programming models, containing over 1,000 tasks that involve translating code between different programming language pairs, such as Python to Java or JavaScript to C++. This dataset tests the model’s ability to adapt and transform code across languages, highlighting the challenges of understanding varied syntax and semantics. CodeUltraFeedback <cite class="ltx_cite ltx_citemacro_citep">(Weyssow et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib244" title="">2024</a>)</cite> is designed to evaluate and enhance the alignment between LLMs and user-defined programming preferences. It includes 10,000 programming instructions, each paired with four responses from 14 different LLMs. These responses are scored by GPT-3.5 based on five distinct programming preferences, such as readability, efficiency, and adherence to user specifications. The dataset emphasizes fine-grained feedback and user-centered evaluation, making it a useful tool for analyzing preference alignment.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2. </span>Machine Translation</h4>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1">Machine Translation (MT) refers to the process of automatically translating text from a source language to a target language. Over time, MT technology has progressed significantly, evolving from rule-based methods to Statistical Machine Translation (SMT), and more recently to Neural Machine Translation (NMT), which is now the dominant approach. With the widespread adoption of NMT and the emergence of LLMs, evaluating translation quality has become a complex task, requiring robust evaluation frameworks that can assess accuracy, fluency, and contextual relevance across diverse language pairs.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p2">
<p class="ltx_p" id="S6.SS1.SSS2.p2.1">The Workshop on Machine Translation (WMT) <cite class="ltx_cite ltx_citemacro_citep">(Freitag et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib67" title="">2021b</a>)</cite> is a prominent annual evaluation event in the field of MT. It provides large-scale, human-annotated datasets for a variety of language pairs, including English-French, English-German, and English-Russian. Each year, WMT releases benchmark datasets that include source texts, model-generated translations, reference translations, and human evaluation scores. These datasets are widely used for assessing the performance of automated evaluation metrics by comparing their outputs against human judgments. WMT covers a broad range of tasks, from sentence-level translation to document-level and domain-specific challenges, making it a comprehensive resource for evaluating the correlation between automated evaluators and human assessments. However, WMT primarily focuses on high-resource languages, which may limit its applicability to low-resource or underrepresented languages.
Literary Translation Comparisons <cite class="ltx_cite ltx_citemacro_citep">(Karpinska and Iyyer, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib105" title="">2023</a>)</cite> is designed to assess document-level translation quality, particularly in the context of literary works. It includes carefully selected paragraphs from various literary pieces, covering 18 language pairs such as Japanese-English, Polish-English, and French-English. Unlike sentence-level benchmarks, this dataset emphasizes the importance of evaluating translations in a broader context, as literary texts often require understanding of stylistic elements and cultural subtleties. This makes it particularly useful for evaluating the performance of LLMs, which may excel in capturing broader contextual information.
The MQM <cite class="ltx_cite ltx_citemacro_citep">(Freitag et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib66" title="">2021a</a>)</cite> study is the largest evaluation effort to date focusing on machine translation quality. It involves professional translators annotating the outputs of top-performing systems from the WMT 2020 shared task, specifically targeting English-German and Chinese-English translations. MQM introduces a multidimensional quality assessment framework that goes beyond traditional metrics like BLEU or ROUGE. It evaluates translations across multiple dimensions, including accuracy, fluency, terminology, style, and locale, providing a more nuanced understanding of translation quality.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3. </span>Text Summarization</h4>
<div class="ltx_para" id="S6.SS1.SSS3.p1">
<p class="ltx_p" id="S6.SS1.SSS3.p1.1">Text Summarization (TS) is the task of generating a concise and coherent summary from a given piece of text while preserving its essential meaning. The main goal is to provide a quick, accurate overview of the source content, capturing key information and eliminating unnecessary details. As LLMs have shown impressive capabilities in generating summaries, the need for robust meta-evaluation benchmarks is critical to effectively assess their performance across various dimensions like coherence, relevance, consistency, and fluency.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p2">
<p class="ltx_p" id="S6.SS1.SSS3.p2.1">SummEval <cite class="ltx_cite ltx_citemacro_citep">(Fabbri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib60" title="">2021</a>)</cite> is one of the most widely used benchmarks for evaluating summarization models. It includes summaries generated by 16 different models based on 100 news articles randomly sampled from the CNN/DailyMail test set. Each summary was annotated by five independent crowd-sourced workers and three expert evaluators, using a Likert scale from 1 to 5 across four key dimensions: coherence, consistency, fluency, and relevance. The dataset is valuable for analyzing the correlation between human judgments and automated evaluation metrics.
The FRANK <cite class="ltx_cite ltx_citemacro_citep">(Pagnoni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib170" title="">2021</a>)</cite> dataset is dedicated to assessing the factual accuracy of summaries generated by automatic summarization systems. It provides detailed human annotations of factual errors, including semantic frame errors, discourse errors, and content verifiability issues. The dataset includes summaries from both the CNN/DailyMail and XSum datasets, making it a comprehensive resource for evaluating factual correctness. FRANK’s detailed categorization of errors offers valuable insights into the types of factual inaccuracies common in generated summaries, highlighting areas where LLMs often struggle. However, focusing solely on factual errors may overlook other aspects of summary quality, such as coherence and fluency.
OpinsummEval <cite class="ltx_cite ltx_citemacro_citep">(Shen and Wan, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib195" title="">2023</a>)</cite> is a meta-evaluation benchmark specifically designed for opinion summarization tasks, where the goal is to extract and summarize opinions from a large volume of user reviews. This dataset includes outputs from 14 different opinion summarization models and provides human annotations across four dimensions: aspect relevance, self-consistency, sentiment consistency, and readability.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.4. </span>Dialogue Generation</h4>
<div class="ltx_para" id="S6.SS1.SSS4.p1">
<p class="ltx_p" id="S6.SS1.SSS4.p1.1">Dialogue Generation is the task of automatically generating natural language conversations that are relevant to a given context. The primary goal is to develop dialogue systems that can understand context, generate fluent responses, and maintain logical consistency and contextual accuracy. Dialogue generation encompasses a wide range of applications, from chatbots and virtual assistants to social conversational agents. With the increasing capabilities of large language models (LLMs), evaluating dialogue generation has become more complex, requiring multi-faceted evaluation frameworks to assess various aspects of conversational quality.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS4.p2">
<p class="ltx_p" id="S6.SS1.SSS4.p2.1">In the field of dialogue generation, the most commonly used datasets include Topical-Chat <cite class="ltx_cite ltx_citemacro_citep">(Gopalakrishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib73" title="">2023</a>)</cite> and PERSONA-CHAT <cite class="ltx_cite ltx_citemacro_citep">(Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib284" title="">2018</a>)</cite>. The Topical-Chat <cite class="ltx_cite ltx_citemacro_citep">(Gopalakrishnan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib73" title="">2023</a>)</cite> dataset aims to advance research in open-domain conversational AI, covering eight major topics such as entertainment, health, and technology. The PERSONA-CHAT <cite class="ltx_cite ltx_citemacro_citep">(Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib284" title="">2018</a>)</cite> dataset, on the other hand, focuses on enhancing dialogue systems by incorporating predefined personas to generate more personalized responses. Each dialogue participant is assigned a persona profile, consisting of several descriptive sentences about their personality or preferences.
Mehri and Eskenazi <cite class="ltx_cite ltx_citemacro_citep">(Mehri and Eskenazi, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib157" title="">2020</a>)</cite> conducted a meta-evaluation study on these two widely-used open-domain dialogue corpora. They manually annotated 60 dialogue contexts from each dataset, with six responses per context for Topical-Chat and five for PERSONA-CHAT <cite class="ltx_cite ltx_citemacro_citep">(Zhang, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib284" title="">2018</a>)</cite>, including both model-generated and human responses. Each response was evaluated across six key dimensions: naturalness, coherence, engagement, groundedness, understandability, and overall quality. This study highlights the importance of multi-dimensional evaluation in dialogue generation, providing valuable insights into the strengths and weaknesses of different dialogue models.
Additionally, the dataset from DSTC10 Track 5 <cite class="ltx_cite ltx_citemacro_citep">(Yoshino et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib273" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib280" title="">2021</a>)</cite> focuses on evaluating open-domain dialogue systems and is designed for automatic evaluation and moderation of dialogue systems. The challenge aims to develop automatic evaluation mechanisms that accurately reflect human judgments while effectively handling harmful user inputs, maintaining conversational flow and engagement. The dataset includes annotations across four aspects: coherence, appropriateness, naturalness, and toxicity control.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.5. </span>Automatic Story Generation</h4>
<div class="ltx_para" id="S6.SS1.SSS5.p1">
<p class="ltx_p" id="S6.SS1.SSS5.p1.1">Automatic Story Generation (ASG) is a challenging task that aims to enable models to create coherent, engaging narratives based on a given prompt or context. It emulates human storytelling abilities by generating stories that exhibit a logical structure, compelling characters, and interesting plot developments. Evaluating story generation systems is inherently complex, as it involves assessing not only linguistic quality but also narrative elements like coherence, engagement, and surprise.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS5.p2">
<p class="ltx_p" id="S6.SS1.SSS5.p2.2">The HANNA <cite class="ltx_cite ltx_citemacro_citep">(Chhun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib35" title="">2022</a>)</cite> dataset is tailored for evaluating automatic story generation (ASG), featuring 1,056 stories generated by 10 different systems from 96 prompts. Each story is annotated by three human reviewers across six criteria: relevance, coherence, resonance, surprise, engagement, and complexity. This comprehensive annotation framework provides a detailed assessment of narrative quality, making HANNA a valuable benchmark for comparing ASG models.
Another notable dataset is the MANS <cite class="ltx_cite ltx_citemacro_citep">(Guan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib74" title="">2021</a>)</cite>, which forms part of the OpenMEVA <cite class="ltx_cite ltx_citemacro_citep">(Guan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib74" title="">2021</a>)</cite> framework. It compiles stories from various natural language generation models using well-known corpora like ROCStories <cite class="ltx_cite ltx_citemacro_citep">(Mostafazadeh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib160" title="">2016</a>)</cite> and WritingPrompts <cite class="ltx_cite ltx_citemacro_citep">(Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib61" title="">2018</a>)</cite>. MANS <cite class="ltx_cite ltx_citemacro_citep">(Guan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib74" title="">2021</a>)</cite> focuses on manual annotations of narrative elements, serving as a robust testbed for exploring diverse evaluation metrics.
The StoryER <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib27" title="">2023b</a>)</cite> dataset offers a distinct approach to evaluating story generation by focusing on preference prediction and aspect-based rating. StoryER is divided into two primary components: the first is a 100k Story Ranking Data, which pairs stories from the WritingPrompts dataset. Each pair includes one story with high user engagement (upvotes <math alttext="\geq" class="ltx_Math" display="inline" id="S6.SS1.SSS5.p2.1.m1.1"><semantics id="S6.SS1.SSS5.p2.1.m1.1a"><mo id="S6.SS1.SSS5.p2.1.m1.1.1" xref="S6.SS1.SSS5.p2.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS5.p2.1.m1.1b"><geq id="S6.SS1.SSS5.p2.1.m1.1.1.cmml" xref="S6.SS1.SSS5.p2.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS5.p2.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.SSS5.p2.1.m1.1d">≥</annotation></semantics></math> 50) and another with low engagement (upvotes <math alttext="\leq" class="ltx_Math" display="inline" id="S6.SS1.SSS5.p2.2.m2.1"><semantics id="S6.SS1.SSS5.p2.2.m2.1a"><mo id="S6.SS1.SSS5.p2.2.m2.1.1" xref="S6.SS1.SSS5.p2.2.m2.1.1.cmml">≤</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS5.p2.2.m2.1b"><leq id="S6.SS1.SSS5.p2.2.m2.1.1.cmml" xref="S6.SS1.SSS5.p2.2.m2.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS5.p2.2.m2.1c">\leq</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.SSS5.p2.2.m2.1d">≤</annotation></semantics></math> 0). This component leverages real-world user feedback to capture implicit preferences, providing a practical basis for training models to predict story quality. The second component, Aspect Rating and Reasoning Data, contains 46,000 entries where annotators provide detailed ratings (on a scale of 1-5) for various story aspects such as introduction, character development, and plot description, along with explanatory comments. This combination of quantitative rankings and qualitative reasoning enables a nuanced evaluation of stories, making StoryER particularly useful for both automated scoring and interpretability research.
The PERSER <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib230" title="">2023e</a>)</cite> dataset takes a different approach by addressing the subjectivity inherent in open-domain text generation evaluations. PERSER restructures existing datasets and introduces personalized tags, resulting in two sub-datasets: Per-MPST and Per-DOC. Per-MPST is an adapted version of the Movie Plot Synopsis Dataset, while Per-DOC includes 7,000 instances of paired stories generated from the same premise. These stories are evaluated based on dimensions such as interestingness, adaptability, surprise, character development, and the quality of the ending.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.6. </span>Values Alignment</h4>
<div class="ltx_para" id="S6.SS1.SSS6.p1">
<p class="ltx_p" id="S6.SS1.SSS6.p1.1">Values alignment is a critical task in the development of AI systems, focused on ensuring that their behavior and decisions consistently reflect core human values and ethical standards. In the context of LLM-as-Judge, the alignment process is vital to verify that the model’s outputs adhere to societal norms and ethical principles, minimizing risks related to harmful, biased, or unethical behavior. To support research and model development in values alignment, several datasets have been created, each with unique characteristics designed to evaluate or enhance the ethical behavior of LLMs.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS6.p2">
<p class="ltx_p" id="S6.SS1.SSS6.p2.1">One notable dataset is PKU-SafeRLHF <cite class="ltx_cite ltx_citemacro_citep">(Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib97" title="">2024</a>)</cite>, which was specifically curated for studying safe alignment in large language models. The dataset comprises 83.4K preference entries, focusing on two primary dimensions: harmlessness and usefulness. In each sample, the dataset presents a pair of model responses to a given prompt, annotated with safety meta-labels and preferences based on the levels of safety and utility.
Another influential dataset is the HHH <cite class="ltx_cite ltx_citemacro_citep">(Askell et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib7" title="">2021</a>)</cite> (Honesty, Helpfulness, and Harmlessness) dataset, designed to evaluate LLM performance across various human-model interaction scenarios. The dataset emphasizes three core human-centered values: honesty, helpfulness, and harmlessness. It includes a diverse collection of conversational examples where models are tested on their adherence to these values. By exposing models to a wide range of contexts, the HHH dataset serves as a comprehensive benchmark for assessing whether LLMs align with essential ethical standards and effectively mitigate risks of misinformation, harmful advice, or biased outputs.
Moreover, the CVALUES <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib256" title="">2023b</a>)</cite> benchmark is a more recent contribution aimed at evaluating human values alignment specifically for Chinese LLMs. It represents the first comprehensive framework tailored to assess values alignment in the Chinese language context, focusing on two critical criteria: Safety and Responsibility.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS7">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.7. </span>Recommendation</h4>
<div class="ltx_para" id="S6.SS1.SSS7.p1">
<p class="ltx_p" id="S6.SS1.SSS7.p1.1">Recommendation systems aim to provide personalized suggestions based on users’ preferences and historical behavior. As the use of large language models (LLMs) expands, their role in evaluating the performance of recommendation systems has garnered increasing attention. LLMs can serve as versatile evaluators, offering insights into multiple aspects of recommendation systems beyond traditional metrics like accuracy. They can assess factors such as user engagement, satisfaction, and the quality of generated explanations.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS7.p2">
<p class="ltx_p" id="S6.SS1.SSS7.p2.1">The MovieLens <cite class="ltx_cite ltx_citemacro_citep">(Harper and Konstan, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib81" title="">2015</a>)</cite> dataset is a widely-used public dataset for movie recommendations, available in multiple versions with varying scales, ranging from thousands of users and ratings to millions. Zhang et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib285" title="">2024a</a>)</cite> further annotated the MovieLens <cite class="ltx_cite ltx_citemacro_citep">(Harper and Konstan, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib81" title="">2015</a>)</cite> data to create a sub-dataset featuring user self-explanation texts. In this sub-dataset, users write explanatory texts after being presented with a recommended movie. These explanations are then rated on a five-point Likert scale across four dimensions: Persuasiveness, Transparency, Accuracy, and Satisfaction. This annotated data provides valuable reference texts for LLMs in the context of explainability evaluation.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS7.p3">
<p class="ltx_p" id="S6.SS1.SSS7.p3.1">Another commonly used dataset is the Yelp dataset <cite class="ltx_cite ltx_citemacro_citep">(Asghar, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib5" title="">2016</a>)</cite>, which contains detailed review data from 11 metropolitan areas, covering approximately 150,000 businesses, nearly 7 million user reviews, and over 200,000 images. User reviews include ratings for businesses, such as hotel ratings (1-5 stars), as well as additional feedback like “cool” and “funny” votes. Furthermore, the Yelp dataset provides extensive business attribute information (e.g., operating hours, parking availability, and delivery options), offering rich contextual information that can be leveraged for developing and evaluating recommendation systems.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS8">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.8. </span>Search</h4>
<div class="ltx_para" id="S6.SS1.SSS8.p1">
<p class="ltx_p" id="S6.SS1.SSS8.p1.1">The search task is a fundamental component of information retrieval (IR), focusing on identifying the most relevant documents from extensive text collections based on user queries. Traditionally, relevance assessments in search tasks have been conducted by human annotators following established guidelines. However, recent advances in large language models (LLMs) have opened up new opportunities for utilizing these models as evaluators, offering an automated and scalable approach to relevance assessment.
With the advent of retrieval-augmented generation (RAG) models, the role of LLMs as evaluators has expanded. There is now a growing need to assess various dimensions of retrieved contexts, including context relevance, answer faithfulness, and answer relevance. This shift highlights the potential of LLMs to provide nuanced judgments that go beyond simple topical relevance.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS8.p2">
<p class="ltx_p" id="S6.SS1.SSS8.p2.1">A key resource for evaluating the performance of LLMs as relevance assessors is the series of datasets from the Text Retrieval Conference (TREC). TREC workshops aim to advance research in IR by offering large-scale test collections, standardized evaluation procedures, and a platform for benchmarking retrieval models.
The datasets from the TREC Deep Learning Track <cite class="ltx_cite ltx_citemacro_citep">(Lawrie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib119" title="">2024</a>)</cite>, specifically from 2021 (DL21) <cite class="ltx_cite ltx_citemacro_citep">(Craswell et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib43" title="">2021</a>)</cite> and 2022 (DL22) <cite class="ltx_cite ltx_citemacro_citep">(Craswell et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib44" title="">2022</a>)</cite>, are commonly used for this purpose. These datasets are derived from the expanded MS MARCO v2 collection <cite class="ltx_cite ltx_citemacro_citep">(Bajaj et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib12" title="">2016</a>)</cite>, which contains approximately 138 million passages. Relevance judgments are provided by assessors from the National Institute of Standards and Technology (NIST) using a 4-point scale (0 to 3). This structured and fine-grained annotation scheme allows for a detailed comparison between LLM-generated relevance scores and human judgments.
While general-purpose datasets offer valuable benchmarks, specialized retrieval tasks often require domain-specific datasets that reflect unique relevance criteria. One notable example is LeCaRDv2 <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib130" title="">2024d</a>)</cite>, a large-scale dataset tailored for legal case retrieval. LeCaRDv2 enriches the concept of relevance by incorporating three distinct aspects: characterization, penalty, and procedure. These additional criteria provide a more comprehensive perspective on relevance.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS9">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.9. </span>Comprehensive Data</h4>
<div class="ltx_para" id="S6.SS1.SSS9.p1">
<p class="ltx_p" id="S6.SS1.SSS9.p1.1">To thoroughly assess the role of LLMs-as-Judges and better align them with human preferences, a diverse set of comprehensive datasets has been developed. These datasets provide large-scale, well-annotated data, allowing for the effective training and evaluation of LLMs in complex, real-world contexts. As a result, they contribute to improving the models’ reliability and effectiveness in their role as evaluators.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS9.p2">
<p class="ltx_p" id="S6.SS1.SSS9.p2.1">Datasets such as HelpSteer <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib239" title="">2023a</a>)</cite> and HelpSteer2 <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib238" title="">2024a</a>)</cite> are designed to improve the alignment and usefulness of LLMs. They provide multi-attribute data, enabling the training of models that can generate responses that are factually correct, coherent, and tailored to diverse user preferences. These open-source datasets support adjustments in response complexity and verbosity, catering to varying user needs. Additionally, UltraFeedback <cite class="ltx_cite ltx_citemacro_citep">(Cui et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib45" title="">2024</a>)</cite> offers a large-scale dataset with around 64,000 prompts from sources like UltraChat <cite class="ltx_cite ltx_citemacro_citep">(Ding et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib50" title="">2023</a>)</cite>, ShareGPT <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib38" title="">2023</a>)</cite>, and TruthfulQA <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib141" title="">2021</a>)</cite>. It includes multiple responses per prompt generated by different LLMs, with high-quality preference labels and textual feedback covering aspects like instruction-following, truthfulness, and helpfulness. UltraFeedback’s fine-grained annotations and diverse prompts provide a robust resource for training reward and critic models, enhancing the evaluative capabilities of LLMs.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS9.p3">
<p class="ltx_p" id="S6.SS1.SSS9.p3.1">In exploring instruction following and dialogue capabilities, specialized tools like AlpacaEval <cite class="ltx_cite ltx_citemacro_citep">(Dubois et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib57" title="">2024</a>)</cite>, alongside interactive platforms such as Chatbot Arena <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>)</cite> and benchmarks like MT-Bench <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>)</cite>, provide critical insights. AlpacaEval is an automated evaluation tool using GPT-4 or Claude as evaluators. It assesses chat-based LLMs against the AlpacaFarm dataset, providing win-rate calculations across a variety of tasks, enabling rapid and cost-effective comparisons with baseline models like GPT-3.5 (Davinci-003). Chatbot Arena, on the other hand, offers a user-driven evaluation framework where participants interact with anonymous models and vote based on their preferences. The platform has collected over 1,000,000 user votes, using the Bradley-Terry model to rank LLMs and chatbots, providing valuable insights into user preferences and model performance in open-domain dialogue.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS9.p4">
<p class="ltx_p" id="S6.SS1.SSS9.p4.1">Benchmarks like WildBench <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib139" title="">2024</a>)</cite> and FLASK <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib270" title="">2023b</a>)</cite> aim to evaluate LLMs on tasks more reflective of real-world applications. WildBench <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib139" title="">2024</a>)</cite> collects challenging examples from real users via the AI2 WildChat project, providing fine-grained annotations, task types, and checklists for response quality evaluation, and employs length-penalized Elo ratings to ensure unbiased assessments. FLASK <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib270" title="">2023b</a>)</cite> introduces a fine-grained evaluation protocol that decomposes overall scoring into skill set-level scoring for each instruction, enhancing interpretability and reliability in both human-based and model-based evaluations. Additionally, comprehensive evaluations covering multiple domains—including factual question answering, reading comprehension, summarization, mathematical problem-solving, reasoning, poetry generation, and programming—have been conducted. These evaluations involve assessing models across multiple criteria such as correctness, fluency, informativeness, logicality, and harmlessness.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS9.p5">
<p class="ltx_p" id="S6.SS1.SSS9.p5.1">Reward models and LLM-based judges face the crucial task of ensuring alignment with human expectations, a challenge addressed by datasets like RewardBench <cite class="ltx_cite ltx_citemacro_citep">(Lambert et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib117" title="">2024</a>)</cite>, RM-Bench <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib149" title="">2024b</a>)</cite>, and JudgerBench <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib21" title="">2024a</a>)</cite>.
RewardBench <cite class="ltx_cite ltx_citemacro_citep">(Lambert et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib117" title="">2024</a>)</cite> focuses on assessing models through complex prompt-choice trios, covering diverse areas like chat, reasoning, and safety, with a particular emphasis on out-of-distribution scenarios. RM-Bench <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib149" title="">2024b</a>)</cite> introduces a new benchmark for evaluating reward models based on their sensitivity to subtle content differences and resistance to stylistic biases, emphasizing the need for refined assessments that correlate highly with aligned language models’ performance.
JudgerBench <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib21" title="">2024a</a>)</cite>, with its dual components (JDB-A and JDB-B), offers a structured framework for evaluating alignment and critique abilities. By including data from human voting results and combining insights from varied sources, JudgerBench <cite class="ltx_cite ltx_citemacro_citep">(Cao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib21" title="">2024a</a>)</cite> provides a nuanced understanding of model performance across different languages and dialogue formats.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS9.p6">
<p class="ltx_p" id="S6.SS1.SSS9.p6.1">With the growing complexity of tasks handled by LLMs, there is an increasing demand for more objective and reliable evaluation frameworks. JUDGEBENCH <cite class="ltx_cite ltx_citemacro_citep">(Tan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib214" title="">2024</a>)</cite> proposes a novel approach to assessing LLM-based judges on challenging response pairs across domains like knowledge, reasoning, mathematics, and coding. It addresses the limitations of existing benchmarks by introducing preference labels that reflect objective correctness, providing a robust platform for evaluating the capabilities of advanced LLM-based judges.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS9.p7">
<p class="ltx_p" id="S6.SS1.SSS9.p7.1">As LLMs evolve beyond text-only tasks, evaluation frameworks have expanded to encompass multimodal and multilingual contexts. MLLM-as-a-Judge <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib25" title="">2024b</a>)</cite> serves as a benchmark for assessing Multimodal LLMs, covering tasks like image description, mathematical reasoning, and infographic interpretation. By integrating human annotations, it provides a comprehensive evaluation across visual and textual domains, reflecting the growing demand for models capable of processing diverse inputs. In a parallel effort, MM-Eval <cite class="ltx_cite ltx_citemacro_citep">(Son et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib203" title="">2024b</a>)</cite> addresses the multilingual aspect, offering extensive analysis across 18 languages. With core subsets like Chat, Reasoning, and Linguistics, alongside a broader Language Resource subset spanning 122 languages, MM-Eval <cite class="ltx_cite ltx_citemacro_citep">(Son et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib203" title="">2024b</a>)</cite> highlights performance discrepancies, especially in low-resource languages where models tend to default to neutral scores.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Metric</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">The evaluation of LLMs-as-Judges models centers around assessing the extent to which the model’s judgments align with human evaluations, which are typically considered the benchmark for quality. Given the complexity and subjectivity of many evaluation tasks, achieving high agreement with human ratings is a key indicator of the LLM’s performance. To quantify this agreement, a range of statistical metrics is employed. Below, we outline these metrics and their applications in evaluating LLMs-as-Judges models.</p>
</div>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1. </span>Accuracy</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">Accuracy is a fundamental metric used to assess the proportion of correct judgments made by the LLM compared to human evaluations. In classification tasks, it is defined as:</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p2">
<table class="ltx_equation ltx_eqn_table" id="S6.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Accuracy}=\frac{\text{Number of Correct Predictions}}{\text{Total Number%
 of Predictions}}," class="ltx_Math" display="block" id="S6.E2.m1.1"><semantics id="S6.E2.m1.1a"><mrow id="S6.E2.m1.1.1.1" xref="S6.E2.m1.1.1.1.1.cmml"><mrow id="S6.E2.m1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.cmml"><mtext id="S6.E2.m1.1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.2a.cmml">Accuracy</mtext><mo id="S6.E2.m1.1.1.1.1.1" xref="S6.E2.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S6.E2.m1.1.1.1.1.3" xref="S6.E2.m1.1.1.1.1.3.cmml"><mtext id="S6.E2.m1.1.1.1.1.3.2" xref="S6.E2.m1.1.1.1.1.3.2a.cmml">Number of Correct Predictions</mtext><mtext id="S6.E2.m1.1.1.1.1.3.3" xref="S6.E2.m1.1.1.1.1.3.3a.cmml">Total Number of Predictions</mtext></mfrac></mrow><mo id="S6.E2.m1.1.1.1.2" xref="S6.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E2.m1.1b"><apply id="S6.E2.m1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1"><eq id="S6.E2.m1.1.1.1.1.1.cmml" xref="S6.E2.m1.1.1.1.1.1"></eq><ci id="S6.E2.m1.1.1.1.1.2a.cmml" xref="S6.E2.m1.1.1.1.1.2"><mtext id="S6.E2.m1.1.1.1.1.2.cmml" xref="S6.E2.m1.1.1.1.1.2">Accuracy</mtext></ci><apply id="S6.E2.m1.1.1.1.1.3.cmml" xref="S6.E2.m1.1.1.1.1.3"><divide id="S6.E2.m1.1.1.1.1.3.1.cmml" xref="S6.E2.m1.1.1.1.1.3"></divide><ci id="S6.E2.m1.1.1.1.1.3.2a.cmml" xref="S6.E2.m1.1.1.1.1.3.2"><mtext id="S6.E2.m1.1.1.1.1.3.2.cmml" xref="S6.E2.m1.1.1.1.1.3.2">Number of Correct Predictions</mtext></ci><ci id="S6.E2.m1.1.1.1.1.3.3a.cmml" xref="S6.E2.m1.1.1.1.1.3.3"><mtext id="S6.E2.m1.1.1.1.1.3.3.cmml" xref="S6.E2.m1.1.1.1.1.3.3">Total Number of Predictions</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E2.m1.1c">\text{Accuracy}=\frac{\text{Number of Correct Predictions}}{\text{Total Number%
 of Predictions}},</annotation><annotation encoding="application/x-llamapun" id="S6.E2.m1.1d">Accuracy = divide start_ARG Number of Correct Predictions end_ARG start_ARG Total Number of Predictions end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p3">
<p class="ltx_p" id="S6.SS2.SSS1.p3.1">where the number of correct predictions corresponds to instances where the LLM’s judgment matches the human evaluator’s judgment. While accuracy is simple to compute and intuitive, it may not fully capture the quality of the model, especially when dealing with tasks that involve nuanced or continuous evaluations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2. </span>Pearson Correlation Coefficient</h4>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">The Pearson Correlation Coefficient <cite class="ltx_cite ltx_citemacro_citep">(Cohen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib42" title="">2009</a>)</cite> measures the linear relationship between two continuous variables, in this case, the evaluation scores assigned by the LLM and those assigned by human evaluators. It is defined as:</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S6.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(3)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="r=\frac{\sum(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sqrt{\sum(x_{i}-\bar{x})^{2}\sum(%
y_{i}-\bar{y})^{2}}}," class="ltx_Math" display="block" id="S6.E3.m1.5"><semantics id="S6.E3.m1.5a"><mrow id="S6.E3.m1.5.5.1" xref="S6.E3.m1.5.5.1.1.cmml"><mrow id="S6.E3.m1.5.5.1.1" xref="S6.E3.m1.5.5.1.1.cmml"><mi id="S6.E3.m1.5.5.1.1.2" xref="S6.E3.m1.5.5.1.1.2.cmml">r</mi><mo id="S6.E3.m1.5.5.1.1.1" xref="S6.E3.m1.5.5.1.1.1.cmml">=</mo><mfrac id="S6.E3.m1.4.4" xref="S6.E3.m1.4.4.cmml"><mrow id="S6.E3.m1.2.2.2" xref="S6.E3.m1.2.2.2.cmml"><mo id="S6.E3.m1.2.2.2.3" rspace="0em" xref="S6.E3.m1.2.2.2.3.cmml">∑</mo><mrow id="S6.E3.m1.2.2.2.2" xref="S6.E3.m1.2.2.2.2.cmml"><mrow id="S6.E3.m1.1.1.1.1.1.1" xref="S6.E3.m1.1.1.1.1.1.1.1.cmml"><mo id="S6.E3.m1.1.1.1.1.1.1.2" stretchy="false" xref="S6.E3.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E3.m1.1.1.1.1.1.1.1" xref="S6.E3.m1.1.1.1.1.1.1.1.cmml"><msub id="S6.E3.m1.1.1.1.1.1.1.1.2" xref="S6.E3.m1.1.1.1.1.1.1.1.2.cmml"><mi id="S6.E3.m1.1.1.1.1.1.1.1.2.2" xref="S6.E3.m1.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S6.E3.m1.1.1.1.1.1.1.1.2.3" xref="S6.E3.m1.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S6.E3.m1.1.1.1.1.1.1.1.1" xref="S6.E3.m1.1.1.1.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S6.E3.m1.1.1.1.1.1.1.1.3" xref="S6.E3.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S6.E3.m1.1.1.1.1.1.1.1.3.2" xref="S6.E3.m1.1.1.1.1.1.1.1.3.2.cmml">x</mi><mo id="S6.E3.m1.1.1.1.1.1.1.1.3.1" xref="S6.E3.m1.1.1.1.1.1.1.1.3.1.cmml">¯</mo></mover></mrow><mo id="S6.E3.m1.1.1.1.1.1.1.3" stretchy="false" xref="S6.E3.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S6.E3.m1.2.2.2.2.3" xref="S6.E3.m1.2.2.2.2.3.cmml">⁢</mo><mrow id="S6.E3.m1.2.2.2.2.2.1" xref="S6.E3.m1.2.2.2.2.2.1.1.cmml"><mo id="S6.E3.m1.2.2.2.2.2.1.2" stretchy="false" xref="S6.E3.m1.2.2.2.2.2.1.1.cmml">(</mo><mrow id="S6.E3.m1.2.2.2.2.2.1.1" xref="S6.E3.m1.2.2.2.2.2.1.1.cmml"><msub id="S6.E3.m1.2.2.2.2.2.1.1.2" xref="S6.E3.m1.2.2.2.2.2.1.1.2.cmml"><mi id="S6.E3.m1.2.2.2.2.2.1.1.2.2" xref="S6.E3.m1.2.2.2.2.2.1.1.2.2.cmml">y</mi><mi id="S6.E3.m1.2.2.2.2.2.1.1.2.3" xref="S6.E3.m1.2.2.2.2.2.1.1.2.3.cmml">i</mi></msub><mo id="S6.E3.m1.2.2.2.2.2.1.1.1" xref="S6.E3.m1.2.2.2.2.2.1.1.1.cmml">−</mo><mover accent="true" id="S6.E3.m1.2.2.2.2.2.1.1.3" xref="S6.E3.m1.2.2.2.2.2.1.1.3.cmml"><mi id="S6.E3.m1.2.2.2.2.2.1.1.3.2" xref="S6.E3.m1.2.2.2.2.2.1.1.3.2.cmml">y</mi><mo id="S6.E3.m1.2.2.2.2.2.1.1.3.1" xref="S6.E3.m1.2.2.2.2.2.1.1.3.1.cmml">¯</mo></mover></mrow><mo id="S6.E3.m1.2.2.2.2.2.1.3" stretchy="false" xref="S6.E3.m1.2.2.2.2.2.1.1.cmml">)</mo></mrow></mrow></mrow><msqrt id="S6.E3.m1.4.4.4" xref="S6.E3.m1.4.4.4.cmml"><mrow id="S6.E3.m1.4.4.4.2.2" xref="S6.E3.m1.4.4.4.2.2.cmml"><mo id="S6.E3.m1.4.4.4.2.2.3" rspace="0em" xref="S6.E3.m1.4.4.4.2.2.3.cmml">∑</mo><mrow id="S6.E3.m1.4.4.4.2.2.2" xref="S6.E3.m1.4.4.4.2.2.2.cmml"><msup id="S6.E3.m1.3.3.3.1.1.1.1" xref="S6.E3.m1.3.3.3.1.1.1.1.cmml"><mrow id="S6.E3.m1.3.3.3.1.1.1.1.1.1" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.cmml"><mo id="S6.E3.m1.3.3.3.1.1.1.1.1.1.2" stretchy="false" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.cmml"><msub id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2.cmml"><mi id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2.2" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2.2.cmml">x</mi><mi id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2.3" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.1" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.3" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.3.cmml"><mi id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.3.2" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.3.2.cmml">x</mi><mo id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.3.1" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.3.1.cmml">¯</mo></mover></mrow><mo id="S6.E3.m1.3.3.3.1.1.1.1.1.1.3" stretchy="false" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="S6.E3.m1.3.3.3.1.1.1.1.3" xref="S6.E3.m1.3.3.3.1.1.1.1.3.cmml">2</mn></msup><mo id="S6.E3.m1.4.4.4.2.2.2.3" xref="S6.E3.m1.4.4.4.2.2.2.3.cmml">⁢</mo><mrow id="S6.E3.m1.4.4.4.2.2.2.2" xref="S6.E3.m1.4.4.4.2.2.2.2.cmml"><mo id="S6.E3.m1.4.4.4.2.2.2.2.2" rspace="0em" xref="S6.E3.m1.4.4.4.2.2.2.2.2.cmml">∑</mo><msup id="S6.E3.m1.4.4.4.2.2.2.2.1" xref="S6.E3.m1.4.4.4.2.2.2.2.1.cmml"><mrow id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.cmml"><mo id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.2" stretchy="false" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.cmml">(</mo><mrow id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.cmml"><msub id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2.cmml"><mi id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2.2" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2.2.cmml">y</mi><mi id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2.3" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.1" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.1.cmml">−</mo><mover accent="true" id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.3" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.3.cmml"><mi id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.3.2" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.3.2.cmml">y</mi><mo id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.3.1" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.3.1.cmml">¯</mo></mover></mrow><mo id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.3" stretchy="false" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.cmml">)</mo></mrow><mn id="S6.E3.m1.4.4.4.2.2.2.2.1.3" xref="S6.E3.m1.4.4.4.2.2.2.2.1.3.cmml">2</mn></msup></mrow></mrow></mrow></msqrt></mfrac></mrow><mo id="S6.E3.m1.5.5.1.2" xref="S6.E3.m1.5.5.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E3.m1.5b"><apply id="S6.E3.m1.5.5.1.1.cmml" xref="S6.E3.m1.5.5.1"><eq id="S6.E3.m1.5.5.1.1.1.cmml" xref="S6.E3.m1.5.5.1.1.1"></eq><ci id="S6.E3.m1.5.5.1.1.2.cmml" xref="S6.E3.m1.5.5.1.1.2">𝑟</ci><apply id="S6.E3.m1.4.4.cmml" xref="S6.E3.m1.4.4"><divide id="S6.E3.m1.4.4.5.cmml" xref="S6.E3.m1.4.4"></divide><apply id="S6.E3.m1.2.2.2.cmml" xref="S6.E3.m1.2.2.2"><sum id="S6.E3.m1.2.2.2.3.cmml" xref="S6.E3.m1.2.2.2.3"></sum><apply id="S6.E3.m1.2.2.2.2.cmml" xref="S6.E3.m1.2.2.2.2"><times id="S6.E3.m1.2.2.2.2.3.cmml" xref="S6.E3.m1.2.2.2.2.3"></times><apply id="S6.E3.m1.1.1.1.1.1.1.1.cmml" xref="S6.E3.m1.1.1.1.1.1.1"><minus id="S6.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.1"></minus><apply id="S6.E3.m1.1.1.1.1.1.1.1.2.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E3.m1.1.1.1.1.1.1.1.2.1.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S6.E3.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S6.E3.m1.1.1.1.1.1.1.1.2.3.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S6.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.3"><ci id="S6.E3.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.3.1">¯</ci><ci id="S6.E3.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S6.E3.m1.1.1.1.1.1.1.1.3.2">𝑥</ci></apply></apply><apply id="S6.E3.m1.2.2.2.2.2.1.1.cmml" xref="S6.E3.m1.2.2.2.2.2.1"><minus id="S6.E3.m1.2.2.2.2.2.1.1.1.cmml" xref="S6.E3.m1.2.2.2.2.2.1.1.1"></minus><apply id="S6.E3.m1.2.2.2.2.2.1.1.2.cmml" xref="S6.E3.m1.2.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S6.E3.m1.2.2.2.2.2.1.1.2.1.cmml" xref="S6.E3.m1.2.2.2.2.2.1.1.2">subscript</csymbol><ci id="S6.E3.m1.2.2.2.2.2.1.1.2.2.cmml" xref="S6.E3.m1.2.2.2.2.2.1.1.2.2">𝑦</ci><ci id="S6.E3.m1.2.2.2.2.2.1.1.2.3.cmml" xref="S6.E3.m1.2.2.2.2.2.1.1.2.3">𝑖</ci></apply><apply id="S6.E3.m1.2.2.2.2.2.1.1.3.cmml" xref="S6.E3.m1.2.2.2.2.2.1.1.3"><ci id="S6.E3.m1.2.2.2.2.2.1.1.3.1.cmml" xref="S6.E3.m1.2.2.2.2.2.1.1.3.1">¯</ci><ci id="S6.E3.m1.2.2.2.2.2.1.1.3.2.cmml" xref="S6.E3.m1.2.2.2.2.2.1.1.3.2">𝑦</ci></apply></apply></apply></apply><apply id="S6.E3.m1.4.4.4.cmml" xref="S6.E3.m1.4.4.4"><root id="S6.E3.m1.4.4.4a.cmml" xref="S6.E3.m1.4.4.4"></root><apply id="S6.E3.m1.4.4.4.2.2.cmml" xref="S6.E3.m1.4.4.4.2.2"><sum id="S6.E3.m1.4.4.4.2.2.3.cmml" xref="S6.E3.m1.4.4.4.2.2.3"></sum><apply id="S6.E3.m1.4.4.4.2.2.2.cmml" xref="S6.E3.m1.4.4.4.2.2.2"><times id="S6.E3.m1.4.4.4.2.2.2.3.cmml" xref="S6.E3.m1.4.4.4.2.2.2.3"></times><apply id="S6.E3.m1.3.3.3.1.1.1.1.cmml" xref="S6.E3.m1.3.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S6.E3.m1.3.3.3.1.1.1.1.2.cmml" xref="S6.E3.m1.3.3.3.1.1.1.1">superscript</csymbol><apply id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.cmml" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1"><minus id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.1.cmml" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.1"></minus><apply id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2.cmml" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2.1.cmml" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2.2.cmml" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2.2">𝑥</ci><ci id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2.3.cmml" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.2.3">𝑖</ci></apply><apply id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.3.cmml" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.3"><ci id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.3.1.cmml" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.3.1">¯</ci><ci id="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.3.2.cmml" xref="S6.E3.m1.3.3.3.1.1.1.1.1.1.1.3.2">𝑥</ci></apply></apply><cn id="S6.E3.m1.3.3.3.1.1.1.1.3.cmml" type="integer" xref="S6.E3.m1.3.3.3.1.1.1.1.3">2</cn></apply><apply id="S6.E3.m1.4.4.4.2.2.2.2.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2"><sum id="S6.E3.m1.4.4.4.2.2.2.2.2.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2.2"></sum><apply id="S6.E3.m1.4.4.4.2.2.2.2.1.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2.1"><csymbol cd="ambiguous" id="S6.E3.m1.4.4.4.2.2.2.2.1.2.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2.1">superscript</csymbol><apply id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1"><minus id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.1.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.1"></minus><apply id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2.1.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2">subscript</csymbol><ci id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2.2.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2.2">𝑦</ci><ci id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2.3.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.2.3">𝑖</ci></apply><apply id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.3.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.3"><ci id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.3.1.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.3.1">¯</ci><ci id="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.3.2.cmml" xref="S6.E3.m1.4.4.4.2.2.2.2.1.1.1.1.3.2">𝑦</ci></apply></apply><cn id="S6.E3.m1.4.4.4.2.2.2.2.1.3.cmml" type="integer" xref="S6.E3.m1.4.4.4.2.2.2.2.1.3">2</cn></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E3.m1.5c">r=\frac{\sum(x_{i}-\bar{x})(y_{i}-\bar{y})}{\sqrt{\sum(x_{i}-\bar{x})^{2}\sum(%
y_{i}-\bar{y})^{2}}},</annotation><annotation encoding="application/x-llamapun" id="S6.E3.m1.5d">italic_r = divide start_ARG ∑ ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¯ start_ARG italic_x end_ARG ) ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¯ start_ARG italic_y end_ARG ) end_ARG start_ARG square-root start_ARG ∑ ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¯ start_ARG italic_x end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ∑ ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over¯ start_ARG italic_y end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p3">
<p class="ltx_p" id="S6.SS2.SSS2.p3.6">where <math alttext="x_{i}" class="ltx_Math" display="inline" id="S6.SS2.SSS2.p3.1.m1.1"><semantics id="S6.SS2.SSS2.p3.1.m1.1a"><msub id="S6.SS2.SSS2.p3.1.m1.1.1" xref="S6.SS2.SSS2.p3.1.m1.1.1.cmml"><mi id="S6.SS2.SSS2.p3.1.m1.1.1.2" xref="S6.SS2.SSS2.p3.1.m1.1.1.2.cmml">x</mi><mi id="S6.SS2.SSS2.p3.1.m1.1.1.3" xref="S6.SS2.SSS2.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p3.1.m1.1b"><apply id="S6.SS2.SSS2.p3.1.m1.1.1.cmml" xref="S6.SS2.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS2.p3.1.m1.1.1.1.cmml" xref="S6.SS2.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S6.SS2.SSS2.p3.1.m1.1.1.2.cmml" xref="S6.SS2.SSS2.p3.1.m1.1.1.2">𝑥</ci><ci id="S6.SS2.SSS2.p3.1.m1.1.1.3.cmml" xref="S6.SS2.SSS2.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p3.1.m1.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS2.p3.1.m1.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="y_{i}" class="ltx_Math" display="inline" id="S6.SS2.SSS2.p3.2.m2.1"><semantics id="S6.SS2.SSS2.p3.2.m2.1a"><msub id="S6.SS2.SSS2.p3.2.m2.1.1" xref="S6.SS2.SSS2.p3.2.m2.1.1.cmml"><mi id="S6.SS2.SSS2.p3.2.m2.1.1.2" xref="S6.SS2.SSS2.p3.2.m2.1.1.2.cmml">y</mi><mi id="S6.SS2.SSS2.p3.2.m2.1.1.3" xref="S6.SS2.SSS2.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p3.2.m2.1b"><apply id="S6.SS2.SSS2.p3.2.m2.1.1.cmml" xref="S6.SS2.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS2.p3.2.m2.1.1.1.cmml" xref="S6.SS2.SSS2.p3.2.m2.1.1">subscript</csymbol><ci id="S6.SS2.SSS2.p3.2.m2.1.1.2.cmml" xref="S6.SS2.SSS2.p3.2.m2.1.1.2">𝑦</ci><ci id="S6.SS2.SSS2.p3.2.m2.1.1.3.cmml" xref="S6.SS2.SSS2.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p3.2.m2.1c">y_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS2.p3.2.m2.1d">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are the scores from the LLM and the human, respectively, and <math alttext="\bar{x}" class="ltx_Math" display="inline" id="S6.SS2.SSS2.p3.3.m3.1"><semantics id="S6.SS2.SSS2.p3.3.m3.1a"><mover accent="true" id="S6.SS2.SSS2.p3.3.m3.1.1" xref="S6.SS2.SSS2.p3.3.m3.1.1.cmml"><mi id="S6.SS2.SSS2.p3.3.m3.1.1.2" xref="S6.SS2.SSS2.p3.3.m3.1.1.2.cmml">x</mi><mo id="S6.SS2.SSS2.p3.3.m3.1.1.1" xref="S6.SS2.SSS2.p3.3.m3.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p3.3.m3.1b"><apply id="S6.SS2.SSS2.p3.3.m3.1.1.cmml" xref="S6.SS2.SSS2.p3.3.m3.1.1"><ci id="S6.SS2.SSS2.p3.3.m3.1.1.1.cmml" xref="S6.SS2.SSS2.p3.3.m3.1.1.1">¯</ci><ci id="S6.SS2.SSS2.p3.3.m3.1.1.2.cmml" xref="S6.SS2.SSS2.p3.3.m3.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p3.3.m3.1c">\bar{x}</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS2.p3.3.m3.1d">over¯ start_ARG italic_x end_ARG</annotation></semantics></math> and <math alttext="\bar{y}" class="ltx_Math" display="inline" id="S6.SS2.SSS2.p3.4.m4.1"><semantics id="S6.SS2.SSS2.p3.4.m4.1a"><mover accent="true" id="S6.SS2.SSS2.p3.4.m4.1.1" xref="S6.SS2.SSS2.p3.4.m4.1.1.cmml"><mi id="S6.SS2.SSS2.p3.4.m4.1.1.2" xref="S6.SS2.SSS2.p3.4.m4.1.1.2.cmml">y</mi><mo id="S6.SS2.SSS2.p3.4.m4.1.1.1" xref="S6.SS2.SSS2.p3.4.m4.1.1.1.cmml">¯</mo></mover><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p3.4.m4.1b"><apply id="S6.SS2.SSS2.p3.4.m4.1.1.cmml" xref="S6.SS2.SSS2.p3.4.m4.1.1"><ci id="S6.SS2.SSS2.p3.4.m4.1.1.1.cmml" xref="S6.SS2.SSS2.p3.4.m4.1.1.1">¯</ci><ci id="S6.SS2.SSS2.p3.4.m4.1.1.2.cmml" xref="S6.SS2.SSS2.p3.4.m4.1.1.2">𝑦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p3.4.m4.1c">\bar{y}</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS2.p3.4.m4.1d">over¯ start_ARG italic_y end_ARG</annotation></semantics></math> are their means. Pearson correlation values range from <math alttext="-1" class="ltx_Math" display="inline" id="S6.SS2.SSS2.p3.5.m5.1"><semantics id="S6.SS2.SSS2.p3.5.m5.1a"><mrow id="S6.SS2.SSS2.p3.5.m5.1.1" xref="S6.SS2.SSS2.p3.5.m5.1.1.cmml"><mo id="S6.SS2.SSS2.p3.5.m5.1.1a" xref="S6.SS2.SSS2.p3.5.m5.1.1.cmml">−</mo><mn id="S6.SS2.SSS2.p3.5.m5.1.1.2" xref="S6.SS2.SSS2.p3.5.m5.1.1.2.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p3.5.m5.1b"><apply id="S6.SS2.SSS2.p3.5.m5.1.1.cmml" xref="S6.SS2.SSS2.p3.5.m5.1.1"><minus id="S6.SS2.SSS2.p3.5.m5.1.1.1.cmml" xref="S6.SS2.SSS2.p3.5.m5.1.1"></minus><cn id="S6.SS2.SSS2.p3.5.m5.1.1.2.cmml" type="integer" xref="S6.SS2.SSS2.p3.5.m5.1.1.2">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p3.5.m5.1c">-1</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS2.p3.5.m5.1d">- 1</annotation></semantics></math> to <math alttext="1" class="ltx_Math" display="inline" id="S6.SS2.SSS2.p3.6.m6.1"><semantics id="S6.SS2.SSS2.p3.6.m6.1a"><mn id="S6.SS2.SSS2.p3.6.m6.1.1" xref="S6.SS2.SSS2.p3.6.m6.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS2.p3.6.m6.1b"><cn id="S6.SS2.SSS2.p3.6.m6.1.1.cmml" type="integer" xref="S6.SS2.SSS2.p3.6.m6.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS2.p3.6.m6.1c">1</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS2.p3.6.m6.1d">1</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.3. </span>Spearman’s Rank Correlation Coefficient</h4>
<div class="ltx_para" id="S6.SS2.SSS3.p1">
<p class="ltx_p" id="S6.SS2.SSS3.p1.1">Spearman’s Rank Correlation Coefficient (<math alttext="\rho" class="ltx_Math" display="inline" id="S6.SS2.SSS3.p1.1.m1.1"><semantics id="S6.SS2.SSS3.p1.1.m1.1a"><mi id="S6.SS2.SSS3.p1.1.m1.1.1" xref="S6.SS2.SSS3.p1.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS3.p1.1.m1.1b"><ci id="S6.SS2.SSS3.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS3.p1.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS3.p1.1.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS3.p1.1.m1.1d">italic_ρ</annotation></semantics></math>)  <cite class="ltx_cite ltx_citemacro_citep">(Sedgwick, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib191" title="">2014</a>)</cite> assesses the monotonic relationship between two variables by comparing their ranked values rather than the raw scores. It is defined as:</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS3.p2">
<table class="ltx_equation ltx_eqn_table" id="S6.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(4)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\rho=1-\frac{6\sum d_{i}^{2}}{n(n^{2}-1)}," class="ltx_Math" display="block" id="S6.E4.m1.2"><semantics id="S6.E4.m1.2a"><mrow id="S6.E4.m1.2.2.1" xref="S6.E4.m1.2.2.1.1.cmml"><mrow id="S6.E4.m1.2.2.1.1" xref="S6.E4.m1.2.2.1.1.cmml"><mi id="S6.E4.m1.2.2.1.1.2" xref="S6.E4.m1.2.2.1.1.2.cmml">ρ</mi><mo id="S6.E4.m1.2.2.1.1.1" xref="S6.E4.m1.2.2.1.1.1.cmml">=</mo><mrow id="S6.E4.m1.2.2.1.1.3" xref="S6.E4.m1.2.2.1.1.3.cmml"><mn id="S6.E4.m1.2.2.1.1.3.2" xref="S6.E4.m1.2.2.1.1.3.2.cmml">1</mn><mo id="S6.E4.m1.2.2.1.1.3.1" xref="S6.E4.m1.2.2.1.1.3.1.cmml">−</mo><mfrac id="S6.E4.m1.1.1" xref="S6.E4.m1.1.1.cmml"><mrow id="S6.E4.m1.1.1.3" xref="S6.E4.m1.1.1.3.cmml"><mn id="S6.E4.m1.1.1.3.2" xref="S6.E4.m1.1.1.3.2.cmml">6</mn><mo id="S6.E4.m1.1.1.3.1" xref="S6.E4.m1.1.1.3.1.cmml">⁢</mo><mrow id="S6.E4.m1.1.1.3.3" xref="S6.E4.m1.1.1.3.3.cmml"><mo id="S6.E4.m1.1.1.3.3.1" xref="S6.E4.m1.1.1.3.3.1.cmml">∑</mo><msubsup id="S6.E4.m1.1.1.3.3.2" xref="S6.E4.m1.1.1.3.3.2.cmml"><mi id="S6.E4.m1.1.1.3.3.2.2.2" xref="S6.E4.m1.1.1.3.3.2.2.2.cmml">d</mi><mi id="S6.E4.m1.1.1.3.3.2.2.3" xref="S6.E4.m1.1.1.3.3.2.2.3.cmml">i</mi><mn id="S6.E4.m1.1.1.3.3.2.3" xref="S6.E4.m1.1.1.3.3.2.3.cmml">2</mn></msubsup></mrow></mrow><mrow id="S6.E4.m1.1.1.1" xref="S6.E4.m1.1.1.1.cmml"><mi id="S6.E4.m1.1.1.1.3" xref="S6.E4.m1.1.1.1.3.cmml">n</mi><mo id="S6.E4.m1.1.1.1.2" xref="S6.E4.m1.1.1.1.2.cmml">⁢</mo><mrow id="S6.E4.m1.1.1.1.1.1" xref="S6.E4.m1.1.1.1.1.1.1.cmml"><mo id="S6.E4.m1.1.1.1.1.1.2" stretchy="false" xref="S6.E4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E4.m1.1.1.1.1.1.1" xref="S6.E4.m1.1.1.1.1.1.1.cmml"><msup id="S6.E4.m1.1.1.1.1.1.1.2" xref="S6.E4.m1.1.1.1.1.1.1.2.cmml"><mi id="S6.E4.m1.1.1.1.1.1.1.2.2" xref="S6.E4.m1.1.1.1.1.1.1.2.2.cmml">n</mi><mn id="S6.E4.m1.1.1.1.1.1.1.2.3" xref="S6.E4.m1.1.1.1.1.1.1.2.3.cmml">2</mn></msup><mo id="S6.E4.m1.1.1.1.1.1.1.1" xref="S6.E4.m1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S6.E4.m1.1.1.1.1.1.1.3" xref="S6.E4.m1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S6.E4.m1.1.1.1.1.1.3" stretchy="false" xref="S6.E4.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow></mrow><mo id="S6.E4.m1.2.2.1.2" xref="S6.E4.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E4.m1.2b"><apply id="S6.E4.m1.2.2.1.1.cmml" xref="S6.E4.m1.2.2.1"><eq id="S6.E4.m1.2.2.1.1.1.cmml" xref="S6.E4.m1.2.2.1.1.1"></eq><ci id="S6.E4.m1.2.2.1.1.2.cmml" xref="S6.E4.m1.2.2.1.1.2">𝜌</ci><apply id="S6.E4.m1.2.2.1.1.3.cmml" xref="S6.E4.m1.2.2.1.1.3"><minus id="S6.E4.m1.2.2.1.1.3.1.cmml" xref="S6.E4.m1.2.2.1.1.3.1"></minus><cn id="S6.E4.m1.2.2.1.1.3.2.cmml" type="integer" xref="S6.E4.m1.2.2.1.1.3.2">1</cn><apply id="S6.E4.m1.1.1.cmml" xref="S6.E4.m1.1.1"><divide id="S6.E4.m1.1.1.2.cmml" xref="S6.E4.m1.1.1"></divide><apply id="S6.E4.m1.1.1.3.cmml" xref="S6.E4.m1.1.1.3"><times id="S6.E4.m1.1.1.3.1.cmml" xref="S6.E4.m1.1.1.3.1"></times><cn id="S6.E4.m1.1.1.3.2.cmml" type="integer" xref="S6.E4.m1.1.1.3.2">6</cn><apply id="S6.E4.m1.1.1.3.3.cmml" xref="S6.E4.m1.1.1.3.3"><sum id="S6.E4.m1.1.1.3.3.1.cmml" xref="S6.E4.m1.1.1.3.3.1"></sum><apply id="S6.E4.m1.1.1.3.3.2.cmml" xref="S6.E4.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S6.E4.m1.1.1.3.3.2.1.cmml" xref="S6.E4.m1.1.1.3.3.2">superscript</csymbol><apply id="S6.E4.m1.1.1.3.3.2.2.cmml" xref="S6.E4.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="S6.E4.m1.1.1.3.3.2.2.1.cmml" xref="S6.E4.m1.1.1.3.3.2">subscript</csymbol><ci id="S6.E4.m1.1.1.3.3.2.2.2.cmml" xref="S6.E4.m1.1.1.3.3.2.2.2">𝑑</ci><ci id="S6.E4.m1.1.1.3.3.2.2.3.cmml" xref="S6.E4.m1.1.1.3.3.2.2.3">𝑖</ci></apply><cn id="S6.E4.m1.1.1.3.3.2.3.cmml" type="integer" xref="S6.E4.m1.1.1.3.3.2.3">2</cn></apply></apply></apply><apply id="S6.E4.m1.1.1.1.cmml" xref="S6.E4.m1.1.1.1"><times id="S6.E4.m1.1.1.1.2.cmml" xref="S6.E4.m1.1.1.1.2"></times><ci id="S6.E4.m1.1.1.1.3.cmml" xref="S6.E4.m1.1.1.1.3">𝑛</ci><apply id="S6.E4.m1.1.1.1.1.1.1.cmml" xref="S6.E4.m1.1.1.1.1.1"><minus id="S6.E4.m1.1.1.1.1.1.1.1.cmml" xref="S6.E4.m1.1.1.1.1.1.1.1"></minus><apply id="S6.E4.m1.1.1.1.1.1.1.2.cmml" xref="S6.E4.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S6.E4.m1.1.1.1.1.1.1.2.1.cmml" xref="S6.E4.m1.1.1.1.1.1.1.2">superscript</csymbol><ci id="S6.E4.m1.1.1.1.1.1.1.2.2.cmml" xref="S6.E4.m1.1.1.1.1.1.1.2.2">𝑛</ci><cn id="S6.E4.m1.1.1.1.1.1.1.2.3.cmml" type="integer" xref="S6.E4.m1.1.1.1.1.1.1.2.3">2</cn></apply><cn id="S6.E4.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S6.E4.m1.1.1.1.1.1.1.3">1</cn></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E4.m1.2c">\rho=1-\frac{6\sum d_{i}^{2}}{n(n^{2}-1)},</annotation><annotation encoding="application/x-llamapun" id="S6.E4.m1.2d">italic_ρ = 1 - divide start_ARG 6 ∑ italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_n ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - 1 ) end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS2.SSS3.p3">
<p class="ltx_p" id="S6.SS2.SSS3.p3.3">where <math alttext="d_{i}" class="ltx_Math" display="inline" id="S6.SS2.SSS3.p3.1.m1.1"><semantics id="S6.SS2.SSS3.p3.1.m1.1a"><msub id="S6.SS2.SSS3.p3.1.m1.1.1" xref="S6.SS2.SSS3.p3.1.m1.1.1.cmml"><mi id="S6.SS2.SSS3.p3.1.m1.1.1.2" xref="S6.SS2.SSS3.p3.1.m1.1.1.2.cmml">d</mi><mi id="S6.SS2.SSS3.p3.1.m1.1.1.3" xref="S6.SS2.SSS3.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS3.p3.1.m1.1b"><apply id="S6.SS2.SSS3.p3.1.m1.1.1.cmml" xref="S6.SS2.SSS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS3.p3.1.m1.1.1.1.cmml" xref="S6.SS2.SSS3.p3.1.m1.1.1">subscript</csymbol><ci id="S6.SS2.SSS3.p3.1.m1.1.1.2.cmml" xref="S6.SS2.SSS3.p3.1.m1.1.1.2">𝑑</ci><ci id="S6.SS2.SSS3.p3.1.m1.1.1.3.cmml" xref="S6.SS2.SSS3.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS3.p3.1.m1.1c">d_{i}</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS3.p3.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is the difference between the ranks of corresponding scores from the LLM and the human evaluator, and <math alttext="n" class="ltx_Math" display="inline" id="S6.SS2.SSS3.p3.2.m2.1"><semantics id="S6.SS2.SSS3.p3.2.m2.1a"><mi id="S6.SS2.SSS3.p3.2.m2.1.1" xref="S6.SS2.SSS3.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS3.p3.2.m2.1b"><ci id="S6.SS2.SSS3.p3.2.m2.1.1.cmml" xref="S6.SS2.SSS3.p3.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS3.p3.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS3.p3.2.m2.1d">italic_n</annotation></semantics></math> is the number of paired scores.
Spearman’s <math alttext="\rho" class="ltx_Math" display="inline" id="S6.SS2.SSS3.p3.3.m3.1"><semantics id="S6.SS2.SSS3.p3.3.m3.1a"><mi id="S6.SS2.SSS3.p3.3.m3.1.1" xref="S6.SS2.SSS3.p3.3.m3.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS3.p3.3.m3.1b"><ci id="S6.SS2.SSS3.p3.3.m3.1.1.cmml" xref="S6.SS2.SSS3.p3.3.m3.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS3.p3.3.m3.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS3.p3.3.m3.1d">italic_ρ</annotation></semantics></math> is less sensitive to outliers and non-linear relationships compared to Pearson’s correlation, making it a robust choice for evaluating tasks where the relative order of scores is more important than the exact values. It is commonly used in ranking-based evaluations such as preference judgments or ranking tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.4. </span>Kendall’s Tau</h4>
<div class="ltx_para" id="S6.SS2.SSS4.p1">
<p class="ltx_p" id="S6.SS2.SSS4.p1.1">Kendall’s Tau (<math alttext="\tau" class="ltx_Math" display="inline" id="S6.SS2.SSS4.p1.1.m1.1"><semantics id="S6.SS2.SSS4.p1.1.m1.1a"><mi id="S6.SS2.SSS4.p1.1.m1.1.1" xref="S6.SS2.SSS4.p1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS4.p1.1.m1.1b"><ci id="S6.SS2.SSS4.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS4.p1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS4.p1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS4.p1.1.m1.1d">italic_τ</annotation></semantics></math>)  <cite class="ltx_cite ltx_citemacro_citep">(Sen, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib192" title="">1968</a>)</cite> is another rank-based correlation metric that measures the ordinal association between two ranked lists. It is defined as:</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS4.p2">
<table class="ltx_equation ltx_eqn_table" id="S6.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(5)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\tau=\frac{C-D}{\frac{1}{2}n(n-1)}," class="ltx_Math" display="block" id="S6.E5.m1.2"><semantics id="S6.E5.m1.2a"><mrow id="S6.E5.m1.2.2.1" xref="S6.E5.m1.2.2.1.1.cmml"><mrow id="S6.E5.m1.2.2.1.1" xref="S6.E5.m1.2.2.1.1.cmml"><mi id="S6.E5.m1.2.2.1.1.2" xref="S6.E5.m1.2.2.1.1.2.cmml">τ</mi><mo id="S6.E5.m1.2.2.1.1.1" xref="S6.E5.m1.2.2.1.1.1.cmml">=</mo><mfrac id="S6.E5.m1.1.1" xref="S6.E5.m1.1.1.cmml"><mrow id="S6.E5.m1.1.1.3" xref="S6.E5.m1.1.1.3.cmml"><mi id="S6.E5.m1.1.1.3.2" xref="S6.E5.m1.1.1.3.2.cmml">C</mi><mo id="S6.E5.m1.1.1.3.1" xref="S6.E5.m1.1.1.3.1.cmml">−</mo><mi id="S6.E5.m1.1.1.3.3" xref="S6.E5.m1.1.1.3.3.cmml">D</mi></mrow><mrow id="S6.E5.m1.1.1.1" xref="S6.E5.m1.1.1.1.cmml"><mfrac id="S6.E5.m1.1.1.1.3" xref="S6.E5.m1.1.1.1.3.cmml"><mn id="S6.E5.m1.1.1.1.3.2" xref="S6.E5.m1.1.1.1.3.2.cmml">1</mn><mn id="S6.E5.m1.1.1.1.3.3" xref="S6.E5.m1.1.1.1.3.3.cmml">2</mn></mfrac><mo id="S6.E5.m1.1.1.1.2" xref="S6.E5.m1.1.1.1.2.cmml">⁢</mo><mi id="S6.E5.m1.1.1.1.4" xref="S6.E5.m1.1.1.1.4.cmml">n</mi><mo id="S6.E5.m1.1.1.1.2a" xref="S6.E5.m1.1.1.1.2.cmml">⁢</mo><mrow id="S6.E5.m1.1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.1.1.cmml"><mo id="S6.E5.m1.1.1.1.1.1.2" stretchy="false" xref="S6.E5.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S6.E5.m1.1.1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.1.1.cmml"><mi id="S6.E5.m1.1.1.1.1.1.1.2" xref="S6.E5.m1.1.1.1.1.1.1.2.cmml">n</mi><mo id="S6.E5.m1.1.1.1.1.1.1.1" xref="S6.E5.m1.1.1.1.1.1.1.1.cmml">−</mo><mn id="S6.E5.m1.1.1.1.1.1.1.3" xref="S6.E5.m1.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S6.E5.m1.1.1.1.1.1.3" stretchy="false" xref="S6.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mfrac></mrow><mo id="S6.E5.m1.2.2.1.2" xref="S6.E5.m1.2.2.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E5.m1.2b"><apply id="S6.E5.m1.2.2.1.1.cmml" xref="S6.E5.m1.2.2.1"><eq id="S6.E5.m1.2.2.1.1.1.cmml" xref="S6.E5.m1.2.2.1.1.1"></eq><ci id="S6.E5.m1.2.2.1.1.2.cmml" xref="S6.E5.m1.2.2.1.1.2">𝜏</ci><apply id="S6.E5.m1.1.1.cmml" xref="S6.E5.m1.1.1"><divide id="S6.E5.m1.1.1.2.cmml" xref="S6.E5.m1.1.1"></divide><apply id="S6.E5.m1.1.1.3.cmml" xref="S6.E5.m1.1.1.3"><minus id="S6.E5.m1.1.1.3.1.cmml" xref="S6.E5.m1.1.1.3.1"></minus><ci id="S6.E5.m1.1.1.3.2.cmml" xref="S6.E5.m1.1.1.3.2">𝐶</ci><ci id="S6.E5.m1.1.1.3.3.cmml" xref="S6.E5.m1.1.1.3.3">𝐷</ci></apply><apply id="S6.E5.m1.1.1.1.cmml" xref="S6.E5.m1.1.1.1"><times id="S6.E5.m1.1.1.1.2.cmml" xref="S6.E5.m1.1.1.1.2"></times><apply id="S6.E5.m1.1.1.1.3.cmml" xref="S6.E5.m1.1.1.1.3"><divide id="S6.E5.m1.1.1.1.3.1.cmml" xref="S6.E5.m1.1.1.1.3"></divide><cn id="S6.E5.m1.1.1.1.3.2.cmml" type="integer" xref="S6.E5.m1.1.1.1.3.2">1</cn><cn id="S6.E5.m1.1.1.1.3.3.cmml" type="integer" xref="S6.E5.m1.1.1.1.3.3">2</cn></apply><ci id="S6.E5.m1.1.1.1.4.cmml" xref="S6.E5.m1.1.1.1.4">𝑛</ci><apply id="S6.E5.m1.1.1.1.1.1.1.cmml" xref="S6.E5.m1.1.1.1.1.1"><minus id="S6.E5.m1.1.1.1.1.1.1.1.cmml" xref="S6.E5.m1.1.1.1.1.1.1.1"></minus><ci id="S6.E5.m1.1.1.1.1.1.1.2.cmml" xref="S6.E5.m1.1.1.1.1.1.1.2">𝑛</ci><cn id="S6.E5.m1.1.1.1.1.1.1.3.cmml" type="integer" xref="S6.E5.m1.1.1.1.1.1.1.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E5.m1.2c">\tau=\frac{C-D}{\frac{1}{2}n(n-1)},</annotation><annotation encoding="application/x-llamapun" id="S6.E5.m1.2d">italic_τ = divide start_ARG italic_C - italic_D end_ARG start_ARG divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_n ( italic_n - 1 ) end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS2.SSS4.p3">
<p class="ltx_p" id="S6.SS2.SSS4.p3.4">where <math alttext="C" class="ltx_Math" display="inline" id="S6.SS2.SSS4.p3.1.m1.1"><semantics id="S6.SS2.SSS4.p3.1.m1.1a"><mi id="S6.SS2.SSS4.p3.1.m1.1.1" xref="S6.SS2.SSS4.p3.1.m1.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS4.p3.1.m1.1b"><ci id="S6.SS2.SSS4.p3.1.m1.1.1.cmml" xref="S6.SS2.SSS4.p3.1.m1.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS4.p3.1.m1.1c">C</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS4.p3.1.m1.1d">italic_C</annotation></semantics></math> is the number of concordant pairs (where the rank order agrees between the LLM and human), and <math alttext="D" class="ltx_Math" display="inline" id="S6.SS2.SSS4.p3.2.m2.1"><semantics id="S6.SS2.SSS4.p3.2.m2.1a"><mi id="S6.SS2.SSS4.p3.2.m2.1.1" xref="S6.SS2.SSS4.p3.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS4.p3.2.m2.1b"><ci id="S6.SS2.SSS4.p3.2.m2.1.1.cmml" xref="S6.SS2.SSS4.p3.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS4.p3.2.m2.1c">D</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS4.p3.2.m2.1d">italic_D</annotation></semantics></math> is the number of discordant pairs. Kendall’s <math alttext="\tau" class="ltx_Math" display="inline" id="S6.SS2.SSS4.p3.3.m3.1"><semantics id="S6.SS2.SSS4.p3.3.m3.1a"><mi id="S6.SS2.SSS4.p3.3.m3.1.1" xref="S6.SS2.SSS4.p3.3.m3.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS4.p3.3.m3.1b"><ci id="S6.SS2.SSS4.p3.3.m3.1.1.cmml" xref="S6.SS2.SSS4.p3.3.m3.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS4.p3.3.m3.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS4.p3.3.m3.1d">italic_τ</annotation></semantics></math> is particularly useful when evaluating the consistency of rankings produced by LLMs and human evaluators. It is often preferred when the dataset contains many ties, as it provides a more nuanced measure of agreement than Spearman’s <math alttext="\rho" class="ltx_Math" display="inline" id="S6.SS2.SSS4.p3.4.m4.1"><semantics id="S6.SS2.SSS4.p3.4.m4.1a"><mi id="S6.SS2.SSS4.p3.4.m4.1.1" xref="S6.SS2.SSS4.p3.4.m4.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS4.p3.4.m4.1b"><ci id="S6.SS2.SSS4.p3.4.m4.1.1.cmml" xref="S6.SS2.SSS4.p3.4.m4.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS4.p3.4.m4.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS4.p3.4.m4.1d">italic_ρ</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.5. </span>Cohen’s Kappa</h4>
<div class="ltx_para" id="S6.SS2.SSS5.p1">
<p class="ltx_p" id="S6.SS2.SSS5.p1.1">Cohen’s Kappa (<math alttext="\kappa" class="ltx_Math" display="inline" id="S6.SS2.SSS5.p1.1.m1.1"><semantics id="S6.SS2.SSS5.p1.1.m1.1a"><mi id="S6.SS2.SSS5.p1.1.m1.1.1" xref="S6.SS2.SSS5.p1.1.m1.1.1.cmml">κ</mi><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS5.p1.1.m1.1b"><ci id="S6.SS2.SSS5.p1.1.m1.1.1.cmml" xref="S6.SS2.SSS5.p1.1.m1.1.1">𝜅</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS5.p1.1.m1.1c">\kappa</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS5.p1.1.m1.1d">italic_κ</annotation></semantics></math>)  <cite class="ltx_cite ltx_citemacro_citep">(Warrens, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib241" title="">2015</a>)</cite> measures the level of agreement between two raters (in this case, the LLM and the human) beyond what would be expected by chance. It is defined as:</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS5.p2">
<table class="ltx_equation ltx_eqn_table" id="S6.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_left">(6)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\kappa=\frac{p_{o}-p_{e}}{1-p_{e}}," class="ltx_Math" display="block" id="S6.E6.m1.1"><semantics id="S6.E6.m1.1a"><mrow id="S6.E6.m1.1.1.1" xref="S6.E6.m1.1.1.1.1.cmml"><mrow id="S6.E6.m1.1.1.1.1" xref="S6.E6.m1.1.1.1.1.cmml"><mi id="S6.E6.m1.1.1.1.1.2" xref="S6.E6.m1.1.1.1.1.2.cmml">κ</mi><mo id="S6.E6.m1.1.1.1.1.1" xref="S6.E6.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S6.E6.m1.1.1.1.1.3" xref="S6.E6.m1.1.1.1.1.3.cmml"><mrow id="S6.E6.m1.1.1.1.1.3.2" xref="S6.E6.m1.1.1.1.1.3.2.cmml"><msub id="S6.E6.m1.1.1.1.1.3.2.2" xref="S6.E6.m1.1.1.1.1.3.2.2.cmml"><mi id="S6.E6.m1.1.1.1.1.3.2.2.2" xref="S6.E6.m1.1.1.1.1.3.2.2.2.cmml">p</mi><mi id="S6.E6.m1.1.1.1.1.3.2.2.3" xref="S6.E6.m1.1.1.1.1.3.2.2.3.cmml">o</mi></msub><mo id="S6.E6.m1.1.1.1.1.3.2.1" xref="S6.E6.m1.1.1.1.1.3.2.1.cmml">−</mo><msub id="S6.E6.m1.1.1.1.1.3.2.3" xref="S6.E6.m1.1.1.1.1.3.2.3.cmml"><mi id="S6.E6.m1.1.1.1.1.3.2.3.2" xref="S6.E6.m1.1.1.1.1.3.2.3.2.cmml">p</mi><mi id="S6.E6.m1.1.1.1.1.3.2.3.3" xref="S6.E6.m1.1.1.1.1.3.2.3.3.cmml">e</mi></msub></mrow><mrow id="S6.E6.m1.1.1.1.1.3.3" xref="S6.E6.m1.1.1.1.1.3.3.cmml"><mn id="S6.E6.m1.1.1.1.1.3.3.2" xref="S6.E6.m1.1.1.1.1.3.3.2.cmml">1</mn><mo id="S6.E6.m1.1.1.1.1.3.3.1" xref="S6.E6.m1.1.1.1.1.3.3.1.cmml">−</mo><msub id="S6.E6.m1.1.1.1.1.3.3.3" xref="S6.E6.m1.1.1.1.1.3.3.3.cmml"><mi id="S6.E6.m1.1.1.1.1.3.3.3.2" xref="S6.E6.m1.1.1.1.1.3.3.3.2.cmml">p</mi><mi id="S6.E6.m1.1.1.1.1.3.3.3.3" xref="S6.E6.m1.1.1.1.1.3.3.3.3.cmml">e</mi></msub></mrow></mfrac></mrow><mo id="S6.E6.m1.1.1.1.2" xref="S6.E6.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S6.E6.m1.1b"><apply id="S6.E6.m1.1.1.1.1.cmml" xref="S6.E6.m1.1.1.1"><eq id="S6.E6.m1.1.1.1.1.1.cmml" xref="S6.E6.m1.1.1.1.1.1"></eq><ci id="S6.E6.m1.1.1.1.1.2.cmml" xref="S6.E6.m1.1.1.1.1.2">𝜅</ci><apply id="S6.E6.m1.1.1.1.1.3.cmml" xref="S6.E6.m1.1.1.1.1.3"><divide id="S6.E6.m1.1.1.1.1.3.1.cmml" xref="S6.E6.m1.1.1.1.1.3"></divide><apply id="S6.E6.m1.1.1.1.1.3.2.cmml" xref="S6.E6.m1.1.1.1.1.3.2"><minus id="S6.E6.m1.1.1.1.1.3.2.1.cmml" xref="S6.E6.m1.1.1.1.1.3.2.1"></minus><apply id="S6.E6.m1.1.1.1.1.3.2.2.cmml" xref="S6.E6.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="S6.E6.m1.1.1.1.1.3.2.2.1.cmml" xref="S6.E6.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="S6.E6.m1.1.1.1.1.3.2.2.2.cmml" xref="S6.E6.m1.1.1.1.1.3.2.2.2">𝑝</ci><ci id="S6.E6.m1.1.1.1.1.3.2.2.3.cmml" xref="S6.E6.m1.1.1.1.1.3.2.2.3">𝑜</ci></apply><apply id="S6.E6.m1.1.1.1.1.3.2.3.cmml" xref="S6.E6.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S6.E6.m1.1.1.1.1.3.2.3.1.cmml" xref="S6.E6.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="S6.E6.m1.1.1.1.1.3.2.3.2.cmml" xref="S6.E6.m1.1.1.1.1.3.2.3.2">𝑝</ci><ci id="S6.E6.m1.1.1.1.1.3.2.3.3.cmml" xref="S6.E6.m1.1.1.1.1.3.2.3.3">𝑒</ci></apply></apply><apply id="S6.E6.m1.1.1.1.1.3.3.cmml" xref="S6.E6.m1.1.1.1.1.3.3"><minus id="S6.E6.m1.1.1.1.1.3.3.1.cmml" xref="S6.E6.m1.1.1.1.1.3.3.1"></minus><cn id="S6.E6.m1.1.1.1.1.3.3.2.cmml" type="integer" xref="S6.E6.m1.1.1.1.1.3.3.2">1</cn><apply id="S6.E6.m1.1.1.1.1.3.3.3.cmml" xref="S6.E6.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S6.E6.m1.1.1.1.1.3.3.3.1.cmml" xref="S6.E6.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S6.E6.m1.1.1.1.1.3.3.3.2.cmml" xref="S6.E6.m1.1.1.1.1.3.3.3.2">𝑝</ci><ci id="S6.E6.m1.1.1.1.1.3.3.3.3.cmml" xref="S6.E6.m1.1.1.1.1.3.3.3.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.E6.m1.1c">\kappa=\frac{p_{o}-p_{e}}{1-p_{e}},</annotation><annotation encoding="application/x-llamapun" id="S6.E6.m1.1d">italic_κ = divide start_ARG italic_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_ARG start_ARG 1 - italic_p start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_ARG ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S6.SS2.SSS5.p3">
<p class="ltx_p" id="S6.SS2.SSS5.p3.2">where <math alttext="p_{o}" class="ltx_Math" display="inline" id="S6.SS2.SSS5.p3.1.m1.1"><semantics id="S6.SS2.SSS5.p3.1.m1.1a"><msub id="S6.SS2.SSS5.p3.1.m1.1.1" xref="S6.SS2.SSS5.p3.1.m1.1.1.cmml"><mi id="S6.SS2.SSS5.p3.1.m1.1.1.2" xref="S6.SS2.SSS5.p3.1.m1.1.1.2.cmml">p</mi><mi id="S6.SS2.SSS5.p3.1.m1.1.1.3" xref="S6.SS2.SSS5.p3.1.m1.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS5.p3.1.m1.1b"><apply id="S6.SS2.SSS5.p3.1.m1.1.1.cmml" xref="S6.SS2.SSS5.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS5.p3.1.m1.1.1.1.cmml" xref="S6.SS2.SSS5.p3.1.m1.1.1">subscript</csymbol><ci id="S6.SS2.SSS5.p3.1.m1.1.1.2.cmml" xref="S6.SS2.SSS5.p3.1.m1.1.1.2">𝑝</ci><ci id="S6.SS2.SSS5.p3.1.m1.1.1.3.cmml" xref="S6.SS2.SSS5.p3.1.m1.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS5.p3.1.m1.1c">p_{o}</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS5.p3.1.m1.1d">italic_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> is the observed agreement and <math alttext="p_{e}" class="ltx_Math" display="inline" id="S6.SS2.SSS5.p3.2.m2.1"><semantics id="S6.SS2.SSS5.p3.2.m2.1a"><msub id="S6.SS2.SSS5.p3.2.m2.1.1" xref="S6.SS2.SSS5.p3.2.m2.1.1.cmml"><mi id="S6.SS2.SSS5.p3.2.m2.1.1.2" xref="S6.SS2.SSS5.p3.2.m2.1.1.2.cmml">p</mi><mi id="S6.SS2.SSS5.p3.2.m2.1.1.3" xref="S6.SS2.SSS5.p3.2.m2.1.1.3.cmml">e</mi></msub><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS5.p3.2.m2.1b"><apply id="S6.SS2.SSS5.p3.2.m2.1.1.cmml" xref="S6.SS2.SSS5.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S6.SS2.SSS5.p3.2.m2.1.1.1.cmml" xref="S6.SS2.SSS5.p3.2.m2.1.1">subscript</csymbol><ci id="S6.SS2.SSS5.p3.2.m2.1.1.2.cmml" xref="S6.SS2.SSS5.p3.2.m2.1.1.2">𝑝</ci><ci id="S6.SS2.SSS5.p3.2.m2.1.1.3.cmml" xref="S6.SS2.SSS5.p3.2.m2.1.1.3">𝑒</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS5.p3.2.m2.1c">p_{e}</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS5.p3.2.m2.1d">italic_p start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT</annotation></semantics></math> is the expected agreement by chance.
Cohen’s Kappa is particularly effective in classification tasks where both the LLM and the human evaluators assign categorical labels. It accounts for the possibility of random agreement, making it a more robust metric than simple accuracy.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS6">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.6. </span>Intraclass Correlation Coefficient (ICC)</h4>
<div class="ltx_para" id="S6.SS2.SSS6.p1">
<p class="ltx_p" id="S6.SS2.SSS6.p1.1">The Intraclass Correlation Coefficient (ICC) <cite class="ltx_cite ltx_citemacro_citep">(Bartko, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib14" title="">1966</a>)</cite> assesses the reliability of ratings when there are multiple evaluators. It evaluates the consistency or conformity of measurements made by different raters, including LLMs and human annotators. ICC is defined based on the variance components derived from a one-way or two-way ANOVA model.
The ICC is particularly useful when comparing multiple LLMs or when evaluating the consistency of an LLM across different subsets of data, providing a broader view of its reliability as an evaluator.</p>
</div>
<figure class="ltx_table" id="S6.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 4. </span>Summary of Common Metrics for Evaluating LLMs-as-Judges Models</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T4.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S6.T4.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T4.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.1.1" style="font-size:90%;">Metric</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T4.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.2.1" style="font-size:90%;">Type</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S6.T4.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.3.1" style="font-size:90%;">Use Case</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S6.T4.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.4.1" style="font-size:90%;">Robustness to Outliers</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.3.2.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T4.3.2.1.1"><span class="ltx_text" id="S6.T4.3.2.1.1.1" style="font-size:90%;">Accuracy</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T4.3.2.1.2"><span class="ltx_text" id="S6.T4.3.2.1.2.1" style="font-size:90%;">Agreement measure</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S6.T4.3.2.1.3"><span class="ltx_text" id="S6.T4.3.2.1.3.1" style="font-size:90%;">Proportion of correct judgments</span></td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S6.T4.3.2.1.4"><span class="ltx_text" id="S6.T4.3.2.1.4.1" style="font-size:90%;">Sensitive</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.3.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.3.3.2.1"><span class="ltx_text" id="S6.T4.3.3.2.1.1" style="font-size:90%;">Pearson</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.3.3.2.2"><span class="ltx_text" id="S6.T4.3.3.2.2.1" style="font-size:90%;">Linear correlation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.3.3.2.3"><span class="ltx_text" id="S6.T4.3.3.2.3.1" style="font-size:90%;">Continuous score comparison</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.3.3.2.4"><span class="ltx_text" id="S6.T4.3.3.2.4.1" style="font-size:90%;">Sensitive</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.4.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.3.4.3.1"><span class="ltx_text" id="S6.T4.3.4.3.1.1" style="font-size:90%;">Spearman</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.3.4.3.2"><span class="ltx_text" id="S6.T4.3.4.3.2.1" style="font-size:90%;">Rank correlation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.3.4.3.3"><span class="ltx_text" id="S6.T4.3.4.3.3.1" style="font-size:90%;">Rank-based evaluation tasks</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.3.4.3.4"><span class="ltx_text" id="S6.T4.3.4.3.4.1" style="font-size:90%;">Robust</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.5.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.3.5.4.1"><span class="ltx_text" id="S6.T4.3.5.4.1.1" style="font-size:90%;">Kendall’s Tau</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.3.5.4.2"><span class="ltx_text" id="S6.T4.3.5.4.2.1" style="font-size:90%;">Rank correlation</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.3.5.4.3"><span class="ltx_text" id="S6.T4.3.5.4.3.1" style="font-size:90%;">Consistency in ordinal rankings</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.3.5.4.4"><span class="ltx_text" id="S6.T4.3.5.4.4.1" style="font-size:90%;">Robust, handles ties</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.6.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.3.6.5.1"><span class="ltx_text" id="S6.T4.3.6.5.1.1" style="font-size:90%;">Cohen’s Kappa</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.3.6.5.2"><span class="ltx_text" id="S6.T4.3.6.5.2.1" style="font-size:90%;">Agreement measure</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S6.T4.3.6.5.3"><span class="ltx_text" id="S6.T4.3.6.5.3.1" style="font-size:90%;">Two raters, consistency analysis</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S6.T4.3.6.5.4"><span class="ltx_text" id="S6.T4.3.6.5.4.1" style="font-size:90%;">Adjusts for chance</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.7.6">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T4.3.7.6.1"><span class="ltx_text" id="S6.T4.3.7.6.1.1" style="font-size:90%;">ICC</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T4.3.7.6.2"><span class="ltx_text" id="S6.T4.3.7.6.2.1" style="font-size:90%;">Agreement measure</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S6.T4.3.7.6.3"><span class="ltx_text" id="S6.T4.3.7.6.3.1" style="font-size:90%;">Multiple raters, consistency analysis</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S6.T4.3.7.6.4"><span class="ltx_text" id="S6.T4.3.7.6.4.1" style="font-size:90%;">Robust for group ratings</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Limitation</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Although the application of LLMs-as-judges holds great promise, there are still several significant limitations that can affect their effectiveness, reliability, and fairness <cite class="ltx_cite ltx_citemacro_citep">(Thakur et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib216" title="">2024</a>; Stureborg et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib209" title="">2024</a>)</cite>.
These limitations arise from the inherent characteristics of LLMs, including their reliance on large-scale data for training and token-based decoding mechanisms.
In this section, we will primarily explore the limitations in the following three key aspets: <span class="ltx_text ltx_font_bold" id="S7.p1.1.1">Biases</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1" title="7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1</span></a>), <span class="ltx_text ltx_font_bold" id="S7.p1.1.2">Adversarial Attacks</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS2" title="7.2. Adversarial Attacks ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.2</span></a>), and <span class="ltx_text ltx_font_bold" id="S7.p1.1.3">Inherent Weaknesses</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS3" title="7.3. Inherent Weaknesses ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.3</span></a>).</p>
</div>
<figure class="ltx_table" id="S7.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Overview of different biases.</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S7.T5.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S7.T5.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S7.T5.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.1.1">Bias</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S7.T5.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S7.T5.1.1.1.2.1">Description</span></th>
</tr>
<tr class="ltx_tr" id="S7.T5.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="2" id="S7.T5.1.2.2.1">
<span class="ltx_text ltx_font_bold" id="S7.T5.1.2.2.1.1">Presentation-Related Biases</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS1" title="7.1.1. Presentation-Related Biases ‣ 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1.1</span></a>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S7.T5.1.3.1" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T5.1.3.1.1"><span class="ltx_text" id="S7.T5.1.3.1.1.1" style="background-color:#E6E6E6;">Position Bias</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S7.T5.1.3.1.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.3.1.2.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S7.T5.1.3.1.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.3.1.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.3.1.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.3.1.2.1.1.1.1.1" style="width:303.5pt;">A tendency to make judgments based on the position of the input, where responses earlier or later in the sequence are favored over those in other positions.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.4.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.4.2.1">Verbosity Bias</th>
<td class="ltx_td ltx_align_left" id="S7.T5.1.4.2.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.4.2.2.1">
<tr class="ltx_tr" id="S7.T5.1.4.2.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.4.2.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.4.2.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.4.2.2.1.1.1.1.1" style="width:303.5pt;">A tendency to favor longer responses, potentially equating length with quality, regardless of the content’s actual value.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.5.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="2" id="S7.T5.1.5.3.1">
<span class="ltx_text ltx_font_bold" id="S7.T5.1.5.3.1.1">Social-Related Biases</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS2" title="7.1.2. Social-Related Biases ‣ 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1.2</span></a>)</th>
</tr>
<tr class="ltx_tr" id="S7.T5.1.6.4" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T5.1.6.4.1"><span class="ltx_text" id="S7.T5.1.6.4.1.1" style="background-color:#E6E6E6;">Authority Bias</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S7.T5.1.6.4.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.6.4.2.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S7.T5.1.6.4.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.6.4.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.6.4.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.6.4.2.1.1.1.1.1" style="width:303.5pt;">A tendency to be swayed by references to authoritative sources, such as books, websites, or famous individuals.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.7.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.7.5.1">Bandwagon-Effect Bias</th>
<td class="ltx_td ltx_align_left" id="S7.T5.1.7.5.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.7.5.2.1">
<tr class="ltx_tr" id="S7.T5.1.7.5.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.7.5.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.7.5.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.7.5.2.1.1.1.1.1" style="width:303.5pt;">A tendency to align with majority opinions, where LLMs-as-judges favor prevailing views over objectively assessing the content.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.8.6" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.8.6.1"><span class="ltx_text" id="S7.T5.1.8.6.1.1" style="background-color:#E6E6E6;">Compassion-Fade Bias</span></th>
<td class="ltx_td ltx_align_left" id="S7.T5.1.8.6.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.8.6.2.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S7.T5.1.8.6.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.8.6.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.8.6.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.8.6.2.1.1.1.1.1" style="width:303.5pt;">A tendency to be influenced by anonymization strategies, such as the removal of model names, affecting the judgments of LLMs.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.9.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.9.7.1">Diversity Bias</th>
<td class="ltx_td ltx_align_left" id="S7.T5.1.9.7.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.9.7.2.1">
<tr class="ltx_tr" id="S7.T5.1.9.7.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.9.7.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.9.7.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.9.7.2.1.1.1.1.1" style="width:303.5pt;">A tendency to shift judgments based on identity-related markers, such as gender, ethnicity, or other social categorizations.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.10.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="2" id="S7.T5.1.10.8.1">
<span class="ltx_text ltx_font_bold" id="S7.T5.1.10.8.1.1">Content-Related Biases</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS3" title="7.1.3. Content-Related Biases ‣ 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1.3</span></a>)</th>
</tr>
<tr class="ltx_tr" id="S7.T5.1.11.9" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T5.1.11.9.1"><span class="ltx_text" id="S7.T5.1.11.9.1.1" style="background-color:#E6E6E6;">Sentiment Bias</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S7.T5.1.11.9.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.11.9.2.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S7.T5.1.11.9.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.11.9.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.11.9.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.11.9.2.1.1.1.1.1" style="width:303.5pt;">A tendency to favor responses with specific emotional tones, such as cheerful or neutral, over negative or fearful ones.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.12.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.12.10.1">Token Bias</th>
<td class="ltx_td ltx_align_left" id="S7.T5.1.12.10.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.12.10.2.1">
<tr class="ltx_tr" id="S7.T5.1.12.10.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.12.10.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.12.10.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.12.10.2.1.1.1.1.1" style="width:303.5pt;">A tendency of LLMs to favor more frequent or prominent tokens during pre-training, leading to skewed judgments.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.13.11" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.13.11.1"><span class="ltx_text" id="S7.T5.1.13.11.1.1" style="background-color:#E6E6E6;">Context Bias</span></th>
<td class="ltx_td ltx_align_left" id="S7.T5.1.13.11.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.13.11.2.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S7.T5.1.13.11.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.13.11.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.13.11.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.13.11.2.1.1.1.1.1" style="width:303.5pt;">A tendency of LLMs to produce biased judgments influenced by contextual examples or cultural contexts, potentially leading to biased or culturally insensitive outcomes.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.14.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" colspan="2" id="S7.T5.1.14.12.1">
<span class="ltx_text ltx_font_bold" id="S7.T5.1.14.12.1.1">Cognitive-Related Biases</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS4" title="7.1.4. Cognitive-Related Biases ‣ 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1.4</span></a>)</th>
</tr>
<tr class="ltx_tr" id="S7.T5.1.15.13" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S7.T5.1.15.13.1"><span class="ltx_text" id="S7.T5.1.15.13.1.1" style="background-color:#E6E6E6;">Overconfidence Bias</span></th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S7.T5.1.15.13.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.15.13.2.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S7.T5.1.15.13.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.15.13.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.15.13.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.15.13.2.1.1.1.1.1" style="width:303.5pt;">A tendency to exhibit inflated confidence in evaluation judgments, leading to overly assertive but potentially incorrect conclusions</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.16.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.16.14.1">Self-Enhancement Bias</th>
<td class="ltx_td ltx_align_left" id="S7.T5.1.16.14.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.16.14.2.1">
<tr class="ltx_tr" id="S7.T5.1.16.14.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.16.14.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.16.14.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.16.14.2.1.1.1.1.1" style="width:303.5pt;">A tendency to favor outputs generated by the same model acting as a judge, undermining objectivity.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.17.15" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.17.15.1"><span class="ltx_text" id="S7.T5.1.17.15.1.1" style="background-color:#E6E6E6;">Refinement-Aware Bias</span></th>
<td class="ltx_td ltx_align_left" id="S7.T5.1.17.15.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.17.15.2.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S7.T5.1.17.15.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.17.15.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.17.15.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.17.15.2.1.1.1.1.1" style="width:303.5pt;">A tendency for scoring variations influenced by whether an answer is original, refined, or accompanied by conversation history during evaluation.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.18.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S7.T5.1.18.16.1">Distraction Bias</th>
<td class="ltx_td ltx_align_left" id="S7.T5.1.18.16.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.18.16.2.1">
<tr class="ltx_tr" id="S7.T5.1.18.16.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.18.16.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.18.16.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.18.16.2.1.1.1.1.1" style="width:303.5pt;">A tendency to be influenced by irrelevant content, which can detract from the quality of judgments by diverting attention from critical elements.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S7.T5.1.19.17" style="background-color:#E6E6E6;">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S7.T5.1.19.17.1"><span class="ltx_text" id="S7.T5.1.19.17.1.1" style="background-color:#E6E6E6;">Fallacy-Oversight Bias</span></th>
<td class="ltx_td ltx_align_left ltx_border_b" id="S7.T5.1.19.17.2">
<table class="ltx_tabular ltx_align_middle" id="S7.T5.1.19.17.2.1" style="background-color:#E6E6E6;">
<tr class="ltx_tr" id="S7.T5.1.19.17.2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S7.T5.1.19.17.2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S7.T5.1.19.17.2.1.1.1.1">
<span class="ltx_p" id="S7.T5.1.19.17.2.1.1.1.1.1" style="width:303.5pt;">A tendency to overlook logical fallacies, which can undermine the accuracy of judgments.</span>
</span>
</td>
</tr>
</table>
</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Biases</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">Essentially, LLMs are trained on vast amounts of data gathered from diverse sources. While this allows them to generate human-like responses, it also makes them inherit to the biases inherent in the training data.
These biases are presented in various forms, which can significantly affect evaluation results, compromising the fairness and accuracy of decisions.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1">To gain a deeper understanding of the impact of bias, we have provided a detailed classification of bias. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.T5" title="Table 5 ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">5</span></a>, the biases exhibited by LLMs-as-judges can be systematically categorized into four groups based on their underlying causes and manifestations: <span class="ltx_text ltx_font_bold" id="S7.SS1.p2.1.1">Presentation-Related Biases</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS1" title="7.1.1. Presentation-Related Biases ‣ 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1.1</span></a>), <span class="ltx_text ltx_font_bold" id="S7.SS1.p2.1.2">Social-Related Biases</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS2" title="7.1.2. Social-Related Biases ‣ 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1.2</span></a>), <span class="ltx_text ltx_font_bold" id="S7.SS1.p2.1.3">Content-Related Biases</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS3" title="7.1.3. Content-Related Biases ‣ 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1.3</span></a>), and <span class="ltx_text ltx_font_bold" id="S7.SS1.p2.1.4">Cognitive-Related Biases</span> (§<a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#S7.SS1.SSS4" title="7.1.4. Cognitive-Related Biases ‣ 7.1. Biases ‣ 7. Limitation ‣ LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods"><span class="ltx_text ltx_ref_tag">7.1.4</span></a>). In this section, we provide a detailed overview of the definition, impact, and solutions to these biases.</p>
</div>
<section class="ltx_subsubsection" id="S7.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.1. </span>Presentation-Related Biases</h4>
<div class="ltx_para" id="S7.SS1.SSS1.p1">
<p class="ltx_p" id="S7.SS1.SSS1.p1.1">Presentation-Related Biases refer to tendencies in LLMs where judgments are influenced more by the structure or presentation of information than by its substantive content.
For example, models may prioritize certain formats, styles, or patterns of expression, which can affect the quality of the input. Next, we introduce two biases related to Presentation-Related Biases: position bias and verbosity bias.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS1.p2">
<p class="ltx_p" id="S7.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS1.p2.1.1">Position bias</span> is a prevalent issue not only in the context of LLMs-as-judges but also in human decision-making and across various machine learning domains.
Research has shown that humans are often influenced by the order of options presented to them, leading to biased decision that can impact fairness and objectivity <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib198" title="">2024a</a>; Blunch, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib17" title="">1984</a>; Raghubir and Valenzuela, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib183" title="">2006</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib289" title="">2024a</a>)</cite>.
Similarly, in other ML applications, models trained on ordered data exhibit a positional preference, skewing outcomes based on the sequence of input <cite class="ltx_cite ltx_citemacro_citep">(Ko et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib112" title="">2020</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib235" title="">2018</a>)</cite>.
Position bias in LLMs-as-judges refers to the tendency of LLMs to favor certain answers based on their position in the response set.
For example, when presented with multiple answer choices or compared pairwise, LLMs disproportionately select options that appear earlier in the list, leading to skewed judgment.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS1.p3">
<p class="ltx_p" id="S7.SS1.SSS1.p3.1">Recent studies have further examined position bias in the LLMs-as-judges context.
For instance, a framework <cite class="ltx_cite ltx_citemacro_citep">(LLMS, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib152" title="">2025</a>)</cite> is proposed to investigate position bias in pairwise comparisons, introducing metrics such as repetition stability, position consistency, and preference fairness to better understand how positions affect LLM judgments.
Another study <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>)</cite> explores the limitations of LLMs-as-judges, including position biases, and verifies agreement between LLM judgments and human preferences across multiple benchmarks.
These findings underscore the need for robust debiasing strategies to enhance the fairness and reliableness of LLMs-as-judges.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS1.p4">
<p class="ltx_p" id="S7.SS1.SSS1.p4.1">Several methods are proposed to mitigate position bias.
The naive approach involves excluding inconsistent judgments by swapping the positions of the candidate answers and verifying whether the LLM’s judgment remains consistent. Inconsistent judgments are then filtered out <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib26" title="">2024a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib231" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib131" title="">2023c</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib292" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib128" title="">2024a</a>)</cite>.
The <span class="ltx_text ltx_font_bold" id="S7.SS1.SSS1.p4.1.1">swap-based</span> debiasing method can be further divided into two categories: <span class="ltx_text ltx_font_italic" id="S7.SS1.SSS1.p4.1.2">score-based</span> and <span class="ltx_text ltx_font_italic" id="S7.SS1.SSS1.p4.1.3">comparison-based</span>. Both approaches start by swapping the positions of the candidate answers. The difference lies in how the final judgment is determined. In the score-based method, each candidate answer is scored, and the average score across multiple swaps is taken as the final score for that answer <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib231" title="">2023b</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib131" title="">2023c</a>; Raina et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib185" title="">2024</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib88" title="">2024</a>)</cite>.In contrast, the comparison-based method considers the outcome a tie if the LLM’s judgments are inconsistent after swapping.
The conclusion of a tie is based on an analysis of the quality gap between answers. The larger the quality gap between candidate answers, the smaller the impact of position bias, resulting in higher consistency in predictions after swapping their positions, which is detailed in a recently study <cite class="ltx_cite ltx_citemacro_citep">(LLMS, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib152" title="">2025</a>)</cite>.
In addition to the aforementioned methods, PORTIA <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib135" title="">2023d</a>)</cite> employs an <span class="ltx_text ltx_font_bold" id="S7.SS1.SSS1.p4.1.4">alignment-based</span> approach that simulates human comparison strategies. It divides each answer into multiple segments, aligns similar content across candidate answers, and then merges these aligned segments into a single prompt for the LLM to evaluate. By presenting content in a balanced and aligned format, PORTIA enables the model to make more consistent and unbiased judgments, focusing on answer quality rather than order. This approach is effective across various LLMs, significantly improving evaluation consistency and reducing costs.
Further efforts to enhance LLM-based evaluations have explored new techniques to address position bias and other judgment inconsistencies. <span class="ltx_text ltx_font_bold" id="S7.SS1.SSS1.p4.1.5">Discussion-based</span> methods <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib133" title="">2023b</a>; Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib108" title="">2024</a>)</cite> incorporate peer ranking and discussion to improve evaluation accuracy. Instead of relying solely on a single LLM’s judgment, these methods prompt multiple LLMs to compare answers and discuss preferences to reach a consensus, thereby reducing individual positional bias and enhancing alignment with human judgments. This collaborative evaluation approach represents a promising direction for mitigating biases inherent in LLM assessments.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS1.p5">
<p class="ltx_p" id="S7.SS1.SSS1.p5.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS1.p5.1.1">Verbosity bias</span> <cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib108" title="">2024</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib26" title="">2024a</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>; Nasrabadi, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib164" title="">2024</a>)</cite> refers to the tendency of a judge, whether human or model-based, to favor lengthier responses over shorter ones, irrespective of the actual content quality or relevance. This bias may cause LLMs prefer longer responses, even if the extended content does not contribute substantively to the correctness of the judgments.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS1.p6">
<p class="ltx_p" id="S7.SS1.SSS1.p6.1">To mitigate verbosity bias in LLMs-as-judges, several approaches <cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib108" title="">2024</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib271" title="">a</a>)</cite> have been proposed. One approach <cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib108" title="">2024</a>)</cite> employs persuasive debating techniques, structuring responses to prioritize substance. By training LLMs-as-judges to engage in a debate-like format, this method encourages clarity and relevance, reducing the tendency to favor verbose arguments that lack substantive content. Additionally, the CALM <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite> framework introduces controlled modifications to systematically assess and quantify verbosity’s impact on judgments, using automated perturbations to evaluate robustness against verbosity bias and refine LLMs-as-judges toward objective and concise assessments. Complementing these methods, the contrastive judgments (Con-J) <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib271" title="">2024a</a>)</cite> approach trains models with structured rationale pairs instead of scalar scores, encouraging LLMs-as-judges to focus on well-reasoned content rather than associating verbosity with quality.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.2. </span>Social-Related Biases</h4>
<div class="ltx_para" id="S7.SS1.SSS2.p1">
<p class="ltx_p" id="S7.SS1.SSS2.p1.1">Social-Related Biases refer to biases in language models that resemble social phenomena <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib290" title="">2023a</a>)</cite>. These biases may manifest when models are swayed by references to authoritative sources (Authority Bias), align with prevailing majority opinions without independent evaluation (Bandwagon-Effect Bias), or adjust their judgments based on anonymization strategies or identity markers such as gender and ethnicity (Compassion-Fade Bias and Diversity Bias). Next, we present the details of these biases.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS2.p2">
<p class="ltx_p" id="S7.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS2.p2.1.1">Authority bias</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib26" title="">2024a</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite> in the context of LLMs-as-judges refers to the tendency of the model to attribute greater credibility to statements associated with authoritative references, regardless of the actual evidence supporting them. For instance, LLMs-as-judges may favor responses that include references to well-known sources or experts, even when the content is inaccurate or irrelevant. This bias highlights a critical vulnerability where the appearance of authority can unduly influence judgment outcomes.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS2.p3">
<p class="ltx_p" id="S7.SS1.SSS2.p3.1">While specific solutions to mitigate authority bias in LLMs-as-judges are still under active exploration, potential approaches include using retrieval-augmented generation (RAG) techniques to verify the validity of authoritative claims against external knowledge bases. This approach allows the model to cross-check referenced information and ensure its alignment with factual evidence. Another possible strategy is to design prompts that explicitly emphasize semantic accuracy and relevance over perceived authority. Further research is needed to validate these approaches and develop robust methods for addressing authority bias effectively in evaluative contexts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS2.p4">
<p class="ltx_p" id="S7.SS1.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS2.p4.1.1">Bandwagon-effect bias</span> <cite class="ltx_cite ltx_citemacro_citep">(Koo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib113" title="">2023</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite> in the context of LLMs-as-judges refers to the tendency of the model to align its judgments with the majority opinion or prevailing trends, regardless of the actual quality or correctness of the evaluated content. For instance, when multiple responses are presented with indications of popular support or consensus, LLMs-as-judges may disproportionately favor these responses over alternatives, even when the consensus is flawed or biased. This bias reflects a susceptibility to groupthink dynamics, undermining the objectivity and fairness of the judgment process.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS2.p5">
<p class="ltx_p" id="S7.SS1.SSS2.p5.1">Solutions to address bandwagon-effect bias include designing evaluation prompts that anonymize information about majority opinions, ensuring that judgments are based solely on the intrinsic quality of the responses rather than external indicators of popularity. Further exploration of debiasing strategies tailored to specific evaluative contexts is necessary to mitigate the impact of bandwagon-effect bias effectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS2.p6">
<p class="ltx_p" id="S7.SS1.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS2.p6.1.1">Compassion-fade bias</span> <cite class="ltx_cite ltx_citemacro_citep">(Koo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib113" title="">2023</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite> occurs when the anonymity of model names or the absence of identifiable contextual cues affects the judgments made by LLMs-as-judges. For example, anonymizing model names or using neutral identifiers may lead to shifts in evaluation outcomes. This bias highlights how the lack of personalized or contextual information can diminish the model’s sensitivity to equitable considerations.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS2.p7">
<p class="ltx_p" id="S7.SS1.SSS2.p7.1">To mitigate compassion-fade bias, it is important to design evaluation prompts that standardize judgment criteria, ensuring that assessments remain consistent regardless of whether identifying details are present. Additionally, fairness-driven frameworks that explicitly address anonymization effects can further enhance the reliability of LLMs-as-judges.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS2.p8">
<p class="ltx_p" id="S7.SS1.SSS2.p8.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS2.p8.1.1">Diversity bias</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib26" title="">2024a</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite> in the context of LLMs-as-judges refers to the model’s tendency to exhibit judgment shifts based on identity-related markers, such as gender, ethnicity, religion, or other social categorizations. For example, LLMs-as-judges might favor responses associated with certain demographic groups over others, leading to unfair or skewed judgments. This bias reflects the model’s susceptibility to implicit stereotypes or unequal treatment of diverse identities present in the training data.
Continued efforts to address this bias are crucial for ensuring fairness and inclusivity in the judgments conducted by LLMs-as-judges.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.3. </span>Content-Related Biases</h4>
<div class="ltx_para" id="S7.SS1.SSS3.p1">
<p class="ltx_p" id="S7.SS1.SSS3.p1.1">Content-Related Biases involve preferences or skewed judgments based on the content’s characteristics. An LLM might favor responses with certain emotional tones (Sentiment Bias), prefer frequently occurring words from its training data (Token Bias), or be influenced by specific cultural or domain contexts leading to insensitive outcomes (Context Bias). We present the details of these biases in the following.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS3.p2">
<p class="ltx_p" id="S7.SS1.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS3.p2.1.1">Sentiment bias</span> <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite> in the context of LLMs-as-judges refers to the tendency of the model to favor responses that exhibit certain emotional tones, such as positive or neutral sentiments, over others, regardless of their actual content quality or relevance. For instance, LLMs-as-judges may disproportionately reward responses that are cheerful or optimistic while penalizing those that are negative or emotionally intense, even if the latter are more contextually appropriate or accurate.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS3.p3">
<p class="ltx_p" id="S7.SS1.SSS3.p3.1">To address sentiment bias, potential solution is the use of sentiment-neutralizing mechanisms, such as filtering or adjusting responses to remove sentiment-driven influences during evaluation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS3.p4">
<p class="ltx_p" id="S7.SS1.SSS3.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS3.p4.1.1">Token Bias</span> <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib99" title="">2024</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib128" title="">2024a</a>; Pezeshkpour and Hruschka, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib179" title="">2023</a>; Raina et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib185" title="">2024</a>)</cite> refers to that LLMs favor certain tokens during the evaluation process. This bias often arises from the model’s pre-training data, where more frequently occurring tokens are prioritized over less common ones, regardless of the contextual appropriateness or correctness in judgment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS3.p5">
<p class="ltx_p" id="S7.SS1.SSS3.p5.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS3.p5.1.1">Contextual Bias</span> refers to the tendency of LLMs to produce skewed or biased judgments based on the specific context in which they are applied. For instance, models used in healthcare may propagate biases found in medical datasets, potentially influencing diagnoses or treatment recommendations <cite class="ltx_cite ltx_citemacro_citep">(Poulain et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib180" title="">2024</a>)</cite>, while in finance, they might reflect biases in credit scoring or loan approval processes <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib301" title="">2024d</a>)</cite>.
In addition, the selection of contextual examples may also introduce bias <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib297" title="">2023a</a>; Fei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib63" title="">2023</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib291" title="">2021</a>; Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib79" title="">2022</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.4. </span>Cognitive-Related Biases</h4>
<div class="ltx_para" id="S7.SS1.SSS4.p1">
<p class="ltx_p" id="S7.SS1.SSS4.p1.1">Cognitive-Related Biases pertain to the inherent cognitive tendencies of LLMs in processing information. This includes exhibiting unwarranted confidence in judgments (Overconfidence Bias), favoring outputs generated by themselves (Self-Enhancement Bias), varying scores based on whether an answer is original or refined (Refinement-Aware Bias), being distracted by irrelevant information (Distraction Bias), or overlooking logical fallacies (Fallacy-Oversight Bias). The details of these biases are presented in the following.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS4.p2">
<p class="ltx_p" id="S7.SS1.SSS4.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS4.p2.1.1">Overconfidence bias</span> <cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib108" title="">2024</a>; Jung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib104" title="">2024</a>)</cite> in the context of LLMs-as-judges refers to the tendency of models to exhibit an inflated level of confidence in their judgments, often resulting in overly assertive evaluations that may not accurately reflect the true reliability of the answer. This bias is particularly concerning in evaluative contexts, as it can lead LLMs-as-judges to overstate the correctness of certain outputs, compromising the objectivity and dependability of assessments.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS4.p3">
<p class="ltx_p" id="S7.SS1.SSS4.p3.1">To address Overconfidence bias, researchers have proposed several methods.
Cascaded Selective Evaluation <cite class="ltx_cite ltx_citemacro_citep">(Jung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib104" title="">2024</a>)</cite> addresses overconfidence by using Simulated Annotators to estimate confidence. This involves simulating diverse annotator preferences through in-context learning, which provides a more realistic measure of the likelihood that a human would agree with the LLM’s judgment. By analyzing multiple simulated responses, this method offers a confidence metric that reflects human-like disagreement, which helps to avoid overconfidence bias.
Another method uses an adversarial debate mechanism, where two LLMs argue for different outcomes. Through structured debate rounds, each model is required to substantiate its position, which can reveal overconfidence by prompting self-reflection and critical analysis. This approach has been shown to improve truthfulness and reduces overconfidence by fostering a balanced evaluation, aligning LLMs’ judgments more closely with accurate and reasoned conclusions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS4.p4">
<p class="ltx_p" id="S7.SS1.SSS4.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS4.p4.1.1">Self-enhancement bias</span> is the tendency to favor their own outputs <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib146" title="">2023a</a>; Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>; Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib133" title="">2023b</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib146" title="">2023a</a>; Panickssery et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib173" title="">2024</a>)</cite>.
This concept of self-enhancement is drawn from social psychology, as discussed in Brown’s work in social cognition literature <cite class="ltx_cite ltx_citemacro_citep">(Brown, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib20" title="">1986</a>)</cite>.
In the context of LLMs-as-judges, this bias manifests when a LLM evaluates its own generated outputs more favorably than those of other LLMs. Such bias is particularly concerning in applications involving self-assessment or feedback generation, as it compromises the objectivity of the LLMs-as-judges.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS4.p5">
<p class="ltx_p" id="S7.SS1.SSS4.p5.1">To address self-enhancement bias, PRD <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib133" title="">2023b</a>)</cite> introduces Peer Rank (PR) and Peer Discussion (PD) mechanisms. PR mitigates bias by using multiple LLMs as reviewers, each assessing pairwise comparisons between responses from different LLMs. By aggregating evaluations from several peer LLMs and weighting their preferences based on consistency with human judgments, PR reduces the impact of any single LLM’s self-enhancement bias, as more reliable reviewers have a greater influence. PD further alleviates self-enhancement bias by enabling two LLMs to engage in a dialogue to reach a mutual agreement on their preference between two answers. This multi-turn discussion encourages models to re-evaluate their initial judgments and consider alternative perspectives, focusing on content quality rather than self-generated responses. By promoting collaborative assessment and accountability, PR and PD effectively mitigate self-enhancement bias, aligning evaluations more closely with human standards.
Recently, an automated bias quantification framework named CALM <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite> has been proposed to systematically evaluate biases in LLMs-as-judges. CALM’s findings suggest that one effective way to reduce self-enhancement bias is to avoid using the same model to both generate and judge answers, thereby ensuring that evaluation remains more impartial.
Moreover, the Reference-Guided Verdict method <cite class="ltx_cite ltx_citemacro_citep">(Badshah and Sajjad, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib9" title="">2024</a>)</cite> further addresses self-enhancement bias by providing a definitive gold-standard answer as a reference for LLM judges. This reference anchor helps align judgments to objective criteria, even when an LLM evaluates its own output, thus reducing the tendency to favor self-generated answers. Through structured prompts, this method has been shown to enhance reliability and mitigate variability in judgments, especially when multiple LLMs are used collectively. The integration of multiple LLMs, trained on varied datasets or fine-tuned with different parameters, has proven instrumental in producing less biased, more balanced evaluations, highlighting the effectiveness of model diversity and reference-guided criteria in combating self-enhancement bias.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS4.p6">
<p class="ltx_p" id="S7.SS1.SSS4.p6.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS4.p6.1.1">Refinement-aware bias</span> <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite> in the context of LLMs-as-judges refers to the tendency of the model to evaluate responses differently based on whether they are original, refined, or include revision history. For instance, an answer that has been iteratively refined may be judged more favorably than an original response, even if the refinement process does not significantly improve the content quality. Similarly, responses that explicitly present their improvement process or revision rationale might receive undue preference, skewing the evaluation outcomes.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS4.p7">
<p class="ltx_p" id="S7.SS1.SSS4.p7.1">While research on refinement-aware bias in the specific context of LLMs-as-judges remains limited, solution <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib260" title="">2024c</a>)</cite> developed for general LLMs offer valuable insights. One potential solution involves incorporating external feedback mechanisms during judgment, as it introduces an objective and independent judgment mechanism that is not influenced by the LLM’s internal iterations or self-perception.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS4.p8">
<p class="ltx_p" id="S7.SS1.SSS4.p8.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS4.p8.1.1">Distraction bias</span> in the context of LLMs-as-judges refers to the model’s tendency to be influenced by irrelevant or unimportant details when making judgments. For instance, introducing unrelated information, such as a meaningless statement like “System Star likes to eat oranges and apples,” <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>; Koo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib113" title="">2023</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib196" title="">2023</a>)</cite> can significantly alter the model’s evaluation outcomes. This bias highlights the vulnerability of LLMs to attentional diversion caused by inconsequential content.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS4.p9">
<p class="ltx_p" id="S7.SS1.SSS4.p9.1">While existing studies <cite class="ltx_cite ltx_citemacro_citep">(Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>; Koo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib113" title="">2023</a>)</cite> have analyzed and discussed distraction bias, effective strategies to mitigate this issue in LLMs-as-judges remain underexplored. Potential solutions could involve input sanitization to preprocess and remove irrelevant information before presenting it to the model, ensuring that evaluations focus solely on relevant content. Additionally, explicit prompting with clear and strict guidelines could be designed to direct the LLM to evaluate only task-related aspects, reducing its susceptibility to distractions. Further research is needed to develop and validate robust methods that can systematically address distraction bias in the context of LLMs-as-judges.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.SS1.SSS4.p10">
<p class="ltx_p" id="S7.SS1.SSS4.p10.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS4.p10.1.1">Fallacy-oversight bias</span> <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib26" title="">2024a</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib268" title="">2024b</a>)</cite> refers to the tendency of LLMs-as-judges to overlook logical fallacies or inconsistencies within the evaluated responses. For instance, when presented with arguments or answers containing reasoning errors—such as circular reasoning, false dilemmas, or strawman arguments—LLMs-as-judges may fail to identify these issues and treat the responses as valid, potentially compromising the integrity of their evaluations.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS4.p11">
<p class="ltx_p" id="S7.SS1.SSS4.p11.1">In summary, while LLMs-as-judges have garnered significant attention for their effectiveness in diverse scenarios, the exploration of various biases that impact their performance remains relatively underdeveloped.
These biases pose significant challenges to ensuring fair, objective, and reliable judgments across tasks, particularly in various applications where the implications of biased judgments can be severe.
Future research must focus on systematically identifying, quantifying, and addressing these biases within the LLMs-as-judges framework.
Drawing from methodologies developed for general LLMs, such as external feedback mechanisms, balanced datasets, and fairness-aware prompting, could offer initial insights. However, domain-specific challenges require tailored solutions that align with the unique demands of LLMs-as-judges.
</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Adversarial Attacks</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Adversarial attacks involve carefully crafted inputs designed to deceive the model into producing incorrect or unintended outputs. For LLM judges, attackers may subtly modify the input content, alter the wording of questions, or introduce misleading context to influence the model’s evaluation results.
Researchers have found that for LLMs, even small, seemingly insignificant changes to the input data, such as adding or removing words, changing word order, or introducing ambiguous phrasing, can significantly affect the model’s response <cite class="ltx_cite ltx_citemacro_citep">(Shen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib194" title="">2023</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib101" title="">2023a</a>; Zou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib306" title="">2023</a>)</cite>. Such attacks can lead to inaccurate ratings or assessments, particularly when evaluating complex or high-risk tasks.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">In this section, we first review research on adversarial attacks on LLMs within general domains. Then, we specifically focus on adversarial attacks in the context of LLMs-as-judges.</p>
</div>
<section class="ltx_subsubsection" id="S7.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.1. </span>Adversarial Attacks on LLMs</h4>
<div class="ltx_para" id="S7.SS2.SSS1.p1">
<p class="ltx_p" id="S7.SS2.SSS1.p1.1">Adversarial attacks on LLMs focus on exploiting vulnerabilities within the general framework of language model functionality. These attacks can be classified into three main categories based on the manipulation level: text-level manipulations, structural and semantic distortions, and optimization-based attacks.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS1.p2">
<p class="ltx_p" id="S7.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.SSS1.p2.1.1">Text-Level Manipulations</span> involve subtle changes to the input text to deceive the model. Character-level perturbations, such as introducing typos, swapping letters, or inserting unnecessary characters, can cause significant changes in predictions despite minimal visible alterations <cite class="ltx_cite ltx_citemacro_citep">(Ebrahimi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib58" title="">2017</a>; Jiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib101" title="">2023a</a>)</cite>. Sentence-level modifications, such as rearranging phrases, adding irrelevant information, or paraphrasing inputs, further exploit the model’s sensitivity to surface-level changes <cite class="ltx_cite ltx_citemacro_citep">(Branch et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib19" title="">2022</a>; Perez and Ribeiro, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib178" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS1.p3">
<p class="ltx_p" id="S7.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.SSS1.p3.1.1">Structural and Semantic Distortions</span> focus on the syntactic and semantic properties of the input. Syntactic attacks rewrite sentence structures while preserving semantic meaning, targeting the model’s reliance on specific linguistic patterns <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib261" title="">2023a</a>)</cite>. Semantic preservation with perturbations modifies critical tokens identified through saliency analysis, ensuring the attack minimally affects meaning but significantly alters predictions.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS1.p4">
<p class="ltx_p" id="S7.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.SSS1.p4.1.1">Optimization-Based Attacks</span> leverage algorithmic techniques to craft adversarial inputs. Gradient-based methods utilize the model’s gradients to identify and manipulate influential input features, causing substantial shifts in predictions <cite class="ltx_cite ltx_citemacro_citep">(Sun, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib211" title="">2020</a>; Sun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib212" title="">2020</a>)</cite>. Population-based optimization techniques iteratively generate adversarial examples in black-box settings, exploiting the model’s outputs to refine attacks <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib121" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS1.p5">
<p class="ltx_p" id="S7.SS2.SSS1.p5.1">These attacks highlight the vulnerabilities in LLMs, demonstrating their susceptibility to subtle manipulations. Studying these adversarial attacks is essential, as it provides insights that can guide the development of robust defense mechanisms, ensuring that LLMs maintain reliability against such manipulations.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.2. </span>Adversarial Attacks on LLMs-as-judges</h4>
<div class="ltx_para" id="S7.SS2.SSS2.p1">
<p class="ltx_p" id="S7.SS2.SSS2.p1.1">Recent studies have unveiled significant vulnerabilities in LLMs-as-judges to adversarial attacks <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib294" title="">2024</a>; Doddapaneni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib52" title="">2024</a>; Raina et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib185" title="">2024</a>; Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib197" title="">2024b</a>)</cite>.
Zheng et al. <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib294" title="">2024</a>)</cite> and Doddapaneni et al. <cite class="ltx_cite ltx_citemacro_citep">(Doddapaneni et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib52" title="">2024</a>)</cite> demonstrated that automatic benchmarking systems like MT-Bench <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib293" title="">2023a</a>)</cite> can be easily deceived to yield artificially high scores. These findings highlight that malicious inputs can manipulate evaluation metrics, undermining the reliability of such benchmarks.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS2.p2">
<p class="ltx_p" id="S7.SS2.SSS2.p2.1">Building on this, Raina et al. <cite class="ltx_cite ltx_citemacro_citep">(Raina et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib185" title="">2024</a>)</cite> investigated the robustness of LLMs-as-judges against universal adversarial attacks. Their work showed that appending short, carefully crafted phrases to evaluated texts can effortlessly manipulate LLM scores, inflating them to their maximum regardless of the actual quality. Remarkably, these universal attack phrases are transferable across models; phrases optimized on smaller surrogate models (e.g., FlanT5-xl <cite class="ltx_cite ltx_citemacro_citep">(Chung et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib41" title="">2024</a>)</cite>) can successfully deceive larger models like GPT-3.5 and Llama2 <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib219" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS2.p3">
<p class="ltx_p" id="S7.SS2.SSS2.p3.1">Furthermore, Shi et al. <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib197" title="">2024b</a>)</cite> introduced <span class="ltx_text ltx_font_italic" id="S7.SS2.SSS2.p3.1.1">JudgeDeceiver</span>, an optimization-based prompt injection attack tailored for the LLMs-as-judges framework. Unlike handcrafted methods, JudgeDeceiver formulates a precise optimization objective to efficiently generate adversarial sequences. These sequences can mislead LLMs-as-judges into selecting biased or incorrect responses among candidate answers, thereby compromising the evaluation process.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS2.p4">
<p class="ltx_p" id="S7.SS2.SSS2.p4.1">Although preliminary studies <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib197" title="">2024b</a>; Raina et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib185" title="">2024</a>)</cite> have highlighted the vulnerability of LLMs-as-judges to adversarial manipulations, this field remains largely underexplored.
It is imperative to advance our understanding of these weaknesses and devise effective defense strategies.
As the use of LLMs-as-judges grows across diverse applications, future research should focus on uncovering new attack methods and strengthening the models against such adversarial threats.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Inherent Weaknesses</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">Despite the remarkable capabilities of LLMs, they possess several inherent weaknesses that can compromise their reliability and robustness in LLMs-as-judges. This subsection discusses key limitations, including issues related to knowledge recency, hallucination, and other domain-specific knowledge gaps <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib288" title="">2023b</a>)</cite>.</p>
</div>
<section class="ltx_subsubsection" id="S7.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.1. </span>Knowledge Recency</h4>
<div class="ltx_para" id="S7.SS3.SSS1.p1">
<p class="ltx_p" id="S7.SS3.SSS1.p1.1">One significant limitation of LLMs is their inability to access or incorporate up-to-date information reliably. LLMs are generally trained on static datasets that may become outdated over time, limiting their ability to evaluate scenarios that require knowledge of recent events, legislation, or rapidly evolving fields. The most straightforward solution is to retrain the model on new data; however, this approach is resource-intensive and risks catastrophic forgetting <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib153" title="">2023</a>)</cite>, where previously learned knowledge is overwritten during training.
This temporal disconnect can lead to judgments based on invalid data, or obsolete practices, compromising their reliability in real-world, time-sensitive applications. Consider a case where LLMs-as-judges are used to evaluate which of two responses from LLMs better answers a prompt about the COVID-19 pandemic. Suppose one response references the WHO guidelines updated timely, while the other relies on outdated 2020 guidelines. If the LLM-as-Judge has not been updated with the latest guidelines, it might erroneously prefer the outdated response, incorrectly deeming it more accurate. This failure to account for recent developments highlights the importance of addressing knowledge recency in LLMs-as-judges.</p>
</div>
<div class="ltx_para" id="S7.SS3.SSS1.p2">
<p class="ltx_p" id="S7.SS3.SSS1.p2.1">Addressing the issue of knowledge recency can involve integrating retrieval-augmented generation (RAG) methods <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib70" title="">2023</a>; Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib126" title="">2020</a>)</cite>, which enable LLMs to query external, dynamically updated databases or knowledge sources during evaluation.
Additionally, periodic fine-tuning with updated datasets or leveraging continual learning frameworks <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib245" title="">2024a</a>)</cite> can ensure that LLMs-as-judges remain aligned with the latest information. Combining these approaches with robust fact-checking mechanisms <cite class="ltx_cite ltx_citemacro_citep">(Dierickx et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib49" title="">2024</a>)</cite> can further enhance temporal reliability in judgment contexts.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.2. </span>Hallucination</h4>
<div class="ltx_para" id="S7.SS3.SSS2.p1">
<p class="ltx_p" id="S7.SS3.SSS2.p1.1">Another critical issue in LLMs is the hallucination problem, where models generate incorrect or fabricated information with high confidence. In the context of LLMs-as-judges, hallucination can manifest as the invention of non-existent precedents, misinterpretation of facts, or fabrication of sources, which can severely undermine the reliability of their judgments. This issue is particularly concerning in various applications, where such errors can lead to unfair or harmful outcomes.</p>
</div>
<div class="ltx_para" id="S7.SS3.SSS2.p2">
<p class="ltx_p" id="S7.SS3.SSS2.p2.1">Employing fact-checking mechanisms <cite class="ltx_cite ltx_citemacro_citep">(Dierickx et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib49" title="">2024</a>; Ji et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib98" title="">2023</a>; Tonmoy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib217" title="">2024</a>)</cite> during evaluation is crucial to mitigate hallucination. By cross-verifying the outputs of LLMs-as-judges with trusted databases and external knowledge sources, hallucinated information can be identified and corrected.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.3. </span>Domain-Specific Knowledge Gaps</h4>
<div class="ltx_para" id="S7.SS3.SSS3.p1">
<p class="ltx_p" id="S7.SS3.SSS3.p1.1">While LLMs demonstrate broad generalization capabilities, they often lack the depth of understanding required for specialized domains <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib64" title="">2023</a>; Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib172" title="">2024b</a>; Szymanski et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib213" title="">2024</a>; Dorner et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib56" title="">2024</a>)</cite>. For instance, legal judgments demand intricate knowledge of statutes, precedents, and contextual nuances, which may not be adequately captured in the training data of general-purpose LLMs. This limitation can lead to shallow or incorrect judgments in domain-specific contexts.</p>
</div>
<div class="ltx_para" id="S7.SS3.SSS3.p2">
<p class="ltx_p" id="S7.SS3.SSS3.p2.1">Domain adaptation techniques, such as integrating LLMs with domain-specific knowledge graphs <cite class="ltx_cite ltx_citemacro_citep">(Feng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib64" title="">2023</a>; Pan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib172" title="">2024b</a>)</cite> or leveraging RAG systems <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib70" title="">2023</a>)</cite>, can substantially improve their performance in specialized domains. Knowledge graphs provide structured, expert-curated information that enhances context-awareness, while RAG enables LLMs to dynamically retrieve relevant knowledge from specific domain.</p>
</div>
<div class="ltx_para" id="S7.SS3.SSS3.p3">
<p class="ltx_p" id="S7.SS3.SSS3.p3.1">The inherent weaknesses of LLMs highlight the need for continued research and innovation. Addressing these limitations through RAG, enhanced training methods, and knowledge graph techniques is crucial for ensuring that LLMs-as-judges deliver reliable, accurate, and trustworthy evaluations in diverse applications.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Future Work</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this section, we will explore the key directions for future work, focusing on how to build more efficient, more effective, and more reliable LLM judges. These directions aim to address the bottlenecks and challenges in current technologies and practices, while also promoting broader applications and deeper integration of LLM judges in diverse scenarios.</p>
</div>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1. </span>More Efficient LLMs-as-Judges</h3>
<section class="ltx_subsubsection" id="S8.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.1. </span>Automated Construction of Evaluation Criteria and Tasks</h4>
<div class="ltx_para" id="S8.SS1.SSS1.p1">
<p class="ltx_p" id="S8.SS1.SSS1.p1.1">Current LLM judges often rely on manually predefined evaluation criteria, lacking the ability to adapt dynamically during the assessment process. Designing prompts for these systems is not only tedious and time-consuming but also struggles to address the diverse requirements of various task scenarios <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib11" title="">2024</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib287" title="">2024b</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib274" title="">2024</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib281" title="">2024c</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib234" title="">2024f</a>)</cite>. To overcome these limitations, future LLM judges could incorporate enhanced adaptability by tailoring evaluation criteria based on task types, target audiences, and domain-specific knowledge. Such advancements would significantly streamline the configuration process of LLM judges, while also greatly improving their practicality and efficiency in real-world applications.</p>
</div>
<div class="ltx_para" id="S8.SS1.SSS1.p2">
<p class="ltx_p" id="S8.SS1.SSS1.p2.1">Moreover, existing static evaluation datasets are prone to issues such as training data contamination, which can compromise their effectiveness in accurately assessing the evolving capabilities of LLMs. To address this, future LLM judges could focus on dynamically constructing more suitable evaluation tasks and continuously optimizing the evaluation process, thereby enhancing applicability and precision <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib11" title="">2024</a>; Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib287" title="">2024b</a>)</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.2. </span>Scalable Evaluation Systems</h4>
<div class="ltx_para" id="S8.SS1.SSS2.p1">
<p class="ltx_p" id="S8.SS1.SSS2.p1.1">Existing LLM judges often exhibit limited adaptability in practical applications. While these judges may perform effectively on specific downstream tasks, they frequently struggle in cross-domain or multi-task settings, thereby falling short of meeting the diverse and broader demands of real-world applications.</p>
</div>
<div class="ltx_para" id="S8.SS1.SSS2.p2">
<p class="ltx_p" id="S8.SS1.SSS2.p2.1">To address these limitations, future research could focus on modular design principles to create scalable evaluation frameworks <cite class="ltx_cite ltx_citemacro_citep">(Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib258" title="">2024a</a>)</cite>. Such frameworks would allow users to flexibly add or customize evaluation modules to suit their specific needs. This modular approach not only enhances the usability and flexibility of the system but also significantly reduces the cost and complexity of transferring the framework across different domains.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.3. </span>Accelerating Evaluation Processes</h4>
<div class="ltx_para" id="S8.SS1.SSS3.p1">
<p class="ltx_p" id="S8.SS1.SSS3.p1.1">Existing LLM systems often face significant computational costs when performing evaluation tasks. For example, pairwise comparison methods require multiple rounds of comparisons for each candidate, which becomes extremely time-consuming as the number of candidates grows. In resource-constrained environments, such high-cost evaluation methods are challenging to deploy effectively. To address this issue, future research could focus on developing more efficient candidate selection algorithms, thereby unlocking new opportunities for the use of LLMs in low-resource settings <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib124" title="">2024b</a>; Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib150" title="">2024c</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S8.SS1.SSS3.p2">
<p class="ltx_p" id="S8.SS1.SSS3.p2.1">Similarly, the multi-LLM evaluation paradigm, which relies on multiple rounds of interaction, further exacerbates computational demands. To mitigate these challenges, future efforts could explore streamlined communication frameworks that support high-quality evaluation tasks while minimizing resource requirements <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib32" title="">2024f</a>)</cite>. Advances in these areas could lead to the development of more efficient and scalable evaluation systems, making LLM-based evaluations more practical across diverse and resource-limited scenarios.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2. </span>More Effective LLMs-as-Judges</h3>
<section class="ltx_subsubsection" id="S8.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.2.1. </span>Integration of Reasoning and Judge Capabilities</h4>
<div class="ltx_para" id="S8.SS2.SSS1.p1">
<p class="ltx_p" id="S8.SS2.SSS1.p1.1">Current LLMs-as-judges systems often treat reasoning and evaluation capabilities as distinct and independent modules, which can hinder effectiveness when addressing complex tasks. As the demand for evaluating increasingly complex systems grows, future LLM-as-Judge systems should prioritize the deep integration of reasoning and evaluation capabilities to achieve a seamless synergy <cite class="ltx_cite ltx_citemacro_citep">(Zhuo, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib305" title="">2023</a>; Yi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib272" title="">2024</a>; Stephan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib208" title="">2024</a>)</cite>. For instance, in legal scenarios, the model could first infer the relevant legal provisions and then assess the case’s relevance, making the evaluation process more effective.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.2.2. </span>Establishing a Collective Judgment Mechanism</h4>
<div class="ltx_para" id="S8.SS2.SSS2.p1">
<p class="ltx_p" id="S8.SS2.SSS2.p1.1">Current LLMs-as-Judge systems typically rely on a single model for evaluation. While this approach is straightforward, it is prone to biases inherent in individual models, leading to reduced accuracy and stabilitys. Moreover, a single model often struggles to comprehensively address the diverse requirements of such tasks. Future research could investigate collaborative multi-agent mechanisms to enable “collective judgment” where multiple LLMs work together, leveraging their respective strengths in reasoning and knowledge <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib23" title="">2023</a>; Chu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib40" title="">2024</a>)</cite>,. Additionally, ensemble techniques could be employed to dynamically balance the contributions of different models, leading to more stable and reliable judgment outcomes.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.2.3. </span>Enhancing Domain Knowledge</h4>
<div class="ltx_para" id="S8.SS2.SSS3.p1">
<p class="ltx_p" id="S8.SS2.SSS3.p1.1">Current LLMs-as-Judge systems often fall short when handling tasks in specialized fields due to insufficient domain knowledge. Furthermore, as domain knowledge continues to evolve, these models struggle to keep up with the latest developments, further limiting their effectiveness and applicability in real-world scenarios.</p>
</div>
<div class="ltx_para" id="S8.SS2.SSS3.p2">
<p class="ltx_p" id="S8.SS2.SSS3.p2.1">To address these challenges, future LLMs-as-judges systems should focus on integrating comprehensive domain knowledge to enhance their performance in specialized tasks <cite class="ltx_cite ltx_citemacro_citep">(Raju et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib186" title="">2024</a>)</cite>. This can be achieved by utilizing knowledge graphs, embedding domain-specific expertise, and fine-tuning models based on feedback from subject-matter experts.
In addition, these systems should incorporate dynamic knowledge updating capabilities. For instance, in the legal domain, models could regularly acquire and integrate updates on new statutes, case law, and policy changes, ensuring that their judgments remain current and aligned with the latest legal standards.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.2.4. </span>Cross-Domain and Cross-Language Transferability</h4>
<div class="ltx_para" id="S8.SS2.SSS4.p1">
<p class="ltx_p" id="S8.SS2.SSS4.p1.1">Current LLMs-as-Judge systems are often confined to specific domains or languages, making it challenging for them to transfer across different fields. For instance, an LLM proficient in processing legal texts may struggle to effectively handle evaluation tasks in the medical or financial domains. This limitation greatly restricts the applicability of such systems.</p>
</div>
<div class="ltx_para" id="S8.SS2.SSS4.p2">
<p class="ltx_p" id="S8.SS2.SSS4.p2.1">Future research can focus on exploring cross-domain and cross-language transfer learning techniques to enhance the adaptability of LLMs in diverse fields. By leveraging shared general knowledge across fields, models can quickly adapt to new tasks with minimal additional training costs <cite class="ltx_cite ltx_citemacro_citep">(Son et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib203" title="">2024b</a>; Hada et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib78" title="">2023</a>; Watts et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib242" title="">2024</a>)</cite>. For example, evaluation capabilities developed in English could be transferred to contexts in German, thereby improving the evaluation performance in these new areas.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS2.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.2.5. </span>Multimodal Integration Evaluation</h4>
<div class="ltx_para" id="S8.SS2.SSS5.p1">
<p class="ltx_p" id="S8.SS2.SSS5.p1.1">Current LLM-as-Judge systems primarily focus on processing textual data, with limited attention to integrating other modalities like images, audio, and video. This single-modal approach falls short in complex scenarios requiring multimodal analysis, such as combining visual and textual information in medical assessments. Future systems should develop cross-modal integration capabilities to process and evaluate multimodal data simultaneously <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib25" title="">2024b</a>)</cite>. Leveraging cross-modal validation can enhance evaluation accuracy. Key research areas include efficient multimodal feature extraction, integration, and the design of unified frameworks for more comprehensive and precise evaluations.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S8.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.3. </span>More Reliable LLMs-as-Judges</h3>
<section class="ltx_subsubsection" id="S8.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.3.1. </span>Enhancing Interpretability and Transparency</h4>
<div class="ltx_para" id="S8.SS3.SSS1.p1">
<p class="ltx_p" id="S8.SS3.SSS1.p1.1">Current LLM-as-Judge systems often operate as black boxes, with their rulings lacking transparency and a clear reasoning process. This opacity is particularly concerning in high-stakes domains such as legal judgments, where users cannot fully understand the basis of the model’s decisions or trust its outputs.
Future research should focus on improving the interpretability of LLMs <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib148" title="">2024a</a>)</cite>. For example, the LLM judges should not only provide evaluation results but also present a clear explanation. Research could explore designing validation models based on logical frameworks to make the decision-making process more transparent.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.3.2. </span>Mitigating Bias and Ensuring Fairness</h4>
<div class="ltx_para" id="S8.SS3.SSS2.p1">
<p class="ltx_p" id="S8.SS3.SSS2.p1.1">LLMs may be influenced by biases present in their training data, leading to unfair judgments in different social, cultural, or legal contexts. These biases could be amplified by the model and compromise the fairness of its decisions. Future research could focus on ensuring fairness in model outputs through debiasing algorithms and fairness constraints <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib128" title="">2024a</a>)</cite>. Targeted approaches, such as adversarial debiasing training or bias detection tools, can dynamically identify and mitigate potential biases during the model’s reasoning process.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.3.3. </span>Enhancing Robustness</h4>
<div class="ltx_para" id="S8.SS3.SSS3.p1">
<p class="ltx_p" id="S8.SS3.SSS3.p1.1">LLMs are sensitive to noise, incompleteness, or ambiguity in input instructions, which may lead to errors or instability in evaluation results when handling complex or highly uncertain texts. This lack of robustness significantly limits their reliability in practical applications.
Future research can adopt several methods to enable LLMs robust and reliable performance in real-world environments <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib197" title="">2024b</a>; Elangovan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2412.05579v2#bib.bib59" title="">2024</a>)</cite>. For instance, introducing more advanced data augmentation techniques to generate diverse and uncertain simulated cases can help train models to adapt to various complex input conditions.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Conclusion</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">This survey systematically examined the LLMs-as-Judges framework across five dimensions: functionality, methodology, applications, meta-evaluation, and limitations, providing a comprehensive understanding of its advantages, limitations, practical implementations, applications, and methods for evaluating its effectiveness.
To advance research in this field, we also outlined several promising directions for future exploration, including the development of more efficient, effective, and reliable LLM judges. We hope to promote the ongoing development of this field by providing foundational resources and will continue to update relevant content.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Achiam et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al<span class="ltx_text" id="bib.bib2.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.4.1">arXiv preprint arXiv:2303.08774</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arif et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Samee Arif, Sualeha Farid, Abdul Hameed Azeemi, Awais Athar, and Agha Ali Raza. 2024.

</span>
<span class="ltx_bibblock">The fellowship of the llms: Multi-agent workflows for synthetic preference optimization dataset generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">arXiv preprint arXiv:2408.08688</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock">Self-rag: Learning to retrieve, generate, and critique through self-reflection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">arXiv preprint arXiv:2310.11511</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asghar (2016)</span>
<span class="ltx_bibblock">
Nabiha Asghar. 2016.

</span>
<span class="ltx_bibblock">Yelp dataset challenge: Review rating prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:1605.05362</em> (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ashktorab et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zahra Ashktorab, Michael Desmond, Qian Pan, James M Johnson, Martin Santillan Cooper, Elizabeth M Daly, Rahul Nair, Tejaswini Pedapati, Swapnaja Achintalwar, and Werner Geyer. 2024.

</span>
<span class="ltx_bibblock">Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">arXiv preprint arXiv:2410.00873</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Askell et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
A Askell, Y Bai, A Chen, D Drain, D Ganguli, T Henighan, A Jones, N Joseph, B Mann, N DasSarma, et al<span class="ltx_text" id="bib.bib7.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">A general language assistant as a laboratory for alignment. arXiv.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.4.1">Preprint posted online December</em> 1 (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Babaei and Giudici (2024)</span>
<span class="ltx_bibblock">
Golnoosh Babaei and Paolo Giudici. 2024.

</span>
<span class="ltx_bibblock">GPT classifications, with application to credit lending.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Machine Learning with Applications</em> 16 (2024), 100534.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Badshah and Sajjad (2024)</span>
<span class="ltx_bibblock">
Sher Badshah and Hassan Sajjad. 2024.

</span>
<span class="ltx_bibblock">Reference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2408.09235</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al<span class="ltx_text" id="bib.bib10.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.4.1">arXiv preprint arXiv:2309.16609</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al<span class="ltx_text" id="bib.bib11.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Benchmarking foundation models with language-model-as-an-examiner.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.4.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bajaj et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et al<span class="ltx_text" id="bib.bib12.3.1">.</span> 2016.

</span>
<span class="ltx_bibblock">Ms marco: A human generated machine reading comprehension dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.4.1">arXiv preprint arXiv:1611.09268</em> (2016).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bandi and Harrasse (2024)</span>
<span class="ltx_bibblock">
Chaithanya Bandi and Abir Harrasse. 2024.

</span>
<span class="ltx_bibblock">Adversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2410.04663</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bartko (1966)</span>
<span class="ltx_bibblock">
John J Bartko. 1966.

</span>
<span class="ltx_bibblock">The intraclass correlation coefficient as a measure of reliability.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Psychological reports</em> 19, 1 (1966), 3–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bavaresco et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et al<span class="ltx_text" id="bib.bib15.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.4.1">arXiv preprint arXiv:2406.18403</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Besta et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al<span class="ltx_text" id="bib.bib16.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Graph of thoughts: Solving elaborate problems with large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.4.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 38. 17682–17690.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blunch (1984)</span>
<span class="ltx_bibblock">
Niels J Blunch. 1984.

</span>
<span class="ltx_bibblock">Position bias in multiple-choice questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Journal of Marketing Research</em> 21, 2 (1984), 216–220.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brake and Schaaf (2024)</span>
<span class="ltx_bibblock">
Nathan Brake and Thomas Schaaf. 2024.

</span>
<span class="ltx_bibblock">Comparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2404.06503</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Branch et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Hezekiah J Branch, Jonathan Rodriguez Cefalu, Jeremy McHugh, Leyla Hujer, Aditya Bahl, Daniel del Castillo Iglesias, Ron Heichman, and Ramesh Darwishi. 2022.

</span>
<span class="ltx_bibblock">Evaluating the susceptibility of pre-trained language models via handcrafted adversarial examples.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">arXiv preprint arXiv:2209.02128</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown (1986)</span>
<span class="ltx_bibblock">
Jonathon D Brown. 1986.

</span>
<span class="ltx_bibblock">Evaluations of self and others: Self-enhancement biases in social judgments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Social cognition</em> 4, 4 (1986), 353–376.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Maosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, and Kai Chen. 2024a.

</span>
<span class="ltx_bibblock">CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">arXiv preprint arXiv:2410.16256</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cao et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Meng Cao, Lei Shu, Lei Yu, Yun Zhu, Nevan Wichers, Yinxiao Liu, and Lei Meng. 2024b.

</span>
<span class="ltx_bibblock">Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>. 9119–9138.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023.

</span>
<span class="ltx_bibblock">Chateval: Towards better llm-based evaluators through multi-agent debate.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.3.1">arXiv preprint arXiv:2308.07201</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chang et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al<span class="ltx_text" id="bib.bib24.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">A survey on evaluation of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.4.1">ACM Transactions on Intelligent Systems and Technology</em> 15, 3 (2024), 1–45.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024b.

</span>
<span class="ltx_bibblock">Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">arXiv preprint arXiv:2402.04788</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Guiming Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. 2024a.

</span>
<span class="ltx_bibblock">Humans or llms as the judge? a study on judgement biases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">arXiv preprint arXiv:2402.10669</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Hong Chen, Duc Minh Vo, Hiroya Takamura, Yusuke Miyao, and Hideki Nakayama. 2023b.

</span>
<span class="ltx_bibblock">StoryER: Automatic story evaluation via ranking, rating and reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">Journal of Natural Language Processing</em> 30, 1 (2023), 243–249.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2024e)</span>
<span class="ltx_bibblock">
Junjie Chen, Weihang Su, Zhumin Chu, Haitao Li, Qinyao Ai, Yiqun Liu, Min Zhang, and Shaoping Ma. 2024e.

</span>
<span class="ltx_bibblock">An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2410.12265 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2410.12265" title="">https://arxiv.org/abs/2410.12265</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Jiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan O Arik, Tomas Pfister, and Somesh Jha. 2023c.

</span>
<span class="ltx_bibblock">Adaptation with self-evaluation to improve selective prediction in llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">arXiv preprint arXiv:2310.11689</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Kai Chen, Yanze Li, Wenhua Zhang, Yanxin Liu, Pengxiang Li, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao, Zhenguo Li, et al<span class="ltx_text" id="bib.bib30.3.1">.</span> 2024d.

</span>
<span class="ltx_bibblock">Automated evaluation of large vision-language models on self-driving corner cases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.4.1">arXiv preprint arXiv:2404.10595</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al<span class="ltx_text" id="bib.bib31.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.4.1">arXiv preprint arXiv:2107.03374</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2024f)</span>
<span class="ltx_bibblock">
Weize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2024f.

</span>
<span class="ltx_bibblock">Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">arXiv preprint arXiv:2407.07061</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou. 2023a.

</span>
<span class="ltx_bibblock">Teaching large language models to self-debug.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">arXiv preprint arXiv:2304.05128</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Yen-Shan Chen, Jing Jin, Peng-Ting Kuo, Chao-Wei Huang, and Yun-Nung Chen. 2024c.

</span>
<span class="ltx_bibblock">LLMs are Biased Evaluators But Not Biased for Retrieval Augmented Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">arXiv preprint arXiv:2410.20833</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chhun et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Cyril Chhun, Pierre Colombo, Chloé Clavel, and Fabian M Suchanek. 2022.

</span>
<span class="ltx_bibblock">Of human criteria and automatic metrics: A benchmark of the evaluation of story generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">arXiv preprint arXiv:2208.11646</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Cheng-Han Chiang, Wei-Chih Chen, Chun-Yi Kuan, Chienchou Yang, and Hung-yi Lee. 2024.

</span>
<span class="ltx_bibblock">Large Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">arXiv preprint arXiv:2407.05216</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang and Lee (2023)</span>
<span class="ltx_bibblock">
Cheng-Han Chiang and Hung-yi Lee. 2023.

</span>
<span class="ltx_bibblock">A closer look into automatic evaluation using large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">arXiv preprint arXiv:2310.05657</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al<span class="ltx_text" id="bib.bib38.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.4.1">See https://vicuna. lmsys. org (accessed 14 April 2023)</em> 2, 3 (2023), 6.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Juhwan Choi, Jungmin Yun, Kyohoon Jin, and YoungBin Kim. 2024.

</span>
<span class="ltx_bibblock">Multi-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">arXiv preprint arXiv:2404.09682</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu. 2024.

</span>
<span class="ltx_bibblock">Pre: A peer review based large language model evaluator.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">arXiv preprint arXiv:2401.15641</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al<span class="ltx_text" id="bib.bib41.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.4.1">Journal of Machine Learning Research</em> 25, 70 (2024), 1–53.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cohen et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2009)</span>
<span class="ltx_bibblock">
Israel Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009.

</span>
<span class="ltx_bibblock">Pearson correlation coefficient.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Noise reduction in speech processing</em> (2009), 1–4.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Craswell et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Jimmy Lin. 2021.

</span>
<span class="ltx_bibblock">Overview of the TREC 2021 Deep Learning Track. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.3.1">TREC</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://trec.nist.gov/pubs/trec30/papers/Overview-DL.pdf" title="">https://trec.nist.gov/pubs/trec30/papers/Overview-DL.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Craswell et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, and Ian Soboroff. 2022.

</span>
<span class="ltx_bibblock">Overview of the TREC 2022 Deep Learning Track. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">TREC</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://trec.nist.gov/pubs/trec31/papers/Overview_deep.pdf" title="">https://trec.nist.gov/pubs/trec31/papers/Overview_deep.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cui et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al<span class="ltx_text" id="bib.bib45.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.4.1">Forty-first International Conference on Machine Learning</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Daynauth and Mars (2024)</span>
<span class="ltx_bibblock">
Roland Daynauth and Jason Mars. 2024.

</span>
<span class="ltx_bibblock">Aligning Model Evaluations with Human Preferences: Mitigating Token Count Bias in Language Model Assessments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2407.12847</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shijian Deng, Wentian Zhao, Yu-Jhe Li, Kun Wan, Daniel Miranda, Ajinkya Kale, and Yapeng Tian. 2024.

</span>
<span class="ltx_bibblock">Efficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">arXiv preprint arXiv:2411.17760</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deshwal and Chawla (2024)</span>
<span class="ltx_bibblock">
Mahesh Deshwal and Apoorva Chawla. 2024.

</span>
<span class="ltx_bibblock">PHUDGE: Phi-3 as Scalable Judge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2405.08029</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dierickx et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Laurence Dierickx, Arjen Van Dalen, Andreas L Opdahl, and Carl-Gustav Lindén. 2024.

</span>
<span class="ltx_bibblock">Striking the balance in using LLMs for fact-checking: A narrative literature review. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Multidisciplinary International Symposium on Disinformation in Open Online Media</em>. Springer, 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023.

</span>
<span class="ltx_bibblock">Enhancing chat language models by scaling high-quality instructional conversations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">arXiv preprint arXiv:2305.14233</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ding et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, et al<span class="ltx_text" id="bib.bib51.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Crosscodeeval: A diverse and multilingual benchmark for cross-file code completion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.4.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doddapaneni et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Sshubam Verma, and Mitesh M Khapra. 2024.

</span>
<span class="ltx_bibblock">Finding Blind Spots in Evaluator LLMs with Interpretable Checklists.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">arXiv preprint arXiv:2406.13439</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Qingxiu Dong, Li Dong, Xingxing Zhang, Zhifang Sui, and Furu Wei. 2024a.

</span>
<span class="ltx_bibblock">Self-Boosting Large Language Models with Synthetic Preference Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">arXiv preprint arXiv:2410.06961</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et al<span class="ltx_text" id="bib.bib54.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">A survey on in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.4.1">arXiv preprint arXiv:2301.00234</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Yijiang River Dong, Tiancheng Hu, and Nigel Collier. 2024b.

</span>
<span class="ltx_bibblock">Can LLM be a Personalized Judge?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.3.1">arXiv preprint arXiv:2406.11657</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dorner et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Florian E. Dorner, Vivian Y. Nastl, and Moritz Hardt. 2024.

</span>
<span class="ltx_bibblock">Limits to scalable evaluation at the frontier: LLM as Judge won’t beat twice the data.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2410.13341 [cs.LG]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2410.13341" title="">https://arxiv.org/abs/2410.13341</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B Hashimoto. 2024.

</span>
<span class="ltx_bibblock">Length-controlled alpacaeval: A simple way to debias automatic evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">arXiv preprint arXiv:2404.04475</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ebrahimi et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2017.

</span>
<span class="ltx_bibblock">Hotflip: White-box adversarial examples for text classification.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">arXiv preprint arXiv:1712.06751</em> (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elangovan et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Aparna Elangovan, Jongwoo Ko, Lei Xu, Mahsa Elyasi, Ling Liu, Sravan Bodapati, and Dan Roth. 2024.

</span>
<span class="ltx_bibblock">Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">arXiv preprint arXiv:2410.03775</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fabbri et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alexander R Fabbri, Wojciech Kryściński, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021.

</span>
<span class="ltx_bibblock">Summeval: Re-evaluating summarization evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">Transactions of the Association for Computational Linguistics</em> 9 (2021), 391–409.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Angela Fan, Mike Lewis, and Yann Dauphin. 2018.

</span>
<span class="ltx_bibblock">Hierarchical neural story generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">arXiv preprint arXiv:1805.04833</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhiting Fan, Ruizhe Chen, Ruiling Xu, and Zuozhu Liu. 2024.

</span>
<span class="ltx_bibblock">Biasalert: A plug-and-play tool for social bias detection in llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">arXiv preprint arXiv:2407.10241</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fei et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut. 2023.

</span>
<span class="ltx_bibblock">Mitigating label biases for in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">arXiv preprint arXiv:2305.19148</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chao Feng, Xinyu Zhang, and Zichu Fei. 2023.

</span>
<span class="ltx_bibblock">Knowledge solver: Teaching llms to search for domain knowledge from knowledge graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">arXiv preprint arXiv:2309.03118</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhaopeng Feng, Yan Zhang, Hao Li, Wenqiang Liu, Jun Lang, Yang Feng, Jian Wu, and Zuozhu Liu. 2024.

</span>
<span class="ltx_bibblock">Improving llm-based machine translation with systematic self-correction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">arXiv preprint arXiv:2402.16379</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a.

</span>
<span class="ltx_bibblock">Experts, errors, and context: A large-scale study of human evaluation for machine translation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">Transactions of the Association for Computational Linguistics</em> 9 (2021), 1460–1474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Freitag et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Markus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ondřej Bojar. 2021b.

</span>
<span class="ltx_bibblock">Results of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In <em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">Proceedings of the Sixth Conference on Machine Translation</em>. 733–774.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">French (2000)</span>
<span class="ltx_bibblock">
Robert M French. 2000.

</span>
<span class="ltx_bibblock">The Turing Test: the first 50 years.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Trends in cognitive sciences</em> 4, 3 (2000), 115–122.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.

</span>
<span class="ltx_bibblock">Gptscore: Evaluate as you desire.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">arXiv preprint arXiv:2302.04166</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib70.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.3.1">arXiv preprint arXiv:2312.10997</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yicheng Gao, Gonghan Xu, Zhe Wang, and Arman Cohan. 2024.

</span>
<span class="ltx_bibblock">Bayesian Calibration of Win Rate Estimation with LLM Evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">arXiv preprint arXiv:2411.04424</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilardi et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023.

</span>
<span class="ltx_bibblock">ChatGPT outperforms crowd workers for text-annotation tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">Proceedings of the National Academy of Sciences</em> 120, 30 (2023), e2305016120.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gopalakrishnan et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tur. 2023.

</span>
<span class="ltx_bibblock">Topical-chat: Towards knowledge-grounded open-domain conversations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">arXiv preprint arXiv:2308.11995</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guan et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Jian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie Huang. 2021.

</span>
<span class="ltx_bibblock">OpenMEVA: A benchmark for evaluating open-ended story generation metrics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">arXiv preprint arXiv:2105.08920</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et al<span class="ltx_text" id="bib.bib75.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Direct language model alignment from online ai feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib75.4.1">arXiv preprint arXiv:2402.04792</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et al<span class="ltx_text" id="bib.bib76.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Evaluating large language models: A comprehensive survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.4.1">arXiv preprint arXiv:2310.19736</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Taneesh Gupta, Shivam Shandilya, Xuchao Zhang, Supriyo Ghosh, Chetan Bansal, Huaxiu Yao, and Saravan Rajmohan. 2024.

</span>
<span class="ltx_bibblock">Unveiling Context-Aware Criteria in Self-Assessing LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">arXiv preprint arXiv:2410.21545</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hada et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. 2023.

</span>
<span class="ltx_bibblock">Are large language model-based evaluators the solution to scaling up multilingual evaluation?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">arXiv preprint arXiv:2309.07462</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhixiong Han, Yaru Hao, Li Dong, Yutao Sun, and Furu Wei. 2022.

</span>
<span class="ltx_bibblock">Prototypical calibration for few-shot learning of language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">arXiv preprint arXiv:2205.10183</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hao et al<span class="ltx_text" id="bib.bib80.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jing Hao, Yuxiang Zhao, Song Chen, Yanpeng Sun, Qiang Chen, Gang Zhang, Kun Yao, Errui Ding, and Jingdong Wang. 2024.

</span>
<span class="ltx_bibblock">Fullanno: A data engine for enhancing image comprehension of mllms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.3.1">arXiv preprint arXiv:2409.13540</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harper and Konstan (2015)</span>
<span class="ltx_bibblock">
F Maxwell Harper and Joseph A Konstan. 2015.

</span>
<span class="ltx_bibblock">The movielens datasets: History and context.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">Acm transactions on interactive intelligent systems (tiis)</em> 5, 4 (2015), 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasanbeig et al<span class="ltx_text" id="bib.bib82.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe Vieira Frujeri, and Ida Momennejad. 2023.

</span>
<span class="ltx_bibblock">ALLURE: auditing and improving llm-based evaluation of text using iterative in-context-learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.3.1">arXiv e-prints</em> (2023), arXiv–2309.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Hangfeng He, Hongming Zhang, and Dan Roth. 2023b.

</span>
<span class="ltx_bibblock">Socreval: Large language models with the socratic method for reference-free reasoning evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib83.3.1">arXiv preprint arXiv:2310.00074</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib84.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Xingwei He, Zhenghao Lin, Yeyun Gong, Alex Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, Weizhu Chen, et al<span class="ltx_text" id="bib.bib84.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">Annollm: Making large language models to be better crowdsourced annotators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.4.1">arXiv preprint arXiv:2303.16854</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib85.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Yuanqin He, Yan Kang, Lixin Fan, and Qiang Yang. 2024b.

</span>
<span class="ltx_bibblock">FedEval-LLM: Federated Evaluation of Large Language Models on Downstream Tasks with Collective Wisdom.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib85.3.1">arXiv preprint arXiv:2404.12273</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Zeyu He, Chieh-Yang Huang, Chien-Kuang Cornelia Ding, Shaurya Rohatgi, and Ting-Hao Kenneth Huang. 2024a.

</span>
<span class="ltx_bibblock">If in a Crowdsourced Data Annotation Pipeline, a GPT-4. In <em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–25.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hijazi et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hashem Hijazi, Diego Molla, Vincent Nguyen, and Sarvnaz Karimi. 2024.

</span>
<span class="ltx_bibblock">Using Large Language Models to Evaluate Biomedical Query-Focused Summarisation. In <em class="ltx_emph ltx_font_italic" id="bib.bib87.3.1">Proceedings of the 23rd Workshop on Biomedical Natural Language Processing</em>. 236–242.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib88.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne Xin Zhao. 2024.

</span>
<span class="ltx_bibblock">Large language models are zero-shot rankers for recommender systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib88.3.1">European Conference on Information Retrieval</em>. Springer, 364–381.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib89.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Xinyu Hu, Li Lin, Mingqi Gao, Xunjian Yin, and Xiaojun Wan. 2024a.

</span>
<span class="ltx_bibblock">Themis: A reference-free nlg evaluation language model with flexibility and interpretability.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.3.1">arXiv preprint arXiv:2406.18365</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib90.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zhengyu Hu, Linxin Song, Jieyu Zhang, Zheyuan Xiao, Jingang Wang, Zhenyu Chen, and Hui Xiong. 2024b.

</span>
<span class="ltx_bibblock">Rethinking llm-based preference evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib90.3.1">arXiv preprint arXiv:2407.01085</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib91.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Zhengyu Hu, Jieyu Zhang, Zhihan Xiong, Alexander Ratner, Hui Xiong, and Ranjay Krishna. 2024c.

</span>
<span class="ltx_bibblock">Language Model Preference Evaluation with Multiple Weak Evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.3.1">arXiv preprint arXiv:2410.12869</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib92.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Hui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. 2024a.

</span>
<span class="ltx_bibblock">An empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib92.3.1">arXiv preprint arXiv:2403.02839</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Hui Huang, Yingqi Qu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, and Tiejun Zhao. 2024b.

</span>
<span class="ltx_bibblock">On the limitations of fine-tuned judge models for llm evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.3.1">arXiv preprint arXiv:2403.02839</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Large language models cannot self-correct reasoning yet.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">arXiv preprint arXiv:2310.01798</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al<span class="ltx_text" id="bib.bib95.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. 2023.

</span>
<span class="ltx_bibblock">Multi-dimensional evaluation of text summarization with in-context learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.3.1">arXiv preprint arXiv:2306.01200</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeong et al<span class="ltx_text" id="bib.bib96.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Minbyul Jeong, Jiwoong Sohn, Mujeen Sung, and Jaewoo Kang. 2024.

</span>
<span class="ltx_bibblock">Improving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.3.1">Bioinformatics</em> 40, Supplement_1 (2024), i119–i129.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al<span class="ltx_text" id="bib.bib97.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen, Josef Dai, Boren Zheng, Tianyi Qiu, Boxun Li, and Yaodong Yang. 2024.

</span>
<span class="ltx_bibblock">PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib97.3.1">arXiv preprint arXiv:2406.15513</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ji et al<span class="ltx_text" id="bib.bib98.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and Pascale Fung. 2023.

</span>
<span class="ltx_bibblock">Survey of hallucination in natural language generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.3.1">Comput. Surveys</em> 55, 12 (2023), 1–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J Su, Camillo J Taylor, and Dan Roth. 2024.

</span>
<span class="ltx_bibblock">A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib99.3.1">arXiv preprint arXiv:2406.11050</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib100.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill Yuchen Lin, and Wenhu Chen. 2023b.

</span>
<span class="ltx_bibblock">Tigerscore: Towards building explainable metric for all text generation tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib100.3.1">Transactions on Machine Learning Research</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al<span class="ltx_text" id="bib.bib101.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Shuyu Jiang, Xingshu Chen, and Rui Tang. 2023a.

</span>
<span class="ltx_bibblock">Prompt packer: Deceiving llms through compositional instruction with hidden attacks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.3.1">arXiv preprint arXiv:2310.10077</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jimenez et al<span class="ltx_text" id="bib.bib102.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023.

</span>
<span class="ltx_bibblock">Swe-bench: Can language models resolve real-world github issues?, 2024.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.3.1">URL https://arxiv. org/abs/2310.06770</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jinnai et al<span class="ltx_text" id="bib.bib103.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yuu Jinnai, Tetsuro Morimura, Kaito Ariu, and Kenshi Abe. 2024.

</span>
<span class="ltx_bibblock">Regularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib103.3.1">arXiv preprint arXiv:2404.01054</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jung et al<span class="ltx_text" id="bib.bib104.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jaehun Jung, Faeze Brahman, and Yejin Choi. 2024.

</span>
<span class="ltx_bibblock">Trust or Escalate: LLM Judges with Provable Guarantees for Human Agreement.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib104.3.1">arXiv preprint arXiv:2407.18370</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Karpinska and Iyyer (2023)</span>
<span class="ltx_bibblock">
Marzena Karpinska and Mohit Iyyer. 2023.

</span>
<span class="ltx_bibblock">Large language models effectively leverage document-level context for literary translation, but critical errors persist.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">arXiv preprint arXiv:2304.03245</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kawabata and Sugawara (2024)</span>
<span class="ltx_bibblock">
Akira Kawabata and Saku Sugawara. 2024.

</span>
<span class="ltx_bibblock">Rationale-Aware Answer Verification by Pairwise Self-Evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">arXiv preprint arXiv:2410.04838</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al<span class="ltx_text" id="bib.bib107.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Pei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, et al<span class="ltx_text" id="bib.bib107.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Critiquellm: Towards an informative critique generation model for evaluation of large language model generation. In <em class="ltx_emph ltx_font_italic" id="bib.bib107.4.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 13034–13054.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al<span class="ltx_text" id="bib.bib108.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R Bowman, Tim Rocktäschel, and Ethan Perez. 2024.

</span>
<span class="ltx_bibblock">Debating with more persuasive llms leads to more truthful answers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib108.3.1">arXiv preprint arXiv:2402.06782</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib109.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Dongyoung Kim, Kimin Lee, Jinwoo Shin, and Jaehyung Kim. 2024a.

</span>
<span class="ltx_bibblock">Aligning Large Language Models with Self-generated Preference Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.3.1">arXiv preprint arXiv:2406.04412</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib110.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al<span class="ltx_text" id="bib.bib110.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Prometheus: Inducing fine-grained evaluation capability in language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib110.4.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib111.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024b.

</span>
<span class="ltx_bibblock">Prometheus 2: An open source language model specialized in evaluating other language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.3.1">arXiv preprint arXiv:2405.01535</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ko et al<span class="ltx_text" id="bib.bib112.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Miyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, and Jaewoo Kang. 2020.

</span>
<span class="ltx_bibblock">Look at the first sentence: Position bias in question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib112.3.1">arXiv preprint arXiv:2004.14602</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koo et al<span class="ltx_text" id="bib.bib113.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, and Dongyeop Kang. 2023.

</span>
<span class="ltx_bibblock">Benchmarking cognitive biases in large language models as evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib113.3.1">arXiv preprint arXiv:2309.17012</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kotonya et al<span class="ltx_text" id="bib.bib114.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Neema Kotonya, Saran Krishnasamy, Joel Tetreault, and Alejandro Jaimes. 2023.

</span>
<span class="ltx_bibblock">Little giants: Exploring the potential of small llms as evaluation metrics in summarization in the eval4nlp 2023 shared task.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.3.1">arXiv preprint arXiv:2311.00686</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krolik et al<span class="ltx_text" id="bib.bib115.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jack Krolik, Herprit Mahal, Feroz Ahmad, Gaurav Trivedi, and Bahador Saket. 2024.

</span>
<span class="ltx_bibblock">Towards Leveraging Large Language Models for Automated Medical Q&amp;A Evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib115.3.1">arXiv preprint arXiv:2409.01941</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumar et al<span class="ltx_text" id="bib.bib116.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Abhishek Kumar, Sonia Haiduc, Partha Pratim Das, and Partha Pratim Chakrabarti. 2024.

</span>
<span class="ltx_bibblock">LLMs as Evaluators: A Novel Approach to Evaluate Bug Report Summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib116.3.1">arXiv preprint arXiv:2409.00630</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lambert et al<span class="ltx_text" id="bib.bib117.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al<span class="ltx_text" id="bib.bib117.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Rewardbench: Evaluating reward models for language modeling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib117.4.1">arXiv preprint arXiv:2403.13787</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Latif et al<span class="ltx_text" id="bib.bib118.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Siddique Latif, Muhammad Usama, Mohammad Ibrahim Malik, and Björn W Schuller. 2023.

</span>
<span class="ltx_bibblock">Can large language models aid in annotating speech emotional data? uncovering new frontiers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.3.1">arXiv preprint arXiv:2307.06090</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lawrie et al<span class="ltx_text" id="bib.bib119.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Dawn Lawrie, Sean MacAvaney, James Mayfield, Paul McNamee, Douglas W Oard, Luca Soldaini, and Eugene Yang. 2024.

</span>
<span class="ltx_bibblock">Overview of the TREC 2023 NeuCLIR Track.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib119.3.1">arXiv preprint arXiv:2404.08071</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeCun et al<span class="ltx_text" id="bib.bib120.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015.

</span>
<span class="ltx_bibblock">Deep learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.3.1">nature</em> 521, 7553 (2015), 436–444.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib121.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Deokjae Lee, Seungyong Moon, Junhyeok Lee, and Hyun Oh Song. 2022.

</span>
<span class="ltx_bibblock">Query-efficient and scalable black-box adversarial attacks on discrete sequential data via bayesian optimization. In <em class="ltx_emph ltx_font_italic" id="bib.bib121.3.1">International Conference on Machine Learning</em>. PMLR, 12478–12497.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib122.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Harrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Ren Lu, Thomas Mesnard, Johan Ferret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. 2023.

</span>
<span class="ltx_bibblock">Rlaif: Scaling reinforcement learning from human feedback with ai feedback.

</span>
<span class="ltx_bibblock">(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib123.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Seongyun Lee, Seungone Kim, Sue Hyun Park, Geewook Kim, and Minjoon Seo. 2024a.

</span>
<span class="ltx_bibblock">Prometheusvision: Vision-language model as a judge for fine-grained evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib123.3.1">arXiv preprint arXiv:2401.06591</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib124.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min Yoo, and Youngjae Yu. 2024b.

</span>
<span class="ltx_bibblock">Aligning Large Language Models by On-Policy Self-Judgment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib124.3.1">arXiv preprint arXiv:2402.11253</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lei et al<span class="ltx_text" id="bib.bib125.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. 2024.

</span>
<span class="ltx_bibblock">RecExplainer: Aligning Large Language Models for Explaining Recommendation Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib125.3.1">Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em>. 1530–1541.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib126.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al<span class="ltx_text" id="bib.bib126.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for knowledge-intensive nlp tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib126.4.1">Advances in Neural Information Processing Systems</em> 33 (2020), 9459–9474.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib127.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Anqi Li, Yu Lu, Nirui Song, Shuai Zhang, Lizhi Ma, and Zhenzhong Lan. 2024c.

</span>
<span class="ltx_bibblock">Automatic evaluation for mental health counseling using llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib127.3.1">arXiv preprint arXiv:2402.11958</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib128.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Haitao Li, Junjie Chen, Qingyao Ai, Zhumin Chu, Yujia Zhou, Qian Dong, and Yiqun Liu. 2024a.

</span>
<span class="ltx_bibblock">Calibraeval: Calibrating prediction distribution to mitigate selection bias in llms-as-judges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib128.3.1">arXiv preprint arXiv:2410.15393</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib129.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Haitao Li, You Chen, Qingyao Ai, Yueyue Wu, Ruizhe Zhang, and Yiqun Liu. 2024b.

</span>
<span class="ltx_bibblock">LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2409.20288 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2409.20288" title="">https://arxiv.org/abs/2409.20288</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib130.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Haitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, and Yiqun Liu. 2024d.

</span>
<span class="ltx_bibblock">Lecardv2: A large-scale chinese legal case retrieval dataset. In <em class="ltx_emph ltx_font_italic" id="bib.bib130.3.1">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</em>. 2251–2260.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib131.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2023c.

</span>
<span class="ltx_bibblock">Generative judge for evaluating alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib131.3.1">arXiv preprint arXiv:2310.05470</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib132.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Qintong Li, Leyang Cui, Lingpeng Kong, and Wei Bi. 2023a.

</span>
<span class="ltx_bibblock">Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib132.3.1">arXiv preprint arXiv:2310.19740</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib133.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Ruosen Li, Teerth Patel, and Xinya Du. 2023b.

</span>
<span class="ltx_bibblock">Prd: Peer rank and discussion improve large language model based evaluations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib133.3.1">arXiv preprint arXiv:2307.02762</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib134.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017.

</span>
<span class="ltx_bibblock">DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset. In <em class="ltx_emph ltx_font_italic" id="bib.bib134.3.1">Proceedings of The 8th International Joint Conference on Natural Language Processing (IJCNLP 2017)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib135.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Zongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2023d.

</span>
<span class="ltx_bibblock">Split and merge: Aligning position biases in large language model based evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib135.3.1">arXiv preprint arXiv:2310.01432</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span class="ltx_text" id="bib.bib136.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Jingcong Liang, Rong Ye, Meng Han, Ruofei Lai, Xinyu Zhang, Xuanjing Huang, and Zhongyu Wei. 2024a.

</span>
<span class="ltx_bibblock">Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib136.3.1">arXiv preprint arXiv:2403.08010</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span class="ltx_text" id="bib.bib137.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Sirui Liang, Baoli Zhang, Jun Zhao, and Kang Liu. 2024b.

</span>
<span class="ltx_bibblock">ABSEval: An Agent-based Framework for Script Evaluation. In <em class="ltx_emph ltx_font_italic" id="bib.bib137.3.1">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>. 12418–12434.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lightman et al<span class="ltx_text" id="bib.bib138.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023.

</span>
<span class="ltx_bibblock">Let’s verify step by step.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib138.3.1">arXiv preprint arXiv:2305.20050</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib139.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Bill Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. 2024.

</span>
<span class="ltx_bibblock">WILDBENCH: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib139.3.1">arXiv preprint arXiv:2406.04770</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin (2004)</span>
<span class="ltx_bibblock">
Chin-Yew Lin. 2004.

</span>
<span class="ltx_bibblock">Rouge: A package for automatic evaluation of summaries. In <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">Text summarization branches out</em>. 74–81.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib141.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Stephanie Lin, Jacob Hilton, and Owain Evans. 2021.

</span>
<span class="ltx_bibblock">Truthfulqa: Measuring how models mimic human falsehoods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib141.3.1">arXiv preprint arXiv:2109.07958</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin and Chen (2023)</span>
<span class="ltx_bibblock">
Yen-Ting Lin and Yun-Nung Chen. 2023.

</span>
<span class="ltx_bibblock">Llm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">arXiv preprint arXiv:2305.13711</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib143.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Minqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, and Lifu Huang. 2023c.

</span>
<span class="ltx_bibblock">X-eval: Generalizable multi-aspect text evaluation via augmented instruction tuning with auxiliary evaluation aspects.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib143.3.1">arXiv preprint arXiv:2311.08788</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib144.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al<span class="ltx_text" id="bib.bib144.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">Alignbench: Benchmarking chinese alignment of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib144.4.1">arXiv preprint arXiv:2311.18743</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib145.2.2.1">.</span> (2023e)</span>
<span class="ltx_bibblock">
Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al<span class="ltx_text" id="bib.bib145.3.1">.</span> 2023e.

</span>
<span class="ltx_bibblock">Agentbench: Evaluating llms as agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib145.4.1">arXiv preprint arXiv:2308.03688</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib146.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023a.

</span>
<span class="ltx_bibblock">G-eval: Nlg evaluation using gpt-4 with better human alignment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib146.3.1">arXiv preprint arXiv:2303.16634</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib147.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2023d.

</span>
<span class="ltx_bibblock">Calibrating llm-based evaluator.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib147.3.1">arXiv preprint arXiv:2309.13308</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib148.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Yuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2024a.

</span>
<span class="ltx_bibblock">HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib148.3.1">arXiv preprint arXiv:2402.15754</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib149.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Yantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2024b.

</span>
<span class="ltx_bibblock">RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib149.3.1">arXiv preprint arXiv:2410.16184</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib150.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Yinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vulić, Anna Korhonen, and Nigel Collier. 2024c.

</span>
<span class="ltx_bibblock">Aligning with human judgement: The role of pairwise preference in large language model evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib150.3.1">arXiv preprint arXiv:2403.16950</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liusie et al<span class="ltx_text" id="bib.bib151.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Adian Liusie, Vatsal Raina, Yassir Fathullah, and Mark Gales. 2024.

</span>
<span class="ltx_bibblock">Efficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib151.3.1">arXiv preprint arXiv:2405.05894</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LLMS (2025)</span>
<span class="ltx_bibblock">
SESSMENTS BY LLMS. 2025.

</span>
<span class="ltx_bibblock">JUDING THE JUDGES: ASYSTEMATIC INVESTIGATION OF POSITION BIAS IN PAIRWISE COMPARATIVE AS.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">Under review as a conference paper at ICLR 2025</em> (2025).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib153.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023.

</span>
<span class="ltx_bibblock">An empirical study of catastrophic forgetting in large language models during continual fine-tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib153.3.1">arXiv preprint arXiv:2308.08747</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib154.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ziyang Luo, Haoning Wu, Dongxu Li, Jing Ma, Mohan Kankanhalli, and Junnan Li. 2024.

</span>
<span class="ltx_bibblock">VideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib154.3.1">arXiv preprint arXiv:2411.13281</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib155.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shengjie Ma, Chong Chen, Qi Chu, and Jiaxin Mao. 2024.

</span>
<span class="ltx_bibblock">Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib155.3.1">arXiv preprint arXiv:2403.18405</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Madaan et al<span class="ltx_text" id="bib.bib156.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al<span class="ltx_text" id="bib.bib156.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Self-refine: Iterative refinement with self-feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib156.4.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mehri and Eskenazi (2020)</span>
<span class="ltx_bibblock">
Shikib Mehri and Maxine Eskenazi. 2020.

</span>
<span class="ltx_bibblock">USR: An unsupervised and reference free evaluation metric for dialog generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">arXiv preprint arXiv:2005.00456</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mendonça et al<span class="ltx_text" id="bib.bib158.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
John Mendonça, Isabel Trancoso, and Alon Lavie. 2024.

</span>
<span class="ltx_bibblock">Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib158.3.1">arXiv preprint arXiv:2408.10902</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moniri et al<span class="ltx_text" id="bib.bib159.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Behrad Moniri, Hamed Hassani, and Edgar Dobriban. 2024.

</span>
<span class="ltx_bibblock">Evaluating the Performance of Large Language Models via Debates.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib159.3.1">arXiv preprint arXiv:2406.11044</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mostafazadeh et al<span class="ltx_text" id="bib.bib160.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016.

</span>
<span class="ltx_bibblock">A corpus and cloze evaluation for deeper understanding of commonsense stories. In <em class="ltx_emph ltx_font_italic" id="bib.bib160.3.1">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>. 839–849.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Musolesi (2024)</span>
<span class="ltx_bibblock">
Mirco Musolesi. 2024.

</span>
<span class="ltx_bibblock">Creative Beam Search: LLM-as-a-Judge for Improving Response Generation. ICCC.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Myrzakhan et al<span class="ltx_text" id="bib.bib162.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Aidar Myrzakhan, Sondos Mahmoud Bsharat, and Zhiqiang Shen. 2024.

</span>
<span class="ltx_bibblock">Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib162.3.1">arXiv preprint arXiv:2406.07545</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Narayan et al<span class="ltx_text" id="bib.bib163.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Shashi Narayan, Shay B Cohen, and Mirella Lapata. 2018.

</span>
<span class="ltx_bibblock">Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib163.3.1">arXiv preprint arXiv:1808.08745</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nasrabadi (2024)</span>
<span class="ltx_bibblock">
Dom Nasrabadi. 2024.

</span>
<span class="ltx_bibblock">JurEE not Judges: safeguarding llm interactions with small, specialised Encoder Ensembles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib164.1.1">arXiv preprint arXiv:2410.08442</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nilsson (2014)</span>
<span class="ltx_bibblock">
Nils J Nilsson. 2014.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib165.1.1">Principles of artificial intelligence</em>.

</span>
<span class="ltx_bibblock">Morgan Kaufmann.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ning et al<span class="ltx_text" id="bib.bib166.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Kun-Peng Ning, Shuo Yang, Yuyang Liu, Jia-Yu Yao, Zhenhui Liu, Yu Wang, Ming Pang, and Li Yuan. 2024.

</span>
<span class="ltx_bibblock">PiCO: Peer Review in LLMs based on the Consistency Optimization.

</span>
<span class="ltx_bibblock">(2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Niu et al<span class="ltx_text" id="bib.bib167.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Tong Niu, Shafiq Joty, Ye Liu, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. 2024.

</span>
<span class="ltx_bibblock">JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib167.3.1">arXiv preprint arXiv:2411.00142</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">O’Donoghue et al<span class="ltx_text" id="bib.bib168.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Odhran O’Donoghue, Aleksandar Shtedritski, John Ginger, Ralph Abboud, Ali Essa Ghareeb, Justin Booth, and Samuel G Rodriques. 2023.

</span>
<span class="ltx_bibblock">BioPlanner: automatic evaluation of LLMs on protocol planning in biology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib168.3.1">arXiv preprint arXiv:2310.10632</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Owens et al<span class="ltx_text" id="bib.bib169.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Deonna M Owens, Ryan A Rossi, Sungchul Kim, Tong Yu, Franck Dernoncourt, Xiang Chen, Ruiyi Zhang, Jiuxiang Gu, Hanieh Deilamsalehy, and Nedim Lipka. 2024.

</span>
<span class="ltx_bibblock">A multi-llm debiasing framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib169.3.1">arXiv preprint arXiv:2409.13884</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pagnoni et al<span class="ltx_text" id="bib.bib170.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021.

</span>
<span class="ltx_bibblock">Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib170.3.1">arXiv preprint arXiv:2104.13346</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span class="ltx_text" id="bib.bib171.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Qian Pan, Zahra Ashktorab, Michael Desmond, Martin Santillan Cooper, James Johnson, Rahul Nair, Elizabeth Daly, and Werner Geyer. 2024a.

</span>
<span class="ltx_bibblock">Human-Centered Design Recommendations for LLM-as-a-judge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib171.3.1">arXiv preprint arXiv:2407.03479</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pan et al<span class="ltx_text" id="bib.bib172.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2024b.

</span>
<span class="ltx_bibblock">Unifying large language models and knowledge graphs: A roadmap.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib172.3.1">IEEE Transactions on Knowledge and Data Engineering</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Panickssery et al<span class="ltx_text" id="bib.bib173.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Arjun Panickssery, Samuel R Bowman, and Shi Feng. 2024.

</span>
<span class="ltx_bibblock">Llm evaluators recognize and favor their own generations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib173.3.1">arXiv preprint arXiv:2404.13076</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al<span class="ltx_text" id="bib.bib174.2.2.1">.</span> (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation. In <em class="ltx_emph ltx_font_italic" id="bib.bib174.3.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>. 311–318.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al<span class="ltx_text" id="bib.bib175.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. 2024.

</span>
<span class="ltx_bibblock">Offsetbias: Leveraging debiased data for tuning evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib175.3.1">arXiv preprint arXiv:2407.06551</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel et al<span class="ltx_text" id="bib.bib176.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Bhrij Patel, Souradip Chakraborty, Wesley A Suttle, Mengdi Wang, Amrit Singh Bedi, and Dinesh Manocha. 2024.

</span>
<span class="ltx_bibblock">AIME: AI System Optimization via Multiple LLM Evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib176.3.1">arXiv preprint arXiv:2410.03131</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Paul et al<span class="ltx_text" id="bib.bib177.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023.

</span>
<span class="ltx_bibblock">Refiner: Reasoning feedback on intermediate representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib177.3.1">arXiv preprint arXiv:2304.01904</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez and Ribeiro (2022)</span>
<span class="ltx_bibblock">
Fábio Perez and Ian Ribeiro. 2022.

</span>
<span class="ltx_bibblock">Ignore previous prompt: Attack techniques for language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib178.1.1">arXiv preprint arXiv:2211.09527</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pezeshkpour and Hruschka (2023)</span>
<span class="ltx_bibblock">
Pouya Pezeshkpour and Estevam Hruschka. 2023.

</span>
<span class="ltx_bibblock">Large language models sensitivity to the order of options in multiple-choice questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib179.1.1">arXiv preprint arXiv:2308.11483</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poulain et al<span class="ltx_text" id="bib.bib180.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Raphael Poulain, Hamed Fayyaz, and Rahmatollah Beheshti. 2024.

</span>
<span class="ltx_bibblock">Bias patterns in the application of LLMs for clinical decision support: A comprehensive study.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib180.3.1">arXiv preprint arXiv:2404.15149</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qin et al<span class="ltx_text" id="bib.bib181.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, et al<span class="ltx_text" id="bib.bib181.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Large language models are effective text rankers with pairwise ranking prompting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib181.4.1">arXiv preprint arXiv:2306.17563</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rafailov et al<span class="ltx_text" id="bib.bib182.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024.

</span>
<span class="ltx_bibblock">Direct preference optimization: Your language model is secretly a reward model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib182.3.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raghubir and Valenzuela (2006)</span>
<span class="ltx_bibblock">
Priya Raghubir and Ana Valenzuela. 2006.

</span>
<span class="ltx_bibblock">Center-of-inattention: Position biases in decision-making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib183.1.1">Organizational Behavior and Human Decision Processes</em> 99, 1 (2006), 66–80.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rahmani et al<span class="ltx_text" id="bib.bib184.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hossein A Rahmani, Emine Yilmaz, Nick Craswell, Bhaskar Mitra, Paul Thomas, Charles LA Clarke, Mohammad Aliannejadi, Clemencia Siro, and Guglielmo Faggioli. 2024.

</span>
<span class="ltx_bibblock">LLMJudge: LLMs for Relevance Judgments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib184.3.1">arXiv preprint arXiv:2408.08896</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raina et al<span class="ltx_text" id="bib.bib185.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Vyas Raina, Adian Liusie, and Mark Gales. 2024.

</span>
<span class="ltx_bibblock">Is LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib185.3.1">arXiv preprint arXiv:2402.14016</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raju et al<span class="ltx_text" id="bib.bib186.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ravi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, and Urmish Thakker. 2024.

</span>
<span class="ltx_bibblock">Constructing domain-specific evaluation sets for llm-as-a-judge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib186.3.1">arXiv preprint arXiv:2408.08808</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib187.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jie Ren, Yao Zhao, Tu Vu, Peter J Liu, and Balaji Lakshminarayanan. 2023.

</span>
<span class="ltx_bibblock">Self-evaluation improves selective generation in large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib187.3.1">Proceedings on</em>. PMLR, 49–64.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ryu et al<span class="ltx_text" id="bib.bib188.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Cheol Ryu, Seolhwa Lee, Subeen Pang, Chanyeol Choi, Hojun Choi, Myeonggee Min, and Jy-Yong Sohn. 2023.

</span>
<span class="ltx_bibblock">Retrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA. In <em class="ltx_emph ltx_font_italic" id="bib.bib188.3.1">Proceedings of the Natural Legal Language Processing Workshop 2023</em>. 132–137.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saad-Falcon et al<span class="ltx_text" id="bib.bib189.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2023.

</span>
<span class="ltx_bibblock">Ares: An automated evaluation framework for retrieval-augmented generation systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib189.3.1">arXiv preprint arXiv:2311.09476</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sahoo et al<span class="ltx_text" id="bib.bib190.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024.

</span>
<span class="ltx_bibblock">A systematic survey of prompt engineering in large language models: Techniques and applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib190.3.1">arXiv preprint arXiv:2402.07927</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sedgwick (2014)</span>
<span class="ltx_bibblock">
Philip Sedgwick. 2014.

</span>
<span class="ltx_bibblock">Spearman’s rank correlation coefficient.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib191.1.1">Bmj</em> 349 (2014).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sen (1968)</span>
<span class="ltx_bibblock">
Pranab Kumar Sen. 1968.

</span>
<span class="ltx_bibblock">Estimates of the regression coefficient based on Kendall’s tau.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib192.1.1">Journal of the American statistical association</em> 63, 324 (1968), 1379–1389.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shankar et al<span class="ltx_text" id="bib.bib193.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shreya Shankar, JD Zamfirescu-Pereira, Björn Hartmann, Aditya Parameswaran, and Ian Arawjo. 2024.

</span>
<span class="ltx_bibblock">Who validates the validators? aligning llm-assisted evaluation of llm outputs with human preferences. In <em class="ltx_emph ltx_font_italic" id="bib.bib193.3.1">Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology</em>. 1–14.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen et al<span class="ltx_text" id="bib.bib194.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2023.

</span>
<span class="ltx_bibblock">" do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib194.3.1">arXiv preprint arXiv:2308.03825</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shen and Wan (2023)</span>
<span class="ltx_bibblock">
Yuchen Shen and Xiaojun Wan. 2023.

</span>
<span class="ltx_bibblock">Opinsummeval: Revisiting automated evaluation for opinion summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib195.1.1">arXiv preprint arXiv:2310.18122</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib196.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, Nathanael Schärli, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Large language models can be easily distracted by irrelevant context. In <em class="ltx_emph ltx_font_italic" id="bib.bib196.3.1">International Conference on Machine Learning</em>. PMLR, 31210–31227.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib197.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Jiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, and Neil Zhenqiang Gong. 2024b.

</span>
<span class="ltx_bibblock">Optimization-based Prompt Injection Attack to LLM-as-a-Judge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib197.3.1">arXiv preprint arXiv:2403.17710</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib198.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Lin Shi, Chiyu Ma, Wenhua Liang, Weicheng Ma, and Soroush Vosoughi. 2024a.

</span>
<span class="ltx_bibblock">Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib198.3.1">arXiv preprint arXiv:2406.07791</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shu et al<span class="ltx_text" id="bib.bib199.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Lei Shu, Nevan Wichers, Liangchen Luo, Yun Zhu, Yinxiao Liu, Jindong Chen, and Lei Meng. 2024.

</span>
<span class="ltx_bibblock">Fusion-Eval: Integrating Assistant Evaluators with LLMs. In <em class="ltx_emph ltx_font_italic" id="bib.bib199.3.1">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track</em>. 225–238.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al<span class="ltx_text" id="bib.bib200.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Avi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et al<span class="ltx_text" id="bib.bib200.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Beyond human data: Scaling self-training for problem-solving with language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib200.4.1">arXiv preprint arXiv:2312.06585</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soboroff (2024)</span>
<span class="ltx_bibblock">
Ian Soboroff. 2024.

</span>
<span class="ltx_bibblock">Don’t Use LLMs to Make Relevance Judgments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib201.1.1">arXiv preprint arXiv:2409.15133</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Son et al<span class="ltx_text" id="bib.bib202.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Guijin Son, Hyunjun Jeon, Chami Hwang, and Hanearl Jung. 2024a.

</span>
<span class="ltx_bibblock">KRX Bench: Automating Financial Benchmark Creation via Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib202.3.1">Proceedings of the Joint Workshop of the 7th Financial Technology and Natural Language Processing, the 5th Knowledge Discovery from Unstructured Data in Financial Services, and the 4th Workshop on Economics and Natural Language Processing@ LREC-COLING 2024</em>. 10–20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Son et al<span class="ltx_text" id="bib.bib203.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Guijin Son, Dongkeun Yoon, Juyoung Suk, Javier Aula-Blasco, Mano Aslan, Vu Trong Kim, Shayekh Bin Islam, Jaume Prats-Cristià, Lucía Tormo-Bañuelos, and Seungone Kim. 2024b.

</span>
<span class="ltx_bibblock">MM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib203.3.1">arXiv preprint arXiv:2410.17578</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib204.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, and Saab Mansour. 2024a.

</span>
<span class="ltx_bibblock">FineSurE: Fine-grained summarization evaluation using LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib204.3.1">arXiv preprint arXiv:2407.00908</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib205.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Mingyang Song, Mao Zheng, and Xuan Luo. 2024b.

</span>
<span class="ltx_bibblock">Can Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib205.3.1">arXiv preprint arXiv:2406.11629</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Song et al<span class="ltx_text" id="bib.bib206.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Yishen Song, Qianta Zhu, Huaibo Wang, and Qinhua Zheng. 2024c.

</span>
<span class="ltx_bibblock">Automated Essay Scoring and Revising Based on Open-Source Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib206.3.1">IEEE Transactions on Learning Technologies</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sottana et al<span class="ltx_text" id="bib.bib207.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan. 2023.

</span>
<span class="ltx_bibblock">Evaluation metrics in the era of GPT-4: reliably evaluating large language models on sequence to sequence tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib207.3.1">arXiv preprint arXiv:2310.13800</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stephan et al<span class="ltx_text" id="bib.bib208.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Andreas Stephan, Dawei Zhu, Matthias Aßenmacher, Xiaoyu Shen, and Benjamin Roth. 2024.

</span>
<span class="ltx_bibblock">From calculation to adjudication: Examining llm judges on mathematical reasoning tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib208.3.1">arXiv preprint arXiv:2409.04168</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stureborg et al<span class="ltx_text" id="bib.bib209.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Rickard Stureborg, Dimitris Alikaniotis, and Yoshi Suhara. 2024.

</span>
<span class="ltx_bibblock">Large language models are inconsistent and biased evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib209.3.1">arXiv preprint arXiv:2405.01724</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib210.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Hanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024.

</span>
<span class="ltx_bibblock">Fast Best-of-N Decoding via Speculative Rejection.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib210.3.1">arXiv preprint arXiv:2410.20290</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun (2020)</span>
<span class="ltx_bibblock">
Lichao Sun. 2020.

</span>
<span class="ltx_bibblock">Natural backdoor attack on text data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib211.1.1">arXiv preprint arXiv:2006.16176</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al<span class="ltx_text" id="bib.bib212.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Lichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai, Jia Li, Philip Yu, and Caiming Xiong. 2020.

</span>
<span class="ltx_bibblock">Adv-bert: Bert is not robust on misspellings! generating nature adversarial samples on bert.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib212.3.1">arXiv preprint arXiv:2003.04985</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Szymanski et al<span class="ltx_text" id="bib.bib213.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Annalisa Szymanski, Noah Ziems, Heather A Eicher-Miller, Toby Jia-Jun Li, Meng Jiang, and Ronald A Metoyer. 2024.

</span>
<span class="ltx_bibblock">Limitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib213.3.1">arXiv preprint arXiv:2410.20266</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al<span class="ltx_text" id="bib.bib214.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Sijun Tan, Siyuan Zhuang, Kyle Montgomery, William Y Tang, Alejandro Cuadron, Chenguang Wang, Raluca Ada Popa, and Ion Stoica. 2024.

</span>
<span class="ltx_bibblock">JudgeBench: A Benchmark for Evaluating LLM-based Judges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib214.3.1">arXiv preprint arXiv:2410.12784</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tessler et al<span class="ltx_text" id="bib.bib215.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Michael Henry Tessler, Michiel A Bakker, Daniel Jarrett, Hannah Sheahan, Martin J Chadwick, Raphael Koster, Georgina Evans, Lucy Campbell-Gillingham, Tantum Collins, David C Parkes, et al<span class="ltx_text" id="bib.bib215.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">AI can help humans find common ground in democratic deliberation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib215.4.1">Science</em> 386, 6719 (2024), eadq2852.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thakur et al<span class="ltx_text" id="bib.bib216.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. 2024.

</span>
<span class="ltx_bibblock">Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib216.3.1">arXiv preprint arXiv:2406.12624</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tonmoy et al<span class="ltx_text" id="bib.bib217.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
SM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024.

</span>
<span class="ltx_bibblock">A comprehensive survey of hallucination mitigation techniques in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib217.3.1">arXiv preprint arXiv:2401.01313</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Törnberg (2023)</span>
<span class="ltx_bibblock">
Petter Törnberg. 2023.

</span>
<span class="ltx_bibblock">Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib218.1.1">arXiv preprint arXiv:2304.06588</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib219.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al<span class="ltx_text" id="bib.bib219.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib219.4.1">arXiv preprint arXiv:2307.09288</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et al<span class="ltx_text" id="bib.bib220.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Harsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. 2024b.

</span>
<span class="ltx_bibblock">Appworld: A controllable world of apps and people for benchmarking interactive coding agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib220.3.1">arXiv preprint arXiv:2407.18901</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trivedi et al<span class="ltx_text" id="bib.bib221.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Prapti Trivedi, Aditya Gulati, Oliver Molenschot, Meghana Arakkal Rajeev, Rajkumar Ramamurthy, Keith Stevens, Tanveesh Singh Chaudhery, Jahnavi Jambholkar, James Zou, and Nazneen Rajani. 2024a.

</span>
<span class="ltx_bibblock">Self-rationalization improves LLM as a fine-grained judge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib221.3.1">arXiv preprint arXiv:2410.05495</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tseng et al<span class="ltx_text" id="bib.bib222.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yu-Min Tseng, Wei-Lin Chen, Chung-Chi Chen, and Hsin-Hsi Chen. 2024.

</span>
<span class="ltx_bibblock">Are Expert-Level Language Models Expert-Level Annotators?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib222.3.1">arXiv preprint arXiv:2410.03254</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Turing (2009)</span>
<span class="ltx_bibblock">
Alan M Turing. 2009.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib223.1.1">Computing machinery and intelligence</em>.

</span>
<span class="ltx_bibblock">Springer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tyen et al<span class="ltx_text" id="bib.bib224.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Gladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, and Victor Cărbune. 2023.

</span>
<span class="ltx_bibblock">LLMs cannot find reasoning errors, but can correct them!

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib224.3.1">arXiv preprint arXiv:2311.08516</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Valmeekam et al<span class="ltx_text" id="bib.bib225.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Karthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023.

</span>
<span class="ltx_bibblock">Can large language models really improve by self-critiquing their own plans?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib225.3.1">arXiv preprint arXiv:2310.08118</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Verga et al<span class="ltx_text" id="bib.bib226.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Pat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. 2024.

</span>
<span class="ltx_bibblock">Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib226.3.1">arXiv preprint arXiv:2404.18796</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vu et al<span class="ltx_text" id="bib.bib227.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, and Yun-Hsuan Sung. 2024.

</span>
<span class="ltx_bibblock">Foundational autoraters: Taming large language models for better automatic evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib227.3.1">arXiv preprint arXiv:2407.10817</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib228.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Binjie Wang, Steffi Chern, Ethan Chern, and Pengfei Liu. 2024b.

</span>
<span class="ltx_bibblock">Halu-j: Critique-based hallucination judge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib228.3.1">arXiv preprint arXiv:2407.12943</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib229.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Chihang Wang, Yuxin Dong, Zhenhong Zhang, Ruotong Wang, Shuo Wang, and Jiajing Chen. 2024c.

</span>
<span class="ltx_bibblock">Automated Genre-Aware Article Scoring and Feedback Using Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib229.3.1">arXiv preprint arXiv:2410.14165</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib230.2.2.1">.</span> (2023e)</span>
<span class="ltx_bibblock">
Chenglong Wang, Hang Zhou, Kaiyan Chang, Tongran Liu, Chunliang Zhang, Quan Du, Tong Xiao, and Jingbo Zhu. 2023e.

</span>
<span class="ltx_bibblock">Learning Evaluation Models from Large Language Models for Sequence Generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib230.3.1">arXiv preprint arXiv:2308.04386</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib231.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b.

</span>
<span class="ltx_bibblock">Large language models are not fair evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib231.3.1">arXiv preprint arXiv:2305.17926</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib232.2.2.1">.</span> (2024e)</span>
<span class="ltx_bibblock">
Tianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. 2024e.

</span>
<span class="ltx_bibblock">Self-taught evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib232.3.1">arXiv preprint arXiv:2408.02666</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib233.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O’Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023c.

</span>
<span class="ltx_bibblock">Shepherd: A critic for language model generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib233.3.1">arXiv preprint arXiv:2308.04592</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib234.2.2.1">.</span> (2024f)</span>
<span class="ltx_bibblock">
Wanying Wang, Zeyu Ma, Pengfei Liu, and Mingang Chen. 2024f.

</span>
<span class="ltx_bibblock">Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic Evaluation Framework for LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib234.3.1">arXiv preprint arXiv:2410.11507</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib235">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib235.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Xuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc Najork. 2018.

</span>
<span class="ltx_bibblock">Position bias estimation for unbiased learning to rank in personal search. In <em class="ltx_emph ltx_font_italic" id="bib.bib235.3.1">Proceedings of the eleventh ACM international conference on web search and data mining</em>. 610–618.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib236">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib236.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock">Self-consistency improves chain of thought reasoning in language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib236.3.1">arXiv preprint arXiv:2203.11171</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib237">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib237.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et al<span class="ltx_text" id="bib.bib237.3.1">.</span> 2023d.

</span>
<span class="ltx_bibblock">Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib237.4.1">arXiv preprint arXiv:2306.05087</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib238">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib238.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. 2024a.

</span>
<span class="ltx_bibblock">HelpSteer2-Preference: Complementing Ratings with Preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib238.3.1">arXiv preprint arXiv:2410.01257</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib239">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib239.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Zhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane Polak Scowcroft, Neel Kant, Aidan Swope, et al<span class="ltx_text" id="bib.bib239.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">Helpsteer: Multi-attribute helpfulness dataset for steerlm.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib239.4.1">arXiv preprint arXiv:2311.09528</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib240">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib240.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, and Huaxiu Yao. 2024d.

</span>
<span class="ltx_bibblock">Cream: Consistency regularized self-rewarding language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib240.3.1">arXiv preprint arXiv:2410.12735</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib241">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Warrens (2015)</span>
<span class="ltx_bibblock">
Matthijs J Warrens. 2015.

</span>
<span class="ltx_bibblock">Five ways to look at Cohen’s kappa.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib241.1.1">Journal of Psychology &amp; Psychotherapy</em> 5 (2015).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib242">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Watts et al<span class="ltx_text" id="bib.bib242.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ishaan Watts, Varun Gumma, Aditya Yadavalli, Vivek Seshadri, Manohar Swaminathan, and Sunayana Sitaram. 2024.

</span>
<span class="ltx_bibblock">PARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib242.3.1">arXiv preprint arXiv:2406.15053</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib243">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib243.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al<span class="ltx_text" id="bib.bib243.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib243.4.1">Advances in neural information processing systems</em> 35 (2022), 24824–24837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib244">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weyssow et al<span class="ltx_text" id="bib.bib244.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Martin Weyssow, Aton Kamanda, and Houari Sahraoui. 2024.

</span>
<span class="ltx_bibblock">CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib244.3.1">arXiv preprint arXiv:2403.09032</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib245">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib245.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. 2024a.

</span>
<span class="ltx_bibblock">Continual learning for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib245.3.1">arXiv preprint arXiv:2402.01364</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib246">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib246.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. 2024b.

</span>
<span class="ltx_bibblock">Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib246.3.1">arXiv preprint arXiv:2407.19594</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib247">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al<span class="ltx_text" id="bib.bib247.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Shijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. 2024a.

</span>
<span class="ltx_bibblock">Evaluating Mathematical Reasoning Beyond Accuracy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib247.3.1">arXiv preprint arXiv:2404.05692</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib248">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xia et al<span class="ltx_text" id="bib.bib248.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Tingyu Xia, Bowen Yu, Yuan Wu, Yi Chang, and Chang Zhou. 2024b.

</span>
<span class="ltx_bibblock">Language Models can Evaluate Themselves via Probability Discrepancy.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib248.3.1">arXiv preprint arXiv:2405.10516</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib249">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib249.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023.

</span>
<span class="ltx_bibblock">Pixiu: A large language model, instruction data and evaluation benchmark for finance.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib249.3.1">arXiv preprint arXiv:2306.05443</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib250">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib250.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, et al<span class="ltx_text" id="bib.bib250.3.1">.</span> 2024b.

</span>
<span class="ltx_bibblock">Sorry-bench: Systematically evaluating large language model safety refusal behaviors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib250.4.1">arXiv preprint arXiv:2406.14598</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib251">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib251.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. 2024a.

</span>
<span class="ltx_bibblock">Self-evaluation guided beam search for reasoning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib251.3.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib252">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib252.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Yiqing Xie, Sheng Zhang, Hao Cheng, Pengfei Liu, Zelalem Gero, Cliff Wong, Tristan Naumann, Hoifung Poon, and Carolyn Rose. 2024c.

</span>
<span class="ltx_bibblock">DOCLENS: Multi-aspect fine-grained evaluation for medical text generation.. In <em class="ltx_emph ltx_font_italic" id="bib.bib252.3.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib253">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xie et al<span class="ltx_text" id="bib.bib253.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Yiqing Xie, Wenxuan Zhou, Pradyot Prakash, Di Jin, Yuning Mao, Quintin Fettes, Arya Talebzadeh, Sinong Wang, Han Fang, Carolyn Rose, et al<span class="ltx_text" id="bib.bib253.3.1">.</span> 2024d.

</span>
<span class="ltx_bibblock">Improving Model Factuality with Fine-grained Critique-based Evaluator.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib253.4.1">arXiv preprint arXiv:2410.18359</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib254">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong et al<span class="ltx_text" id="bib.bib254.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Tianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. 2024.

</span>
<span class="ltx_bibblock">Llava-critic: Learning to evaluate multimodal models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib254.3.1">arXiv preprint arXiv:2410.02712</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib255">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib255.2.2.1">.</span> (2023d)</span>
<span class="ltx_bibblock">
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023d.

</span>
<span class="ltx_bibblock">Wizardlm: Empowering large language models to follow complex instructions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib255.3.1">arXiv preprint arXiv:2304.12244</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib256">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib256.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Guohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, et al<span class="ltx_text" id="bib.bib256.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">Cvalues: Measuring the values of chinese large language models from safety to responsibility.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib256.4.1">arXiv preprint arXiv:2307.09705</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib257">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib257.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Shuying Xu, Junjie Hu, and Ming Jiang. 2024b.

</span>
<span class="ltx_bibblock">Large Language Models Are Active Critics in NLG Evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib257.3.1">arXiv preprint arXiv:2410.10724</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib258">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib258.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Tengyu Xu, Eryk Helenowski, Karthik Abinav Sankararaman, Di Jin, Kaiyan Peng, Eric Han, Shaoliang Nie, Chen Zhu, Hejia Zhang, Wenxuan Zhou, et al<span class="ltx_text" id="bib.bib258.3.1">.</span> 2024a.

</span>
<span class="ltx_bibblock">The perfect blend: Redefining RLHF with mixture of judges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib258.4.1">arXiv preprint arXiv:2409.20370</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib259">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib259.2.2.1">.</span> (2023e)</span>
<span class="ltx_bibblock">
Wenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William Yang Wang, and Lei Li. 2023e.

</span>
<span class="ltx_bibblock">INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib259.3.1">arXiv preprint arXiv:2305.14282</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib260">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib260.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Wang. 2024c.

</span>
<span class="ltx_bibblock">Pride and prejudice: LLM amplifies self-bias in self-refinement. In <em class="ltx_emph ltx_font_italic" id="bib.bib260.3.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 15474–15492.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib261">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib261.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, and Mohan Kankanhalli. 2023a.

</span>
<span class="ltx_bibblock">An LLM can Fool Itself: A Prompt-Based Adversarial Attack.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib261.3.1">arXiv preprint arXiv:2310.13345</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib262">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib262.2.2.1">.</span> (2023c)</span>
<span class="ltx_bibblock">
Zhenran Xu, Senbao Shi, Baotian Hu, Jindi Yu, Dongfang Li, Min Zhang, and Yuxiang Wu. 2023c.

</span>
<span class="ltx_bibblock">Towards reasoning in large language models via multi-agent peer review collaboration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib262.3.1">arXiv preprint arXiv:2311.08152</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib263">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib263.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Le Yan, Zhen Qin, Honglei Zhuang, Rolf Jagerman, Xuanhui Wang, Michael Bendersky, and Harrie Oosterhuis. 2024a.

</span>
<span class="ltx_bibblock">Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib263.3.1">arXiv preprint arXiv:2404.11791</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib264">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib264.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Le Yan, Zhen Qin, Honglei Zhuang, Rolf Jagerman, Xuanhui Wang, Michael Bendersky, and Harrie Oosterhuis. 2024b.

</span>
<span class="ltx_bibblock">Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing. In <em class="ltx_emph ltx_font_italic" id="bib.bib264.3.1">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 410–423.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2024.emnlp-main.25" title="">https://doi.org/10.18653/v1/2024.emnlp-main.25</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib265">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib265.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Nakyeong Yang, Taegwan Kang, Stanley Jungkyu Choi, Honglak Lee, and Kyomin Jung. 2024.

</span>
<span class="ltx_bibblock">Mitigating biases for instruction-following language models via bias neurons elimination. In <em class="ltx_emph ltx_font_italic" id="bib.bib265.3.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 9061–9073.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib266">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao et al<span class="ltx_text" id="bib.bib266.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024.

</span>
<span class="ltx_bibblock">Tree of thoughts: Deliberate problem solving with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib266.3.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib267">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye and Ng (2024)</span>
<span class="ltx_bibblock">
Hai Ye and Hwee Tou Ng. 2024.

</span>
<span class="ltx_bibblock">Self-Judge: Selective Instruction Following with Alignment Self-Evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib267.1.1">arXiv preprint arXiv:2409.00935</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib268">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span class="ltx_text" id="bib.bib268.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Jiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et al<span class="ltx_text" id="bib.bib268.3.1">.</span> 2024b.

</span>
<span class="ltx_bibblock">Justice or prejudice? quantifying biases in llm-as-a-judge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib268.4.1">arXiv preprint arXiv:2410.02736</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib269">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span class="ltx_text" id="bib.bib269.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. 2023a.

</span>
<span class="ltx_bibblock">Selfee: Iterative self-revising llm empowered by self-feedback generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib269.3.1">Blog post</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib270">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span class="ltx_text" id="bib.bib270.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. 2023b.

</span>
<span class="ltx_bibblock">Flask: Fine-grained language model evaluation based on alignment skill sets.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib270.3.1">arXiv preprint arXiv:2307.10928</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib271">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span class="ltx_text" id="bib.bib271.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Ziyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yujia Zhou, Wei Shen, Dong Yan, and Yiqun Liu. 2024a.

</span>
<span class="ltx_bibblock">Beyond Scalar Reward Model: Learning Generative Judge from Preference Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib271.3.1">arXiv preprint arXiv:2410.03742</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib272">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yi et al<span class="ltx_text" id="bib.bib272.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Seungjun Yi, Jaeyoung Lim, and Juyong Yoon. 2024.

</span>
<span class="ltx_bibblock">ProtocoLLM: Automatic Evaluation Framework of LLMs on Domain-Specific Scientific Protocol Formulation Tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib272.3.1">arXiv preprint arXiv:2410.04601</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib273">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yoshino et al<span class="ltx_text" id="bib.bib273.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Koichiro Yoshino, Yun-Nung Chen, Paul Crook, Satwik Kottur, Jinchao Li, Behnam Hedayatnia, Seungwhan Moon, Zhengcong Fei, Zekang Li, Jinchao Zhang, et al<span class="ltx_text" id="bib.bib273.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Overview of the Tenth Dialog System Technology Challenge: DSTC10.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib273.4.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib274">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib274.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, and Shikun Zhang. 2024.

</span>
<span class="ltx_bibblock">Kieval: A knowledge-grounded interactive evaluation framework for large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib274.3.1">arXiv preprint arXiv:2402.15043</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib275">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yuan et al<span class="ltx_text" id="bib.bib275.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024.

</span>
<span class="ltx_bibblock">Self-rewarding language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib275.3.1">arXiv preprint arXiv:2401.10020</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib276">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib276.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Xuanjing Huang, et al<span class="ltx_text" id="bib.bib276.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">Disc-lawllm: Fine-tuning large language models for intelligent legal services.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib276.4.1">arXiv preprint arXiv:2309.11325</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib277">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yue et al<span class="ltx_text" id="bib.bib277.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, and Huan Sun. 2023b.

</span>
<span class="ltx_bibblock">Automatic evaluation of attribution by large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib277.3.1">arXiv preprint arXiv:2305.06311</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib278">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zelikman et al<span class="ltx_text" id="bib.bib278.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Eric Zelikman, YH Wu, Jesse Mu, and Noah D Goodman. 2024.

</span>
<span class="ltx_bibblock">STaR: Self-taught reasoner bootstrapping reasoning with reasoning. In <em class="ltx_emph ltx_font_italic" id="bib.bib278.3.1">Proc. the 36th International Conference on Neural Information Processing Systems</em>, Vol. 1126.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib279">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al<span class="ltx_text" id="bib.bib279.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Weihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, and Weizhu Chen. 2024.

</span>
<span class="ltx_bibblock">Automatic Instruction Evolving for Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib279.3.1">arXiv preprint arXiv:2406.00770</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib280">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib280.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Chen Zhang, João Sedoc, Luis Fernando D’Haro, Rafael Banchs, and Alexander Rudnicky. 2021.

</span>
<span class="ltx_bibblock">Automatic evaluation and moderation of open-domain dialogue systems.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib280.3.1">arXiv preprint arXiv:2111.02110</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib281">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib281.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Kaiqi Zhang, Shuai Yuan, and Honghan Zhao. 2024c.

</span>
<span class="ltx_bibblock">TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib281.3.1">arXiv preprint arXiv:2407.10999</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib282">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib282.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Qiyuan Zhang, Yufei Wang, Tiezheng Yu, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, et al<span class="ltx_text" id="bib.bib282.3.1">.</span> 2024b.

</span>
<span class="ltx_bibblock">RevisEval: Improving LLM-as-a-Judge via Response-Adapted References.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib282.4.1">arXiv preprint arXiv:2410.05193</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib283">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib283.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Ruoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, and Lei Zou. 2023a.

</span>
<span class="ltx_bibblock">Llmaaa: Making large language models as active annotators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib283.3.1">arXiv preprint arXiv:2310.19596</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib284">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang (2018)</span>
<span class="ltx_bibblock">
Saizheng Zhang. 2018.

</span>
<span class="ltx_bibblock">Personalizing dialogue agents: I have a dog, do you have pets too.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib284.1.1">arXiv preprint arXiv:1801.07243</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib285">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib285.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Xiaoyu Zhang, Yishan Li, Jiayin Wang, Bowen Sun, Weizhi Ma, Peijie Sun, and Min Zhang. 2024a.

</span>
<span class="ltx_bibblock">Large language models as evaluators for recommendation explanations. In <em class="ltx_emph ltx_font_italic" id="bib.bib285.3.1">Proceedings of the 18th ACM Conference on Recommender Systems</em>. 33–42.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib286">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib286.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023b.

</span>
<span class="ltx_bibblock">Wider and deeper llm networks are fairer llm evaluators.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib286.3.1">arXiv preprint arXiv:2308.01862</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib287">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib287.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Ruochen Zhao, Wenxuan Zhang, Yew Ken Chia, Deli Zhao, and Lidong Bing. 2024b.

</span>
<span class="ltx_bibblock">Auto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib287.3.1">arXiv preprint arXiv:2405.20267</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib288">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib288.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al<span class="ltx_text" id="bib.bib288.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib288.4.1">arXiv preprint arXiv:2303.18223</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib289">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib289.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Xiutian Zhao, Ke Wang, and Wei Peng. 2024a.

</span>
<span class="ltx_bibblock">Measuring the inconsistency of large language models in preferential ranking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib289.3.1">arXiv preprint arXiv:2410.08851</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib290">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib290.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yachao Zhao, Bo Wang, Dongming Zhao, Kun Huang, Yan Wang, Ruifang He, and Yuexian Hou. 2023a.

</span>
<span class="ltx_bibblock">Mind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib290.3.1">arXiv preprint arXiv:2308.12578</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib291">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib291.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021.

</span>
<span class="ltx_bibblock">Calibrate before use: Improving few-shot performance of language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib291.3.1">International conference on machine learning</em>. PMLR, 12697–12706.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib292">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib292.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2023b.

</span>
<span class="ltx_bibblock">Large language models are not robust multiple choice selectors. In <em class="ltx_emph ltx_font_italic" id="bib.bib292.3.1">The Twelfth International Conference on Learning Representations</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib293">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib293.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al<span class="ltx_text" id="bib.bib293.3.1">.</span> 2023a.

</span>
<span class="ltx_bibblock">Judging llm-as-a-judge with mt-bench and chatbot arena.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib293.4.1">Advances in Neural Information Processing Systems</em> 36 (2023), 46595–46623.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib294">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib294.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, and Min Lin. 2024.

</span>
<span class="ltx_bibblock">Cheating automatic llm benchmarks: Null models achieve high win rates.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib294.3.1">arXiv preprint arXiv:2410.07137</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib295">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib295.2.2.1">.</span> (2024c)</span>
<span class="ltx_bibblock">
Hongli Zhou, Hui Huang, Yunfei Long, Bing Xu, Conghui Zhu, Hailong Cao, Muyun Yang, and Tiejun Zhao. 2024c.

</span>
<span class="ltx_bibblock">Mitigating the Bias of Large Language Model Evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib295.3.1">arXiv preprint arXiv:2409.16788</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib296">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib296.2.2.1">.</span> (2024e)</span>
<span class="ltx_bibblock">
Han Zhou, Xingchen Wan, Yinhong Liu, Nigel Collier, Ivan Vulić, and Anna Korhonen. 2024e.

</span>
<span class="ltx_bibblock">Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib296.3.1">arXiv preprint arXiv:2406.11370</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib297">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib297.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, and Subhrajit Roy. 2023a.

</span>
<span class="ltx_bibblock">Batch calibration: Rethinking calibration for in-context learning and prompt engineering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib297.3.1">arXiv preprint arXiv:2309.17249</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib298">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib298.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Ruiyang Zhou, Lu Chen, and Kai Yu. 2024a.

</span>
<span class="ltx_bibblock">Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib298.3.1">Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)</em>. 9340–9351.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib299">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib299.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al<span class="ltx_text" id="bib.bib299.3.1">.</span> 2023b.

</span>
<span class="ltx_bibblock">Sotopia: Interactive evaluation for social intelligence in language agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib299.4.1">arXiv preprint arXiv:2310.11667</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib300">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib300.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Yiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. 2024b.

</span>
<span class="ltx_bibblock">Calibrated self-rewarding vision language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib300.3.1">arXiv preprint arXiv:2405.14622</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib301">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib301.2.2.1">.</span> (2024d)</span>
<span class="ltx_bibblock">
Yuhang Zhou, Yuchen Ni, Xiang Liu, Jian Zhang, Sen Liu, Guangnan Ye, and Hongfeng Chai. 2024d.

</span>
<span class="ltx_bibblock">Are Large Language Models Rational Investors?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib301.3.1">arXiv preprint arXiv:2402.12713</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib302">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al<span class="ltx_text" id="bib.bib302.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Lianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023.

</span>
<span class="ltx_bibblock">Judgelm: Fine-tuned large language models are scalable judges.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib302.3.1">arXiv preprint arXiv:2310.17631</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib303">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuang et al<span class="ltx_text" id="bib.bib303.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. 2024.

</span>
<span class="ltx_bibblock">A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib303.3.1">Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval</em> (Washington DC, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib303.4.2">(SIGIR ’24)</em>. Association for Computing Machinery, New York, NY, USA, 38–47.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3626772.3657813" title="">https://doi.org/10.1145/3626772.3657813</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib304">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuge et al<span class="ltx_text" id="bib.bib304.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Mingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, et al<span class="ltx_text" id="bib.bib304.3.1">.</span> 2024.

</span>
<span class="ltx_bibblock">Agent-as-a-Judge: Evaluate Agents with Agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib304.4.1">arXiv preprint arXiv:2410.10934</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib305">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhuo (2023)</span>
<span class="ltx_bibblock">
Terry Yue Zhuo. 2023.

</span>
<span class="ltx_bibblock">ICE-Score: Instructing Large Language Models to Evaluate Code.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib305.1.1">arXiv preprint arXiv:2304.14317</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib306">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zou et al<span class="ltx_text" id="bib.bib306.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J Zico Kolter, and Matt Fredrikson. 2023.

</span>
<span class="ltx_bibblock">Universal and transferable adversarial attacks on aligned language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib306.3.1">arXiv preprint arXiv:2307.15043</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Dec 10 05:49:16 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
