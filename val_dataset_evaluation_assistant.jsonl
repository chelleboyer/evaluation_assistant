{"questions": {"56a1f74c-cef6-4d12-a543-1c7eb6a96c17": "What is the main focus of the Crosscodeeval benchmark introduced by Ding et al. (2024)?", "27d91486-a8af-45b1-a74a-7dac53175852": "What issue do Doddapaneni et al. (2024) address in their work on evaluator LLMs?", "118bbb13-e380-43ce-95db-cf106b51142e": "What is the main focus of the paper \"Self-Boosting Large Language Models with Synthetic Preference Data\" by Dong et al. (2024a)?", "155af4b9-570a-4a58-86b5-8b682b619cd4": "How do Dong et al. (2022) contribute to the understanding of in-context learning in their survey?", "722a8d5f-4ade-4766-89b1-8517e910f81a": "What are the main limitations discussed in \"Limits to scalable evaluation at the frontier: LLM as Judge won\u2019t beat twice the data\"?", "b85e3d0f-5ce3-4934-99ef-da98915502ef": "How does the \"Length-controlled alpacaeval\" method proposed by Dubois et al. (2024) aim to debias automatic evaluators?", "11c505a7-ce06-4abe-8c74-9d617961dedf": "What is the main focus of the study conducted by Elangovan et al. (2024) regarding automatic evaluation and LLM-as-a-judge?", "dde587ed-7ce3-43f5-9068-29a30524ae54": "How does the work of Fabbri et al. (2021) contribute to the field of summarization evaluation?", "11679c9a-c317-4437-95ec-3ad54f89c078": "What is the main focus of the paper titled \"Hierarchical neural story generation\" published in 2018?", "c3933a30-9b40-4ec6-b17f-ad089a0076d6": "How does the tool \"Biasalert\" introduced by Fan et al. (2024) contribute to social bias detection in large language models?", "497bfb7d-d64b-4a58-92fc-42d699faa4c3": "What approach do Feng et al. (2024) propose to improve LLM-based machine translation?", "8a74e154-7305-4b3d-8dd6-62b36d1faaba": "What aspects of human evaluation for machine translation are studied by Freitag et al. (2021a)?", "5c20b118-2902-40d5-a515-e78e0ad0c67d": "What was the focus of the WMT21 metrics shared task as reported by Freitag et al. (2021b)?", "c0073696-547d-439e-8454-6b42465fd66b": "What is the main topic discussed by Robert M French in his 2000 publication?", "27b06885-ffb7-413a-8fcd-fe2d80ac7b40": "What are the main topics covered by Gao et al. (2023) in their survey on retrieval-augmented generation for large language models?", "72907d44-759d-48b0-9be3-8359b812e30f": "How do Gilardi et al. (2023) compare the performance of ChatGPT to crowd workers in text-annotation tasks?", "1985a1dc-da0d-45a5-b7d3-cf5d9e6668b8": "What is the main focus of the paper \"Topical-chat: Towards knowledge-grounded open-domain conversations\" by Gopalakrishnan et al. (2023)?", "9cfc90e7-4bf8-4b54-be00-d5f6c0cfc542": "What contribution does the paper \"OpenMEVA: A benchmark for evaluating open-ended story generation metrics\" by Guan et al. (2021) make to the field of story generation?", "495fc332-c348-4b20-8acf-76407b883e2f": "What is the main focus of the paper by Guo et al. (2024) titled \"Direct language model alignment from online AI feedback\"?", "762a9fbf-a3d1-4343-8258-55ed637855e1": "How does the work of Guo et al. (2023) contribute to the evaluation of large language models?", "d8838f63-d1fa-4755-9687-eebc69773930": "What are the main contributions of the paper titled \"Unveiling Context-Aware Criteria in Self-Assessing LLMs\"?", "4858e3b5-b2a9-45f9-aed2-847ced0b1e3f": "How do Hada et al. (2023) address the challenge of scaling up multilingual evaluation using large language model-based evaluators?", "d7e6ac21-af94-4303-b59f-d0ea23783522": "What is the main contribution of Hao et al. (2024) in their work titled \"Fullanno: A data engine for enhancing image comprehension of mllms\"?", "9f2a4857-b16e-4914-b5e6-fd27066b353d": "How do Harper and Konstan (2015) describe the history and context of the MovieLens datasets in their publication?", "019866f6-dc56-41df-b51b-c649c75b15fa": "What is the main focus of the paper \"Socreval\" by Hangfeng He, Hongming Zhang, and Dan Roth (2023b)?", "fcc15491-3d7a-4d7e-b49a-0f8d48a88cc0": "How does the work \"Annollm\" by Xingwei He et al. (2023a) aim to improve large language models?", "201c2b45-1489-4f83-852c-7e2308b921bf": "What is the main focus of the FedEval-LLM framework as described in the context?", "3bea0ddb-3134-4a02-bb4b-0a0cab43cbff": "How have He et al. (2024a) and Hijazi et al. (2024) contributed to the evaluation of large language models according to the provided references?", "e6710214-6bd9-4156-b781-b0788ccd633a": "What is the main contribution of Hou et al. (2024) in the context of recommender systems?", "60d9d39d-1aa3-41a3-9114-b7c3ed052cba": "How do Hu et al. (2024a) describe the capabilities of Themis in natural language generation evaluation?", "8837e7dd-2d5c-443d-9494-4f907359b8fb": "What are the main contributions of Hu et al. (2024c) in the context of language model preference evaluation?", "1efbd6b0-a954-4aca-a3b9-7d52ac6de266": "How do Huang et al. (2024a) characterize fine-tuned judge models in their empirical study of LLM-as-a-judge for LLM evaluation?", "2a841c5f-d0bf-4e92-84da-b0047e5a9215": "What are the main limitations of fine-tuned judge models for evaluating large language models as discussed in the referenced works?", "f3ab63df-0693-48cc-b66e-62c0dea05c8b": "How do Huang et al. (2023) characterize the current capabilities of large language models in self-correcting their reasoning?", "7e0ab47a-678d-41e0-bd87-0d3917300c26": "What methods do Jeong et al. (2024) propose to improve medical reasoning in large language models?", "a1c5f861-bc7f-4403-9e36-48c704e19ded": "How does the PKU-SafeRLHF approach by Ji et al. (2024) aim to enhance safety alignment in large language models?", "984095b1-3cf7-4c0c-bf0a-fb265f41356b": "What are the main topics covered in the survey of hallucination in natural language generation published in Computational Surveys in 2023?", "5a964d6f-0dfe-4581-93da-4d17a221e4de": "How do the works by Jiang et al. from 2023 and 2024 contribute to understanding and evaluating large language models and text generation tasks?", "0cd51b66-7e3b-43cd-8ae3-d9e4212b496b": "What is the main focus of the paper \"Deceiving llms through compositional instruction with hidden attacks\" by Shuyu Jiang, Xingshu Chen, and Rui Tang (2023)?", "a275cd7b-f565-4d8d-b0e1-b51bc07379db": "How does the study \"Swe-bench: Can language models resolve real-world github issues?\" by Carlos E Jimenez et al. (2023) contribute to understanding language model capabilities?", "e77ae99b-e080-489c-b11e-f0ede7453815": "What methods do Jung et al. (2024) propose for ensuring large language model judgments align with human agreement?", "bd8c12a6-1bb9-477d-88d6-4ef9da1c973c": "According to Karpinska and Iyyer (2023), what are the limitations of large language models in literary translation despite their effective use of document-level context?", "69c35936-5855-48d8-91b6-35dc71fa23fb": "What is the main focus of the paper \"Critiquellm\" by Ke et al. (2024)?", "41a8c8e0-dbb8-47f5-b26a-b967a33b41d2": "According to Khan et al. (2024), what effect does debating with more persuasive large language models have on the truthfulness of answers?", "bfed637c-03e4-46ee-9722-cb78867b314f": "What is the main focus of the paper \"Aligning Large Language Models with Self-generated Preference Data\" by Kim et al. (2024a)?", "2ec1cf59-8c9e-4931-a7fc-9e68a704a8a9": "What contribution does the 2023 paper \"Prometheus: Inducing fine-grained evaluation capability in language models\" by Kim et al. make to the field of language models?", "052d035c-f555-4705-8d38-74f9bb516e86": "What is the main focus of the Prometheus 2 language model introduced by Kim et al. (2024b)?", "0025c5ba-35ac-4960-ab7f-b8bdd02f7967": "What issue is addressed in the paper by Ko et al. (2020) related to question answering?", "1c5c6778-f732-43bd-ace5-e3d2239372b2": "What are the main findings of Kotonya et al. (2023) regarding the use of small large language models as evaluation metrics in summarization tasks?", "c1bbca8e-1e50-47b2-8de4-81fb09009593": "How do Krolik et al. (2024) propose leveraging large language models for automated medical question and answer evaluation?", "039f59a6-cbfa-4f0f-a40b-53f3cd9c7437": "What novel approach do Kumar et al. (2024) propose for evaluating bug report summarization?", "fd45a2bc-cb81-46d4-a9f8-0d82d03ee1aa": "What is the main focus of the study conducted by Lambert et al. (2024) as described in the context?", "487b08ad-0686-46e0-9172-84237749598b": "How can large language models contribute to the annotation of speech emotional data according to recent research?", "3a9d4a17-f795-4f94-9fa2-3d2235363b7f": "What are the key insights from Lawrie et al. (2024) regarding the TREC 2023 NeuCLIR Track?", "7da4c05a-859a-4e1d-92f8-f3216c37b5f3": "What is the main focus of the paper titled \"Query-efficient and scalable black-box adversarial attacks on discrete sequential data via bayesian optimization\"?", "91f42c76-e16c-4dfa-ba8b-40ee476f39bb": "How does the work \"Prometheusvision: Vision-language model as a judge for fine-grained evaluation\" contribute to the evaluation of vision-language models?", "175afe05-ec7b-404a-92cf-58d5e9993ed9": "What is the main focus of the paper \"Aligning Large Language Models by On-Policy Self-Judgment\" by Lee et al. (2024b)?", "5be37f96-1783-47fa-aeb3-c79c6576a132": "In which conference was the paper \"RecExplainer: Aligning Large Language Models for Explaining Recommendation Models\" by Lei et al. (2024) presented?", "7db6c0a1-6680-4a05-8f1b-c513e258803b": "What is the main focus of the research conducted by Lewis et al. (2020) as mentioned in the context?", "8122d289-96f7-4d5a-85eb-ee482ee4973a": "Which topic do Li et al. (2024c) address in their work according to the provided information?", "40904770-f01e-4e4f-93d4-163321a6aee2": "What is the main objective of the Calibraeval method in the context of large language models as judges?", "3f5a9e3c-97ed-4e12-877d-b86a079fc57e": "What are the key contributions of Haitao Li and colleagues in the field of Chinese legal benchmarks and datasets for evaluating large language models?", "140d66cf-a705-467a-a737-19d90183347b": "What is the main focus of the paper by Junlong Li et al. (2023c) titled \"Generative judge for evaluating alignment\"?", "9f4a8a27-87d9-4b57-a73a-bc98e8cdc4e4": "How do Qintong Li et al. (2023a) explore the synergy between large language models and humans in their work on collaborative evaluation?", "71d9a287-a6fa-4479-b4da-f75b3fb412d4": "What is the main contribution of Li et al. (2017) in the field of natural language processing?", "1c145c3a-35d1-4d33-947b-9ab7cc39f28c": "What topic do Li et al. (2023d) address in their arXiv preprint related to large language model evaluators?", "3cb9af15-d2bd-46b9-9e7b-c64e328cebc6": "What is the primary focus of the Debatrix framework as described in the context?", "5a3ade83-c3c5-4741-a027-a701fbdcd34a": "Which publication details the ABSEval framework and who are its authors?", "9bbdd5f0-0a2a-47c9-a8b7-eba672e56e13": "What is the main focus of the WILDBENCH benchmark introduced by Lin et al. (2024)?", "84509d00-8d55-4b27-8f17-2020e76b9a3d": "How does the work of Stephanie Lin et al. (2021) contribute to understanding model behavior in terms of truthfulness?", "64cd6e2f-f932-423d-877c-a59c425ec7c4": "What is the main focus of the paper \"Llm-eval\" by Yen-Ting Lin and Yun-Nung Chen (2023)?", "b1732735-433c-4987-86b1-dad7819d7e7f": "How does the \"X-eval\" approach by Minqian Liu et al. (2023c) aim to improve text evaluation?", "c0950b3f-a1cd-4b5e-ba97-70300bb18c9b": "What is the main focus of the Alignbench benchmark mentioned in the context?", "20161cf0-56ad-4295-8794-a769f4ce5703": "Which large language model is used in G-eval for natural language generation evaluation according to the context?", "0f6f8aa6-f7c4-4a05-a2cc-b16b9d00393c": "What is the main focus of the paper \"Calibrating llm-based evaluator\" by Liu et al. (2023d)?", "304f8199-6872-4cc3-9d12-d0ebdcfeed79": "How does the work \"HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition\" by Liu et al. (2024a) contribute to the evaluation of large language models?", "8e4121bc-5b79-4e7c-96b7-f172b5eabefc": "What is the main focus of the study by Liu et al. (2024c) regarding large language model evaluators?", "3d8950b6-3e55-424d-9da2-a3943528854c": "How does the work by Liusie et al. (2024) contribute to the assessment of large language models?", "dacfb083-576e-48e6-bada-5cb6c72fe277": "What is the main focus of the study conducted by Luo et al. (2023) on large language models?", "daa682a2-dca5-43e1-b5a8-66b175eeee55": "What is the purpose of the VideoAutoArena introduced by Luo et al. (2024)?", "0918ea54-364f-42c4-82c2-bf6259445fa4": "What role do large language models play in improving relevance judgments for legal case retrieval according to the 2024 study by Madaan et al.?", "8fb91cd2-5083-4fea-8c12-6de641dfab23": "How does the self-refine iterative refinement method with self-feedback, as discussed by Madaan et al. (2024), contribute to advancements in neural information processing systems?", "616068db-0cf7-41ff-91a2-dd023ae831c8": "What is the main focus of the paper \"Soda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs\" by Mendon\u00e7a et al. (2024)?", "b3dda3fa-2349-420d-8821-22783dc414ba": "How do Moniri et al. (2024) propose to evaluate the performance of large language models?", "25e29c7d-ac6e-42a6-9d7b-6ea3a2789be6": "What is the main focus of the work presented by Myrzakhan et al. (2024) in their paper on Open-LLM-Leaderboard?", "93564f59-372b-4cfe-b34c-57bf183c4fe9": "How does Musolesi (2024) contribute to improving response generation according to the provided context?", "efdf762b-8b77-4694-b8a4-d0609d3d4ce8": "What are the key features of topic-aware convolutional neural networks for extreme summarization as discussed in the 2018 arXiv preprint?", "31b46412-5fd7-4269-a146-5b44bcbbcd51": "How does the JurEE framework utilize small, specialized encoder ensembles to safeguard large language model interactions according to Nasrabadi (2024)?", "732a44a3-632f-47e0-9208-d47547279f67": "What is the main focus of the paper \"JudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking\" by Niu et al. (2024)?", "a435438b-9712-4b7e-a450-eee1f640e89c": "What contribution does the work \"BioPlanner: automatic evaluation of LLMs on protocol planning in biology\" by O\u2019Donoghue et al. (2023) make to the field of biology?", "5db91814-186a-427a-bac6-2a00d5d14992": "What is the main contribution of Owens et al. (2024) in their work on multi-LLM debiasing frameworks?", "dcee3911-64d4-4538-b164-868584ef4023": "How does the FRANK benchmark introduced by Pagnoni et al. (2021) help in understanding factuality in abstractive summarization?", "84fa5f54-37ac-42b7-b79c-e5f8ea9135c1": "What are the key human-centered design recommendations proposed for using large language models as judges?", "db18b0be-6f7b-471b-891c-051f308a9bbf": "How do large language models interact with knowledge graphs according to Pan et al. (2024b)?", "7c950c1d-027b-4774-ad9a-83c0cdd379d9": "What is the main focus of the paper titled \"Offsetbias: Leveraging debiased data for tuning evaluators\" by Park et al. (2024)?", "1ebde6db-56cb-4f4a-ab11-239fd5cc8cd3": "How does the work \"AIME: AI System Optimization via Multiple LLM Evaluators\" by Patel et al. (2024) contribute to AI system optimization?", "15b8efb4-ada5-4f56-8831-626936569ab5": "What is the main focus of the paper by Paul et al. (2023) titled \"Refiner: Reasoning feedback on intermediate representations\"?", "bc989430-7f70-4cb5-8a78-00e2174f4eaf": "According to Pezeshkpour and Hruschka (2023), what aspect of large language models is analyzed in their study on multiple-choice questions?", "7b1ba95c-bc1d-4c44-9150-766c80be9d83": "What are the main findings of Poulain et al. (2024) regarding bias patterns in the application of large language models for clinical decision support?", "00f5dcb3-6a1d-4d0f-8048-00098fa0e3f1": "How do Qin et al. (2023) demonstrate the effectiveness of large language models as text rankers using pairwise ranking prompting?"}, "relevant_contexts": {"56a1f74c-cef6-4d12-a543-1c7eb6a96c17": ["ab3c5590-40c5-4517-82e2-153afb9ccaa8"], "27d91486-a8af-45b1-a74a-7dac53175852": ["ab3c5590-40c5-4517-82e2-153afb9ccaa8"], "118bbb13-e380-43ce-95db-cf106b51142e": ["d2f1df11-2ad9-4806-a54d-f6120e588e2f"], "155af4b9-570a-4a58-86b5-8b682b619cd4": ["d2f1df11-2ad9-4806-a54d-f6120e588e2f"], "722a8d5f-4ade-4766-89b1-8517e910f81a": ["bcc7c482-300a-476e-bf16-6be57a696481"], "b85e3d0f-5ce3-4934-99ef-da98915502ef": ["bcc7c482-300a-476e-bf16-6be57a696481"], "11c505a7-ce06-4abe-8c74-9d617961dedf": ["7d5cf900-0cf4-445f-975b-0edc944cc0d5"], "dde587ed-7ce3-43f5-9068-29a30524ae54": ["7d5cf900-0cf4-445f-975b-0edc944cc0d5"], "11679c9a-c317-4437-95ec-3ad54f89c078": ["abde692d-fbd2-4913-87e3-604e4b863a81"], "c3933a30-9b40-4ec6-b17f-ad089a0076d6": ["abde692d-fbd2-4913-87e3-604e4b863a81"], "497bfb7d-d64b-4a58-92fc-42d699faa4c3": ["8f49d348-5e5a-406f-b604-980afc59c891"], "8a74e154-7305-4b3d-8dd6-62b36d1faaba": ["8f49d348-5e5a-406f-b604-980afc59c891"], "5c20b118-2902-40d5-a515-e78e0ad0c67d": ["ae9f1779-aa4a-4bab-82c0-911706b9d597"], "c0073696-547d-439e-8454-6b42465fd66b": ["ae9f1779-aa4a-4bab-82c0-911706b9d597"], "27b06885-ffb7-413a-8fcd-fe2d80ac7b40": ["dcd28063-070c-4149-8910-42980ab0bae4"], "72907d44-759d-48b0-9be3-8359b812e30f": ["dcd28063-070c-4149-8910-42980ab0bae4"], "1985a1dc-da0d-45a5-b7d3-cf5d9e6668b8": ["99d83c15-6b17-4bef-9ecd-d742cc633604"], "9cfc90e7-4bf8-4b54-be00-d5f6c0cfc542": ["99d83c15-6b17-4bef-9ecd-d742cc633604"], "495fc332-c348-4b20-8acf-76407b883e2f": ["79ba891a-6062-43cd-826f-6da880f619db"], "762a9fbf-a3d1-4343-8258-55ed637855e1": ["79ba891a-6062-43cd-826f-6da880f619db"], "d8838f63-d1fa-4755-9687-eebc69773930": ["ca353861-d5de-48de-8c90-72a55d2ac255"], "4858e3b5-b2a9-45f9-aed2-847ced0b1e3f": ["ca353861-d5de-48de-8c90-72a55d2ac255"], "d7e6ac21-af94-4303-b59f-d0ea23783522": ["a97d74a1-0340-4eff-a0cb-1239e05011ab"], "9f2a4857-b16e-4914-b5e6-fd27066b353d": ["a97d74a1-0340-4eff-a0cb-1239e05011ab"], "019866f6-dc56-41df-b51b-c649c75b15fa": ["57adbacf-7753-46a3-8eee-4bc3d1379f07"], "fcc15491-3d7a-4d7e-b49a-0f8d48a88cc0": ["57adbacf-7753-46a3-8eee-4bc3d1379f07"], "201c2b45-1489-4f83-852c-7e2308b921bf": ["1d1b7134-2299-4aeb-a64e-e1bc5cb92e31"], "3bea0ddb-3134-4a02-bb4b-0a0cab43cbff": ["1d1b7134-2299-4aeb-a64e-e1bc5cb92e31"], "e6710214-6bd9-4156-b781-b0788ccd633a": ["68a8a158-0008-4fe8-948b-223bc400cecd"], "60d9d39d-1aa3-41a3-9114-b7c3ed052cba": ["68a8a158-0008-4fe8-948b-223bc400cecd"], "8837e7dd-2d5c-443d-9494-4f907359b8fb": ["6ab5545b-eb80-41cb-838f-5d589ac625a3"], "1efbd6b0-a954-4aca-a3b9-7d52ac6de266": ["6ab5545b-eb80-41cb-838f-5d589ac625a3"], "2a841c5f-d0bf-4e92-84da-b0047e5a9215": ["e3902b87-6979-43d3-a47a-234d85f1ae2a"], "f3ab63df-0693-48cc-b66e-62c0dea05c8b": ["e3902b87-6979-43d3-a47a-234d85f1ae2a"], "7e0ab47a-678d-41e0-bd87-0d3917300c26": ["5ca9089a-5808-4243-a876-05289920ec99"], "a1c5f861-bc7f-4403-9e36-48c704e19ded": ["5ca9089a-5808-4243-a876-05289920ec99"], "984095b1-3cf7-4c0c-bf0a-fb265f41356b": ["e9d4b1c8-b321-48fe-a168-01b07d462e5a"], "5a964d6f-0dfe-4581-93da-4d17a221e4de": ["e9d4b1c8-b321-48fe-a168-01b07d462e5a"], "0cd51b66-7e3b-43cd-8ae3-d9e4212b496b": ["e00d6c4d-cba6-4487-9e69-5349c658ac04"], "a275cd7b-f565-4d8d-b0e1-b51bc07379db": ["e00d6c4d-cba6-4487-9e69-5349c658ac04"], "e77ae99b-e080-489c-b11e-f0ede7453815": ["6f1a2f91-7990-4ffb-bc11-17eae541ae01"], "bd8c12a6-1bb9-477d-88d6-4ef9da1c973c": ["6f1a2f91-7990-4ffb-bc11-17eae541ae01"], "69c35936-5855-48d8-91b6-35dc71fa23fb": ["8a358560-ec19-4dff-9cc0-f85e698c37a1"], "41a8c8e0-dbb8-47f5-b26a-b967a33b41d2": ["8a358560-ec19-4dff-9cc0-f85e698c37a1"], "bfed637c-03e4-46ee-9722-cb78867b314f": ["19fd7223-2121-4710-a525-e0abf6e6e897"], "2ec1cf59-8c9e-4931-a7fc-9e68a704a8a9": ["19fd7223-2121-4710-a525-e0abf6e6e897"], "052d035c-f555-4705-8d38-74f9bb516e86": ["b0665698-4034-4e80-8560-fcef1fbefa28"], "0025c5ba-35ac-4960-ab7f-b8bdd02f7967": ["b0665698-4034-4e80-8560-fcef1fbefa28"], "1c5c6778-f732-43bd-ace5-e3d2239372b2": ["a7010010-2e3a-44ae-8437-796e5824dd11"], "c1bbca8e-1e50-47b2-8de4-81fb09009593": ["a7010010-2e3a-44ae-8437-796e5824dd11"], "039f59a6-cbfa-4f0f-a40b-53f3cd9c7437": ["36b2de72-780a-4530-a8c3-a7c816300737"], "fd45a2bc-cb81-46d4-a9f8-0d82d03ee1aa": ["36b2de72-780a-4530-a8c3-a7c816300737"], "487b08ad-0686-46e0-9172-84237749598b": ["6d5987d5-2e3c-4230-9831-9e9305a3d95d"], "3a9d4a17-f795-4f94-9fa2-3d2235363b7f": ["6d5987d5-2e3c-4230-9831-9e9305a3d95d"], "7da4c05a-859a-4e1d-92f8-f3216c37b5f3": ["07c4c182-c108-4511-8a19-38bdb46fbf07"], "91f42c76-e16c-4dfa-ba8b-40ee476f39bb": ["07c4c182-c108-4511-8a19-38bdb46fbf07"], "175afe05-ec7b-404a-92cf-58d5e9993ed9": ["5012737f-669e-4078-835b-f79197feeb81"], "5be37f96-1783-47fa-aeb3-c79c6576a132": ["5012737f-669e-4078-835b-f79197feeb81"], "7db6c0a1-6680-4a05-8f1b-c513e258803b": ["919bbd9e-32ee-4e08-aec6-3f6bde5faffb"], "8122d289-96f7-4d5a-85eb-ee482ee4973a": ["919bbd9e-32ee-4e08-aec6-3f6bde5faffb"], "40904770-f01e-4e4f-93d4-163321a6aee2": ["45948a2d-a1f6-413c-ac04-ea2de38f556a"], "3f5a9e3c-97ed-4e12-877d-b86a079fc57e": ["45948a2d-a1f6-413c-ac04-ea2de38f556a"], "140d66cf-a705-467a-a737-19d90183347b": ["bf3f1f6e-f950-4498-8224-4144b51a85a9"], "9f4a8a27-87d9-4b57-a73a-bc98e8cdc4e4": ["bf3f1f6e-f950-4498-8224-4144b51a85a9"], "71d9a287-a6fa-4479-b4da-f75b3fb412d4": ["9a825bd9-251b-4c56-8182-e67997a4a712"], "1c145c3a-35d1-4d33-947b-9ab7cc39f28c": ["9a825bd9-251b-4c56-8182-e67997a4a712"], "3cb9af15-d2bd-46b9-9e7b-c64e328cebc6": ["df38f65d-78a5-4c7a-97b4-284bac62fc2d"], "5a3ade83-c3c5-4741-a027-a701fbdcd34a": ["df38f65d-78a5-4c7a-97b4-284bac62fc2d"], "9bbdd5f0-0a2a-47c9-a8b7-eba672e56e13": ["53b7b232-a20b-40c6-aed5-79d1045ccc6e"], "84509d00-8d55-4b27-8f17-2020e76b9a3d": ["53b7b232-a20b-40c6-aed5-79d1045ccc6e"], "64cd6e2f-f932-423d-877c-a59c425ec7c4": ["580d7e98-1804-4d9c-afe7-4ed8bf19301d"], "b1732735-433c-4987-86b1-dad7819d7e7f": ["580d7e98-1804-4d9c-afe7-4ed8bf19301d"], "c0950b3f-a1cd-4b5e-ba97-70300bb18c9b": ["c85552f4-60dc-49c8-9d99-329c75c2582f"], "20161cf0-56ad-4295-8794-a769f4ce5703": ["c85552f4-60dc-49c8-9d99-329c75c2582f"], "0f6f8aa6-f7c4-4a05-a2cc-b16b9d00393c": ["a6cc0094-d6c2-47b5-ae31-9c59d2f44b58"], "304f8199-6872-4cc3-9d12-d0ebdcfeed79": ["a6cc0094-d6c2-47b5-ae31-9c59d2f44b58"], "8e4121bc-5b79-4e7c-96b7-f172b5eabefc": ["6e904bce-e177-439b-9d14-3fc72e5638dd"], "3d8950b6-3e55-424d-9da2-a3943528854c": ["6e904bce-e177-439b-9d14-3fc72e5638dd"], "dacfb083-576e-48e6-bada-5cb6c72fe277": ["bd2e300f-0774-4bd3-8e6a-1023073cec9c"], "daa682a2-dca5-43e1-b5a8-66b175eeee55": ["bd2e300f-0774-4bd3-8e6a-1023073cec9c"], "0918ea54-364f-42c4-82c2-bf6259445fa4": ["f8d0411d-031d-4ceb-a143-baffcb82e04a"], "8fb91cd2-5083-4fea-8c12-6de641dfab23": ["f8d0411d-031d-4ceb-a143-baffcb82e04a"], "616068db-0cf7-41ff-91a2-dd023ae831c8": ["15f3cd3d-e0dd-49ef-ab47-d89896ccccdd"], "b3dda3fa-2349-420d-8821-22783dc414ba": ["15f3cd3d-e0dd-49ef-ab47-d89896ccccdd"], "25e29c7d-ac6e-42a6-9d7b-6ea3a2789be6": ["d02d4c00-c1d4-43bf-ab45-cedd5cea10d3"], "93564f59-372b-4cfe-b34c-57bf183c4fe9": ["d02d4c00-c1d4-43bf-ab45-cedd5cea10d3"], "efdf762b-8b77-4694-b8a4-d0609d3d4ce8": ["cf43a9a1-8617-46a1-80eb-5ba6cd6c7a1b"], "31b46412-5fd7-4269-a146-5b44bcbbcd51": ["cf43a9a1-8617-46a1-80eb-5ba6cd6c7a1b"], "732a44a3-632f-47e0-9208-d47547279f67": ["d31e77fd-84b2-4737-8bec-433c1c7e4cbf"], "a435438b-9712-4b7e-a450-eee1f640e89c": ["d31e77fd-84b2-4737-8bec-433c1c7e4cbf"], "5db91814-186a-427a-bac6-2a00d5d14992": ["241ed0a7-f2be-49df-8f2e-cc7651655871"], "dcee3911-64d4-4538-b164-868584ef4023": ["241ed0a7-f2be-49df-8f2e-cc7651655871"], "84fa5f54-37ac-42b7-b79c-e5f8ea9135c1": ["0c85ff0e-6227-469c-97a8-b621b67bfdf4"], "db18b0be-6f7b-471b-891c-051f308a9bbf": ["0c85ff0e-6227-469c-97a8-b621b67bfdf4"], "7c950c1d-027b-4774-ad9a-83c0cdd379d9": ["6462b72f-e6f7-45f8-9e40-ed14f992f8ca"], "1ebde6db-56cb-4f4a-ab11-239fd5cc8cd3": ["6462b72f-e6f7-45f8-9e40-ed14f992f8ca"], "15b8efb4-ada5-4f56-8831-626936569ab5": ["b210c008-0451-4d01-81f0-3bbe88c02a5b"], "bc989430-7f70-4cb5-8a78-00e2174f4eaf": ["b210c008-0451-4d01-81f0-3bbe88c02a5b"], "7b1ba95c-bc1d-4c44-9150-766c80be9d83": ["32bf79d0-5ed7-4634-99de-0d3965c62e28"], "00f5dcb3-6a1d-4d0f-8048-00098fa0e3f1": ["32bf79d0-5ed7-4634-99de-0d3965c62e28"]}, "corpus": {"ab3c5590-40c5-4517-82e2-153afb9ccaa8": "arXiv preprint arXiv:2305.14233 (2023).\n\n\n\n\n\n\nDing et\u00a0al. (2024)\n\nYangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali\u00a0Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, et\u00a0al. 2024.\n\n\nCrosscodeeval: A diverse and multilingual benchmark for cross-file code completion.\n\n\nAdvances in Neural Information Processing Systems 36 (2024).\n\n\n\n\n\n\nDoddapaneni et\u00a0al. (2024)\n\nSumanth Doddapaneni, Mohammed Safi Ur\u00a0Rahman Khan, Sshubam Verma, and Mitesh\u00a0M Khapra. 2024.\n\n\nFinding Blind Spots in Evaluator LLMs with Interpretable Checklists.\n\n\narXiv preprint arXiv:2406.13439 (2024).\n\n\n\n\n\n\nDong et\u00a0al. (2024a)", "d2f1df11-2ad9-4806-a54d-f6120e588e2f": "Dong et\u00a0al. (2024a)\n\nQingxiu Dong, Li Dong, Xingxing Zhang, Zhifang Sui, and Furu Wei. 2024a.\n\n\nSelf-Boosting Large Language Models with Synthetic Preference Data.\n\n\narXiv preprint arXiv:2410.06961 (2024).\n\n\n\n\n\n\nDong et\u00a0al. (2022)\n\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et\u00a0al. 2022.\n\n\nA survey on in-context learning.\n\n\narXiv preprint arXiv:2301.00234 (2022).\n\n\n\n\n\n\nDong et\u00a0al. (2024b)\n\nYijiang\u00a0River Dong, Tiancheng Hu, and Nigel Collier. 2024b.\n\n\nCan LLM be a Personalized Judge?\n\n\narXiv preprint arXiv:2406.11657 (2024).\n\n\n\n\n\n\nDorner et\u00a0al. (2024)\n\nFlorian\u00a0E. Dorner, Vivian\u00a0Y. Nastl, and Moritz Hardt. 2024.", "bcc7c482-300a-476e-bf16-6be57a696481": "Limits to scalable evaluation at the frontier: LLM as Judge won\u2019t beat twice the data.\n\n\n\n\narXiv:2410.13341\u00a0[cs.LG]\n\nhttps://arxiv.org/abs/2410.13341\n\n\n\nDubois et\u00a0al. (2024)\n\nYann Dubois, Bal\u00e1zs Galambosi, Percy Liang, and Tatsunori\u00a0B Hashimoto. 2024.\n\n\nLength-controlled alpacaeval: A simple way to debias automatic evaluators.\n\n\narXiv preprint arXiv:2404.04475 (2024).\n\n\n\n\n\n\nEbrahimi et\u00a0al. (2017)\n\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2017.\n\n\nHotflip: White-box adversarial examples for text classification.\n\n\narXiv preprint arXiv:1712.06751 (2017).\n\n\n\n\n\n\nElangovan et\u00a0al. (2024)", "7d5cf900-0cf4-445f-975b-0edc944cc0d5": "Elangovan et\u00a0al. (2024)\n\nAparna Elangovan, Jongwoo Ko, Lei Xu, Mahsa Elyasi, Ling Liu, Sravan Bodapati, and Dan Roth. 2024.\n\n\nBeyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge.\n\n\narXiv preprint arXiv:2410.03775 (2024).\n\n\n\n\n\n\nFabbri et\u00a0al. (2021)\n\nAlexander\u00a0R Fabbri, Wojciech Kry\u015bci\u0144ski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021.\n\n\nSummeval: Re-evaluating summarization evaluation.\n\n\nTransactions of the Association for Computational Linguistics 9 (2021), 391\u2013409.\n\n\n\n\n\n\nFan et\u00a0al. (2018)\n\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018.\n\n\nHierarchical neural story generation.", "abde692d-fbd2-4913-87e3-604e4b863a81": "Hierarchical neural story generation.\n\n\narXiv preprint arXiv:1805.04833 (2018).\n\n\n\n\n\n\nFan et\u00a0al. (2024)\n\nZhiting Fan, Ruizhe Chen, Ruiling Xu, and Zuozhu Liu. 2024.\n\n\nBiasalert: A plug-and-play tool for social bias detection in llms.\n\n\narXiv preprint arXiv:2407.10241 (2024).\n\n\n\n\n\n\nFei et\u00a0al. (2023)\n\nYu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut. 2023.\n\n\nMitigating label biases for in-context learning.\n\n\narXiv preprint arXiv:2305.19148 (2023).\n\n\n\n\n\n\nFeng et\u00a0al. (2023)\n\nChao Feng, Xinyu Zhang, and Zichu Fei. 2023.\n\n\nKnowledge solver: Teaching llms to search for domain knowledge from knowledge graphs.\n\n\narXiv preprint arXiv:2309.03118 (2023).\n\n\n\n\n\n\nFeng et\u00a0al. (2024)", "8f49d348-5e5a-406f-b604-980afc59c891": "Feng et\u00a0al. (2024)\n\nZhaopeng Feng, Yan Zhang, Hao Li, Wenqiang Liu, Jun Lang, Yang Feng, Jian Wu, and Zuozhu Liu. 2024.\n\n\nImproving llm-based machine translation with systematic self-correction.\n\n\narXiv preprint arXiv:2402.16379 (2024).\n\n\n\n\n\n\nFreitag et\u00a0al. (2021a)\n\nMarkus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun Tan, and Wolfgang Macherey. 2021a.\n\n\nExperts, errors, and context: A large-scale study of human evaluation for machine translation.\n\n\nTransactions of the Association for Computational Linguistics 9 (2021), 1460\u20131474.\n\n\n\n\n\n\nFreitag et\u00a0al. (2021b)", "ae9f1779-aa4a-4bab-82c0-911706b9d597": "Freitag et\u00a0al. (2021b)\n\nMarkus Freitag, Ricardo Rei, Nitika Mathur, Chi-kiu Lo, Craig Stewart, George Foster, Alon Lavie, and Ond\u0159ej Bojar. 2021b.\n\n\nResults of the WMT21 metrics shared task: Evaluating metrics with expert-based human evaluations on TED and news domain. In Proceedings of the Sixth Conference on Machine Translation. 733\u2013774.\n\n\n\n\n\n\nFrench (2000)\n\nRobert\u00a0M French. 2000.\n\n\nThe Turing Test: the first 50 years.\n\n\nTrends in cognitive sciences 4, 3 (2000), 115\u2013122.\n\n\n\n\n\n\nFu et\u00a0al. (2023)\n\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023.\n\n\nGptscore: Evaluate as you desire.\n\n\narXiv preprint arXiv:2302.04166 (2023).\n\n\n\n\n\n\nGao et\u00a0al. (2023)", "dcd28063-070c-4149-8910-42980ab0bae4": "Gao et\u00a0al. (2023)\n\nYunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, and Haofen Wang. 2023.\n\n\nRetrieval-augmented generation for large language models: A survey.\n\n\narXiv preprint arXiv:2312.10997 (2023).\n\n\n\n\n\n\nGao et\u00a0al. (2024)\n\nYicheng Gao, Gonghan Xu, Zhe Wang, and Arman Cohan. 2024.\n\n\nBayesian Calibration of Win Rate Estimation with LLM Evaluators.\n\n\narXiv preprint arXiv:2411.04424 (2024).\n\n\n\n\n\n\nGilardi et\u00a0al. (2023)\n\nFabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. 2023.\n\n\nChatGPT outperforms crowd workers for text-annotation tasks.\n\n\nProceedings of the National Academy of Sciences 120, 30 (2023), e2305016120.", "99d83c15-6b17-4bef-9ecd-d742cc633604": "Gopalakrishnan et\u00a0al. (2023)\n\nKarthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, and Dilek Hakkani-Tur. 2023.\n\n\nTopical-chat: Towards knowledge-grounded open-domain conversations.\n\n\narXiv preprint arXiv:2308.11995 (2023).\n\n\n\n\n\n\nGuan et\u00a0al. (2021)\n\nJian Guan, Zhexin Zhang, Zhuoer Feng, Zitao Liu, Wenbiao Ding, Xiaoxi Mao, Changjie Fan, and Minlie Huang. 2021.\n\n\nOpenMEVA: A benchmark for evaluating open-ended story generation metrics.\n\n\narXiv preprint arXiv:2105.08920 (2021).\n\n\n\n\n\n\nGuo et\u00a0al. (2024)", "79ba891a-6062-43cd-826f-6da880f619db": "Guo et\u00a0al. (2024)\n\nShangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas Mesnard, Yao Zhao, Bilal Piot, et\u00a0al. 2024.\n\n\nDirect language model alignment from online ai feedback.\n\n\narXiv preprint arXiv:2402.04792 (2024).\n\n\n\n\n\n\nGuo et\u00a0al. (2023)\n\nZishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et\u00a0al. 2023.\n\n\nEvaluating large language models: A comprehensive survey.\n\n\narXiv preprint arXiv:2310.19736 (2023).\n\n\n\n\n\n\nGupta et\u00a0al. (2024)\n\nTaneesh Gupta, Shivam Shandilya, Xuchao Zhang, Supriyo Ghosh, Chetan Bansal, Huaxiu Yao, and Saravan Rajmohan. 2024.", "ca353861-d5de-48de-8c90-72a55d2ac255": "Unveiling Context-Aware Criteria in Self-Assessing LLMs.\n\n\narXiv preprint arXiv:2410.21545 (2024).\n\n\n\n\n\n\nHada et\u00a0al. (2023)\n\nRishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. 2023.\n\n\nAre large language model-based evaluators the solution to scaling up multilingual evaluation?\n\n\narXiv preprint arXiv:2309.07462 (2023).\n\n\n\n\n\n\nHan et\u00a0al. (2022)\n\nZhixiong Han, Yaru Hao, Li Dong, Yutao Sun, and Furu Wei. 2022.\n\n\nPrototypical calibration for few-shot learning of language models.\n\n\narXiv preprint arXiv:2205.10183 (2022).\n\n\n\n\n\n\nHao et\u00a0al. (2024)", "a97d74a1-0340-4eff-a0cb-1239e05011ab": "Hao et\u00a0al. (2024)\n\nJing Hao, Yuxiang Zhao, Song Chen, Yanpeng Sun, Qiang Chen, Gang Zhang, Kun Yao, Errui Ding, and Jingdong Wang. 2024.\n\n\nFullanno: A data engine for enhancing image comprehension of mllms.\n\n\narXiv preprint arXiv:2409.13540 (2024).\n\n\n\n\n\n\nHarper and Konstan (2015)\n\nF\u00a0Maxwell Harper and Joseph\u00a0A Konstan. 2015.\n\n\nThe movielens datasets: History and context.\n\n\nAcm transactions on interactive intelligent systems (tiis) 5, 4 (2015), 1\u201319.\n\n\n\n\n\n\nHasanbeig et\u00a0al. (2023)\n\nHosein Hasanbeig, Hiteshi Sharma, Leo Betthauser, Felipe Vieira\u00a0Frujeri, and Ida Momennejad. 2023.\n\n\nALLURE: auditing and improving llm-based evaluation of text using iterative in-context-learning.", "57adbacf-7753-46a3-8eee-4bc3d1379f07": "arXiv e-prints (2023), arXiv\u20132309.\n\n\n\n\n\n\nHe et\u00a0al. (2023b)\n\nHangfeng He, Hongming Zhang, and Dan Roth. 2023b.\n\n\nSocreval: Large language models with the socratic method for reference-free reasoning evaluation.\n\n\narXiv preprint arXiv:2310.00074 (2023).\n\n\n\n\n\n\nHe et\u00a0al. (2023a)\n\nXingwei He, Zhenghao Lin, Yeyun Gong, Alex Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu\u00a0Ming Yiu, Nan Duan, Weizhu Chen, et\u00a0al. 2023a.\n\n\nAnnollm: Making large language models to be better crowdsourced annotators.\n\n\narXiv preprint arXiv:2303.16854 (2023).\n\n\n\n\n\n\nHe et\u00a0al. (2024b)\n\nYuanqin He, Yan Kang, Lixin Fan, and Qiang Yang. 2024b.", "1d1b7134-2299-4aeb-a64e-e1bc5cb92e31": "FedEval-LLM: Federated Evaluation of Large Language Models on Downstream Tasks with Collective Wisdom.\n\n\narXiv preprint arXiv:2404.12273 (2024).\n\n\n\n\n\n\nHe et\u00a0al. (2024a)\n\nZeyu He, Chieh-Yang Huang, Chien-Kuang\u00a0Cornelia Ding, Shaurya Rohatgi, and Ting-Hao\u00a0Kenneth Huang. 2024a.\n\n\nIf in a Crowdsourced Data Annotation Pipeline, a GPT-4. In Proceedings of the CHI Conference on Human Factors in Computing Systems. 1\u201325.\n\n\n\n\n\n\nHijazi et\u00a0al. (2024)\n\nHashem Hijazi, Diego Molla, Vincent Nguyen, and Sarvnaz Karimi. 2024.\n\n\nUsing Large Language Models to Evaluate Biomedical Query-Focused Summarisation. In Proceedings of the 23rd Workshop on Biomedical Natural Language Processing. 236\u2013242.", "68a8a158-0008-4fe8-948b-223bc400cecd": "Hou et\u00a0al. (2024)\n\nYupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley, and Wayne\u00a0Xin Zhao. 2024.\n\n\nLarge language models are zero-shot rankers for recommender systems. In European Conference on Information Retrieval. Springer, 364\u2013381.\n\n\n\n\n\n\nHu et\u00a0al. (2024a)\n\nXinyu Hu, Li Lin, Mingqi Gao, Xunjian Yin, and Xiaojun Wan. 2024a.\n\n\nThemis: A reference-free nlg evaluation language model with flexibility and interpretability.\n\n\narXiv preprint arXiv:2406.18365 (2024).\n\n\n\n\n\n\nHu et\u00a0al. (2024b)\n\nZhengyu Hu, Linxin Song, Jieyu Zhang, Zheyuan Xiao, Jingang Wang, Zhenyu Chen, and Hui Xiong. 2024b.\n\n\nRethinking llm-based preference evaluation.", "6ab5545b-eb80-41cb-838f-5d589ac625a3": "Rethinking llm-based preference evaluation.\n\n\narXiv preprint arXiv:2407.01085 (2024).\n\n\n\n\n\n\nHu et\u00a0al. (2024c)\n\nZhengyu Hu, Jieyu Zhang, Zhihan Xiong, Alexander Ratner, Hui Xiong, and Ranjay Krishna. 2024c.\n\n\nLanguage Model Preference Evaluation with Multiple Weak Evaluators.\n\n\narXiv preprint arXiv:2410.12869 (2024).\n\n\n\n\n\n\nHuang et\u00a0al. (2024a)\n\nHui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. 2024a.\n\n\nAn empirical study of llm-as-a-judge for llm evaluation: Fine-tuned judge models are task-specific classifiers.\n\n\narXiv preprint arXiv:2403.02839 (2024).\n\n\n\n\n\n\nHuang et\u00a0al. (2024b)\n\nHui Huang, Yingqi Qu, Hongli Zhou, Jing Liu, Muyun Yang, Bing Xu, and Tiejun Zhao. 2024b.", "e3902b87-6979-43d3-a47a-234d85f1ae2a": "On the limitations of fine-tuned judge models for llm evaluation.\n\n\narXiv preprint arXiv:2403.02839 (2024).\n\n\n\n\n\n\nHuang et\u00a0al. (2023)\n\nJie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu\u00a0Steven Zheng, Adams\u00a0Wei Yu, Xinying Song, and Denny Zhou. 2023.\n\n\nLarge language models cannot self-correct reasoning yet.\n\n\narXiv preprint arXiv:2310.01798 (2023).\n\n\n\n\n\n\nJain et\u00a0al. (2023)\n\nSameer Jain, Vaishakh Keshava, Swarnashree\u00a0Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, and Chunting Zhou. 2023.\n\n\nMulti-dimensional evaluation of text summarization with in-context learning.\n\n\narXiv preprint arXiv:2306.01200 (2023).\n\n\n\n\n\n\nJeong et\u00a0al. (2024)", "5ca9089a-5808-4243-a876-05289920ec99": "Jeong et\u00a0al. (2024)\n\nMinbyul Jeong, Jiwoong Sohn, Mujeen Sung, and Jaewoo Kang. 2024.\n\n\nImproving medical reasoning through retrieval and self-reflection with retrieval-augmented large language models.\n\n\nBioinformatics 40, Supplement_1 (2024), i119\u2013i129.\n\n\n\n\n\n\nJi et\u00a0al. (2024)\n\nJiaming Ji, Donghai Hong, Borong Zhang, Boyuan Chen, Josef Dai, Boren Zheng, Tianyi Qiu, Boxun Li, and Yaodong Yang. 2024.\n\n\nPKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference.\n\n\narXiv preprint arXiv:2406.15513 (2024).\n\n\n\n\n\n\nJi et\u00a0al. (2023)\n\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye\u00a0Jin Bang, Andrea Madotto, and Pascale Fung. 2023.", "e9d4b1c8-b321-48fe-a168-01b07d462e5a": "Survey of hallucination in natural language generation.\n\n\nComput. Surveys 55, 12 (2023), 1\u201338.\n\n\n\n\n\n\nJiang et\u00a0al. (2024)\n\nBowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie\u00a0J Su, Camillo\u00a0J Taylor, and Dan Roth. 2024.\n\n\nA Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners.\n\n\narXiv preprint arXiv:2406.11050 (2024).\n\n\n\n\n\n\nJiang et\u00a0al. (2023b)\n\nDongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang, Bill\u00a0Yuchen Lin, and Wenhu Chen. 2023b.\n\n\nTigerscore: Towards building explainable metric for all text generation tasks.\n\n\nTransactions on Machine Learning Research (2023).\n\n\n\n\n\n\nJiang et\u00a0al. (2023a)\n\nShuyu Jiang, Xingshu Chen, and Rui Tang. 2023a.", "e00d6c4d-cba6-4487-9e69-5349c658ac04": "Shuyu Jiang, Xingshu Chen, and Rui Tang. 2023a.\n\n\nPrompt packer: Deceiving llms through compositional instruction with hidden attacks.\n\n\narXiv preprint arXiv:2310.10077 (2023).\n\n\n\n\n\n\nJimenez et\u00a0al. (2023)\n\nCarlos\u00a0E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023.\n\n\nSwe-bench: Can language models resolve real-world github issues?, 2024.\n\n\nURL https://arxiv. org/abs/2310.06770 (2023).\n\n\n\n\n\n\nJinnai et\u00a0al. (2024)\n\nYuu Jinnai, Tetsuro Morimura, Kaito Ariu, and Kenshi Abe. 2024.\n\n\nRegularized Best-of-N Sampling to Mitigate Reward Hacking for Language Model Alignment.\n\n\narXiv preprint arXiv:2404.01054 (2024).\n\n\n\n\n\n\nJung et\u00a0al. (2024)", "6f1a2f91-7990-4ffb-bc11-17eae541ae01": "Jung et\u00a0al. (2024)\n\nJaehun Jung, Faeze Brahman, and Yejin Choi. 2024.\n\n\nTrust or Escalate: LLM Judges with Provable Guarantees for Human Agreement.\n\n\narXiv preprint arXiv:2407.18370 (2024).\n\n\n\n\n\n\nKarpinska and Iyyer (2023)\n\nMarzena Karpinska and Mohit Iyyer. 2023.\n\n\nLarge language models effectively leverage document-level context for literary translation, but critical errors persist.\n\n\narXiv preprint arXiv:2304.03245 (2023).\n\n\n\n\n\n\nKawabata and Sugawara (2024)\n\nAkira Kawabata and Saku Sugawara. 2024.\n\n\nRationale-Aware Answer Verification by Pairwise Self-Evaluation.\n\n\narXiv preprint arXiv:2410.04838 (2024).\n\n\n\n\n\n\nKe et\u00a0al. (2024)", "8a358560-ec19-4dff-9cc0-f85e698c37a1": "Ke et\u00a0al. (2024)\n\nPei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, et\u00a0al. 2024.\n\n\nCritiquellm: Towards an informative critique generation model for evaluation of large language model generation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 13034\u201313054.\n\n\n\n\n\n\nKhan et\u00a0al. (2024)\n\nAkbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel\u00a0R Bowman, Tim Rockt\u00e4schel, and Ethan Perez. 2024.\n\n\nDebating with more persuasive llms leads to more truthful answers.", "19fd7223-2121-4710-a525-e0abf6e6e897": "arXiv preprint arXiv:2402.06782 (2024).\n\n\n\n\n\n\nKim et\u00a0al. (2024a)\n\nDongyoung Kim, Kimin Lee, Jinwoo Shin, and Jaehyung Kim. 2024a.\n\n\nAligning Large Language Models with Self-generated Preference Data.\n\n\narXiv preprint arXiv:2406.04412 (2024).\n\n\n\n\n\n\nKim et\u00a0al. (2023)\n\nSeungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et\u00a0al. 2023.\n\n\nPrometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations.\n\n\n\n\n\n\nKim et\u00a0al. (2024b)", "b0665698-4034-4e80-8560-fcef1fbefa28": "Kim et\u00a0al. (2024b)\n\nSeungone Kim, Juyoung Suk, Shayne Longpre, Bill\u00a0Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2024b.\n\n\nPrometheus 2: An open source language model specialized in evaluating other language models.\n\n\narXiv preprint arXiv:2405.01535 (2024).\n\n\n\n\n\n\nKo et\u00a0al. (2020)\n\nMiyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, and Jaewoo Kang. 2020.\n\n\nLook at the first sentence: Position bias in question answering.\n\n\narXiv preprint arXiv:2004.14602 (2020).\n\n\n\n\n\n\nKoo et\u00a0al. (2023)\n\nRyan Koo, Minhwa Lee, Vipul Raheja, Jong\u00a0Inn Park, Zae\u00a0Myung Kim, and Dongyeop Kang. 2023.", "a7010010-2e3a-44ae-8437-796e5824dd11": "Benchmarking cognitive biases in large language models as evaluators.\n\n\narXiv preprint arXiv:2309.17012 (2023).\n\n\n\n\n\n\nKotonya et\u00a0al. (2023)\n\nNeema Kotonya, Saran Krishnasamy, Joel Tetreault, and Alejandro Jaimes. 2023.\n\n\nLittle giants: Exploring the potential of small llms as evaluation metrics in summarization in the eval4nlp 2023 shared task.\n\n\narXiv preprint arXiv:2311.00686 (2023).\n\n\n\n\n\n\nKrolik et\u00a0al. (2024)\n\nJack Krolik, Herprit Mahal, Feroz Ahmad, Gaurav Trivedi, and Bahador Saket. 2024.\n\n\nTowards Leveraging Large Language Models for Automated Medical Q&A Evaluation.\n\n\narXiv preprint arXiv:2409.01941 (2024).\n\n\n\n\n\n\nKumar et\u00a0al. (2024)", "36b2de72-780a-4530-a8c3-a7c816300737": "Kumar et\u00a0al. (2024)\n\nAbhishek Kumar, Sonia Haiduc, Partha\u00a0Pratim Das, and Partha\u00a0Pratim Chakrabarti. 2024.\n\n\nLLMs as Evaluators: A Novel Approach to Evaluate Bug Report Summarization.\n\n\narXiv preprint arXiv:2409.00630 (2024).\n\n\n\n\n\n\nLambert et\u00a0al. (2024)\n\nNathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill\u00a0Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et\u00a0al. 2024.\n\n\nRewardbench: Evaluating reward models for language modeling.\n\n\narXiv preprint arXiv:2403.13787 (2024).\n\n\n\n\n\n\nLatif et\u00a0al. (2023)\n\nSiddique Latif, Muhammad Usama, Mohammad\u00a0Ibrahim Malik, and Bj\u00f6rn\u00a0W Schuller. 2023.", "6d5987d5-2e3c-4230-9831-9e9305a3d95d": "Can large language models aid in annotating speech emotional data? uncovering new frontiers.\n\n\narXiv preprint arXiv:2307.06090 (2023).\n\n\n\n\n\n\nLawrie et\u00a0al. (2024)\n\nDawn Lawrie, Sean MacAvaney, James Mayfield, Paul McNamee, Douglas\u00a0W Oard, Luca Soldaini, and Eugene Yang. 2024.\n\n\nOverview of the TREC 2023 NeuCLIR Track.\n\n\narXiv preprint arXiv:2404.08071 (2024).\n\n\n\n\n\n\nLeCun et\u00a0al. (2015)\n\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015.\n\n\nDeep learning.\n\n\nnature 521, 7553 (2015), 436\u2013444.\n\n\n\n\n\n\nLee et\u00a0al. (2022)\n\nDeokjae Lee, Seungyong Moon, Junhyeok Lee, and Hyun\u00a0Oh Song. 2022.", "07c4c182-c108-4511-8a19-38bdb46fbf07": "Query-efficient and scalable black-box adversarial attacks on discrete sequential data via bayesian optimization. In International Conference on Machine Learning. PMLR, 12478\u201312497.\n\n\n\n\n\n\nLee et\u00a0al. (2023)\n\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie\u00a0Ren Lu, Thomas Mesnard, Johan Ferret, Colton Bishop, Ethan Hall, Victor Carbune, and Abhinav Rastogi. 2023.\n\n\nRlaif: Scaling reinforcement learning from human feedback with ai feedback.\n\n\n(2023).\n\n\n\n\n\n\nLee et\u00a0al. (2024a)\n\nSeongyun Lee, Seungone Kim, Sue\u00a0Hyun Park, Geewook Kim, and Minjoon Seo. 2024a.\n\n\nPrometheusvision: Vision-language model as a judge for fine-grained evaluation.\n\n\narXiv preprint arXiv:2401.06591 (2024).", "5012737f-669e-4078-835b-f79197feeb81": "arXiv preprint arXiv:2401.06591 (2024).\n\n\n\n\n\n\nLee et\u00a0al. (2024b)\n\nSangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang\u00a0Min Yoo, and Youngjae Yu. 2024b.\n\n\nAligning Large Language Models by On-Policy Self-Judgment.\n\n\narXiv preprint arXiv:2402.11253 (2024).\n\n\n\n\n\n\nLei et\u00a0al. (2024)\n\nYuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, and Xing Xie. 2024.\n\n\nRecExplainer: Aligning Large Language Models for Explaining Recommendation Models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 1530\u20131541.\n\n\n\n\n\n\nLewis et\u00a0al. (2020)", "919bbd9e-32ee-4e08-aec6-3f6bde5faffb": "Lewis et\u00a0al. (2020)\n\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, et\u00a0al. 2020.\n\n\nRetrieval-augmented generation for knowledge-intensive nlp tasks.\n\n\nAdvances in Neural Information Processing Systems 33 (2020), 9459\u20139474.\n\n\n\n\n\n\nLi et\u00a0al. (2024c)\n\nAnqi Li, Yu Lu, Nirui Song, Shuai Zhang, Lizhi Ma, and Zhenzhong Lan. 2024c.\n\n\nAutomatic evaluation for mental health counseling using llms.\n\n\narXiv preprint arXiv:2402.11958 (2024).\n\n\n\n\n\n\nLi et\u00a0al. (2024a)\n\nHaitao Li, Junjie Chen, Qingyao Ai, Zhumin Chu, Yujia Zhou, Qian Dong, and Yiqun Liu. 2024a.", "45948a2d-a1f6-413c-ac04-ea2de38f556a": "Calibraeval: Calibrating prediction distribution to mitigate selection bias in llms-as-judges.\n\n\narXiv preprint arXiv:2410.15393 (2024).\n\n\n\n\n\n\nLi et\u00a0al. (2024b)\n\nHaitao Li, You Chen, Qingyao Ai, Yueyue Wu, Ruizhe Zhang, and Yiqun Liu. 2024b.\n\n\nLexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models.\n\n\n\n\narXiv:2409.20288\u00a0[cs.CL]\n\nhttps://arxiv.org/abs/2409.20288\n\n\n\nLi et\u00a0al. (2024d)\n\nHaitao Li, Yunqiu Shao, Yueyue Wu, Qingyao Ai, Yixiao Ma, and Yiqun Liu. 2024d.\n\n\nLecardv2: A large-scale chinese legal case retrieval dataset. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2251\u20132260.", "bf3f1f6e-f950-4498-8224-4144b51a85a9": "Li et\u00a0al. (2023c)\n\nJunlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. 2023c.\n\n\nGenerative judge for evaluating alignment.\n\n\narXiv preprint arXiv:2310.05470 (2023).\n\n\n\n\n\n\nLi et\u00a0al. (2023a)\n\nQintong Li, Leyang Cui, Lingpeng Kong, and Wei Bi. 2023a.\n\n\nCollaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation.\n\n\narXiv preprint arXiv:2310.19740 (2023).\n\n\n\n\n\n\nLi et\u00a0al. (2023b)\n\nRuosen Li, Teerth Patel, and Xinya Du. 2023b.\n\n\nPrd: Peer rank and discussion improve large language model based evaluations.\n\n\narXiv preprint arXiv:2307.02762 (2023).\n\n\n\n\n\n\nLi et\u00a0al. (2017)", "9a825bd9-251b-4c56-8182-e67997a4a712": "Li et\u00a0al. (2017)\n\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017.\n\n\nDailyDialog: A Manually Labelled Multi-turn Dialogue Dataset. In Proceedings of The 8th International Joint Conference on Natural Language Processing (IJCNLP 2017).\n\n\n\n\n\n\nLi et\u00a0al. (2023d)\n\nZongjie Li, Chaozheng Wang, Pingchuan Ma, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang Liu. 2023d.\n\n\nSplit and merge: Aligning position biases in large language model based evaluators.\n\n\narXiv preprint arXiv:2310.01432 (2023).\n\n\n\n\n\n\nLiang et\u00a0al. (2024a)\n\nJingcong Liang, Rong Ye, Meng Han, Ruofei Lai, Xinyu Zhang, Xuanjing Huang, and Zhongyu Wei. 2024a.", "df38f65d-78a5-4c7a-97b4-284bac62fc2d": "Debatrix: Multi-dimensinal Debate Judge with Iterative Chronological Analysis Based on LLM.\n\n\narXiv preprint arXiv:2403.08010 (2024).\n\n\n\n\n\n\nLiang et\u00a0al. (2024b)\n\nSirui Liang, Baoli Zhang, Jun Zhao, and Kang Liu. 2024b.\n\n\nABSEval: An Agent-based Framework for Script Evaluation. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 12418\u201312434.\n\n\n\n\n\n\nLightman et\u00a0al. (2023)\n\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023.\n\n\nLet\u2019s verify step by step.\n\n\narXiv preprint arXiv:2305.20050 (2023).\n\n\n\n\n\n\nLin et\u00a0al. (2024)", "53b7b232-a20b-40c6-aed5-79d1045ccc6e": "Lin et\u00a0al. (2024)\n\nBill\u00a0Yuchen Lin, Yuntian Deng, Khyathi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan\u00a0Le Bras, and Yejin Choi. 2024.\n\n\nWILDBENCH: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild.\n\n\narXiv preprint arXiv:2406.04770 (2024).\n\n\n\n\n\n\nLin (2004)\n\nChin-Yew Lin. 2004.\n\n\nRouge: A package for automatic evaluation of summaries. In Text summarization branches out. 74\u201381.\n\n\n\n\n\n\nLin et\u00a0al. (2021)\n\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\n\n\nTruthfulqa: Measuring how models mimic human falsehoods.\n\n\narXiv preprint arXiv:2109.07958 (2021).\n\n\n\n\n\n\nLin and Chen (2023)\n\nYen-Ting Lin and Yun-Nung Chen. 2023.", "580d7e98-1804-4d9c-afe7-4ed8bf19301d": "Yen-Ting Lin and Yun-Nung Chen. 2023.\n\n\nLlm-eval: Unified multi-dimensional automatic evaluation for open-domain conversations with large language models.\n\n\narXiv preprint arXiv:2305.13711 (2023).\n\n\n\n\n\n\nLiu et\u00a0al. (2023c)\n\nMinqian Liu, Ying Shen, Zhiyang Xu, Yixin Cao, Eunah Cho, Vaibhav Kumar, Reza Ghanadan, and Lifu Huang. 2023c.\n\n\nX-eval: Generalizable multi-aspect text evaluation via augmented instruction tuning with auxiliary evaluation aspects.\n\n\narXiv preprint arXiv:2311.08788 (2023).\n\n\n\n\n\n\nLiu et\u00a0al. (2023b)\n\nXiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng\u00a0Lam Tam, et\u00a0al. 2023b.", "c85552f4-60dc-49c8-9d99-329c75c2582f": "Alignbench: Benchmarking chinese alignment of large language models.\n\n\narXiv preprint arXiv:2311.18743 (2023).\n\n\n\n\n\n\nLiu et\u00a0al. (2023e)\n\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et\u00a0al. 2023e.\n\n\nAgentbench: Evaluating llms as agents.\n\n\narXiv preprint arXiv:2308.03688 (2023).\n\n\n\n\n\n\nLiu et\u00a0al. (2023a)\n\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023a.\n\n\nG-eval: Nlg evaluation using gpt-4 with better human alignment.\n\n\narXiv preprint arXiv:2303.16634 (2023).\n\n\n\n\n\n\nLiu et\u00a0al. (2023d)", "a6cc0094-d6c2-47b5-ae31-9c59d2f44b58": "Liu et\u00a0al. (2023d)\n\nYuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2023d.\n\n\nCalibrating llm-based evaluator.\n\n\narXiv preprint arXiv:2309.13308 (2023).\n\n\n\n\n\n\nLiu et\u00a0al. (2024a)\n\nYuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, and Qi Zhang. 2024a.\n\n\nHD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition.\n\n\narXiv preprint arXiv:2402.15754 (2024).\n\n\n\n\n\n\nLiu et\u00a0al. (2024b)\n\nYantao Liu, Zijun Yao, Rui Min, Yixin Cao, Lei Hou, and Juanzi Li. 2024b.\n\n\nRM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style.", "6e904bce-e177-439b-9d14-3fc72e5638dd": "arXiv preprint arXiv:2410.16184 (2024).\n\n\n\n\n\n\nLiu et\u00a0al. (2024c)\n\nYinhong Liu, Han Zhou, Zhijiang Guo, Ehsan Shareghi, Ivan Vuli\u0107, Anna Korhonen, and Nigel Collier. 2024c.\n\n\nAligning with human judgement: The role of pairwise preference in large language model evaluators.\n\n\narXiv preprint arXiv:2403.16950 (2024).\n\n\n\n\n\n\nLiusie et\u00a0al. (2024)\n\nAdian Liusie, Vatsal Raina, Yassir Fathullah, and Mark Gales. 2024.\n\n\nEfficient LLM Comparative Assessment: a Product of Experts Framework for Pairwise Comparisons.\n\n\narXiv preprint arXiv:2405.05894 (2024).\n\n\n\n\n\n\nLLMS (2025)\n\nSESSMENTS\u00a0BY LLMS. 2025.\n\n\nJUDING THE JUDGES: ASYSTEMATIC INVESTIGATION OF POSITION BIAS IN PAIRWISE COMPARATIVE AS.", "bd2e300f-0774-4bd3-8e6a-1023073cec9c": "Under review as a conference paper at ICLR 2025 (2025).\n\n\n\n\n\n\nLuo et\u00a0al. (2023)\n\nYun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue Zhang. 2023.\n\n\nAn empirical study of catastrophic forgetting in large language models during continual fine-tuning.\n\n\narXiv preprint arXiv:2308.08747 (2023).\n\n\n\n\n\n\nLuo et\u00a0al. (2024)\n\nZiyang Luo, Haoning Wu, Dongxu Li, Jing Ma, Mohan Kankanhalli, and Junnan Li. 2024.\n\n\nVideoAutoArena: An Automated Arena for Evaluating Large Multimodal Models in Video Analysis through User Simulation.\n\n\narXiv preprint arXiv:2411.13281 (2024).\n\n\n\n\n\n\nMa et\u00a0al. (2024)\n\nShengjie Ma, Chong Chen, Qi Chu, and Jiaxin Mao. 2024.", "f8d0411d-031d-4ceb-a143-baffcb82e04a": "Leveraging Large Language Models for Relevance Judgments in Legal Case Retrieval.\n\n\narXiv preprint arXiv:2403.18405 (2024).\n\n\n\n\n\n\nMadaan et\u00a0al. (2024)\n\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et\u00a0al. 2024.\n\n\nSelf-refine: Iterative refinement with self-feedback.\n\n\nAdvances in Neural Information Processing Systems 36 (2024).\n\n\n\n\n\n\nMehri and Eskenazi (2020)\n\nShikib Mehri and Maxine Eskenazi. 2020.\n\n\nUSR: An unsupervised and reference free evaluation metric for dialog generation.\n\n\narXiv preprint arXiv:2005.00456 (2020).\n\n\n\n\n\n\nMendon\u00e7a et\u00a0al. (2024)", "15f3cd3d-e0dd-49ef-ab47-d89896ccccdd": "Mendon\u00e7a et\u00a0al. (2024)\n\nJohn Mendon\u00e7a, Isabel Trancoso, and Alon Lavie. 2024.\n\n\nSoda-Eval: Open-Domain Dialogue Evaluation in the age of LLMs.\n\n\narXiv preprint arXiv:2408.10902 (2024).\n\n\n\n\n\n\nMoniri et\u00a0al. (2024)\n\nBehrad Moniri, Hamed Hassani, and Edgar Dobriban. 2024.\n\n\nEvaluating the Performance of Large Language Models via Debates.\n\n\narXiv preprint arXiv:2406.11044 (2024).\n\n\n\n\n\n\nMostafazadeh et\u00a0al. (2016)\n\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016.", "d02d4c00-c1d4-43bf-ab45-cedd5cea10d3": "A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 839\u2013849.\n\n\n\n\n\n\nMusolesi (2024)\n\nMirco Musolesi. 2024.\n\n\nCreative Beam Search: LLM-as-a-Judge for Improving Response Generation. ICCC.\n\n\n\n\n\n\nMyrzakhan et\u00a0al. (2024)\n\nAidar Myrzakhan, Sondos\u00a0Mahmoud Bsharat, and Zhiqiang Shen. 2024.\n\n\nOpen-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena.\n\n\narXiv preprint arXiv:2406.07545 (2024).\n\n\n\n\n\n\nNarayan et\u00a0al. (2018)\n\nShashi Narayan, Shay\u00a0B Cohen, and Mirella Lapata. 2018.", "cf43a9a1-8617-46a1-80eb-5ba6cd6c7a1b": "Don\u2019t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization.\n\n\narXiv preprint arXiv:1808.08745 (2018).\n\n\n\n\n\n\nNasrabadi (2024)\n\nDom Nasrabadi. 2024.\n\n\nJurEE not Judges: safeguarding llm interactions with small, specialised Encoder Ensembles.\n\n\narXiv preprint arXiv:2410.08442 (2024).\n\n\n\n\n\n\nNilsson (2014)\n\nNils\u00a0J Nilsson. 2014.\n\n\nPrinciples of artificial intelligence.\n\n\nMorgan Kaufmann.\n\n\n\n\n\n\nNing et\u00a0al. (2024)\n\nKun-Peng Ning, Shuo Yang, Yuyang Liu, Jia-Yu Yao, Zhenhui Liu, Yu Wang, Ming Pang, and Li Yuan. 2024.\n\n\nPiCO: Peer Review in LLMs based on the Consistency Optimization.\n\n\n(2024).\n\n\n\n\n\n\nNiu et\u00a0al. (2024)", "d31e77fd-84b2-4737-8bec-433c1c7e4cbf": "(2024).\n\n\n\n\n\n\nNiu et\u00a0al. (2024)\n\nTong Niu, Shafiq Joty, Ye Liu, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. 2024.\n\n\nJudgeRank: Leveraging Large Language Models for Reasoning-Intensive Reranking.\n\n\narXiv preprint arXiv:2411.00142 (2024).\n\n\n\n\n\n\nO\u2019Donoghue et\u00a0al. (2023)\n\nOdhran O\u2019Donoghue, Aleksandar Shtedritski, John Ginger, Ralph Abboud, Ali\u00a0Essa Ghareeb, Justin Booth, and Samuel\u00a0G Rodriques. 2023.\n\n\nBioPlanner: automatic evaluation of LLMs on protocol planning in biology.\n\n\narXiv preprint arXiv:2310.10632 (2023).\n\n\n\n\n\n\nOwens et\u00a0al. (2024)", "241ed0a7-f2be-49df-8f2e-cc7651655871": "Owens et\u00a0al. (2024)\n\nDeonna\u00a0M Owens, Ryan\u00a0A Rossi, Sungchul Kim, Tong Yu, Franck Dernoncourt, Xiang Chen, Ruiyi Zhang, Jiuxiang Gu, Hanieh Deilamsalehy, and Nedim Lipka. 2024.\n\n\nA multi-llm debiasing framework.\n\n\narXiv preprint arXiv:2409.13884 (2024).\n\n\n\n\n\n\nPagnoni et\u00a0al. (2021)\n\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021.\n\n\nUnderstanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics.\n\n\narXiv preprint arXiv:2104.13346 (2021).\n\n\n\n\n\n\nPan et\u00a0al. (2024a)\n\nQian Pan, Zahra Ashktorab, Michael Desmond, Martin\u00a0Santillan Cooper, James Johnson, Rahul Nair, Elizabeth Daly, and Werner Geyer. 2024a.", "0c85ff0e-6227-469c-97a8-b621b67bfdf4": "Human-Centered Design Recommendations for LLM-as-a-judge.\n\n\narXiv preprint arXiv:2407.03479 (2024).\n\n\n\n\n\n\nPan et\u00a0al. (2024b)\n\nShirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu. 2024b.\n\n\nUnifying large language models and knowledge graphs: A roadmap.\n\n\nIEEE Transactions on Knowledge and Data Engineering (2024).\n\n\n\n\n\n\nPanickssery et\u00a0al. (2024)\n\nArjun Panickssery, Samuel\u00a0R Bowman, and Shi Feng. 2024.\n\n\nLlm evaluators recognize and favor their own generations.\n\n\narXiv preprint arXiv:2404.13076 (2024).\n\n\n\n\n\n\nPapineni et\u00a0al. (2002)\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.", "6462b72f-e6f7-45f8-9e40-ed14f992f8ca": "Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics. 311\u2013318.\n\n\n\n\n\n\nPark et\u00a0al. (2024)\n\nJunsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. 2024.\n\n\nOffsetbias: Leveraging debiased data for tuning evaluators.\n\n\narXiv preprint arXiv:2407.06551 (2024).\n\n\n\n\n\n\nPatel et\u00a0al. (2024)\n\nBhrij Patel, Souradip Chakraborty, Wesley\u00a0A Suttle, Mengdi Wang, Amrit\u00a0Singh Bedi, and Dinesh Manocha. 2024.\n\n\nAIME: AI System Optimization via Multiple LLM Evaluators.\n\n\narXiv preprint arXiv:2410.03131 (2024).\n\n\n\n\n\n\nPaul et\u00a0al. (2023)", "b210c008-0451-4d01-81f0-3bbe88c02a5b": "Paul et\u00a0al. (2023)\n\nDebjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi Faltings. 2023.\n\n\nRefiner: Reasoning feedback on intermediate representations.\n\n\narXiv preprint arXiv:2304.01904 (2023).\n\n\n\n\n\n\nPerez and Ribeiro (2022)\n\nF\u00e1bio Perez and Ian Ribeiro. 2022.\n\n\nIgnore previous prompt: Attack techniques for language models.\n\n\narXiv preprint arXiv:2211.09527 (2022).\n\n\n\n\n\n\nPezeshkpour and Hruschka (2023)\n\nPouya Pezeshkpour and Estevam Hruschka. 2023.\n\n\nLarge language models sensitivity to the order of options in multiple-choice questions.\n\n\narXiv preprint arXiv:2308.11483 (2023).\n\n\n\n\n\n\nPoulain et\u00a0al. (2024)", "32bf79d0-5ed7-4634-99de-0d3965c62e28": "Poulain et\u00a0al. (2024)\n\nRaphael Poulain, Hamed Fayyaz, and Rahmatollah Beheshti. 2024.\n\n\nBias patterns in the application of LLMs for clinical decision support: A comprehensive study.\n\n\narXiv preprint arXiv:2404.15149 (2024).\n\n\n\n\n\n\nQin et\u00a0al. (2023)\n\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Le Yan, Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, et\u00a0al. 2023.\n\n\nLarge language models are effective text rankers with pairwise ranking prompting.\n\n\narXiv preprint arXiv:2306.17563 (2023).\n\n\n\n\n\n\nRafailov et\u00a0al. (2024)\n\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher\u00a0D Manning, Stefano Ermon, and Chelsea Finn. 2024."}}