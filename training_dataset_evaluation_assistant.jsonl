{"questions": {"6612a33e-4f9f-4e15-b622-4eb44aaa982e": "What are the different types of reasoning evaluated in the core ability assessment of Large Language Models?", "e56d00cc-381c-4076-bf14-35658f46195a": "Which aspects are considered under the societal impact evaluation of Large Language Models?", "bc841e6c-afed-4775-a6c8-dab5db157044": "What are the different application scenarios discussed for agent evaluation in the context of LLMs?", "d0573f3a-7f60-4dec-a201-f8ff302b359a": "What future directions are proposed for improving LLM agent evaluation?", "4ec76778-4556-4909-8394-920588f8e85a": "What is the main focus of the paper titled \"A Survey of Useful LLM Evaluation\"?", "6f060a87-69a4-4e8a-a526-e73e8adf94c7": "Which institution are the authors of the paper affiliated with?", "129d1f15-0401-4190-a7be-2f940c859fc5": "What is the main focus or objective presented in the abstract?", "d1770a9d-b37a-48f1-9b1c-c7359cc0ed18": "What key findings or conclusions are summarized in the abstract?", "6743a51b-dfd3-4353-8439-e3324a640902": "What is the two-stage framework proposed for evaluating LLMs, and what does each stage represent?", "f951bf65-4db1-46c5-8108-4cf7e419a0aa": "Why is it important to refine methods for evaluating the capabilities of LLMs according to the study?", "615cfcdb-f85e-4239-9526-3f270a32c7d3": "What core abilities of LLMs were discussed in the initial stage of evaluation?", "74bd2e91-d7a9-42fd-8f1f-c68a6ad4090c": "What challenges and future directions were identified regarding the evaluation methods for LLMs?", "49effd7d-90b3-4f14-9703-247f903b51a5": "What does the footnote \"Equal contribution\" indicate about the authors' involvement?", "ef154f9e-a089-4f9c-b17b-6cfcf6fdfff6": "How is the phrase \"Equal contribution\" typically used in academic or research papers?", "c9334af7-4d2d-4fcb-b416-4e2cefa99876": "What is the main focus of the survey conducted by Ji-Lun Peng and colleagues at National Taiwan University?", "498d7f7a-f1ca-4415-bd16-6dc8217ad2c5": "How is the two-stage framework used in the evaluation of Large Language Models described in the introduction?", "d00f080d-45ef-40a8-a27f-50fb90bce93c": "What was the primary focus of the first models of artificial intelligence?", "80477f6d-692e-48c9-8364-d679f7fb8479": "How does the Transformer architecture improve the handling of word embeddings compared to previous models?", "cb6602fc-8cb6-41db-b393-f389ce4034cc": "How has the number of parameters changed across the GPT series from GPT-1 to GPT-3?", "41057151-f121-4192-8b32-96db96e56604": "What new capabilities does GPT-4 have compared to its predecessors?", "4f5a46e2-81aa-4a6d-8abb-e80a1b69d54e": "What types of inputs can the model accept, and what type of outputs does it produce?", "3c428a05-2a14-4ab0-9c2f-d85bccb319b6": "Why are the models mentioned referred to as LLMs?", "82925fa3-3643-4f28-97f6-d5d13f865a8e": "Why is evaluating large language models (LLMs) considered important?", "1dc96380-1de4-4924-9587-beb99b64591c": "What are the key reasons for assessing the performance of LLMs?", "a5218f28-1b1a-4fad-810e-0396c86c52f4": "What was the original purpose of the Turing Test in evaluating machine intelligence?", "78d820ec-71c2-4999-9b81-2ecdebb46168": "Why is it important to have clear benchmarks when assessing the capabilities of AI technologies?", "024f7a76-76a6-482e-b44e-98ae63826b5f": "Why is it important to evaluate the capabilities of LLMs before determining their tasks and responsibilities?", "1234b014-355c-40a2-849a-3f8fc52e94e7": "What are some of the broad capabilities exhibited by LLMs beyond predicting the next words in human-written texts?", "c8990614-4192-409c-ab3b-8e6cfe87c207": "What is the purpose of having evaluation methods tailored to specific tasks for large language models (LLMs)?", "ef707913-5ea2-4d12-a25b-f3b5ccc0aa88": "How does the study categorize and review the abilities and evaluation methods of LLMs?", "e01e4bdb-de2d-4259-bd70-d1da8de8017d": "What are the two categories into which LLMs' capabilities are divided to assess their usefulness?", "275575c3-22ab-4191-897f-507c7705bc2f": "What does the \"core ability\" of LLMs refer to in the context of generating high-quality natural language texts?", "1efcfb36-fd9e-4af7-ac40-101384e255d2": "Why is the capability for reasoning important for LLMs during interactions with humans?", "c4c557b6-4c45-41e7-8cd2-deae5af2f1f7": "What are the key qualities that LLMs must have to be perceived as safe and trustworthy by society?", "83698269-6d17-49c3-bc05-529f8c6cf604": "What core ability allows LLMs to perform complex behaviors and be defined as agents?", "579ca86f-023f-46ca-8506-c878838048b0": "In what ways can LLM agents solve tasks in various scenarios?", "c15b51d9-d350-4f83-b250-8cef06a8154e": "What gap in the existing literature on LLM evaluation methods does this paper aim to address?", "f1bbd4a3-2f5c-4a2a-9975-fbba017c25a2": "What is the main purpose of the two-stage framework proposed in the paper?", "482c3e57-dbe6-493a-a97f-d0b4d1ee9f2a": "What are the main categories and subsections used to evaluate the core abilities of LLMs in this study?", "e737b3a7-a58b-4a88-ac57-6481da0890a7": "How does the study approach the evaluation of LLM agents and what aspects are covered in this evaluation?", "23653912-20b1-4112-88cd-8ce3a33c83b9": "What are the two stages in the framework used to evaluate whether LLMs are sufficiently useful tools?", "e050c6ab-ba80-4e4a-9b23-42098c257439": "What challenges and future directions are identified in the evaluation methods for LLMs?", "a788e79a-b91b-42da-a4ef-6b57354d088f": "What are the different types of reasoning included under Core Ability Evaluation in the given context?", "f7e1bd7f-ce0e-447c-acf8-6b3fdae6a919": "Which studies are referenced under Logical Reasoning in the provided context?", "6707dc3b-c0ec-4400-8763-e5ea8194e85b": "Which studies are referenced in the context of Structured Data Reasoning?", "cd0f4653-f13c-47be-bdc9-89eca4e31db9": "What are the main subtopics discussed under Societal Impact according to the provided context?", "884c9372-548e-40d9-a81b-e29f1cffa295": "Which authors have contributed to the field of Medicine according to the provided context?", "2537d39b-1736-441c-884c-93b77b759ba1": "Can you list the researchers associated with Education as mentioned in the context?", "3334cca0-9ed6-4f24-abdf-9d68f32d1610": "What are the three essential dimensions used to evaluate the core abilities of LLMs according to Figure 2?", "a071fa8e-d2f0-4a09-816b-a196bea52336": "Which sections of the evaluation focus on reasoning, societal impact, and domain-specific knowledge respectively?", "c5b48e9d-55e5-4945-b529-f22702ef0e07": "What core abilities of LLMs are recognized as having the potential to evolve into more complex behaviors?", "b85d6d8a-7769-42d9-8f10-3e4f132b317d": "How does the development of LLMs emphasize their adaptability and scalability for advanced applications?", "cbf9920c-8fbf-4082-be3e-7601f4ae1dc0": "What are the different categories of reasoning tasks mentioned in the context?", "eb20ba6e-5db8-49e5-a065-f65023477b81": "Why is proficiency in reasoning important for both humans and machines according to the context?", "9844e29e-6bea-4a62-abfc-ea01e5dc3b2a": "What type of reasoning is demonstrated by the example where Gertrude is a sheep and the question asks what Gertrude is afraid of?", "0a9843e5-2c92-4e2d-a121-21a42a1e4a07": "In the deductive reasoning example, if Lily is a white swan and Greg is also a swan, what conclusion is drawn about Greg's color?", "6d6a48a3-4a46-4551-9b25-7ccf4557ec79": "What types of logical reasoning are illustrated in Table 1?", "e55fa411-5b60-46b4-8095-2f75f4637aa8": "How does the context \"hyp2\" relate to the examples provided in Table 1?", "ea3a7026-ac10-48ab-a57a-a23f4ba9ec54": "What are the three different types of logical reasoning mentioned in the context?", "ba6cd7a5-3d44-4cd8-a8f6-08e113eb32bb": "Which benchmarks are commonly used for testing inductive reasoning according to the context?", "565eb1c8-f82b-4081-a62f-abbb0890c69f": "What is abductive reasoning and how is it characterized based on the given context?", "68a80c8e-b984-490c-9451-2bb7807a1469": "Which benchmarks are mentioned for evaluating abductive reasoning?", "2d811bfd-64c5-40c0-9c7a-259d1fa23a8c": "According to Xu et al. (2023a), which language model generally performs best in logical reasoning tasks among text-davinci-003, ChatGPT, and BARD?", "47e3cf26-aabd-465f-bf53-43b0cbbb0baa": "How does GPT-4's performance in logical reasoning compare to humans according to Han et al. (2023) and Liu et al. (2023)?", "f319bf68-fa2a-4b17-9437-4d05d8fe670c": "What role do models play in mathematical reasoning according to the context?", "8791f94c-e1e3-4da1-82c5-7c1b424b7cb5": "Can you name two notable examples mentioned that involve mathematical reasoning tasks?", "bada7e86-9338-47d4-910e-84353f19e97d": "According to Stolfo et al. (2023), how do instruction-tuned large language models (LLMs) perform on mathematical problems compared to non-instruction-tuned models?", "488e9927-37a9-46bd-9176-9694c70f0c31": "What did Yuan et al. (2023) discover about GPT-4's arithmetic capabilities compared to the other 12 models they evaluated?", "59e2a5bd-0e68-479b-925d-d4eea6e39ea6": "What is the role of commonsense reasoning in enabling machines to interact similarly to humans?", "1135dc3f-064d-4cec-a4fe-3ce3ecba4019": "In what types of reasoning processes is commonsense cognition particularly important?", "8a070389-de6a-4bd3-ad9b-3f3433716c45": "What type of prior knowledge is required to answer questions in the CommonsenseQA dataset?", "c2cbf044-8eeb-4859-bd01-30c3d7571586": "What is the primary focus of the questions in the OpenBookQA dataset?", "f99a9713-cc13-49a2-8a75-be44e1335bfd": "According to Bang et al. (2023), on which types of benchmarks does ChatGPT demonstrate commonsense reasoning capability?", "b2c8c8d2-c2b8-46ed-957e-887c57eb48e4": "How do instruction tuning models improve commonsense ability according to Bian et al. (2024)?", "dabbe08b-36e7-4ad2-a4a5-90cf132638c4": "What is the primary purpose of multi-hop reasoning tasks in evaluating large language models (LLMs)?", "68664d87-4b94-4ca7-81b0-5f088b9192e0": "How does the example question about the director of \"Interstellar\" illustrate the multi-hop reasoning process?", "de1293a5-895e-4c8d-82d3-ece90b470e43": "What type of reasoning does StrategyQA require models to perform in order to answer questions?", "a5c49788-5821-4756-b41c-40a9de048900": "How does HotpotQA differ from HoVer in terms of the sources models must use to answer questions?", "8ff197bd-5677-41d9-ab08-6424419cfd0c": "What did Zheng et al. (2023b) identify as the primary reason for ChatGPT's failure to provide reliable answers on HotpotQA?", "c913fe69-e88a-4fd1-a37d-79a6922d40ec": "According to Zheng et al. (2023b), why is knowledge memorization and recall important for large language models?", "2588afab-f37a-49ee-9fce-20584b157756": "What are some examples of structured data formats mentioned in the context?", "e08d4556-c7b0-4cce-834a-b68f41252ad5": "Why is structured data reasoning considered more challenging than reasoning with plain text data?", "1fc46ee4-3d82-4cf8-8d31-2f91a6804646": "What types of information does HybridQA require models to aggregate in order to generate answers?", "88c79617-dd16-4046-8d4e-a8912ffab8bd": "How does MetaQA challenge models in terms of reasoning and entity matching within its knowledge graph?", "774c2daf-4b60-45db-9374-03d75600b73c": "What is the primary task required by models in the Spider Realistic dataset?", "718bd332-bbbe-4763-8b23-fd6d60cb6ab6": "How must models handle textual references in the Spider Realistic dataset to generate accurate SQL queries?", "0986ec96-0593-494e-a32e-c772854774fd": "What methods did Gao et al., 2023 use to investigate the text-to-SQL task across multiple LLMs?", "3f8d6208-5266-45e0-ab92-7b295a24c5a0": "How did the performance of fine-tuned open-source models compare to proprietary models in Gao et al.'s study?", "c3531bd8-4d6b-4558-a910-d013c4c9794c": "What are the two critical aspects of LLMs discussed in relation to their societal impacts?", "1289347e-b104-4673-9ad7-b6b7f6d3974b": "Why is it important to understand the implications of LLMs in modern society?", "c56f698d-7fb4-4c2d-a068-4a0664280574": "What are the dimensions being explored to understand the societal implications of LLMs?", "c756c50d-913b-499d-a031-df63a895ae82": "How do these dimensions help in understanding the broader societal implications of LLMs?", "3ae8e29d-b77a-4716-9a66-ae4f45a59086": "What are the three main areas of safety concerns addressed in the context of LLMs?", "ff242757-435a-4912-8f95-bc2dfa613456": "Why is expert verification important when using models like ChatGPT according to Oviedo-Trespalacios et al. (2023)?", "76d6a3b3-5b66-4492-8bd8-0fe06a8ae060": "What unique challenges does ToxicChat highlight in detecting toxicity within user-AI conversations?", "d4b78bc0-0c02-45e4-b1b0-46ee1407f980": "How does the Open AI Moderation Dataset contribute to identifying undesired content in real-world applications?", "8487a799-384f-481c-9881-637e8517f77e": "What is the primary focus of the AI Safety Benchmark v0.5 created by the MLCommons AI Safety Working Group?", "1ce0133a-dee0-46e2-9a04-d11fa5c4e423": "How many human-LLM interaction instances are annotated in the AEGISSAFETYDATASET by Ghosh et al. (2024)?", "cb531d92-2d80-4989-bbfb-3d54cdc13c23": "What is the primary purpose of the CValues benchmark introduced by Xu et al. (2023b)?", "b416db1e-fb02-4156-add6-1d3fb0228466": "How does the KCDD dataset categorize its dialogues, and what standards does it align with?", "08a75d4e-5e38-425f-a657-9cce809499bd": "How do benchmarks and datasets contribute to evaluating the safety alignment of LLMs?", "340711f2-12f4-4c65-8b42-a5b25c56e3d2": "What types of content must LLMs avoid generating to ensure safety, especially concerning minors and illegal activities?", "d4c4fbe5-ee74-4543-a4e6-c5f9f0a68aaa": "What privacy risks are highlighted by Staab et al. (2023) regarding the inference capabilities of LLMs from innocuous text inputs?", "776c2c96-b70e-420e-8fae-30cab93c95bf": "How does ProPILE, introduced by Kim et al. (2024b), help in detecting potential PII leakage in LLM-based services?", "79265b5a-9f86-48dd-8613-a8723fe05ccd": "What specific vulnerabilities do Das et al. (2024) highlight in their examination of LLMs?", "1a8200b8-8aff-4450-bb74-91f25e2ecafa": "How do Carlini et al. (2023) and Yao et al. (2024) describe the privacy risks associated with LLMs?", "d0685267-f2d0-4d57-b652-6b2dc5d7263b": "What framework do Yip et al. (2024) introduce to quantify the resilience of applications against prompt inject attacks?", "68e5fbb1-16ee-4bb4-828a-7b4da60483ca": "How do Liu et al. (2024b) and Jin et al. (2024) propose to enhance the evaluation of adversarial resilience in large language models?", "a9ec91fe-8ecc-4155-a018-49ccb74b621f": "What techniques does RigorLLM employ to enhance the moderation of harmful content and improve resilience against adversarial attacks?", "ecd647bb-0a96-4c67-b992-bd3b511b93d3": "What is the primary focus of the InjecAgent benchmark in evaluating tool-integrated LLM agents?", "a094a7cd-e6ba-49e2-8e34-a8e3d96db697": "What challenges are associated with ethical evaluation in sensitive areas like medical ethics and moral decision-making according to the context?", "fb3952df-91a1-4eb7-a4fe-2dffd694d914": "How do the studies mentioned contribute to the ethical functioning and responsible application of LLMs in real-world settings?", "3f973a4e-79ff-4840-9bd8-2735ce78a8f4": "What is the purpose of the ValuePrism dataset introduced by Sorensen et al. (2024)?", "a5e6dfa8-e5f7-454f-b9a9-9860a609b0e5": "How does the Value Kaleidoscope (Kaleido) model compare to GPT-4 in terms of explaining and assessing human values?", "15c57a23-284e-4ad2-af57-d7be5165050a": "What is the purpose of the DeNEVIL algorithm proposed by Duan et al. (2024) in the context of LLMs' ethical values?", "080de8c7-12b4-4bb7-8ebe-e2444d3ddf34": "How does the VILMO method contribute to improving value compliance in language models according to Duan et al. (2024)?", "7bca9f57-abec-4037-8d7e-f7c99fc2cce5": "What are the two possible responses presented in the moral dilemma involving a driver approaching a pedestrian crossing the street in the MoralChoice datasets?", "bd259dc8-768f-483a-accf-83bed92c9c44": "How does the scenario in the MoralChoice datasets illustrate the evaluation of ethical behavior in realistic settings?", "2c97dc3b-7470-4242-afaf-1cf83b53ff6e": "What novel approach did Scherrer et al. (2023) introduce to study the moral beliefs of large language models (LLMs)?", "e8f95f28-3473-40c5-83c8-143bb76aac1a": "How do LLMs' decision-making tendencies differ between high-ambiguity and low-ambiguity moral dilemmas according to Scherrer et al. (2023)?", "6c8b172b-6d80-470e-b675-845b1b504cdd": "How do Chain-of-Thought (CoT) explanations affect the perceived reasoning behind a model\u2019s predictions according to Turpin et al. (2023)?", "fe34eec0-4319-4cc6-85dc-93aab92d811e": "What challenge does Khan et al. (2024) highlight regarding the evaluation of increasingly complex LLMs?", "6a2a7685-4c9e-4a34-872c-22cc059c65b4": "What evaluation strategies are researchers implementing to ensure the reliability of LLM outputs?", "5123a6d6-113a-4af0-b4c4-f8f6ce6a742d": "Why do hallucinations in LLMs pose challenges to their trustworthiness?", "78d897db-d329-4a87-b09b-bd23a05a7a5a": "What are some of the benchmarks developed for effective hallucination detection mentioned in the context?", "2d304397-960e-4544-b3a5-575085aa0fc6": "How does the FEWL method measure hallucinations without relying on gold-standard answers?", "1c7e3bc2-8163-49f1-ac40-76e02e081ec8": "What types of tasks does the AMBER benchmark evaluate?", "cb253d76-2360-4b49-ba23-5b2ef736452a": "How does the AMBER benchmark assist in mitigating hallucinations in mainstream MLLMs?", "2d9fad43-ddd0-4f83-b237-0f10f505ad6d": "How does the method proposed by Feldman et al. (2023) help improve the accuracy of information provided by LLMs?", "b91eced4-6bb3-403e-b518-5cf667d5fa2e": "What role does accompanying context play in reducing hallucinations according to Feldman et al. (2023)?", "7cee07a4-9122-4049-a612-f9e41f45d6da": "What is the main purpose of the self-check approach introduced by Yang et al. (2023) for LLMs?", "1bb34796-e29e-4e24-b851-46b6980c3b5c": "How does the PHD benchmark improve the evaluation of hallucination detection methods compared to existing approaches?", "b9ef00e2-32c0-4efc-a178-272bb8e46135": "What is the main focus of studies addressing bias in the evaluation and operation of LLMs?", "1744f3e8-f0a0-48f2-840e-77a11b0a420a": "Why is it important to diminish biases in large language models?", "91549689-758b-4885-9375-ad9a33f88918": "What is the purpose of the BBQ benchmark created by Parrish et al. (2021)?", "aea70b8b-cf08-4f9b-9542-1916d1e26e06": "How does the BIAS benchmark by Vermetten et al. (2022) detect structural bias?", "cfaf72cf-f0ca-46f6-9bf7-bd79c5e8408b": "What dimensions does MERS Wu and Aji (2023) assess in machine-generated text?", "bba2542e-fbb2-4ed9-aca1-da7cbd2a62a6": "How does MERS Wu and Aji (2023) aim to reduce biases in LLM evaluations?", "aa9690fc-57bf-45cf-9790-e92b1d1c06e6": "What is the \"look-ahead benchmark bias\" identified by Daniel et al. (2008) in the financial sector?", "c93488c7-a1ec-4f8b-b790-841576835554": "According to Hort et al. (2021), what challenges do current ML bias mitigation methods face?", "e8f1c227-6f26-40aa-8ef1-b756c1ad08de": "What is the purpose of the Media Bias Identification Benchmark (MBIB) introduced by Wessel et al. (2023)?", "30b446d3-4eb0-4514-a0dd-f153c4960e2d": "How does the MBIB framework improve the detection and evaluation of media biases?", "3a525226-6fb3-478c-bd8e-8906798b8ca8": "What are the five domains explored to evaluate the knowledge and applications of LLMs?", "c0625577-ade7-473c-852d-80f1a9320353": "How do experts utilize LLMs to enhance their capabilities in specific tasks within various domains?", "0d90cecf-2ce2-410b-8414-c66d5793a4b7": "What are some examples of language models specifically designed for financial use mentioned in the context?", "0d8c9001-24cb-4969-b36b-2e9b363be559": "How was BloombergGPT evaluated according to Wu et al. (2023)?", "5ac5e06d-8688-492b-ae45-2b39e5e51c3d": "What datasets and benchmarks were used to evaluate BloombergGPT's performance in aspect-specific sentiment analysis and general purpose tasks?", "5bb7546e-2f18-4dfc-a2dd-4c41b77f6685": "What is PIXIU, and how does it support the fine-tuning of financial large language models like LLaMA?", "fcd7cdf2-af2e-4d19-a56c-5e97c1eda54c": "What are the two major challenges identified by Li et al. (2023b) regarding the use of LLMs in the financial domain?", "e38d4782-20a8-4df9-8903-996e0c5e1f6c": "According to Lee et al. (2024), what is the primary challenge in evaluating LLMs for financial NLP tasks?", "5eaf0afb-5689-4a17-96d8-f123d3706382": "What are the key components covered under section 2.3.2 Legislation?", "9a789465-833f-4d7a-bb06-41ce5a395b80": "How does section 2.3.2 address the implementation of relevant laws and regulations?", "158a198d-4f9d-4b06-a588-5c77d651dab9": "How did GPT-4 perform on the uniform bar examination according to Katz et al. (2024)?", "335faf2b-8087-43e5-b37f-424514bb7297": "What was the accuracy of GPT-3 in statutory reasoning on the SARA dataset in a zero-shot condition as found by Blair-Stanek et al. (2023)?", "d1a0e17f-a0a9-4c32-ac17-7f16ebfe26e1": "How did Chat 3.5 Turbo's interpretation of the statutory term \"vehicle\" compare to the responses of 2,800 English speakers according to Engel and Mcadams (2024)?", "3941d085-7d21-425c-90d3-88f69587b8b4": "What distinctions was GPT-3 able to recognize in legal rules based on the findings of Liga and Robaldo (2023) using LegalDocML and LegalRuleML datasets?", "f7f58d87-195b-46a4-9220-77f92090b60b": "What is the main reason pre-trained LLMs are not yet ready for fully automatic deployment in case judgement summarization?", "9081af7d-e954-419e-b5ca-14c9448f0153": "What issue has been identified in the abstractive summaries generated by pre-trained LLMs according to Deroy et al. (2023)?", "634f2932-248f-4175-9e58-893962f71ca4": "What are the main topics covered under the section 2.3.3 Psychology?", "b79ef8aa-0d00-48e2-8d13-0c191de6d48c": "How does section 2.3.3 Psychology contribute to the overall understanding of the subject?", "0a2d076d-2ef8-4667-a5f3-a3635ea0c7e3": "What psychological tasks have LLMs been tested on according to Rathje et al. (2023)?", "f38da9cb-8ea3-4bbf-b8e5-ff8904722ef8": "How did LLMs perform compared to existing English-language dictionary analysis in detecting psychological constructs?", "8f438a68-fb8f-4b2a-ac63-d1686deca389": "What datasets were used by Lu et al. (2024) to evaluate GPT-4V\u2019s performance in affective computing tasks?", "7cef151c-eed4-4f66-9cad-1fff9936b7bd": "In which affective computing tasks did GPT-4V perform satisfactorily, and in which did it fail according to Lu et al. (2024)?", "256728c3-d715-49b3-ac43-d231f88c6ab1": "What are the two methods proposed by Demszky et al. (2023) to evaluate the effects of features on human thought and behaviour?", "b223c5b2-f231-470b-8c0f-e6ae7fd2acb0": "How did Karinshak et al. (2023) use impact evaluation in their study involving GPT-3-generated messages?", "a6560213-55e2-4d80-b6bc-4f9c0a8711f5": "How can expert evaluation be used as a manipulation check or measure of construct validity in assessing the capability of LLMs for psychological tasks?", "c937e005-c042-4f3d-b542-8883cfd40cc3": "In what way might text aligned with expert evaluations be utilized in an impact evaluation study to measure the intended effects on third-party participants?", "f6d43989-6fa3-4513-9bad-576f3d595863": "What topics are covered under section 2.3.4 Medicine?", "0474aa6a-dfff-4ceb-bd1f-2b5617f36dab": "How does section 2.3.4 Medicine contribute to the overall subject it is part of?", "177c8847-7d56-48b3-bef1-af323957be5d": "How did ChatGPT perform on the United States Medical Licensing Exam (USMLE) according to Kung et al. (2023)?", "70996d17-3084-4cbf-b503-d52f6d129787": "What applications of large language models (LLMs) in the medical field have been explored in recent research?", "289ad596-6154-420e-8903-cbd19b48b313": "What diagnostic and triage accuracy rates did ChatGPT achieve when tested with 45 standardized vignettes, and how did these compare to physicians' performance?", "54433f32-17c6-4bf0-9267-01bc7234fcc6": "Why are current large language models like ChatGPT unable to meet ICMJE authorship criteria when writing academic clinical papers?", "75274291-5698-4ae8-b261-ebc72d443681": "What are the current limitations of deploying LLM applications in the medical field according to the context?", "918260c2-fa18-477f-a74d-b72852aa847a": "Who remains responsible for delivering optimal knowledge and care despite the use of LLM applications?", "7360f4fd-23c4-4165-b400-203659dccfa6": "What topics are covered under section 2.3.5 Education?", "7cea5988-4563-4d33-9325-c49660b9e767": "How does section 2.3.5 address the role of education?", "4d2fae0e-c901-49f8-bd0f-42403199eda1": "What are the two general categories of current evaluation methods for LLMs in the education field?", "ebd4b5f3-9b59-45ab-86f8-774f42c33322": "How did Abdelghani et al. (2023) utilize GPT-3 in their study related to education?", "b3ca46d7-321f-47d9-9b23-f0642d4ec6ce": "How did Menick et al. (2022) evaluate their Self-Supported Question Answering model using paid contractors?", "cd1dcfda-0c4d-4626-a782-665cdc9be51d": "What approach did Dijkstra et al. (2022) propose for generating quizzes automatically?", "8eaa32a2-8b89-4937-9fa9-9ba3aa4bb142": "What metrics were used to compare prediction and ground truth instances in the GPT-3 model evaluation?", "20f39bdd-9430-411e-bf79-d0da8963d7f1": "Which dataset did Raina and Gales (2022) use to train their deep learning model?", "71c23d54-4519-4ef3-9269-0cd5e0a1d92d": "What are some of the key benefits of integrating large language models (LLMs) into the educational area according to Kasneci et al. (2023)?", "04cfd980-6ca4-4070-b17d-fd35fcde62f5": "What strict requirements must be considered when integrating LLMs into education, as highlighted by Kasneci et al. (2023)?", "d740bf18-2506-4b61-b5b4-e76d968af4b1": "What criteria are used in the evaluation of the three agents described in the context?", "093a4a3a-dbcb-4626-b6bc-1b5ccbe835d8": "How does the forest environment impact the performance of the agents during their evaluation?", "ee70a9df-9739-46d9-ad66-86a7c3b7599d": "What are the main sections and subsections outlined in the tree structure related to agent evaluation and application scenarios?", "6515dfe0-47be-403e-b2ae-8ad85517b468": "Which specific studies are referenced under the Planning subsection within Agent Evaluation?", "bcc96150-9c93-4103-952a-40f7a20646c4": "Which studies are associated with API Calls according to the provided context?", "b55b35dd-e357-4a7b-bf51-0c00fcbf1763": "Who are the authors mentioned in relation to Robotic Manipulation?", "567e0a56-5459-48c6-8ab5-d7ca35da516e": "What methods are used to assess the planning capabilities of LLM agents?", "544dbbdc-cfca-4374-a38f-2d15df9cada4": "How are LLM agents evaluated across different application scenarios?", "93d0712d-574e-489e-972b-47a25e674984": "What does planning by an agent involve according to the given context?", "f7a22cd1-113a-4f4c-b88b-23d541a03ebc": "How do agents typically decide the best course of action in planning?", "e214c7c2-1222-41a2-b4aa-e78a2c5aec4b": "How do large language models (LLMs) contribute to robot planning in executing complex tasks?", "866abe49-0ac3-4910-bda8-371a00914190": "What role does the Inner Monologue system play in dynamic planning for robotic tasks?", "8ea04020-83bb-49bf-b13f-7a91ad212525": "How does SayPlan enhance the task planning capabilities of large language models (LLMs)?", "1ec70427-6c95-4b48-9097-d73ac2a083f8": "What dynamic elements do DEPS, AdaPlanner, and Robots That Ask For Help introduce to improve robotics in real-world settings?", "9d907393-5a7d-4830-8464-6f6e5fc99b98": "How are robotics being evaluated to ensure adaptability in real-world settings?", "8bd9add6-a51a-468f-bafb-454fcaeae457": "What does the advancement in robotics towards more intelligent systems signify for their application in complex situations?", "75e453d7-bd6a-4680-94f8-d7eb73e2c8bc": "What is the main advantage of using LLM-based agents in decision-making and strategic planning?", "8fa55992-0293-4fac-8260-1bd5f6d5472c": "How does the Reflexion framework enable language agents to improve their skills without updating model weights?", "7fd2b6df-5097-4915-8dc1-0e144bce3a71": "What are some of the environments where the evaluated method shows marked enhancements over traditional approaches?", "28760634-7038-4d54-9bc4-b548ee27d1f1": "How does the SelfCheck mechanism improve the accuracy of LLMs in multi-step math problem-solving tasks?", "740d551c-0916-4c4a-80a2-41dcdf5c4613": "What are the three main approaches used to evaluate WebGPT models in the text-based web-browsing environment?", "2dee9fc8-53d3-4a28-b304-25865f0363cc": "How does WebGPT enable interaction with language models to generate more faithful outputs in web environments?", "1e0282c9-1033-45d3-9918-e3c3424115df": "What are the four sub-tasks used to evaluate WebCPM Qin et al. (2023a) in answering long-form questions through web searches?", "f2ccf3ae-8a94-40c3-9e08-b672fd786110": "How is the holistic evaluation of WebCPM conducted according to the provided context?", "c9dcc27d-49d9-4d2e-9c0f-b4cae5ac2451": "What is the size and composition of the dataset introduced by WebShop Yao et al. (2023a) for evaluating LLM-based agents in product search and retrieval?", "f74186e1-1b4c-4d66-9ebb-8bce6605e743": "According to the evaluation metrics in WebShop Yao et al. (2023a), how do human performance and LLM performance compare in the product search and retrieval tasks?", "b1909c26-6f21-4229-a747-b54133828c25": "What is the purpose of using the Code as Policies paradigm in robot control according to the context?", "0bf8aed5-b0b3-403e-9858-e92109aee9c6": "How does the RoboCodeGen benchmark evaluate the quality of generated code?", "74a6d87c-bbfc-4539-b4ca-9bcd53a8fdbf": "What is the primary purpose of the CODEAGENTBENCH benchmark introduced by Zhang et al. (2024a)?", "40cf331c-6f02-4dd2-b37e-39e33742fdb2": "What types of input information does the CODEAGENTBENCH benchmark provide to challenge LLMs in code generation tasks?", "a57d6a5e-2b7a-45bc-8c91-077f5df75ba4": "How does ChatDB utilize SQL statements to improve the realism of agent actions?", "9df9616c-6bea-45bd-969f-ff45234491d0": "What evidence is provided to show that ChatDB outperforms ChatGPT in database query tasks?", "0b797f9e-2b7c-4764-968f-3bc87c5a9f2c": "What is the purpose of the API-Bank benchmark introduced by Li et al. (2023a) in evaluating tool-augmented LLM performance?", "9b2e95bc-6bf8-4f0a-9af4-46317f34f9ef": "Which evaluation metrics are used in the API-Bank benchmark to assess the effectiveness of API calls and task planning?", "e4e115e8-ff6f-48c7-8da3-c21b96467c0a": "What methodology did Qin et al. (2023b) use to evaluate the effectiveness of tool learning in contemporary Language Models?", "c2627e56-b09e-4a3b-96ae-3b456664b411": "Which additional tasks did Qin et al. (2023b) include in their extended study beyond the initial six tasks?", "dbcf62f6-a5b6-4a7e-9918-bc2c26828e61": "What types of evaluations are included in the Berkeley Function-Calling Leaderboard (BFCL)?", "8316595a-749c-4020-8ead-cbd70f04eb04": "How does the BFCL support the integration of large language models into platforms such as Langchain and AutoGPT?", "4e5bbe4c-b950-4ebc-bcd2-e432921ce48c": "What is the dependency for the usage of tools according to Schick et al. (2023)?", "b35a8f31-ef19-4f8b-a12c-1cd7b37f29d9": "How have recent efforts utilized LLMs in the context of tool creation as mentioned by Ruan et al. (2023)?", "2d0d24f3-fd00-44ad-91e7-dcf0669f6e18": "What types of datasets does LATM Cai et al. (2024) use to evaluate their models?", "3e53ad48-70a1-46f7-ad63-0fd2a0cc2e7b": "How does LATM Cai et al. (2024) demonstrate the real-world utility of their model?", "53ff8a47-08f0-45a5-928a-369801d7e045": "What dataset does Qian et al. (2023) use to evaluate LLMs' ability to create tools?", "a6e2f8bc-cf27-44d8-ab5c-2f9a3784bf95": "How does providing additional hints affect ChatGPT\u2019s tool-making performance according to the evaluation?", "364d8764-a16e-4f67-8498-8903efb47ac9": "What are the key components involved in robotic navigation by an embodied agent?", "44ce44dc-061e-4e06-a79f-7ce59dc572a7": "How do sensors and algorithms contribute to the navigation process of a robotic or virtual entity?", "32fb0a5d-31cc-4350-a38b-e169b8a81e87": "What components does the LM-Nav system proposed by Shah et al. (2022) utilize to enable robotic navigation using natural language instructions?", "4c4f410f-434c-4321-90e4-b91cd050593d": "How does the approach by LFG Shah et al. (2023) improve robotic navigation in unfamiliar environments?", "586f33e8-28fa-407d-8c08-d6f28ba1069c": "How does NavGPT utilize large language models (LLMs) to improve navigation tasks?", "13c35e57-3043-4223-8a22-2d08482e57da": "What role do schema-based instructions play in the NaviLLM model for embodied navigation?", "7c47b8b7-ca21-489c-8965-dea51cc9c623": "What is the role of an embodied agent in robotic manipulation?", "8858acb6-5c05-4218-a1fd-21a527c8ca85": "What types of tasks can robotic manipulation enable?", "c0812d2f-6b02-436f-8ffc-a99920f06b8a": "What is the key novelty of VoxPoser Huang et al. (2023) in using LLMs for robotic tasks?", "4ccb0660-e4b7-47e2-85a7-3c28cba6046d": "How does L2R Yu et al. (2023) utilize LLMs to improve robot task execution?", "fb45ab41-7416-46cd-bc5c-11f86df9370d": "What types of tasks are involved in complex locomotion and manipulation in simulated environments?", "33b8c86e-a873-43f1-9001-6a609ec274a1": "How are simulated environments used to study complex locomotion and manipulation tasks?", "c9fd5148-2c5e-4ba6-b6db-253ac39cb646": "What are the key features of the APIBench evaluation system described by Patil et al., 2023?", "b7a979af-d87c-4977-a78f-6904c47af111": "How does ToolEval, constructed by Qin et al., 2023c, differ in scale and content from ToolAlpaca?", "b308bb1b-c5d1-487c-9527-7e0e230d0385": "What are the two real-world scenarios included in the RestBench dataset, and how many APIs do they each contain?", "2d0e28db-91b7-4796-81bd-d42b59b4a2bf": "How many long-horizon tasks are featured in the WebArena environment, and what types of web applications does it include?", "5c9cc342-c6cb-4e59-9ea1-38e8175c0d0d": "What is the scope of tasks and domains covered by MIND2WEB as described by Deng et al., 2023?", "d4efd5bc-1364-4cf4-9aa3-176cebc9e6f3": "How does MIND2WEB facilitate the creation of agents capable of handling complex web interactions?", "d53d3398-3f8c-4c10-9a37-e01e006e83ae": "What is the primary focus when evaluating LLMs\u2019 capability on tool manipulation according to the context?", "13fe8689-8802-4f44-a752-614f87c1c3c1": "Why are researchers increasingly interested in evaluating LLMs using multiple tools instead of a single tool?", "471a20f5-de5d-4a08-a951-644615e85580": "What sources does APIBench use to assemble its comprehensive API corpus?", "601204b7-95d1-4ebc-b8d4-b253bb281838": "How does ToolBench evaluate large language models\u2019 generalization and reasoning skills?", "ae782217-7305-40d6-843d-41e10b17b252": "What are the two metrics used by ToolEval to evaluate performance?", "d5c7e143-56b1-4dec-9d37-edf4dbde2fa2": "How does ToolEval reduce potential human biases and unfairness in its evaluation process?", "2dabdd33-5b0e-4d7a-b16f-045b010d5877": "What methodology does ToolAlpaca Tang et al. (2023) use to evaluate new tools in real-world scenarios?", "3dfac66b-44bb-409d-be0c-80d942cccec3": "How do human reviewers contribute to the evaluation process in the ToolAlpaca study?", "8b24aca0-ec35-4b64-a86b-74fc4d232d92": "What are the two main scenarios explored by RestBench Song et al. (2023b) in their study of real-world user instructions using APIs?", "989047e8-b548-46c4-8e8d-7952dc1b2b7f": "How does RestBench evaluate the performance of RestGPT in handling complex tasks?", "726d9606-accc-43d4-8d33-51c9647c69fb": "What are the four common domains covered by the WebArena environment?", "b3ad944e-1bfd-4b1a-81b9-38ef6b4c803f": "What is the primary purpose of the WebArena environment?", "404e2320-b833-4fbd-93a5-ce39c834d81f": "What is the primary purpose of the MIND2WEB dataset as described by Deng et al. (2023)?", "ffdb4bbb-8982-455c-9e8e-41e601db37fb": "How does MIND2WEB differ from other datasets in terms of the environments it uses for developing generalist agents?", "dc177dac-d77d-4b08-a043-745af077508c": "What challenges are associated with current evaluation methodologies for LLMs as their capabilities expand?", "27f2ea5e-3552-4a45-80bc-ba55dfc81df4": "What is the purpose of proposing five future directions for developing evaluation methods for LLMs?", "0ec16069-b401-4f35-ac41-e7d76b1fd9e7": "How can LLMs be made a more \"useful\" presence in the eyes of the public?", "9080e99a-4724-4504-a934-cb216f3509a7": "What factors influence the public perception of LLMs as useful tools?", "e36b10c8-f31a-4be1-a7d6-ee5800732e44": "Why can static benchmarks present problems when used for evaluating large language models (LLMs)?", "f8525da1-479e-49d6-9cf4-5211c518d05b": "How does the changing nature of factual knowledge, such as the presidency, impact the evaluation of LLMs?", "d21eeb74-e0f2-44ae-8c14-4c0011661a32": "Why is it important for evaluation questions within datasets to be automatically replaced and updated as LLM models expand?", "8f9b35fc-ba72-455c-b551-37f37f0d1257": "How does the framework proposed by Wang et al. (2024c) contribute to maintaining the effectiveness of benchmarks for evaluating LLMs?", "8ead70a8-48e5-4880-a02c-4fd4a2bddbc8": "What are the advantages of using LLMs as evaluators compared to human annotators in labeling datasets?", "4a564750-e506-4fa6-9035-69573e7bde6c": "What potential issues have Li et al. (2024b) identified when using LLMs as scorers?", "b3bd4e00-dbe7-465a-af3e-e0e0b7da2f0a": "What biases are mentioned as being inherent in LLMs when used as evaluators?", "cbadb57d-ba6e-49d5-b9d2-25c14076f9a2": "How could addressing these biases impact the development and self-assessment of LLM applications?", "d11d86b9-e0a6-4c17-aa69-e4912ba398dd": "What is the purpose of Root Cause Analysis in problem-solving?", "165b1687-6e01-489c-a982-0875c402c585": "What are the common steps involved in conducting a Root Cause Analysis?", "04e143d3-d9d0-44dd-9323-eb303cbca778": "What is the primary basis for the evaluation methods mentioned for assessing LLMs?", "755c99fb-6837-4514-be78-1c0bf20393a7": "Why is it difficult to determine the root cause of a model\u2019s response by only examining its output?", "53e2fdd1-3363-465c-ae70-683eab2b8e87": "What challenge is mentioned regarding understanding why a model made an error?", "bb2a7f2a-5c49-49e9-a791-00bf1b7de9ec": "What do the authors propose to include in future evaluation methods for LLMs?", "173a9a00-e617-4f2d-8622-3308910f9f6c": "What are the limitations of existing benchmarks in evaluating LLM agents as mentioned in the context?", "dfb7f8c9-f6b8-4ce6-93e9-58c4969ee645": "How might high-intelligence models like LLM contribute to the development of more realistic evaluation environments?", "d12561dc-065e-465f-858e-559c910891af": "What role do simulation environments play in improving the generalization capabilities of robots according to recent research?", "5de4d646-c6a7-474d-ac7b-b96ad48eb2c6": "Why is there a need to develop large-scale benchmarks similar to ImageNet in the field of robotics?", "e4a0b935-9fa9-4cc6-b57c-aec87e0e4805": "How can digital twins help reduce the sim-2-real gap in robotics?", "e7f6f68d-0e88-4072-a05f-145e958035da": "Why is addressing out-of-domain data important for evaluating computer vision models in robotics?", "742fbb63-c789-4f05-8f62-88003a1b9ef8": "What aspects are considered critical for effectively deploying robots in real-world scenarios according to the context?", "5148fe56-5021-4a8b-85d2-4caf6c13cb8a": "Why is evaluating robot foundational models like RT-2 and PaLM-E important for advancing robotics?", "19cb2574-fefb-46fd-b982-bfa2330ab7a6": "What is the purpose of using various evaluation methods for LLMs according to the study?", "bd12c39b-e8fd-4a4b-8763-81159fd2b0fb": "What are the proposed directions for advancing LLM evaluation methods mentioned in the context?", "ff527326-bdbf-416a-a9f8-f2dfe167b658": "How can future research improve the usefulness of LLMs in aiding human society?", "4f4a4f08-0465-433c-ad02-f7c21c2f09b1": "What are the potential benefits of making LLMs more effective tools for human society?", "d0de0f78-5185-4f37-b945-78e4df7309ab": "What is the main focus of the study conducted by Abdelghani et al. (2023) as mentioned in the references?", "616e99a6-6a9f-4fa6-8f86-6bf6fd41c02e": "Which publication contains the technical report authored by Achiam et al. (2023) on GPT-4?", "c99c3b5f-b6a6-4cdc-ba86-1ac6ca1e4eb4": "What is the main focus of the research conducted by Agrawal et al. (2022) as mentioned in the context?", "658d15a8-35b0-4cb7-8246-47d63732acd7": "Which application domain is addressed by Alvarado et al. (2015) in their work on named entity recognition?", "1e7a1c44-1a5e-4fcd-b83a-77123bde3ade": "What is the focus of the paper by Balas et al. (2024) regarding AI large language models?", "389b8cd6-6265-4313-8a98-992df404269c": "What contribution did Banerjee and Lavie (2005) make to machine translation evaluation?", "8d87da82-9abb-4369-af65-997ec4cfcb52": "What are the main evaluation aspects covered in Bang et al. (2023) regarding ChatGPT?", "8b62230f-91bd-4ac8-a7f4-1eb4b38fcdcb": "How does Benoit (2023) utilize ChatGPT in the context of clinical vignette generation and evaluation?", "46ac4d01-4680-454b-96f6-cf7c02d6b6d8": "What is the main focus of the research conducted by Bhagavatula et al. (2019)?", "f33ea0dc-d5e6-48ae-9313-c1609df05e41": "According to Bian et al. (2024), how is ChatGPT characterized in terms of its commonsense problem-solving abilities?", "430174e7-5a13-4506-b82a-3925702f0d0c": "What topics are addressed by Blair-Stanek et al. (2023) in their publication at the Nineteenth International Conference on Artificial Intelligence and Law?", "0f6597ce-48c3-4aef-8a95-7826369eb09f": "How do Carlini et al. (2023) contribute to the understanding of neural language models in their work?", "5f704986-e0af-435c-a0c6-71efd7e7ccc2": "What are the main research trends and challenges discussed by Casino et al. (2022) in the field of digital forensics?", "788c0cdf-7d5a-479d-8898-d42463f9546a": "How do Chang et al. (2023) approach the evaluation of large language models in their survey?", "dd029f8e-6f23-4cb6-b5ef-f299db13aab2": "What is the focus of the HybridQA dataset mentioned in the context?", "8bf00b5c-44f8-47cc-a322-10738c96c32e": "Which study explores numerical reasoning in conversational finance question answering?", "78075cd6-8c7b-4abc-8d63-d0ef94645ba7": "What is the main focus of the research conducted by Cobbe et al. (2021) as mentioned in the context?", "c8420a50-c6db-4b81-9ae8-c25f60dc0d77": "Which publication discusses the concept of \"look-ahead benchmark bias\" in portfolio performance evaluation?", "d5c1bd2f-a80d-4cd8-b426-7dcb0667cd3c": "What are the main security and privacy challenges associated with large language models as discussed by Badhan Chandra Das, M Hadi Amini, and Yanzhao Wu in their 2024 survey?", "6456fa39-7975-4360-9d6c-d6ddf672a83b": "How do Demszky et al. (2023) propose using large language models in the field of psychology according to their publication in Nature Reviews Psychology?", "be6ea7c0-9a4e-4cde-bf55-f1b873737178": "What is the focus of the paper titled \"Structure-grounded pretraining for text-to-sql\"?", "4e030707-3f3a-4f50-89e9-323023220aa1": "How do Dijkstra et al. (2022) utilize generative pre-trained transformers in their research?", "5f4262a0-2d7b-421b-a3ff-2f51833c1b4c": "What are the main findings of Du et al. (2014) regarding compound facial expressions of emotion?", "4eec5904-f05a-41e6-b4ac-10d0de45ff6d": "How does Duan et al. (2024) propose to address the ethical values of large language models through instruction learning?", "10d544e9-2a1a-4a5a-9f20-de3cf74b7a36": "What is the main focus of the paper by Feldman et al. (2023) mentioned in the context?", "a165f551-89c2-4fe7-8aa1-fd0387e5434f": "Which publication discusses long form question answering and who are its authors?", "75eadbeb-1f13-4d2e-8ee1-9f4e2e58159c": "What is the main focus of the paper titled \"Text-to-SQL empowered by large language models: A benchmark evaluation\"?", "5c612444-e868-4080-9abe-57485b2a9d90": "Which authors contributed to the 2021 work on a question answering benchmark involving implicit reasoning strategies?", "4b9445a1-f5ef-4932-ae0e-39a661300ddb": "What is the primary focus of the Aegis system mentioned in the context?", "ad8e0480-0fa0-48ea-887b-eaaa401c95e7": "Which publication provides a comprehensive survey on evaluating large language models?", "ee6006a8-ef60-41cd-aaa0-010454a48e8a": "What is the focus of the dataset introduced by Holzenberger et al. (2020) in their arXiv preprint arXiv:2005.05257?", "086f56be-4cd8-4405-8a5e-c10dd112f97d": "What approach do Hort et al. (2021) propose for benchmarking bias mitigation methods in their paper presented at the 29th ACM joint meeting?", "2f0f88ff-9f0b-4b73-9943-85978b3e00f1": "What is the main focus of the work titled \"Chatdb: Augmenting llms with databases as their symbolic memory\"?", "27ba1612-2d03-418b-a8c8-8d19520b1575": "How do Huang et al. (2022a) propose to use language models in the context of embodied agents?", "c5f79245-fdc3-4deb-b97d-290b68cf1329": "What is the main focus of the paper by Huang et al. (2022b) titled \"Inner monologue: Embodied reasoning through planning with language models\"?", "9eda981d-2da9-4183-a559-81c9bbc91e33": "What contribution does the work by Ji et al. (2023) titled \"Beavertails\" make towards the safety alignment of large language models?", "f92e7050-f57b-4ec3-b387-b59d74adb652": "What is the main focus of the research conducted by Jia et al. (2021) as described in the context?", "83428902-100c-4b75-80da-b6637db6a461": "How does the work of Jiang et al. (2024) contribute to the evaluation of large vision language models?", "376cb100-cf50-4629-9ce0-9ad380a26826": "What is the primary focus of the Hover dataset mentioned by Jin et al. (2024)?", "99a12c6e-b535-4812-8cdb-37f593491b89": "How do Karamizadeh et al. (2023) approach adult content image recognition in their study?", "ce28e924-6d12-4551-ad40-737f179cb402": "How do large language models demonstrate their ability to generate pro-vaccination messages according to Kasneci et al. (2023)?", "3a32f0a9-ba59-4055-b659-96f4072c316c": "What are the identified opportunities and challenges of using large language models like ChatGPT in education as discussed by Katz et al. (2024)?", "0fa1eabd-0113-4fc1-a0e8-0ace31885ff9": "What achievement is highlighted about GPT-4 in the provided context?", "9c8670fb-d554-426f-bc93-da518d0bd18b": "What is the focus of the research conducted by Kim et al. (2024a) as mentioned in the context?", "6876fb92-2658-4846-b2ab-697c83e9c2fa": "What is the main focus of the study conducted by Kim et al. (2024b) on large language models?", "c0aa4427-fea2-465c-b5d1-f989274144f3": "How does Kosinski (2023) contribute to the understanding of theory of mind in large language models?", "6355a28d-2197-426e-8655-a61386d5f31b": "What was the focus of the study conducted by Kung et al. (2023) regarding ChatGPT?", "446a7d6b-209e-4d15-a067-22e1895c920e": "What contribution did Kwiatkowski et al. (2019) make to the field of question answering research?", "69d1deff-2013-433c-8de6-7a1670429685": "What are the main topics covered by Lee et al. (2024) in their survey of large language models in finance?", "6c6ccc85-6a96-44d3-84d6-03a82faa7b2c": "How do the benchmarks introduced by Li et al. (2024a) and Li et al. (2023a) differ in their focus and application for large language models?", "dceaf995-9885-461a-97e4-6c0c1fd3c138": "What are the main applications of large language models in finance as discussed in the survey presented at the Fourth ACM International Conference on AI in Finance?", "48d6f7d3-26ac-4c49-936f-9d7a7de6190e": "How do Liang et al. (2023) propose using language models for embodied control in robotics according to their paper at the IEEE International Conference on Robotics and Automation?", "2273b5e9-88f0-4c57-a2bc-9cd62cd6f4cb": "What is the main focus of the study conducted by Liang et al. (2022) as mentioned in the context?", "601c871b-9fb1-4c05-a50d-cdcf9172379e": "Which dataset did Liang et al. (2019) introduce, and for what purpose?", "d76d70f5-a376-48fe-9a32-895e6c83f139": "What is the main focus of Chin-Yew Lin's 2004 work on Rouge in the context of text summarization?", "bf4f0887-b49f-41cd-a71c-64af97a19dad": "How do Lin et al. (2023) address the challenges of toxicity detection in user-AI conversations in their study \"Toxicchat\"?", "96ad2744-111b-4fc7-86b4-5b3f057b0b3c": "What are the main topics addressed in the works by Liu et al. from 2021a, 2023, and 2024b as mentioned in the context?", "3da29286-6d4d-4e43-9ba4-da17ba5facac": "How do the studies by Liu et al. contribute to the understanding and evaluation of large language models and video-based emotion analysis?", "0b504667-87a0-4366-ae77-5bd2f5eb28b7": "What is the main focus of the paper by Liu et al. (2021b) presented at the twenty-ninth international conference on artificial intelligence?", "6448e232-c698-4d38-a45e-91ae4d6cc452": "What preliminary evaluations are discussed in the work by Lu et al. (2024) regarding GPT-4V?", "5edaf5c9-75d5-4990-b4fc-7c496834e70a": "What is the main focus of the study by Mahowald et al. (2023) as described in the context?", "73481c62-564c-4853-814f-44ea0d0e4b1c": "Which event or conference is associated with the work of Macedo Maia et al. (2018) on financial opinion mining and question answering?", "1fd9ce4c-a2ba-4bf9-b3a4-4359da0b0b5b": "What is the main focus of the study titled \"Good debt or bad debt: Detecting semantic orientations in economic texts\"?", "0f4a2a64-8ffa-4dce-aa9e-0e68c1cf9bc2": "How do Mann et al. (2020) contribute to the understanding of language models in their work on few-shot learning?", "ab249647-02cb-4f68-a948-c5109ef0f32f": "What is the main focus of the Disfa database introduced by Mavadati et al. (2013)?", "91e512e4-baf9-4501-b579-cc832d0087c6": "How do Menick et al. (2022) propose to improve language model answers?", "8bd25fc5-2432-4bf7-9b1d-307c90a0d06b": "What is the focus of the dataset introduced by Mihaylov et al. (2018) in their EMNLP paper?", "4b49e1ed-e185-4bc8-8ab3-805607edbe34": "According to Min et al. (2023), what recent advances in natural language processing are surveyed in their ACM Computing Surveys article?", "e6c7fcd2-9dd2-4349-8ce0-e553562bb531": "What resources were used to create the sense inventory for clinical abbreviations and acronyms?", "ccd8073e-2626-4050-99c5-0ee31be3adb1": "Which publication featured the article titled \"Brief history of artificial intelligence\"?", "bb91d044-c074-4900-bad5-3aaa4360331c": "What is the main focus of the work by Nakano et al. (2022) titled \"WebGPT: Browser-assisted question-answering with human feedback\"?", "25b02628-60e3-4fd8-a8c8-856c647b8097": "What topic is addressed in the systematic literature review by Nayerifard et al. (2023) on machine learning?", "80cbdbd3-d5f3-431d-a9ce-0c2167e2708c": "What are the main risks identified by Oviedo-Trespalacios et al. (2023) regarding the use of ChatGPT for obtaining safety-related information and advice?", "09ead5b8-16b7-4b74-8fa1-52fa9d48d95e": "How do Palmirani and Vitali (2011) describe the role of Akoma-ntoso in managing legal documents within the Semantic Web framework?", "46db5d9b-d49e-4f58-b5c6-ec82be26b6da": "What is the purpose of the Bleu method mentioned in the context?", "c058c324-bfda-420a-bc1f-fac2e3226bd7": "Who are the authors of the Bbq bias benchmark for question answering?", "845c053a-95a7-47b9-8398-058c973f3933": "What is the main research question addressed by Petroni et al. (2019) in their paper \"Language models as knowledge bases?\"", "e4056b12-d211-4577-a0be-1736a7e8fac6": "How do Pinar Saygin et al. (2000) reflect on the Turing test 50 years after its inception?", "dafddd4f-7c00-421d-a643-7927a49985f7": "What is the main focus of the work by Qin et al. (2023a) titled \"Webcpm: Interactive web search for Chinese long-form question answering\"?", "13d8d0d6-2c75-4fce-b24e-b431066e0f8e": "Who are the authors involved in the study conducted by Qin et al. (2023a) on interactive web search for Chinese long-form question answering?", "e35f1cef-ed53-4868-aa77-9c366e69b8a0": "What is the main focus of the work by Qin et al. (2023b) titled \"Tool learning with foundation models\"?", "69453724-0f0d-4302-9497-e1b22b2e5474": "Who are some of the key contributors to the study \"Tool learning with foundation models\" by Qin et al. (2023b)?", "ece6ffc1-205f-4a0d-9b0a-dd5f420e4568": "What is the main contribution of Qin et al. (2023c) in the context of large language models?", "c8568a75-8fd9-4747-b8b4-02c7b919b106": "How do the works of Radford et al. (2018) and Radford et al. (2019) differ in their approach to language model development?", "058b7ea3-d4df-48a4-ac23-0281da61e885": "What is the main focus of the paper by Raina and Gales (2022) titled \"Multiple-choice question generation: Towards an automated assessment framework\"?", "2746ee62-cbfc-421c-9277-6a0e96ba43ef": "How do Rana et al. (2023) propose to use 3D scene graphs in their work on grounding large language models for task planning?", "310a2e9b-a739-4d2a-82c6-d7a7bb4b844d": "What is the main focus of the research conducted by Ren et al. (2023) on robots and large language model planners?", "3f532c99-11b9-4b34-ad08-e65d9ae7ef59": "How do Ruan et al. (2023) utilize large language models in their work on AI agents for task planning and tool usage?", "c07af445-cc07-44bf-bd5c-509c4e9192df": "What are the key findings of Scherrer et al. (2023) regarding the moral beliefs encoded in large language models (LLMs)?", "fc1cb5a1-6c80-4a45-baf1-b0bf26a5dde4": "How does the Toolformer model, as described by Schick et al. (2023), enable language models to teach themselves to use tools?", "f4aa7230-04b2-4f25-928d-a2be92dd6b63": "What is the main focus of the research conducted by Shah et al. in their 2023 paper on navigation with large language models?", "a4b46362-ac8f-46df-8e12-2be148acea43": "How do Shan and Deng (2018) approach facial expression recognition in their study?", "752a9928-d622-4cbf-9734-c8d9b5677caa": "What are the main contributions of Singh et al. (2023) in their work on generating situated robot task plans using large language models?", "a4f73f0d-c386-4b9f-8d1d-7b83cabae175": "How do Shinn et al. (2023) utilize verbal reinforcement learning in their Reflexion language agents?", "d2e2ba9e-02e9-49f6-b92b-e100b94091df": "What is the main focus of the study conducted by Sinha and Khandait (2021) as presented in the 2021 Future of Information and Communication Conference?", "12eabdeb-4904-4dfb-8e85-2c02d8a59283": "What is the contribution of Song et al. (2023a) in the field of embodied agents and large language models?", "dbbb8275-c007-46a1-ae2a-d302c46b95e6": "What is the main focus of the work by Song et al. (2023b) titled \"Restgpt: Connecting large language models with real-world restful apis\"?", "ccfc76f9-fc53-401d-a7a5-378d586c82db": "What topics are addressed in Sorensen et al. (2024) in their paper \"Value kaleidoscope: Engaging ai with pluralistic human values, rights, and duties\"?", "d6640e84-01e2-4b88-acc4-aa0ef66be16d": "What is the main focus of the paper \"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models\" by Srivastava et al. (2022)?", "8989d8eb-cfbf-4789-92b0-2d5131cad786": "In which publication format and platform was the 2022 paper by Srivastava et al. released?", "e3852c0c-f494-452f-a3d6-f2f7fc9af9b3": "Who are some of the individuals listed in the provided context?", "137583ee-c8f6-4124-bf81-32bb7cb09621": "Can you name three people whose first names start with the letter \"A\" from the context?", "f368d6d8-7118-40a3-ba1d-b8c2a85dd6b0": "Who are some of the individuals mentioned in the provided context?", "4b99f5b5-8f13-41b2-9fe0-ca8e805a751c": "Can you list any names that appear in the given context?", "94dc1ae9-4abe-4cd8-a84f-9ae7aa3ba91b": "How many individuals are listed in the provided context?", "0e8ae25f-3371-487b-a97a-1a568a05ab06": "Which names in the list have the first name \"Daniel\"?", "6b2599e7-760f-4584-ba72-ae8daeadb593": "Which individuals are mentioned in the provided context?", "be7fe999-816c-482a-8d65-aa898161e56a": "How many people named Derek are listed in the context?", "ab5f4e81-953e-4157-85a3-f04b7404e15b": "How many individuals named Ethan are mentioned in the provided context?", "bd568974-81fe-4c3c-beb7-8bdf372a8d84": "Which names in the list include a middle initial?", "8c4dd40b-b7c8-4677-b232-fe8c5e42df92": "How many individuals named James are mentioned in the context?", "de48bd92-1ed5-458c-8d3f-2b81fe81df4a": "Which names in the context start with the letter \"Jas\"?", "9a98c566-3b24-4de8-b987-8013bbb4fe7e": "How many individuals named John are mentioned in the context?", "9edd08db-fe9d-4052-a7db-ce416f245535": "Which names in the context include a middle initial?", "d6fe62bb-82fc-43b1-aad1-32daa672e08b": "Who are some of the individuals mentioned in the provided context?", "ea844ea8-f931-4aa0-87e7-ac1e49be3575": "Can you list any three names from the given list of people?", "5181db5a-dbf5-4997-9285-4b1da51101dc": "How many individuals named Michael are listed in the provided context?", "202ce89e-3311-4ec0-9431-28e4377b5f8b": "Which names in the list include a middle initial or middle name?", "848dffd1-b843-48c4-973f-216bbd6cf0e4": "Who are some of the individuals mentioned in the provided context?", "2e9e4b03-31b1-4e8c-9f95-fa3f35244655": "Can you list any three names from the given list of people?", "cfcdc1f7-9d1a-4aea-acaa-b1e3014b89e3": "How many individuals are listed in the provided context?", "d8577583-be9f-4caa-a375-ffc17c0528dd": "Can you name three people mentioned in the context whose first names start with the letter \"R\"?", "4da22628-bce5-4f7e-9b69-704e0f55e2bb": "Which individuals are mentioned in the provided context?", "fcfd3b9b-c5df-4b8c-b218-5a2c4f0f382e": "How many people named \"Shyam\" are listed in the context?", "cc60ef94-01ba-4a24-ac2e-a2c63146d12a": "Who are some of the individuals mentioned in the provided context?", "1f8ff266-2372-4138-b4ed-3ed7f62195e5": "Can you list any three names from the given list of people?", "3c22a3c9-47e1-4f70-9fe6-e0de506176ae": "Who are some of the authors listed in the 2023 publication?", "7e77c3cc-42b6-438b-acd1-3a959d3b1d7b": "In what year was the work by Yadollah Yaghoobzadeh and colleagues published?", "d328a0d2-a4c0-4f82-bd4d-5233fbc0fdf0": "What methods do Staab et al. (2023) propose to quantify and extrapolate the capabilities of language models beyond simple imitation?", "14b6d582-668c-428e-b7f9-c2ba6da3d056": "How does the causal framework introduced by Stolfo et al. (2023) help in assessing the robustness of mathematical reasoning in language models?", "7e7765b6-0a23-4a9a-8e26-a4bab6daa8a7": "What is the main topic of the survey conducted by Sun et al. (2024) as mentioned in the context?", "139708f0-fa07-478e-a436-f1e8b6b95626": "In which publication and volume can the work by Sun et al. (2024) be found?", "d0894454-903f-443c-8c9b-7df91f91b377": "What is the main focus of the study conducted by Suzgun et al. (2022) as described in the context?", "c4bb27b8-b81c-4415-afe7-8dd8ec2f552a": "What type of challenge does the CommonsenseQA dataset, introduced by Talmor et al. (2018), aim to address?", "7e0c3e28-1655-4bc6-8d14-32d9dcd24a7f": "What is the main focus of the study conducted by Tang et al. (2024) on topic-focused dialogue summarization?", "990b5a5c-6883-4633-b43c-c801ba936d6e": "How does the work of Tang et al. (2023) contribute to generalized tool learning for language models?", "4d7d1141-c49f-4531-b087-a8f92009121c": "What are some challenges associated with unfaithful explanations in chain-of-thought prompting as discussed by Turpin et al. (2023)?", "55a49fd9-4a9f-48f2-a482-2554e08d305d": "How has the introduction of the \"Attention is all you need\" model by Vaswani et al. (2017) influenced the development of large language models in medicine?", "a986540b-9457-4ff6-8ddf-3bdd0450542f": "What is the main focus of the toolbox introduced by Vermetten et al. (2022) in their study on structural bias?", "f95cecbf-0a17-4720-83cf-68fe54cc4b12": "In which journal and year was the paper by Vermetten et al. on benchmarking structural bias published?", "f4d28131-84e6-4dac-b128-725ab1af01b3": "Who are some of the individuals listed in the provided context?", "482a4559-df60-4c5c-a52c-0eaf05fc971f": "Can you name three contributors mentioned in the context?", "2908d1fa-7932-4bcb-b57b-900577a0c365": "Who are some of the individuals mentioned in the provided context?", "b0d08693-156b-456b-b2a2-c107d5b7cd84": "Can you list any three names from the given list of people?", "18e2d4ba-eab6-4670-b79a-c9e352aeb1a2": "Who are some of the contributors listed in the 2024 publication mentioned in the context?", "32e255ee-a360-4dd8-a51a-6203464a2f48": "In what year was the work by Patlak, William Pietri, Forough Poursabzi-Sangdeh, and others published?", "70c2223a-c14d-4d72-944b-38974b6d5b1f": "What is the purpose of the ai safety benchmark introduced in version 0.5 by mlcommons?", "148e35d9-a3d7-4a01-9a48-a82514256631": "What are the main topics covered by the works of Wang et al. (2024a) and Wang et al. (2024b)?", "4a6ad75d-142d-49df-be16-ec7c09643ab2": "What is the main contribution of the paper \"Benchmark self-evolving: A multi-agent framework for dynamic LLM evaluation\" by Wang et al. (2024c)?", "81901d6d-6079-4cf1-b192-6c213ee34530": "How do Wang et al. (2018) approach modeling semantic plausibility in their research?", "b4434c86-49c3-405b-baf4-5de74695b4d4": "What is the main contribution of Wang et al. (2023b) in the context of interactive planning with large language models?", "f6102452-91ba-461e-9e48-4c5768804fc5": "How do Wei et al. (2024) propose to measure and reduce hallucination in large language models without relying on gold-standard answers?", "2c1d42e9-3b9b-40cc-b837-b16c8096eea4": "What is the primary focus of the mbib benchmark task introduced in the referenced work?", "6949c62e-d251-4538-ba2b-c2bdc309b910": "Which authors contributed to the 2015 paper on AI-complete question answering and what was the nature of the tasks they proposed?", "30c72483-ede4-4ac4-8e3d-e0ce11331f88": "What are the main contributions of Wu et al. (2023) in the development of large language models for finance?", "0ac9cea0-274a-41e2-8752-3db06c915b5a": "How does the Pixiu model introduced by Xie et al. (2023) differ in its approach to instruction data and evaluation benchmarks for finance?", "4b9cd36e-be17-497d-a469-3ed5cd1e7fb2": "What aspects of large language models' logical reasoning abilities are evaluated in the study by Xu et al. (2023b)?", "c4cb2208-a544-44ba-8c41-a296721dec23": "How do the works by Xu et al. (2023c) and Yan et al. (2024a) contribute to understanding the safety, responsibility, and tool manipulation capabilities of large language models?", "3248f589-422a-42f7-a7e4-29ed118a338f": "What are the key challenges discussed by Yan et al. (2024b) in protecting the data privacy of large language models?", "9a814a65-bd2c-429c-bac2-7b11ec231da1": "How does the benchmark and reverse validation method proposed by Yang et al. (2023) contribute to passage-level hallucination detection?", "6bcc7349-f4ac-491d-989b-da3e580a48d5": "What is the main contribution of Yang et al. (2018) in their paper on HotpotQA?", "f6b1dfa7-e3a8-47a8-b4d4-ace428b9e425": "How do Yao et al. (2023a) aim to improve web interaction with grounded language agents in their Webshop project?", "b4a90f75-3e2f-47dd-a8cd-20aaa040c5de": "What are the main topics covered in the survey by Yao et al. (2024) on large language model security and privacy?", "fbc2fed9-8a8f-46d6-a79e-a2b62c3f20f9": "How does the evaluation framework proposed by Yip et al. (2024) assess resilience against prompt injection attacks in large language models?", "c8d936f1-053e-4b61-b177-b615e1dff214": "What is the main focus of the research conducted by Yu et al. (2023) as described in the context?", "f102af8c-d483-4d42-af05-8432a043c17b": "What specific task do Yuan et al. (2023) investigate regarding large language models?", "c1101be1-de7f-48ac-836a-f4e65a46128a": "What is the main focus of the Rigorllm framework mentioned in the context?", "f7125720-08c3-497c-9589-af787701db23": "Which study evaluates the fairness of ChatGPT in recommendation systems according to the provided context?", "432a5b0f-667b-419d-97b1-8b7c7fd9af86": "What is the primary focus of the Codeagent system in addressing real-world coding challenges?", "ed2ed429-d365-4d00-8ed9-1d0e1b3a359c": "How does the self-alignment approach proposed by Zhang et al. (2024b) aim to reduce hallucinations in large language models?", "8a97e320-aea1-44b9-b34c-2928be1bfb6b": "What is the main focus of the research conducted by Zhang et al. (2018) as mentioned in the context?", "52a7053b-8c6e-4889-9a0b-070f8801f83b": "Which topics are covered by the works of Zhao et al. (2023) and Zheng et al. (2023a) according to the provided context?", "0b934e65-4c55-428c-b5fa-402c987c5309": "What methods are used to evaluate large language models as judges in mt-bench and chatbot arena?", "cbfc5395-27ec-43ec-b0db-714a47255e12": "How does Navgpt incorporate explicit reasoning in vision-and-language navigation using large language models?", "ef396fbc-d844-4747-9681-1da6dfccbb64": "What is Webarena and what purpose does it serve in the development of autonomous agents?", "30c8ea02-59e0-431e-84a5-2fb1e1f08fac": "Who are some of the key contributors mentioned in the 2023 work by Zielinski et al. related to Webarena?", "15abe9ab-e9e6-4acf-bdde-ffb6fd6903f2": "What are the main components of the evaluation function \\( E \\) discussed in the survey on LLM-based evaluation methods?", "53a9c5b2-27af-47e5-a92e-9c00d0e30d95": "How do LLMs contribute to model enhancement according to the functionalities outlined in the survey?", "1c75c4e2-9ed7-42d5-89e7-3a7e8a95903c": "What are the different approaches discussed under the Single-LLM System methodology?", "615369f7-4d29-4098-87eb-8c261647ca82": "Which application areas are covered under the section titled \"Application\"?", "ea9b44ed-5e9a-41fb-b540-eb4f84072f07": "What are the different types of correlation coefficients mentioned under the metric section?", "c8d25ef6-dc73-4b9d-84d5-01838cbf1c22": "What categories of biases are identified under the limitations section?", "d7129de1-bff7-4245-a632-dfe2f57bc447": "What methods are discussed under the section \"More Efficient LLMs-as-Judges\" to improve evaluation processes?", "e870a1a0-281d-4337-bcca-819d57d7f27b": "How does the survey address the challenge of ensuring fairness and mitigating bias in LLMs-as-Judges?", "26f0a6e1-f972-4cd2-8cb1-c15ee9ede9d5": "Which institutions are affiliated with Haitao Li and Qian Dong?", "13cf6d2e-5786-429a-b33a-944133c46a39": "What is the email address associated with Huixue Su?", "89cd6a7c-37f4-4b68-acd5-1d8e5d26806a": "Which institution are Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu affiliated with?", "e376bf0f-367a-484e-b423-8aa2bbe63fb8": "What department do all the listed individuals belong to at Tsinghua University?", "1a93645e-6a04-4c3d-ae1e-c6c00fc66cd6": "What are the five key perspectives from which the paper surveys the LLMs-as-judges paradigm?", "45e5b050-47e6-4239-b544-ef57aa9838fe": "Why has the LLMs-as-judges framework attracted growing attention from academia and industry?", "bd531bce-72ff-4a55-a361-34e17fd0a40f": "What is the methodology for constructing an evaluation system using LLM judges?", "81a268fd-64ab-4f3d-b295-bf07b2d8fbc9": "What are the potential domains where LLM judges can be applied?", "567e5c56-6f2b-41f0-93bf-b49c86669fe9": "What is the primary goal of the structured and comprehensive analysis mentioned in the context?", "dee2e12c-d0c6-4787-83cd-13a33fd5e56e": "Where can one find the maintained resource list related to LLMs-as-judges?", "74b7c60c-8ce1-479b-bec3-f00352a934cd": "What are the key challenges in evaluating Large Language Models (LLMs)?", "e02b1532-5457-407e-b1fe-cac7fd9d18a9": "How can LLMs be utilized as judges in the evaluation process of other language models?", "8690eabd-28ad-45bf-ac5a-445bb529d638": "How have AI researchers historically measured and validated the intelligence of AI models?", "3f468737-dd7e-4799-8574-50f42485a800": "What were the primary focuses of AI evaluation methods in the mid-20th century?", "20b10d67-8b24-49f8-a9a7-b27c640ec8b6": "How has the emergence of deep learning influenced the evaluation standards of AI systems?", "94586321-3e96-46d0-871c-36a3725d8380": "What role does the Turing Test play in assessing AI models according to the provided context?", "d0e4f918-8fac-4662-b31b-22b6e294fd20": "What challenges do Large Language Models (LLMs) present for AI evaluation due to their generative and open-ended outputs?", "13300178-e15d-4bc4-a781-d357a9948cfb": "How do Large Language Models demonstrate their generalization and adaptability across different tasks and domains?", "3c17140a-2bb9-4e53-b439-08c349299fa4": "What are some limitations of traditional metrics like BLEU and ROUGE in evaluating natural language generation tasks?", "b727d493-38ea-4319-9065-e204c6dd2127": "Besides task performance, what additional factors should modern AI evaluation consider in real-world scenarios?", "e65fc91b-c6cf-4141-acf0-5300ba8def26": "What are the benefits of using human annotations as the \u201cground truth\u201d in model evaluation?", "8aa9b471-e025-4a49-a8eb-26f792a9acf0": "Why is it challenging to scale up the collection of human annotations for large-scale evaluation?", "c0614aa1-b084-402e-a7f2-110fc7be112c": "What is the new paradigm introduced to replace humans and statistical metrics in evaluation?", "1fa35555-b67c-4fb1-918c-c8eabb7d1629": "How do LLMs-as-judges improve the evaluation process compared to traditional methods?", "0de6caa4-532c-4e5c-846a-8cfebd004d45": "How do LLM judges provide more comprehensive feedback on model performance?", "a9b61c73-744b-472c-94ad-feeb37c26fbf": "In what ways do LLM judges reduce the costs and time compared to human evaluation?", "2183727d-eb13-4108-b797-5bf7b946b47b": "What are some critical challenges faced by LLMs when used as judges?", "2213e9bb-79a4-43cc-ad51-b66ba9bbd9e5": "How can the training data of LLMs impact the fairness and reliability of their assessments?", "986a60fe-784c-49b6-897f-0f74c0e4b651": "What are the five key perspectives discussed in the survey regarding LLMs-as-judges?", "b741fcb3-706e-4341-ad3c-2340811451db": "What is the primary aim of the survey in analyzing LLMs-as-judges?", "6ca99e18-10ad-4313-a835-927f454acabb": "What is the main focus of the survey presented on the emerging paradigm of LLMs-as-judges?", "893860e1-ca07-4b94-8968-a5f6c0202890": "How do LLMs function as performance evaluators according to the survey?", "d52ee58a-dbe9-4b9c-b266-af012f72604f": "What are the five key perspectives used to organize the survey on LLMs as evaluators?", "2906c1eb-5944-4d49-a2b7-1c4e4b1bb787": "How does the structured approach help in understanding the use of LLMs as evaluators?", "1379dc25-ba81-4ed9-8951-f7aa8f7f693a": "What are the current challenges faced in adopting LLMs-as-judges according to the context?", "8f990e8f-93ef-4d60-9602-b1f848851b80": "How does the provided open-source repository aim to support the development of LLMs-as-judges?", "ddf5cfa9-ca56-445e-bc2a-a3e41c4133a0": "What topics are covered in Sections (\u00a72) through (\u00a77) of the paper regarding LLMs-as-judges?", "9cf9d82e-dbf9-4ae6-8708-a9b970f9804e": "Which sections of the paper discuss the future work and conclusion related to LLMs-as-judges?", "a44ba978-820e-49bd-bee8-5a7e6e5cd059": "What styling options are applied to the edges and nodes in the forest diagram described in the context?", "8d5dc1a4-d187-4b2e-bca2-a13c152f2d5f": "How are the anchors and orientations configured for parent and child nodes in the given forest structure?", "2252bbdc-b843-4554-a825-5602908c5d25": "What are some of the response evaluation methods mentioned in the context under section 3.1.1?", "62e56246-6371-44a3-b5b3-26c2cf1c7fee": "Which model evaluation approaches are referenced in section 3.1.2 of the context?", "75f20ae2-b624-47c5-8afd-ac6f77336b5b": "What are some methods mentioned for reward modeling during training in the context provided?", "2136e126-1c31-4a2e-9670-402aeb3ce097": "Which techniques are listed for acting as a verifier during inference according to the context?", "cf7d4527-d1f3-4237-8180-0ec05f6d96a9": "What are some of the key works referenced in the context of feedback for refinement in section 3.2.3?", "a607077d-3975-4120-9f02-18a42b1c5087": "Which studies or methods are mentioned under data annotation in section 3.3.1?", "4e4ebc6d-2f5c-4234-8ecd-543315dead68": "What are some of the data synthesis methods mentioned in the context, and which authors are associated with them?", "66de2991-fc28-4da3-bd8f-171c55ab5aa0": "Which prompt-based in-context learning evaluation techniques are listed under the Single-LLM methodology?", "3b5c9429-04d9-4675-979d-da9be0132946": "What are some of the evaluation methods or frameworks mentioned in the context for assessing language models?", "d6d055e8-b136-48ee-a896-7b707c982dae": "Which approaches or tools are listed under the category of definition augmentation in the context?", "9b676ead-609f-4ba4-a210-3dcd1fab8414": "What are some examples of multi-turn optimization methods mentioned in the context?", "c4322db8-996e-40f4-a459-9feb448934ad": "Which tuning-based score-based methods are listed in the provided context?", "8d04571e-14bb-454d-8db6-da9a83bb4a8c": "What are some examples of preference-based learning methods mentioned in the context?", "bea369e6-85d0-45d0-82c5-f1ada4c81794": "Which authors and years are associated with the development of Meta-Rewarding and JudgeLM?", "52312c90-954a-4d70-a05f-3d71d57e3024": "What are some of the models or methods mentioned under the Post-processing section, specifically related to Probability Calibration?", "89ce062a-ebbf-47e5-b10a-05bb2fe5bfb3": "Which works are listed under the Text Reprocessing subsection in the provided context?", "ed7db102-d6d6-4c3c-8d76-9747a3eb7d66": "What are some examples of cooperation methods mentioned in the Multi-LLM communication section?", "0d4069c2-f158-4663-9318-24ca673292aa": "Which works are cited under the competition category in the Multi-LLM communication subsection?", "53e45b56-6657-4a0a-8d42-ac9058610924": "Which works are referenced under the Human-AI Collaboration section (\u00a74.3)?", "bb1f5ef0-a013-4d41-9a27-952a0d5a0a6e": "What applications are mentioned in the General subsection (\u00a75.1) of the APPLICATION section?", "f83bc992-f86a-44cd-851a-73c3cd03dfa5": "Which works are referenced under the Multimodal section (\u00a75.2) in the provided context?", "1f43ec5e-7c8a-4b6a-b451-f20fb7f17903": "Can you list some of the authors mentioned in the Medical section (\u00a75.3) according to the context?", "b7df62fe-2c78-40cd-a174-0d2280971c79": "Which studies are referenced under the Financial section (\u00a75.5) in the provided context?", "9a48c5c4-41e0-42ed-97bc-e550ae26ed35": "Can you list some of the works mentioned in the Education section (\u00a75.6) according to the context?", "fcde8283-6252-4f38-805f-e1cb01b18ddb": "What are some of the benchmarks mentioned for code generation evaluation in the meta-evaluation section?", "bbbc8c50-6c12-4643-83af-8a9dcaeba194": "Which studies or datasets are referenced under the machine translation benchmarks in the provided context?", "885a1a00-c03e-4116-8fad-340c5ff1fbc9": "What are some of the datasets or benchmarks mentioned for evaluating text summarization models?", "44ee2f2f-ae07-4e67-ba92-ffb8f328f96a": "Which dialogue generation datasets or challenges are referenced in the context?", "f52e1bc7-3186-4959-b926-b33df33d3b46": "What are some of the key datasets or benchmarks mentioned in the context of Values Alignment (\u00a76.1.6)?", "0e09e0ff-fb20-4e68-98f7-710d46402976": "Which datasets are referenced in relation to Recommendation systems (\u00a76.1.7) according to the provided context?", "6d802869-b47f-4857-aa7e-30b55ffa95b3": "What are some of the comprehensive data sources mentioned in the context for evaluating language models?", "8375a89a-aa7a-43f4-89a6-a8d976ccf958": "Which studies or benchmarks listed in the context focus on evaluating the truthfulness or accuracy of chatbot responses?", "e98bb506-269e-45f7-be27-3bd9ac26acc8": "What are some of the metrics mentioned in the context for measuring accuracy or agreement?", "816d3926-5247-4494-b2cd-539fb755e02c": "Which authors are associated with the development or discussion of the metrics listed in the context?", "544c074a-94fe-4c2a-bfd6-db1cfe3f0929": "What are the different categories included in the taxonomy of LLMs-as-judges based on functionality, methodology, application, and meta-evaluation?", "0c934f06-b3b8-43af-90bd-225e56dd9ccb": "How does the taxonomy classify LLMs-as-judges in terms of their methodological approaches?", "d0037d07-9898-47cd-af98-41378934bf9c": "What styling options are applied to the edges and nodes in the described forest diagram?", "4179efcc-d74e-4b83-ae6b-18e26c250890": "How are the anchors and orientations configured for parent and child nodes in the forest structure?", "3df5db72-8962-44c6-a923-66ab42f125c2": "What are some examples of presentation-related biases mentioned in the context?", "19bc90dc-645e-43cc-a55f-17be1a84aefa": "Which specific types of biases are categorized under presentation-related biases in the provided context?", "6a037e4b-cc19-493d-9569-542b05c6ddfd": "What are some examples of social-related biases mentioned in the context?", "884b6899-f887-4824-bf67-217dbf391d62": "Which types of content-related biases are identified in the provided context?", "4f0cb9a7-d59e-4ce0-b097-afe6ec13d084": "What are some examples of cognitive-related biases discussed in the context, and which studies have explored these biases?", "606a6ea4-226e-4357-929c-df027876256d": "How are adversarial attacks on large language models categorized in the provided context?", "363ef18a-da05-4907-85e1-7d4e5159c043": "What are some types of adversarial attacks on large language models (LLMs) discussed in section 7.2.1?", "e1ac292a-8dad-471e-8e53-9e15f5f94014": "Which studies are referenced in relation to adversarial attacks on LLMs-as-Judges in section 7.2.2?", "04b5c3d7-ba7d-4132-a640-56f5e1167f26": "What are some of the key challenges related to knowledge recency discussed in section \u00a77.3.1?", "535678ed-16ea-4f8e-afcb-a35f0994547e": "Which studies are referenced in relation to hallucination issues in section \u00a77.3.2?", "4c8de907-58b2-4988-a6aa-0753b38cb9d4": "What are some approaches mentioned for making evaluation processes more efficient in future work?", "18f35b0e-181e-49b1-a5bc-e0d10d41b20e": "How does the future work propose to make evaluation methods more effective?", "ae15e5a1-26f7-453f-a484-9ec720fd680f": "What are some key areas of focus for improving reliability in AI systems as mentioned in the context?", "86b92c86-4fa7-4add-b9ed-ede67ca2bc03": "Which studies are referenced in relation to cross-domain and cross-language transferability?", "b89d1ca0-3774-42a7-84fd-739764f00e48": "What is the purpose of providing a formal definition of LLMs-as-judges in the preliminaries section?", "9645bb9e-9c24-44fe-b7fa-795f233cb1a4": "What does Figure 3 illustrate in the context of the LLMs-as-judges system?", "475e96e3-e055-47bb-96b7-ae07293bea15": "What is the primary role of LLMs in the LLMs-as-judges evaluation framework?", "3fe90947-390b-41c9-a940-f9689884a8cf": "How does the LLMs-as-judges paradigm unify different evaluation scenarios?", "c02cabb6-7bda-4bad-b3c6-8792a337a782": "What does the expression \\((\\mathcal{Y}, \\mathcal{E}, \\mathcal{F}) = E(\\mathcal{T}, \\mathcal{C}, \\mathcal{X}, \\mathcal{R})\\) represent in the given context?", "0dec50ed-647d-4163-9954-685d540d27ae": "How are the sets \\(\\mathcal{Y}, \\mathcal{E}, \\mathcal{F}\\) related to \\(\\mathcal{T}, \\mathcal{C}, \\mathcal{X}, \\mathcal{R}\\) through the function \\(E\\)?", "d5757160-e2b4-4cdc-9284-8aecf7c0ac06": "What are the inputs required by the evaluation function \ud835\udc38 to produce its outputs?", "b4a145c9-232c-4249-9f10-14ff7d1f8055": "What are the three types of outputs generated by the evaluation function \ud835\udc38 based on the given inputs?", "e45ad7b4-c140-4741-ab3f-31bcfa74e7ae": "What does the unified formulation bring together in the context of LLMs-as-judges?", "fcba6911-6781-49f3-9ae2-7d6d9ad57d9d": "How do different input-output configurations relate to methods and objectives in LLMs-as-judges?", "035f5345-1013-4a88-b4ff-abe23b4aa195": "What are the three primary configurations of the evaluation function E\ud835\udc38Eitalic_E in the context of LLMs-as-judges?", "7305ef4b-2ef7-40b6-834b-63125b0d2fba": "How do the purposes, advantages, and challenges differ among Single-LLM, Multi-LLM, and Hybrid evaluation systems?", "11ed1dd8-1873-4aa6-9ac6-2627b4b2dad8": "What are the advantages of using a single LLM evaluation system?", "97d5ac1c-d5d3-40d5-b28d-bbcc9f1429a8": "What are the potential limitations or challenges associated with a single LLM evaluation system?", "16eace87-73ee-4ea3-820b-b296e6c12896": "What are the main advantages of using a Multi-LLM evaluation system compared to a single model evaluation approach?", "2ac77e66-69f6-4f86-ab83-8011450dda1a": "What challenges are associated with deploying and maintaining Multi-LLM evaluation systems, especially for large-scale tasks?", "1c4d9ec0-fe63-4d24-91f5-f4f051f418a8": "What role does cooperation between models play in enhancing evaluation results?", "7addc0ff-9d6f-4639-961f-0ad54903a400": "Why are the methods for achieving consensus or resolving differences between models considered key areas of ongoing exploration?", "584b54e9-c41e-4ed8-8c2d-97471259dae6": "QUESTION #1", "e284e218-9c38-434e-b9b6-64cd2a9f0784": "QUESTION #2", "4096de6b-a417-4772-a121-14eb325cc919": "What are the main advantages of the Human-AI Collaboration System in evaluation tasks?", "97ee37a3-3f3b-4f5d-9ab4-151d3e192d9f": "What challenges arise from integrating human evaluators with LLMs in the Human-AI Collaboration System?", "a1963da7-612a-4793-ad96-002bff5c3c99": "How does the evaluation process impact the cost and time required compared to purely model-based systems?", "ddae77d8-2cee-4d8f-be56-87aa30367316": "Why is the evaluation process described as less scalable than purely model-based systems?", "a766baee-89ef-417b-a50d-18fd49459598": "What are the three types of inputs that LLM judges typically receive in addition to the evaluation item \ud835\udcb3\\mathcal{X}?", "e8f628c8-df2a-43ce-b228-1e8c11f9373c": "What are the three approaches typically included in the Evaluation Type \ud835\udcaf\\mathcal{T}?", "ec9fca3c-3150-4a18-80b8-1777c6b02611": "What are the main advantages and limitations of pointwise evaluation as described in the context?", "ab96f981-02a6-4b86-a977-13b2e95f9a6c": "How does pointwise evaluation assess candidate items in tasks such as text summarization?", "fdef9d71-99e9-4f48-8436-141f7a8f798f": "What is the main purpose of pairwise evaluation in preference-based tasks?", "800ed847-dfb5-4e7a-91f9-bd19a73002b2": "Why is pairwise evaluation considered effective when differences between outputs are subtle?", "bd1e5e49-92fc-43c4-8b47-2f3fc240ec7f": "What is the primary purpose of listwise evaluation in ranking tasks?", "fc49c74e-0e44-4bd3-87fa-508fc8bdf5dd": "How does listwise evaluation differ from other evaluation methods in terms of candidate item analysis?", "c885ac5f-dfa5-4d57-beb4-7fe24a75120c": "How can pointwise scores be utilized in relation to pairwise comparisons and ranked lists?", "067f0646-d29f-4f65-8cb4-500b9b8a3a8a": "In what way can pairwise preferences contribute to listwise analysis?", "5fb796fa-fb7a-4260-8071-44f16da2185f": "What issue arises in the LLMs-as-judges framework when comparing pointwise evaluation scores to direct pairwise comparisons?", "55151ca2-ab47-4efc-af31-eebed86f38b6": "How do LLM judges demonstrate a lack of transitivity in their pairwise preference judgments?", "a3856391-4067-4b66-894a-eb799df4f644": "What inconsistency is highlighted in the LLM-as-Judge framework regarding the comparison of z-values?", "11f0e981-1fdb-47c1-8005-56a6d1b280d1": "Why do the inconsistencies in the LLM's output raise concerns about its reliability and trustworthiness?", "40c04eef-0317-4fb1-9854-0f1972611dc3": "What role do the evaluation criteria \ud835\udc9e\\mathcal{C} play in the LLMs-as-judges system?", "059ab53d-9bd1-4925-809a-bbda5b3ced02": "Which quality attributes are typically covered by the evaluation criteria \ud835\udc9e\\mathcal{C} in assessing the output?", "a74a69ce-d4cf-458b-91dc-60b5ade44e60": "What aspects of language does the Linguistic Quality category evaluate according to the context?", "143154a0-6811-4f70-a272-d970a80008e7": "Why is Linguistic Quality considered crucial in tasks like text generation, machine translation, and summarization?", "6769c8fa-566a-4fa1-ab83-94fabda027e4": "What aspects are evaluated under the dimension of Content Accuracy according to the given context?", "28cd2bb0-3212-4f5d-945c-3e4ba114d0b4": "Why is Content Accuracy especially important in tasks like code generation and fact-checking?", "eed532b9-000d-42be-9abf-e44dd4d93534": "What are some examples of task-specific metrics mentioned for evaluating outputs in different domains?", "b68a9cf9-1482-460a-a963-ad9903e91efc": "Why is it important to use task-specific metrics like informativeness and completeness in addition to general quality metrics?", "edf5d4b1-499c-4577-8d5a-235b15db75b3": "How can providing well-structured examples benefit the assessment process when working with LLMs?", "4789d9c3-99a1-497c-afeb-0ffefd40aff5": "What are the two broad categories of evaluation processes based on the availability of evaluation references \u211b?", "9cf6e257-cfda-4331-83cc-84b7b5d71921": "What are the key principles behind Reference-Based Evaluation as discussed by Freitag et al. (2021b) and Karpinska and Iyyer (2023)?", "b395610d-4650-4730-9ad0-0a2980a6c554": "How do Freitag et al. (2021b) and Karpinska and Iyyer (2023) compare different methods of Reference-Based Evaluation?", "58f85e92-90f6-49f1-b031-68b9c5976860": "What is the main purpose of reference-based evaluation in Natural Language Generation tasks?", "d5ac96ef-77da-488d-a173-86e9a919087a": "In which types of tasks is reference-based evaluation commonly applied, according to the context?", "57485825-042d-4065-ad46-fe709185f348": "How can the quality of reference data constrain outcomes?", "1fd19084-8272-4e88-8bd0-ebfb797c0fa0": "In what ways does the variety of reference data impact results?", "8b816452-923e-4235-a24b-82b679971a7f": "QUESTION #1", "15cfdc98-c98a-4154-814f-61bb80e431b1": "QUESTION #2", "86cce746-6821-4d06-835a-bac108635211": "What are the key contributions of Shen and Wan (2023) in the field of reference-free evaluation?", "bcad99da-dff9-411a-8a5a-8249c64002df": "How do the approaches of Zheng et al. (2023a) and He et al. (2023b) differ in their methods for reference-free evaluation?", "ffdfe533-b251-42d0-9409-ec4ba925fb39": "What is the main advantage of reference-free evaluation according to the context?", "610d7fa7-b913-45e0-8fac-cf1fd82e87f8": "In which fields is reference-free evaluation widely used as mentioned in the context?", "36d9326d-a420-4655-84af-db39989133dc": "What challenges arise when an LLM lacks relevant knowledge in a specific domain?", "ecb0c72c-bfa0-44dd-8a9c-5ce1064542f9": "How can satisfactory evaluations be obtained for an LLM in areas where it has limited expertise?", "5cab75bd-99e8-4f0d-81cf-da817a0d8cc6": "What are the three types of outputs typically generated by LLMs in the LLMs-as-judges paradigm?", "bc7b68e0-0f79-4984-b2be-830b5ec21422": "What does the evaluation output section describe about the roles of \ud835\udcb4, \u2130, and \u2131 in the LLMs-as-judges framework?", "9fbd1e8a-6bd0-4e02-967e-ab01d0cfc90e": "What forms can the evaluation result \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y take according to the provided context?", "21273ad3-92a0-4cfb-a4f6-4c7e96da69bd": "How does the evaluation result \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y reflect the quality or performance of candidate items in tasks like machine translation or dialogue generation?", "3cd2c869-7011-481c-9504-166b63cd6060": "What does \ud835\udcb4\\mathcal{Y}caligraphic_Y provide in terms of performance evaluation?", "b6e82d9d-4e43-4c4b-be98-52c1c094420d": "How does \ud835\udcb4\\mathcal{Y}caligraphic_Y help researchers when comparing models or outputs?", "c5fba027-c389-4c08-86d9-de4bd2db62c6": "QUESTION #1", "2d8907fe-4da1-403d-91cc-310fde86d07a": "QUESTION #2", "a3fd1c5c-6f95-48d0-bdcb-2486895d2e2a": "What role does the explanation \u2130\u2130\\mathcal{E}caligraphic_E play in the evaluation process according to the context?", "37a53b5b-fbd1-49df-9a71-596ab7ef1e81": "How does the explanation component contribute to the transparency of the LLM's decision-making?", "30e8b347-f13a-4ae5-ac3b-c277ca28be43": "What is the purpose of gaining deeper insights into the evaluated content?", "fabdb213-2d65-4835-a529-7d2476ea2536": "How can understanding the strengths and weaknesses of content improve its overall quality?", "73eaabbd-5bb4-43a0-9765-dfc709ee365b": "QUESTION #1", "5bb0382d-bccb-4b31-b226-423bc613caeb": "QUESTION #2", "b28eb5f6-a46e-4bdf-91c0-a436858450bb": "What is the primary purpose of the feedback component \u2131\u2131\\mathcal{F}caligraphic_F as described in the context?", "d0bf0e71-c12d-45e2-8b09-2169c2cc542d": "How does feedback differ from the evaluation result according to the given information?", "c131fdb5-3ee0-4dd6-b149-fc17a83e7295": "How do creators enhance the quality of the generated outputs?", "ea6864d0-1bfb-4bdf-92d8-50d553d5d01d": "What role do creators play in improving generated outputs?", "fe1ca9eb-acae-4b31-b188-1933061ab417": "What are the three types of outputs that LLM judges can generate for a given task?", "875120d9-eade-4f75-9396-1c17aaabb4bb": "How do the intended purpose and specific requirements of the evaluation influence the combinations of outputs generated by LLM judges?", "b4844465-feaa-4d62-9556-ffe723a55661": "Why does providing explanation \\(\\mathcal{E}\\) help users better understand and trust evaluation results?", "e32b98d3-8966-417d-9bd5-8e782ed2de78": "What additional capabilities are required for generating feedback \\(\\mathcal{F}\\) compared to just assessing input quality?", "eedb6d23-3fa6-465c-a952-b256f27a695d": "What are the three main directions in which LLM evaluators are categorized based on their functionality?", "f0a345f0-adc9-4324-917c-eb7cd1b8858b": "What aspects will be explored in the section discussing the functionality of LLMs-as-judges?", "8a76d073-c790-4b2e-821a-892689bbf14c": "What are the two main components of performance evaluation for LLM judges as described in the context?", "e5e65987-243c-4b5d-a252-0f4c7dc6571e": "How do Response Evaluation and Model Evaluation differ in their focus when assessing LLM performance?", "02b4d63e-6490-42fd-bc78-c5b9070fc61f": "What are the typical criteria used to evaluate responses according to the context?", "5dff3ad8-ff73-407a-94f9-4367d5ef51ec": "Why might evaluation metrics need to be customized for different tasks?", "3e13a9ab-da6a-4153-906b-fc7343ddde39": "What is the purpose of the LLM-Eval framework proposed by Lin and Chen (2023)?", "614637ab-a73d-4780-abb2-aa6f5974cbd8": "Which dimensions does LLM-Eval evaluate in open-domain dialogue systems?", "5addb4be-7f04-4428-aa20-0d4a594d10a0": "What approach did Wang et al. use to develop their article scoring and feedback system for different genres?", "21da9571-3ff5-411a-8c5f-468445daa2b9": "According to Zhou et al., what are the limitations of current LLMs in automated paper review tasks?", "11be9566-b418-4f29-8fbe-a02ed4102ea8": "What aspects of a response can be evaluated beyond the quality of the final answer according to the context?", "e6659dd0-ba9c-4c86-be41-04c9b3a9de89": "How can the response process be analyzed during the evaluation of a single response?", "215714cb-453a-4fd7-9639-b2ed1036ad32": "What are the three dimensions used by ARES to evaluate RAG systems?", "93bba5d7-a396-417d-97cf-9d06f3dbc8bd": "How does SELF-RAG determine whether retrieval is required and assess the quality of generated outputs?", "628a63d3-8dc4-46f5-af5f-0fa5c797b2fc": "What is the initial step in model evaluation according to the provided context?", "999d4631-18d3-44c5-a73e-e15fe59d470b": "Which areas are analyzed when assessing a model\u2019s overall capabilities?", "e6c3b0f8-a396-482b-9270-6286267a2ff7": "What are some of the key metrics used by LLM judges to assess model performance?", "c6ede956-5568-4da9-976c-d1aceb19166d": "Why is the approach of using average performance on static benchmarks widely adopted for evaluating models?", "02add3dc-41f9-44a2-96d2-1e024e53fc2f": "What types of complex situations are discussed by Liu et al. (2023e) and Trivedi et al. (2024b)?", "311d57b6-e600-4212-9e91-6042ab252603": "How do Liu et al. (2023e) and Trivedi et al. (2024b) approach the analysis of complex situations?", "df476a3c-199b-4146-a153-16c08d38e12e": "How has the evaluation process evolved in response to increasing demand according to the context?", "e453b866-beb9-42ae-9410-217e42bf3aac": "What role do LLMs-as-judges and platforms like Chatbot Arena play in the assessment of LLM performance?", "e3866cf4-d937-45fd-a273-192466ccaef8": "What are the key features of the Auto-Arena and LMExam frameworks in assessing model capabilities?", "7b549ec4-3a27-4f36-a39a-606fa433ed7b": "How do Auto-Arena and LMExam utilize large language models in their evaluation processes?", "65b8a85f-929d-4e42-9404-6a62be494353": "What is the role of the LLM-driven \u201cinteractor\u201d introduced by KIEval (Yu et al., 2024)?", "1c5f4eb0-b282-4460-ac88-427bfdf14244": "How do the dynamic multi-turn conversations in KIEval help address issues in traditional benchmark tests?", "981f930c-3d6e-4990-83b1-e4ee13e5972a": "How do LLMs-as-judges contribute to model enhancement from training to inference?", "2e60a2b5-f121-476a-a79f-45a6afac4b57": "What role does reward modeling play during the training phase in the context of model enhancement?", "6e91b3b0-ea0a-4a33-82d8-67bf2ed8a193": "How do LLMs function as judges in reward modeling during training?", "75cfb537-ee2f-47b5-a443-414e82b09f52": "What is the role of human-defined criteria in the reward modeling process involving LLM judges?", "77a4b388-6517-4eca-903a-58a46a3d20a1": "How do SRLMs, OAIF, and RLAIF enable large language models to become their own reward models, and what advantage does this provide over traditional RLHF methods?", "45c9f881-e00c-40f2-bea9-e680cc968490": "What role does the Critic Language Model (Critic LM) introduced by RELC play in addressing the issue of sparse rewards in reinforcement learning?", "d36f1eac-d4a1-4bea-9364-5f8f12006939": "What challenges arise from using the same LLM for both policy generation and reward modeling?", "5a29c809-163f-4d04-ab63-aaf7b3aa7cb2": "How does CREAM address the issue of accumulated biases and preference data noise in training?", "e9a087f4-cb2d-4500-bcfd-d0e91f46e425": "How does the use of \u201cMixed Judges\u201d help in assigning reward models to task groups?", "ed1492ff-da5b-4353-ba10-c4f4f2201c87": "Why is it important to align reward signals closely with task objectives?", "e50a4d23-d059-4dc6-b5e5-56ec29e771b9": "What role do LLM judges play during inference according to the context?", "8f7ae0d7-8d23-4a6e-ba2d-e808b25a141e": "Which metrics are used by LLM judges to select the optimal response during inference?", "3b35b868-60f6-4c50-8caf-edaf66059f03": "What is the main idea behind Best-of-N sampling as described in the context?", "d8d9a91b-b697-4cd0-ada6-ea1a57e403be": "How does the self-consistency sampling method introduced by Wang et al. improve inference stability?", "5172d06e-eb01-42c6-804e-13af89548ec3": "How does the Tree of Thoughts (ToT) method optimize the problem-solving process?", "1a40a162-fdce-426a-905c-970599283051": "In what way does the Graph of Thoughts (GoT) method improve multi-step reasoning compared to the Tree of Thoughts (ToT) method?", "3fbd2dbc-91de-4fcb-8afb-55a91652c710": "How does step-by-step validation improve the performance of LLMs in multi-step reasoning tasks according to Lightman et al. (2023)?", "5b2c2213-0bf0-4111-82b5-9b0cb63c5b0b": "What methods do SE-GBS and REPS use to ensure the accuracy and consistency of reasoning chains in multi-step reasoning?", "d0be9898-c6df-42ab-87c7-08ffbb839bc6": "What is the main purpose of Creative Beam Search as proposed by Musolesi et al. (2024)?", "0e5e230f-922c-4eb3-a4f8-053484ea38f6": "How does the LLM function in the Creative Beam Search method introduced by Musolesi et al.?", "d2ecea54-5e2d-44eb-93a9-731ea99e47a4": "What role do LLM judges play in the iterative refinement process described in the context?", "a1e171d7-6825-4e64-8925-396fefd9ae71": "Which specific task criteria are mentioned as bases for analyzing the initial response to provide feedback?", "51f1b6f4-5e8f-49f0-8e64-c784bdfbca3c": "What is the main purpose of the SELF-REFINE method introduced by Madaan et al., 2024?", "99ba0000-cdf0-4058-85ef-8b5691384aad": "How does SELF-DEBUGGING improve the performance of LLMs in code generation tasks according to Chen et al., 2023a?", "bff98321-794f-49bd-86e3-a532d53a074e": "How does the REFINER framework improve the reasoning performance of large language models?", "1e93aebe-e5c1-4af3-8d55-5de5cb5d79ae": "What is the purpose of the multi-agent collaboration strategy proposed by Xu et al. in enhancing LLM reasoning abilities?", "156a27ca-4d60-46e6-8faf-2ba97581fb93": "What are the three stages of the review process framework?", "1ec81edd-a2da-4119-879a-3e9c2ecf8b97": "How is the final result determined in the review process?", "cf385023-fae7-4ec6-a6e6-6a53f9678f09": "What do research studies by Huang et al. (2023) and Tyen et al. (2023) reveal about the self-correction capabilities of LLMs?", "6ee63bdf-b42a-49e0-95af-8340fdbedaa3": "Why should the limitations of self-feedback in LLMs not be overlooked despite ongoing improvements in feedback and correction mechanisms?", "e9bb97d6-4274-46ef-9434-11608c6fc533": "What concerns do Valmeekam et al. (2023) raise about the effectiveness of LLMs as self-validation tools?", "2c8dda00-969f-4165-a989-282d05be6bf4": "What future research directions are suggested to improve the performance of LLM judges in complex reasoning tasks?", "4c1a7ffc-329b-4abc-8125-b1000e218b61": "What role do LLMs-as-judges play in the data collection process for machine learning systems?", "4ea46be4-f1b7-47af-8e51-d392ee391904": "Which two key perspectives are explored in the context of LLMs-as-judges' impact on data collection?", "2e64c102-ede8-4299-875c-fd8f6309792e": "What role do LLM judges play in the data annotation process described in the context?", "a08f4594-12d0-4c46-9755-792eba77d60b": "How does leveraging LLMs improve the efficiency and quality of labeling large, unlabeled datasets?", "07ee477f-935c-4b33-a402-e0513e963e9d": "How did GPT-4's accuracy in text annotation tasks compare to that of MTurk workers according to He et al. (2024a)?", "7b672a8c-9a97-4a76-b587-7830553b34a2": "What tasks did ChatGPT outperform crowdsourced workers in, as demonstrated by Gilardi et al. (2023)?", "862b2e4f-75ac-4707-8cbd-53f048ed3abc": "How did ChatGPT-4 perform in classifying Twitter users' political leanings compared to human classifiers according to T\u00f6rnberg et al. (2023)?", "0a1730cd-9e64-4be8-a4ae-32514d334c11": "What did T\u00f6rnberg et al. (2023) find regarding the bias levels of ChatGPT-4 in political classification tasks?", "67feae3a-fc0b-47a6-8f7c-04d377d3bc33": "How does the FullAnno system utilize the GPT-4V model in the process of image annotation?", "db365c25-751d-4d47-a7a9-af9472f7ed9f": "What impact does the multi-stage annotation process have on the quality of image descriptions in FullAnno?", "a7fec317-92d6-4866-b7c3-bcd353854553": "How did Latif et al. demonstrate the impact of LLM-annotated samples on speech emotion recognition models?", "44c533ef-fb46-4e9b-a3fa-a1a5a899ca7a": "What additional information was integrated with LLM-based annotations to improve their effectiveness in multimodal annotation tasks?", "e9a781b7-2f32-4636-b9b4-1d7a4b1ecd58": "What is the main purpose of the \"explain-then-annotate\" method introduced by AnnoLLM?", "7a5eff36-8789-40e9-ab53-3a41a01e4432": "How does prompting the LLM to justify its label assignments impact annotation quality?", "a7b957b3-e650-489d-9ea6-62cd331977af": "How does the LLMAAA framework utilize active learning to improve annotation efficiency?", "60f54333-5b72-4c5b-868b-b9d707274a4c": "In what ways does the LLMAAA approach mitigate the effects of noisy labels in task-specific models?", "055285fd-7720-4908-9dc0-be201d472214": "What is the primary goal of Data Synthesis as described in the context?", "3b4cc1c4-d815-4537-811d-bf5ceb45485c": "How does Data Synthesis contribute to improving a model\u2019s performance and data privacy?", "e5d930c7-ec2d-4b7d-a76f-ad3f1c7c739a": "What are the key methods mentioned that have improved the alignment capabilities of LLMs using small amounts of labeled data?", "658bad98-976e-4a53-9687-bf1a820e0a3c": "How does the multi-agent workflow introduced by Arif et al. contribute to the generation of preference datasets?", "4ae5ca0c-46c9-4025-9d1b-8355479230ff": "What is the main method SELF-INSTRUCT uses to align pre-trained models?", "155f927e-bf11-445e-b654-f90065354046": "How does Evol-Instruct improve large language model performance compared to SELF-INSTRUCT?", "4409c2a6-ff80-4c92-8032-61bbd3b89fe4": "What methodology does STaR (Zelikman et al., 2024) use to improve model performance on complex reasoning tasks?", "f4861293-8689-407c-a362-f6234e80ae22": "How does ReSTEM (Singh et al., 2023) utilize the expectation-maximization framework to enhance problem-solving capabilities in large language models?", "dcbf393c-ab29-4704-842d-7e508f11a43f": "What are the three broad methodological approaches categorized for using LLM judges in evaluation tasks?", "b2bdea97-2f9c-4848-836f-7dcfe88d9fe0": "Why is it important to consider methodological approaches when using LLM judges?", "95d896d9-0ec7-4b78-b6a1-a79ae0d17a62": "What are the three fundamental components of a Single-LLM System as described in the context?", "43e778fb-5503-4ad1-aa61-ef780afa4b24": "How does the effectiveness of a Single-LLM System depend on the LLM and input data processing strategies?", "3b805878-9e9d-455d-94f8-7c9930e49da5": "What is the main purpose of prompt engineering in the context of LLM judges?", "ff86c3da-2610-463a-97cb-453b01d47d1b": "How do well-designed prompts impact the need for extensive model training?", "95c86df9-eee1-444b-81c8-3047d88cf457": "What is In-Context Learning (ICL) and how does it enable large language models to adapt to evaluation tasks?", "691d68e3-a808-4a34-b399-e38986233899": "How does GPTScore utilize the few-shot learning capability of generative pre-trained models to evaluate generated text?", "5ee298f8-39db-48fc-9a30-befa6df3954b": "What is the main approach used by LLM-EVAL to evaluate open-domain dialogue?", "d73e4469-03ec-45e9-b9ff-654ae4324dde": "How does TALEC allow users to customize evaluation criteria for LLMs in specific domains?", "f7ef97db-cba8-4218-ab35-33a226f5db4e": "What is the purpose of the In-Context Learning-based Evaluator (ICE) proposed by Jain et al.?", "2cd13b7f-a3d8-4489-b83a-9c562378692d": "How does ICE utilize large language models (LLMs) and in-context examples in text evaluation?", "05f768d0-ded1-4b7d-b92e-25ec38dfb621": "What is one major challenge associated with In-Context Learning (ICL) mentioned in the context?", "9c4a4eb5-76f5-4dea-a1b1-5000062cc5d0": "How can the selection of prompt examples affect the model\u2019s responses in ICL?", "f97eb5ad-58f3-4c40-87ff-0dc922e3d7a4": "What is the purpose of the ALLURE protocol proposed by Hasanbeig et al. in the context of ICL for LLMs?", "380e5367-b8e1-4ec1-8355-60787d1275ff": "What mitigation strategies did Song et al. propose to address symbol bias in LLM evaluators using ICL?", "c344a9d8-1191-4390-b2c3-d95754a31111": "What is the main purpose of the step-by-step approach in evaluation tasks?", "6ede6812-0359-4868-979f-8755141b5394": "How does G-EVAL enhance the Chain-of-Thought method for assessing NLG outputs?", "80debed3-6f43-482c-ac7d-050135c97332": "What is the main purpose of the step-by-step framework introduced by ICE-Score (Zhuo, 2023) in code evaluation?", "bfdce53b-3bfe-41b6-a7a7-49f7c2657a9e": "How does ProtocoLLM (Yi et al., 2024) utilize a step-by-step approach in its evaluation process?", "3ccdd1b1-576f-4a1a-b4a8-5a69e01afa14": "How does Portia divide and process answers for evaluation by the LLM?", "7e164444-afbc-4b07-8f9c-0488910fc02e": "What advantages does Portia achieve according to Li et al., 2023d?", "6349b933-cc0e-42c7-b1bf-658f9e6161b3": "What are the two steps involved in the \"explanation-rating\" approach to evaluations mentioned in the context?", "fa2f298c-d1de-4f5e-875d-178835a69d8c": "According to Chiang et al. (2023), how does combining rating with explanation or explanation with rating affect the quality of LLM evaluations?", "6f492b67-34d9-4032-bb3d-daf90a406ac3": "What are the specific dimensions into which FineSurE decomposes the evaluation of text summarization quality?", "90f9d102-32c2-41c6-9cbb-c6da6da4cee7": "How does FineSurE improve evaluation accuracy compared to traditional methods?", "fa355fb3-e563-4372-adc5-5ff29358278e": "What is the purpose of the Enhanced Definition approach in Definition Augmentation?", "4e33464c-6344-4247-acf6-77ae9f2e8083": "How do some studies improve the evaluation criteria in the Enhanced Definition approach?", "c7976643-5832-4a69-8fd2-a1247c779b78": "What are the key stages involved in the AUTOCALIBRATE method proposed by Liu et al. for calibrating LLM-based evaluators?", "209d9bba-88a0-4440-b8d3-2d348f482fdf": "How does SALC improve upon traditional static, human-defined metrics in the evaluation of natural language generation?", "c1206a83-05aa-4363-9e42-6f1cf3e56fb7": "What is the key innovation introduced by the LLM-as-a-Personalized-Judge approach according to Dong et al., 2024b?", "46fd43bd-14fd-4fd7-b205-0971667458ef": "How does incorporating diverse evaluative roles and principles affect the performance of LLMs in evaluation scenarios?", "99866b18-e3e3-41dc-8ca6-446783e2a494": "How does Definition Augmentation utilize external knowledge to improve the reliability of LLM-generated outputs?", "8d000fb5-a71d-451b-96de-4801c096e1c4": "What did Chen et al. (2024c) discover about the self-preference effect of LLM judges within retrieval-augmented generation frameworks?", "130150e4-54d5-4221-a883-886597207222": "What is the main advantage of multi-turn optimization compared to traditional evaluation methods?", "921ede14-0365-4ac3-8688-cb50be09e37f": "How does the ACTIVE-CRITIC approach enable LLMs to improve evaluation quality?", "0ec368bf-fafe-4616-92fc-ea5b5eea9483": "How do LLMs function as question designers in the evaluation process described by Zhao et al. (2024b) and others?", "fddc7a44-d34a-4f97-a9a5-3ee2073ec011": "What advantage does the real-time adjustment of questions and task design provide in the assessment of evaluated entities?", "110a7e43-e3b6-4a46-a40e-653939be466a": "What is the purpose of tuning a pre-existing LLM on a specialized dataset according to the context?", "2c6076cc-8ccf-4457-bfad-c38b2ea16638": "How does score-based tuning improve a model's performance in judgment tasks?", "31d027a6-436f-4c8b-92c9-c9f3e6382948": "What is the purpose of fine-tuning LLMs on human-labeled datasets in the context of LLM-as-judges?", "72875e3a-489f-459c-adcb-414a358954d2": "How does PHUDGE, fine-tuned from the Phi-3 model, improve the evaluation of LLM outputs?", "3450bc38-0f3a-4254-b194-0288bd743b90": "What is the main purpose of the ECT method introduced by Wang et al. (2023e) in relation to lighter models?", "ecfeaca7-643e-4866-b8ba-f7483e6a3478": "How does AttrScore evaluate attribution and identify attribution errors according to Yue et al. (2023b)?", "f287b17e-66cf-408e-8c77-2bdee7a9d6b1": "How can LLMs improve their decision-making alignment with humans according to the research?", "b5877ef4-4148-4d6c-81d4-46132c1c5fd6": "What role do human-constructed datasets play in the fine-tuning of LLMs?", "e6873844-6005-4f0b-bd0d-ac5836599a54": "What is the purpose of using synthetic datasets like SorryBench in fine-tuning models?", "462c5fc6-9c22-4dc1-8c27-2016d11adaef": "How does the SELF-J framework evaluate LLMs without relying on human-annotated quality scores?", "8e7c2ec2-540e-42be-ac0a-94deb35db279": "What approach does FENCE use to enhance factuality in language model generators?", "52f0d9cb-c4c8-4c5d-b49d-e7d501abc1a5": "How does ARES utilize prediction-powered inference (PPI) in assessing RAG systems?", "f67cb6ed-39ac-44bb-a907-eeb5333c8807": "What is the main focus of preference-based learning in training large language models?", "39d2520b-0e3e-45a9-8259-7e7281286067": "How does preference-based learning contribute to the evaluation capabilities of LLMs?", "1231b987-0c9e-496f-b4ed-bf15a5273435": "What is the main advantage of using Direct Preference Optimization (DPO) in training large language models (LLMs)?", "5c5f3b33-7bfd-499d-b695-a6a04b871796": "How does Con-J utilize DPO and SFT losses to align LLMs with human values?", "28ad1071-db67-4376-ac8b-532600f5f569": "How does JudgeLM address key biases in the fine-tuning process when evaluating other LLMs in open-ended scenarios?", "e38ef16d-0717-4f47-8d37-732c1df69579": "What aspects does PandaLM focus on beyond the objective correctness of responses in its training?", "8f706c1c-ca12-4dfe-b24f-04f6ee2714bb": "How does the Self-Taught approach enable LLMs to become effective evaluators without human-annotated preference judgments?", "e49813ba-d40d-4d4e-84f5-e1cf0bd7cc1d": "What are the key features of FedEval-LLM that help it provide domain-specific evaluation while preserving privacy?", "14a5a59c-0a04-4604-ba97-86f82b85ff73": "What are the key features of FLAMe that improve its generalization to a wide variety of held-out tasks?", "0d4a1103-132f-43b0-8b6e-7a062e43dc5c": "How does AUTO-J address challenges related to generality, flexibility, and interpretability in model evaluation?", "94446cb1-7659-4197-ba90-52342a467d74": "What approach does Shepherd use to critique and refine the outputs of large language models?", "b382e6c6-596d-420a-8206-61c24fd0c7c8": "How does X-EVAL enhance instruction tuning in the domain of natural language generation (NLG)?", "ca38827a-9e68-469d-a289-84dbf7154cc7": "What method does CritiqueLLM use to generate high-quality evaluation data?", "86589b88-31df-433a-8728-583979f13ae6": "How does Self-Rationalization improve LLM performance according to the context?", "8cae3e59-29fd-4759-943d-f50145831131": "What methods does Zhou et al. (2024c) propose for bias reduction in closed-source and open-source models?", "f987bf07-c104-4765-8f9a-ab8750f8599c": "How does HALU-J (Wang et al., 2024b) improve hallucination detection in large language models?", "9b652269-783b-4858-bbf3-7e0302f0a210": "What are the key features of PROMETHEUS and PROMETHEUS 2 that enable them to perform fine-grained evaluation across diverse scoring rubrics?", "a97f2c79-5fe7-4675-9f6a-b680212b7cd7": "How does LLaVA-Critic contribute to the evaluation of multimodal tasks and what potential does it highlight for open-source LMMs?", "b32f24e5-49c9-486b-9c34-58e3123fa75a": "What are the different annotator types used in the fine-tuning methods listed in the table?", "1fdc29f1-3b7a-41ae-b46d-be466453e0a0": "Which base LLMs are associated with the fine-tuning methods ARES and AUTO-J?", "71649ce9-6e8e-4e06-9077-8c005ab3f4b4": "What types of data sources and tuning methods are used in the Con-J and CritiqueLLM models as described in the context?", "4d3f9690-a983-4adc-87c4-642c435085d8": "Which models mentioned in the context utilize pairwise evaluation methods and what are their primary application areas?", "de8b0072-44b7-4dda-9f23-44d48a786e46": "What types of training methods are commonly used across the models listed in the context?", "bf4cb7c3-5498-4847-a14e-d4bf4cd62849": "Which models utilize both pointwise and pairwise evaluation approaches according to the provided information?", "510ff174-cf3f-4ece-8115-427908097e80": "What training methods and model architectures are used in the PROMETHEUS and PROMETHEUS2 projects?", "30a38d08-6c8c-4fe8-a9aa-decac638e4b8": "How do the datasets and feedback types differ between Meta-Rewarding and OffsetBias?", "076c8b86-47f0-4b3b-bfba-bc5c164d6317": "What types of models and training methods are used across the datasets listed in the context?", "817ec9b1-08ed-4e3c-b883-ab6ea0bfed18": "Which datasets involve both human and GPT-4 evaluations, and what are their primary focus areas?", "673d6085-6d86-4005-b8f1-75ccd2f9e032": "What is the purpose of post-processing in the evaluation process described?", "aeae371b-2328-4f23-ace7-849a9842e0dd": "How does post-processing contribute to improving the accuracy and reliability of evaluation results?", "5358b594-90f9-4866-9e62-3322c9e42a46": "What statistical methods does Daynauth et al. use to quantify the bias in language model assessments?", "69971fb6-aba2-4b5c-b7f9-f4faf30ee46c": "How does the recalibration procedure developed by Daynauth et al. address the discrepancy between human preferences and automated evaluations?", "c8a2d1a1-6e61-4543-9614-f9927d460afa": "What is the main approach used by ProbDiff to evaluate the efficacy of large language models?", "3fb77ca4-3006-4e36-befc-6f231498357a": "What are the two types of experts introduced in the Product of Experts framework by Liusie et al. for comparative assessment using LLMs?", "6960e83d-9e9a-4ae8-84e1-eb32d86ade3f": "What is the primary function of the CRISPR method in mitigating bias in large language models?", "9a6b69f5-e08a-40c7-bfbd-05a3d8bb8cdc": "How does CRISPR ensure that reducing bias does not compromise the pre-existing knowledge of the model?", "aa720248-9b0e-4434-b80d-695c1e217f10": "What role do text reprocessing methods play in improving the evaluation outcomes in LLMs-as-judges?", "eb5c2d92-1c3a-4478-baab-7660a26a986a": "How does the multi-round evaluation process described by Sottana et al. help reduce subjectivity in model performance rankings?", "40e645c9-1a4c-4b1d-9e1f-a342e4050c9d": "What strategy does AUTO-J employ for single-response evaluation according to Li et al. (2023c)?", "649baa09-8ebc-4f2b-8f01-f3d047bcb8a7": "How does the post-processing method introduced by Yan et al. (2024a) improve the consolidation of relevance labels generated by LLMs?", "086d5676-89e3-451d-81ae-815d24084b4e": "What is the main purpose of the REVISEVAL evaluation paradigm introduced by Zhang et al. (2024b)?", "295ea405-9ba4-4e69-82a5-73bd60f50306": "How does the \"Habermas Machine\" developed by Tessler et al. (2024) aim to facilitate democratic deliberation?", "3ed20e6f-3502-46e4-b66a-05e846fb290e": "What is the primary focus of the task transformation methods discussed in the context?", "72151c4c-a0f2-4cc6-ad8d-6bc4b89e423d": "How do Ren et al. (2023) improve the quality calibration of large language models in open-ended generation tasks?", "17c3ee42-fd77-48a6-bcc1-3c6f87f4fa2c": "What is the purpose of the Open-LLM-Leaderboard introduced by Myrzakhan et al. (2024)?", "9ff0b2f1-9870-49ac-b576-423063ed2c48": "How does the Open-LLM-Leaderboard address the issues found in multiple-choice question evaluations?", "be925389-5c5b-4310-9cd3-28fdf473c43f": "What are the main benefits of using a Multi-LLM system in evaluations?", "c847b747-3259-4c3d-81a4-b7a113252e63": "How does inter-model communication contribute to the effectiveness of Multi-LLM systems?", "c955ce3c-d16c-4acd-8d88-f63d33eade19": "What role does communication play in the judgment process among LLMs?", "4d6e5ed0-82ee-409f-8bd5-078bc9ba59ff": "How can the Multi-LLM system benefit from interactions between LLMs?", "370f8b69-c809-4cd2-a5ff-648a8b5dde49": "What is the purpose of multi-LLMs working together through information and rationale sharing?", "1097d8e5-88c3-4127-80cd-3af6527a16fb": "How does the multi-agent collaboration strategy introduced by Xu et al. (2023c) enhance complex reasoning in LLMs?", "6e0fa94f-5b5f-404a-a304-0dbfbd2b1dfd": "What roles do the four agents in ABSEval serve in the overall workflow?", "fbd1e2a3-a159-483b-9daa-a68aaa0ac412": "What is a potential risk of using multiple similar models in cooperation according to the context?", "a9946c3e-6092-407f-a798-81794cb67df7": "What are the two types of structures that multi-LLMs systems can be categorized into?", "09b6b366-8f9b-4958-b019-6239a92c70f1": "How do multi-LLMs systems utilize competitive or adversarial communication?", "75fc90af-1366-4cd7-a70b-185ee149e57d": "What role does the central LLM play in the centralized structure described in the context?", "f551b460-4a11-4032-873b-94109f669ccc": "How does the Auto-Arena framework evaluate LLMs according to the provided information?", "93e76e2c-2ce1-47dd-9595-1cb06998be1e": "How does the decentralized structure promote resilience and distributed decision-making among models?", "24cd256c-882f-4466-a9c1-107433afe2fb": "What aspects does the automated benchmarking framework introduced by Moniri et al. evaluate in LLM debates?", "41c9e079-993a-4c57-8a71-83228c1d2683": "What are the key features of the ChatEval framework that contribute to its improved evaluation performance compared to single-agent methods?", "3f4c1602-5134-4956-b899-01b908e3f6c2": "How does the PRD approach address issues such as self-enhancement and positional bias in LLM evaluation methods?", "687c7205-dd75-4db8-94df-627b9e266092": "What are some aggregation strategies used to synthesize judgments from multiple models in multi-LLM systems without communication?", "d7159312-e6f4-4b1f-b841-5d086d7c64e3": "How do aggregation methods like majority vote and weighted averages contribute to the final decision in multi-LLM systems?", "b5e17291-55d0-4d1f-afd8-b9c95d21bce2": "What is the main advantage of using majority voting in synthesizing evaluations?", "df622895-8d04-43a7-8890-dee5941257cb": "How does the reference-guided verdict method introduced by Badshah et al. improve evaluation accuracy?", "6d452ac4-58cb-47dd-a4f5-23be2317bcc4": "What evaluation methods does PoLL use to reduce intra-model bias when assessing LLM performance?", "7934daf9-439d-4fa1-8635-8d9c08d5e39b": "How does the Language-Model-as-an-Examiner framework utilize language models in its peer-examination mechanism?", "991ab183-4af9-4cb4-9e71-b4ecc7811bd0": "How does the cleansing strategy used in MULTI-NEWS+ leverage CoT and majority voting to improve dataset quality?", "41e952a4-23a9-45d0-a32f-99e747cb0af1": "What role do multi-LLM evaluations play in the development of the MULTI-NEWS+ dataset?", "d9d42405-d1c0-4195-96d1-49a02d45c51b": "How does the peer-review mechanism in PiCO enable LLMs to evaluate each other without human annotations?", "7b3323ec-091f-4582-98d8-14080bb71fd5": "What is the objective of the constrained optimization problem formalized by PiCO in the context of weighted scoring aggregation?", "328a3654-c460-4b1a-a563-6e82c202b7b5": "How does PRE select qualified LLMs as reviewers for evaluating text summarization tasks?", "4b674d60-54d8-47ca-892d-84141518254f": "What method do Zhang et al. propose to improve evaluation accuracy and stability in recommendation explanations?", "a447cab6-0bf3-4b3d-a910-54d16211e949": "What is the main approach used by AIME to generate evaluations using multiple LLMs?", "70a1b9ae-7aef-47cc-9555-d6fc5b66735a": "How does HD-EVAL improve evaluation accuracy through Hierarchical Criteria Decomposition?", "59fc1585-8584-4f76-a2d3-7d2933a53929": "What are the two calibration methods introduced by Gao et al. (2024) to address win rate estimation bias in evaluating text generation quality?", "df3fec21-218c-4c6e-8a78-2eba212b71aa": "How do Bayesian methods and graph-based approaches improve the handling of uncertainties and inconsistencies across multiple evaluators?", "3a961e14-e27f-4313-a695-bd5125206f93": "How does GED address inconsistencies in LLM preference evaluations?", "8484d511-9b08-4bf7-8c89-6dc1bf530f4c": "What role does the DAG structure play in GED's method for evaluating preferences?", "0e899ed1-e842-4e55-96fb-2f8ea231aaf3": "What is the main purpose of the LLM-based aggregation framework mentioned in the context?", "42f49760-f504-4961-9066-bca2308e910c": "How does the LLM-based aggregation framework improve the evaluation of natural language systems?", "8a9d0a80-2537-4acf-8a26-dc23fb38b32d": "What is the main advantage of using a tiered cascade framework in model evaluation?", "082cce7e-8bfb-45b1-a58a-5b3882c00691": "How do the approaches proposed by Jung et al. and Huang et al. differ in addressing model evaluation challenges?", "41281500-cc11-4734-b116-17c33b793baf": "What roles do human evaluators play in Human-AI Collaboration Systems?", "fdf43c0b-77e3-4926-88e6-88ae19656c88": "How do Hybrid systems benefit from incorporating human insights in high-stakes domains?", "175ccf4e-7a81-4bd5-ac29-95de8087c6d4": "What role do human evaluators play in Human-AI Collaboration systems during the evaluation process?", "14596261-86aa-459e-9d82-0c4f8ca1bab5": "How does the COEVAL system utilize both LLMs and human evaluators in its collaborative evaluation pipeline?", "eeb35cb0-5f93-4685-b348-a7d0badd30a8": "What are the three strategies proposed by Wang et al. (2023b) to address positional bias in LLM evaluators?", "27cd4dbf-8bc4-47fe-9b77-5d93ad2ef644": "How does EvalGen (Shankar et al., 2024) use human feedback to improve the evaluation process?", "a2150aee-0fee-4141-8b30-5b151acb3afe": "How does human involvement differ in the evaluation process described compared to other systems?", "cb7446a9-f396-4845-8125-b20d067ebf0a": "What role does EvaluLLM play in improving the evaluation results and trust in the model\u2019s performance?", "b485b215-07aa-4c84-afc4-1b77aa6cfaef": "How did Chiang et al. implement LLM TAs in the assignment evaluation process at a large university course?", "dfbc012d-f75f-4710-a4db-387768ffb947": "What role does human oversight play in the evaluation process after LLM-generated feedback is provided?", "2fc97456-b881-4109-8e22-5c59fc4b7c43": "In which specific domains have LLM Judges been widely applied according to the context?", "c32d14f2-8317-4332-bf26-1aece882f6b2": "What is the main focus of the application section regarding LLM Judges?", "27cb7832-7697-451e-a562-9f304079e1dd": "What evaluation criteria are emphasized in dialogue generation tasks according to the context?", "65aebc0b-eb32-4719-b6f8-bc9c298d1b37": "How do the evaluation criteria differ between summarization and translation tasks as described?", "0631cb68-8ba6-45bc-bb92-a86aef730c35": "What are the four dimensions used by Fusion-Eval to evaluate summary quality?", "11bc41cd-f97c-414d-b4f1-602e5bed2cdc": "How does Fusion-Eval leverage LLMs to improve evaluation methods beyond traditional metrics?", "02addd50-0d47-4ea1-9188-a4e640c1e07b": "What evaluation criteria did Xu et al. (2024b) focus on in their study involving the ACTIVE-CRITIC framework?", "16ea7acd-8f41-4ac6-a67b-b5466ffdb28d": "How does the ACTIVE-CRITIC framework improve performance in the story generation task?", "fd3a7263-6a26-4069-9f32-4354afb962ba": "What types of information are included in the evaluation objects of LLMs in the multimodal domain?", "0e0e863e-f23e-49bc-9054-780be264343e": "What is one of the primary challenges in evaluating multimodal tasks according to the context?", "3c76f5e1-8483-43fd-a277-e03ce635e56c": "What advanced techniques are mentioned as necessary for helping LLMs integrate different forms of information for accurate evaluations?", "c5179ea0-fa20-48db-86ab-3e20b3f99010": "How have LLMs been applied as judges in multimodal scenarios according to the examples provided?", "bc848f0c-f6bf-4dba-8fb5-51fc2fb47c26": "How do recent studies suggest multimodal LLMs can improve their performance without external evaluators or human annotations?", "7bbe5377-a88e-47d9-a247-783047afbe9c": "What aspect of speech processing is highlighted as achieving remarkable accuracy in the given context?", "07c17624-abd8-4b6c-bd7a-84c06ff82b86": "What is the purpose of the CODA-LM benchmark proposed by Chen et al. in the context of autonomous driving?", "7e23f612-ff93-47ea-b11e-4ec7e9cb809a": "How did text-only LLM judges compare to Large Vision-Language Models (LVLMs) in aligning with human preferences according to the findings mentioned?", "a73f872f-97b4-431a-ace7-1691f9e08ddf": "What are some key areas in the medical field where LLMs-as-judges have shown significant potential?", "f9828d68-5ec1-43fb-bffa-eee1489578ef": "What capabilities must LLM judges have to provide high-quality evaluation in the medical domain?", "68f1bba2-e620-4461-a6dc-fbea8bb0ca6c": "How did Xie et al. utilize LLMs in evaluating medical texts, and what specific aspects did they focus on?", "93a7885e-649b-47e6-8744-9f0609e7252c": "What were the findings of Brake et al. regarding the use of LLMs like Llama2 in assessing clinical note consistency?", "d96859b5-78f7-41dd-9bc5-2deb8b117d12": "What aspects are important for ensuring clarity of expression in communication?", "76559ec9-946e-4b7c-8e6e-55e024b9788d": "How does the use of appropriate terminology affect the relevance of a response to a question?", "ec6c0f68-f844-4313-9882-c14fc02b2955": "How did Li et al. (2024c) utilize large language models (LLMs) in the evaluation of mental health counseling effectiveness?", "74706422-bf68-4dd9-a6a1-b3a50d1b5974": "In what ways have LLMs been applied to support specialized medical reasoning tasks according to Jeong et al. (2024)?", "9a6bfa74-f489-4136-8153-247535f8eff1": "What topics are covered under section 5.4 Legal?", "a97ad9fc-b69e-4b60-aa83-248ad55b914f": "How does section 5.4 address legal considerations?", "c8b1ac7c-5303-4639-8352-b705e6ee23f8": "What are some key scenarios in the legal domain where LLMs-as-judges have been applied?", "db59606e-a910-4e3f-acf2-640101513857": "Why are interpretability and transparency important in the evaluation results of LLMs used in legal practice?", "5a31b42d-00db-477b-9d46-0631a9b90bc4": "How can bias in legal evaluations by LLM judges affect judicial fairness?", "cc331d52-69ce-4c23-8fb9-6da612896fe3": "Why do LLM judges require higher standards compared to other models?", "49f384da-3dce-4bc8-92df-7698a43f2a1e": "What criteria did Yue et al. use to evaluate the performance of their legal LLM model DISC-LawLLM?", "1051ba44-0b77-47a6-9876-a3ca406fae28": "How did Ryu et al. assess the performance of LLMs in Korean legal question-answering tasks?", "43e91d96-162d-41cb-a13d-03e9045079ba": "How have LLMs been utilized in constructing domain-specific evaluation sets for the legal domain according to Raju et al. (2024)?", "b667ffbe-383a-4a4d-9699-d2fa0423980c": "In what way did Ma et al. (2024) use LLMs to improve legal case retrieval systems?", "090d8c5c-3ee5-4339-9492-3d84eb0fb243": "What potential benefits do LLMs-as-judges offer in the field of law?", "cd68201d-ccd0-481b-9425-701731c55679": "How might the use of LLMs-as-judges impact the future of legal proceedings?", "398c1c4d-4636-40dd-8d4a-bba2b4b3ab7e": "What are some of the unique challenges faced by LLMs in financial risk assessment?", "5d6c177a-8fd6-4534-ad56-639b77f4130f": "Why is real-time processing important for LLMs in high-frequency trading scenarios?", "a52ba0e7-1312-461f-a37e-82536552dc62": "What advantages do large language models (LLMs) offer in investment risk assessment?", "6c97c0ee-2406-4207-a371-cb2fe47bc2c6": "How does the FinMA model developed by Xie et al. improve financial evaluations?", "cb84f40b-2d55-4a5f-914b-04165b2daca5": "How do LLMs improve the accuracy of credit scoring assessments in the financial domain?", "66a6c84b-a66e-4a0a-80d6-2d0d3206097c": "What types of unstructured text data can LLMs analyze to enhance credit scoring, according to Babaei et al.?", "0b0f13df-a595-42c8-8d98-4c9dd893f20d": "What is the purpose of the automated financial evaluation benchmark developed by Son et al. (2024a)?", "08d5b906-7f21-4f20-b903-fbebd1b35129": "How does the framework created by Son et al. support the evaluation and optimization of financial LLMs?", "a634ac46-6ffb-4425-b63f-20b1085c0616": "What are some of the key challenges faced by LLMs when used as judges in educational assessments?", "bb38782e-a182-4935-be9c-a04733a12b35": "Why is it important for LLMs to consider factors beyond correctness, such as creativity and clarity, in educational evaluations?", "8010103e-2ea3-47a6-b9f7-17f2be43116c": "How can certain factors significantly impact students\u2019 development?", "ca69096c-4d8c-4e0e-9c2b-e129bf464e0c": "In what ways do these impacts influence students\u2019 future opportunities?", "841412dc-dbbe-4246-9682-71634636d435": "What role does the LLM Teaching Assistant (LLM TA) play in university classrooms according to Chiang et al. (2024)?", "8984a91d-5307-449d-84be-62db0e5fdffe": "How does the LLM TA ensure consistency and robustness in grading student assignments?", "a77e4024-4879-495c-9b04-2e63a3dfecdf": "What approaches did Wang et al. propose for automated essay scoring and feedback generation using LLMs?", "58ea0138-a3f7-49e8-aaee-df3a6fe88d82": "How did Zhou et al. evaluate the use of LLMs in academic paper reviewing tasks?", "1ddc67c7-2531-4827-9def-59109c426fb6": "What limitations do LLMs have in fully replacing human reviewers according to the context?", "537cd948-b3ae-45b1-80a5-47a09b66fea2": "In which areas are LLMs currently insufficient for automated reviewing?", "82262be5-572a-43cc-8e62-cf172a6bc340": "What additional aspects of the reasoning process does ReasonEval assess beyond the correctness of the final results?", "ce81a6a6-a52f-4315-8463-41c2501ccf1e": "How does the methodology proposed by Xia et al. utilize LLMs-as-judges in evaluating math reasoning?", "559c841d-1bd6-4216-8951-9cba0bc60769": "What are the main aspects assessed by Debatrix when using LLMs to evaluate debates?", "dadfcd8d-8811-4546-a3ab-8102a3dab837": "How does Debatrix leverage LLMs in judging debate performance according to Liang et al. (2024a)?", "314d0ff4-590a-43ce-a522-2b08caaeb2b0": "What are some of the challenges involved in evaluating information retrieval systems?", "a72e5a4c-c76c-40f9-9c21-c38bb3e89b7a": "How are large language models (LLMs) utilized to address challenges in information retrieval evaluation?", "76fd7169-9904-4a8e-b6cb-4cc833a8bbd0": "What is the traditional method for evaluating the relevance of retrieved results to user queries in information retrieval?", "bba7071c-12a4-4ed2-8226-6dd8ba3fc6bc": "How does the LLMJudge framework proposed by Rahmani et al. improve the evaluation of information retrieval system results?", "890b88a6-d3cd-4433-bc43-9eeadca28cfc": "What novel method did Qin et al. (2023) propose for improving text ranking using LLMs?", "b3c546ca-7ca3-47db-b343-7c143316e05d": "How does the JudgeRank framework introduced by Niu et al. (2024) enhance the reranking of results in reasoning-intensive tasks?", "60d04203-a649-4a1a-96c2-cc0ff7e4d0ad": "What was the main goal of the approach mentioned in the context?", "3bad7a62-74bb-43c9-93e9-703419df5376": "How did the approach aim to improve the results?", "e3b64b1f-c4c5-4c0d-b5ab-f6ed3afa6f98": "What dimensions did Zhang et al. (2024a) assess when using LLMs as automated evaluators of recommendation explanations?", "eabf9c45-5ee4-4fd2-96a7-d681542ab4dd": "Why is explanation evaluation important in recommendation systems?", "7da15e82-39e8-456c-81d2-5ca4172f9094": "What challenges do traditional evaluation methods for retrieval-augmented generation (RAG) systems face?", "ba64719c-790b-49a7-bbf7-7363a72c2e9f": "How does the ARES framework proposed by Saad et al. utilize large language models (LLMs) in evaluating RAG systems?", "30606e91-0b06-444e-bd25-9f71ffbbf1c1": "What challenges do LLMs-as-judges face in the software engineering domain according to the context?", "0ff3c395-4a85-4478-8537-66c6c9b54dc8": "How have LLMs been utilized in evaluating code generation and bug report summarization?", "fef92a88-463f-4b6e-88cc-3574a173d757": "What are the main evaluation challenges in the biological field as mentioned in the context?", "b1151b8f-6bfe-436f-8119-c933bac0da50": "How did Hijazi et al. (2024) utilize large language models (LLMs) in evaluating query-focused summarization in biomedical literature?", "f9503905-c2c3-4716-b5ba-5cb5ae378e2f": "How did Tessler et al. (2024) utilize LLMs in democratic discussions within real-world human social contexts?", "85fd73b4-bf61-45ea-994d-6d8b03e747f6": "What is the purpose of the Sotopia framework proposed by Zhou et al. (2023b) in evaluating language agents?", "619b8f1d-8513-4922-a2a4-56f7c89dbb3d": "How do language agents demonstrate emotional understanding?", "fb2cb9fd-f94c-4c71-bda4-a7f399d2f5c4": "In what ways can language agents adapt their responses to improve social interactions?", "9ff35743-ac68-41ea-b2f1-791c9e9d3db3": "What are some of the key applications of LLMs-as-judges mentioned in the context?", "ae962f47-66e5-490a-8e24-26936a258915": "What challenges do LLMs-as-judges currently face despite their potential?", "ab610945-6f23-419f-8c63-f2933911e526": "How might the application of LLMs-as-judges improve model performance and domain adaptation capabilities?", "c68a6214-8024-46ed-ae1b-2de95b30a14f": "In what ways could the use of LLMs-as-judges become more widespread and precise across various domains?", "984bace7-0b05-40a9-aa82-db7559e79a8f": "What is the purpose of meta-evaluation in the context of LLM judges?", "17b61b55-5465-4a51-84b2-06c065df191a": "Which topics are covered in the chapter on meta-evaluation according to the provided context?", "738c38af-6e35-43a0-8a54-1a7e421a3adc": "What evaluation criteria are used in the HumanEval benchmark?", "abd691d5-89fe-4f7d-974e-535ac47c2e1d": "How many tasks are included in the CodeUltraFeedback benchmark and what types of evaluation criteria does it use?", "ad4c34ed-264f-49dc-8188-074f399829e5": "What are the different evaluation criteria used in the MQM framework for assessing translation quality?", "31a0989b-48e2-462f-9950-66fdd1a619eb": "How do SummEval and Opinsummeval differ in their focus when evaluating summaries?", "964a6e8f-062f-43a7-86a9-a9bc87533fa9": "What evaluation criteria are used to assess the Topical-Chat and Personal-Chat dialogue datasets?", "52e35ec8-e711-4868-b45d-507cd681edb2": "How does the DSTC10 Hidden Set dataset differ in size and evaluation focus compared to the HANNA story dataset?", "2a329767-5f7b-46ad-8840-08dce0dcb337": "What are the main evaluation criteria used in the MANS and StoryER datasets for story assessment?", "23b2dbd8-69fc-4665-a109-19ab05260095": "How do the PKU-SafeRLHF and HHH datasets differ in terms of their focus on value-related attributes?", "eb2382bc-58be-4141-8fcb-e28dfd93ac2c": "What are the evaluation criteria used in the Movielens_Explanation benchmark according to the provided context?", "5227c401-6ad4-4e26-a593-8acc583d1a09": "Which benchmarks listed in the context utilize a pointwise evaluation type?", "7b07bb10-0235-4446-a815-5d3797b4944b": "What are the common evaluation criteria used across the benchmarks listed in the context for assessing truthfulness?", "a915766e-673e-45be-839b-23fef266f28d": "Which benchmarks mentioned in the context include instruction-following as part of their evaluation process?", "1b749412-d0d8-4cd3-9c2e-98aa54d19a8b": "What are the main evaluation criteria used in the LLMeval dataset mentioned in the context?", "6e5b4ead-a227-42dc-b04e-f8dc576da7f2": "Which datasets listed in the context include Chinese language data for their evaluations?", "f4371f74-55ca-4230-84fe-0783ea553d3b": "What are the different evaluation criteria used by HELPSTEER and HELPSTEER2 for assessing language models?", "4a89ff33-3616-4072-aa1b-2966fc747954": "How does MM-Eval differ from the other benchmarks in terms of language support and evaluation approach?", "0bcba52f-716d-428a-9cbb-d2d02c5634c9": "What is the common approach used to evaluate LLM-based judges according to the context?", "8bbd3212-8486-4c6e-a237-a5a2d7366a05": "How are the 40 benchmarks categorized to facilitate comparison in the evaluation of LLM-based judges?", "a50f8960-50ee-428d-b80f-63ec5a112310": "What is the primary goal of code generation as described in the context?", "54ed23ad-2cda-42c9-8bd5-59c72084a8f1": "Why is evaluating code generation considered highly challenging?", "69d4e267-d4c3-4d46-9f18-5dbd6d818748": "What types of problems are included in the HumanEval benchmark dataset?", "58b9143f-1d4b-42ec-81d7-da6e8181aeba": "How do input-output examples in HumanEval assist in evaluating programming tasks?", "be5c8eec-deb3-4561-afa2-a57891d444f9": "What types of advanced operations does SWEBench (Jimenez et al., 2023) require models to perform?", "7ba42bc0-d2d2-4191-a77b-4c79b4fdcbb8": "What challenges are associated with evaluating models on SWEBench due to its increased complexity?", "db16b379-6e98-47a1-9239-5180fb042c7d": "What are the main limitations of existing benchmarks in software development as mentioned in the context?", "e02e887b-64a6-4d07-aa41-cd499577f0f2": "How many task requirements are included in the dataset, and what type of programming scenarios do they focus on?", "5a3a13f7-1770-491a-a4d1-afb0cd14a13f": "What is the primary focus of the CrossCodeEval dataset introduced by Ding et al., 2024?", "e2854152-645e-483f-8859-d3d403087372": "How does CodeUltraFeedback evaluate the alignment between large language models and user programming preferences?", "ce091045-27b7-40b5-b648-254c02b37b14": "What are the five distinct programming preferences used by GPT-3.5 to score responses?", "feee5af9-dcbd-404c-941e-98a97d1a6f53": "How does the dataset facilitate analyzing preference alignment in programming responses?", "a834a228-b45c-4d26-a925-c5139c3c817c": "What are the main stages of evolution in Machine Translation technology mentioned in the context?", "a64beffd-340b-4d6c-8237-ea0fba743e8a": "Why has evaluating translation quality become more complex with the adoption of Neural Machine Translation and Large Language Models?", "a49b3a66-5fa6-4638-886c-a98190b8f087": "What types of language pairs are included in the datasets provided by the Workshop on Machine Translation (WMT)?", "4630e4c8-e1b8-4b9c-832d-e7cdf1325d18": "How does WMT contribute to the assessment of automated evaluation metrics in machine translation?", "c2274d54-17a0-4eaf-a97d-9f34e292f810": "What is the primary focus of WMT in terms of language resources?", "7dec4bb7-28ef-49d0-a274-bfe6fb0083d6": "How might WMT's focus on high-resource languages affect its applicability?", "f0dbfbb5-8735-4fb0-a44c-3fdc60e8a77f": "What is the primary focus of the Literary Translation Comparisons dataset introduced by Karpinska and Iyyer in 2023?", "a4a7a28b-7754-4255-b6a1-9f35165d4054": "Why is the Literary Translation Comparisons dataset particularly useful for evaluating the performance of large language models (LLMs)?", "d30ac259-6c87-4d30-9b54-d6f25a8aa041": "What are the key dimensions evaluated by the MQM framework in assessing machine translation quality?", "16385e44-fc48-40d7-b3e0-ed3a8f8f3f32": "How does the MQM study differ from traditional machine translation evaluation metrics like BLEU or ROUGE?", "a74ba28b-1321-419a-b046-1f8b9e719a1c": "What is the primary goal of text summarization as described in the context?", "f4b96358-14ee-430f-b075-220cac01930c": "Why is there a need for robust meta-evaluation benchmarks in assessing the performance of large language models in text summarization?", "3b8144f0-0134-4350-a335-1db64c9223c1": "What are the four key dimensions used to evaluate summaries in the SummEval benchmark?", "de53d57a-28b8-486f-ab1b-cb4ede29c9a8": "How many independent crowd-sourced workers and expert evaluators annotated each summary in the SummEval dataset?", "7cd5ca1b-a8f4-4698-937f-18c575ae4dc8": "What types of factual errors are annotated in the FRANK dataset?", "4ead2c8b-9bd4-4751-aa43-2e5474514184": "Which source datasets are included in the FRANK dataset for evaluating summary factual accuracy?", "96496291-ab54-4324-bac2-63f7b0f9c005": "How can focusing solely on factual errors affect the overall quality of a summary?", "62504734-8216-4187-8712-a47fd06ef86a": "Besides factual accuracy, what other aspects are important for evaluating summary quality?", "3e8f5335-ae14-472e-8104-0c18b551aed6": "What is the primary purpose of the OpinsummEval benchmark introduced by Shen and Wan in 2023?", "8c389671-4f09-40d9-860f-7a1809f3033a": "Which four dimensions are used for human annotations in the OpinsummEval dataset?", "cac1c7ee-a07d-43a8-9246-13a450f28c9f": "What are the primary goals of dialogue generation in developing dialogue systems?", "510bc503-9ad4-44fe-95cf-5f782ce560b8": "Why has evaluating dialogue generation become more complex with the advancement of large language models?", "9b78a42e-9170-41fd-b72e-a88302b5a1c0": "What are the main objectives of the Topical-Chat dataset in dialogue generation research?", "9fd622bb-c87d-4ea6-9819-28b2711d943d": "How does the PERSONA-CHAT dataset contribute to generating more personalized responses in dialogue systems?", "d949c584-8543-475d-99a6-b0ec589cbb1b": "What six key dimensions were used by Mehri and Eskenazi to evaluate responses in their meta-evaluation study?", "d752d135-d444-4138-a68d-bebed082b751": "How many dialogue contexts and responses per context were manually annotated from the Topical-Chat and PERSONA-CHAT datasets in the study by Mehri and Eskenazi?", "22536de6-1b70-47d9-8911-285215b631f2": "What are the four aspects annotated in the DSTC10 Track 5 dataset for evaluating open-domain dialogue systems?", "66604244-f440-4b12-af66-dd9381c9adc9": "What are the main goals of the DSTC10 Track 5 challenge in developing automatic evaluation mechanisms for dialogue systems?", "4c795439-efad-407e-a024-966c22498b7a": "What are the main goals of Automatic Story Generation (ASG)?", "b0a0d048-f0e5-4683-9bc2-e8b314e6ec99": "Why is evaluating story generation systems considered a complex task?", "3d436c04-bb5e-4792-965f-0ced36f31223": "What are the six criteria used by human reviewers to annotate stories in the HANNA dataset?", "fb151dcb-b4c6-47b9-8d66-2fd94da98dc0": "How many stories and prompts are included in the HANNA dataset for evaluating automatic story generation?", "04cb7505-8618-4423-b887-03cd6fd7e079": "What is the primary focus of the MANS dataset within the OpenMEVA framework?", "056f5bb5-17c7-42a9-a899-0bb45ae74889": "Which well-known corpora are used to compile stories in the MANS dataset?", "27199a6e-fa43-42eb-aaf6-6dadfaba6b81": "What are the two primary components of the StoryER dataset?", "dbe51a96-6d02-4bdf-b4ef-59e7963eab55": "How does the 100k Story Ranking Data component of StoryER utilize user engagement metrics?", "51bd0ad8-9188-49ef-899f-61b7792d249d": "How many entries are included in the StoryER dataset?", "d2b9dcd4-6177-4d44-9dcd-5e99c0cf6e4e": "What types of story aspects do annotators rate in the StoryER dataset?", "7c178293-944d-4a37-94e4-67baad919136": "What are the two sub-datasets introduced by the PERSER dataset, and how are they derived?", "27ff4e08-7c29-40dc-b795-a24436caec08": "Which evaluation dimensions are used to assess the stories in the Per-DOC sub-dataset?", "cf14f59d-20c8-407f-a5f0-b20bafefb1cd": "What is the primary goal of values alignment in the development of AI systems?", "d5c245b9-cdf4-46db-9ea7-efdafb296302": "Why is values alignment particularly important in the context of LLM-as-Judge?", "c11b2de6-5b75-496e-bc91-821c2596a3a1": "What are the two primary dimensions focused on in the PKU-SafeRLHF dataset?", "efa9cb34-3023-40d2-9d70-2f5ab648fd5b": "How are the samples in the PKU-SafeRLHF dataset structured and annotated?", "9e8ff1b3-d2c8-477c-b350-4f75c583afa3": "What are the three core human-centered values emphasized by the HHH dataset?", "1c2c51ba-d983-4b11-ba8e-d304a5695a38": "How does the HHH dataset help in assessing the ethical standards of large language models?", "5299e82c-d702-4e69-82f2-ce1ac67d8623": "What are the two critical criteria focused on by the CVALUES benchmark for evaluating human values alignment in Chinese LLMs?", "62b93675-c91c-4e9c-ab51-6e9ad8065640": "How does the CVALUES benchmark contribute to the assessment of values alignment in the Chinese language context?", "07b874f5-c91d-4df0-ba1d-3b5f2836dd23": "How do large language models (LLMs) enhance the evaluation of recommendation systems compared to traditional metrics?", "e316bf9d-9bea-4fa5-a42f-8d425e697702": "What aspects of recommendation systems can LLMs assess beyond accuracy?", "cad07ff3-f6d8-42f9-8123-20dc7dfb35bf": "What are the four dimensions on which user self-explanation texts in the MovieLens sub-dataset are rated?", "f4d4a2f0-5af8-4946-8d56-a0ddb373e699": "How does the sub-dataset created by Zhang et al. enhance the original MovieLens dataset?", "e00552b6-81b9-413a-822a-8448a8293038": "What role do reference texts play in the explainability evaluation of LLMs?", "1ec814bc-fe99-49f0-908f-a8622f6e7a0d": "How can valuable reference texts improve the assessment of explainability in LLMs?", "7ef4bcce-128b-403a-8217-cb1c195481eb": "What types of data are included in the Yelp dataset mentioned in the context?", "29b1c837-f577-4f38-bef5-252f787e97d5": "How can the business attribute information in the Yelp dataset be utilized in recommendation systems?", "497cb6a9-ab5e-4c6c-a550-65e8a2a8d8b0": "What is the primary focus of the search task in information retrieval?", "f53473ea-2212-4be4-b8bb-55e524b98179": "How have large language models (LLMs) impacted the process of relevance assessment in search tasks?", "e60a0149-c5fb-4991-96d5-38230d93443c": "How has the role of LLMs as evaluators changed with the advent of retrieval-augmented generation (RAG) models?", "06085e20-8d12-47bd-8e42-ee456ebc1eec": "What dimensions of retrieved contexts are increasingly important to assess in the evaluation process of RAG models?", "f2f0dab7-714c-4dbc-9200-e5194959b2d4": "What is the purpose of the Text Retrieval Conference (TREC) workshops in the context of information retrieval research?", "ff8fd2db-7783-4f27-a6fc-f38622568157": "How do TREC datasets contribute to evaluating the performance of large language models as relevance assessors?", "93d64f82-77a0-4cf0-bf16-17a832366b74": "What are the sources of the datasets used in the TREC Deep Learning Track for 2021 and 2022?", "df23ae12-b8f9-417b-a534-4b3c0f1a1b21": "How are relevance judgments annotated in the TREC Deep Learning Track datasets, and what scale is used?", "3b91f0c6-8d22-4bf8-b4ac-6ba4edf01123": "What are the three distinct aspects of relevance incorporated in the LeCaRDv2 dataset?", "e80d724e-cd52-4636-ac91-6bc32df51cb0": "Why are domain-specific datasets like LeCaRDv2 important for specialized retrieval tasks?", "a57847d7-56dd-474a-be1a-942b6a3a042b": "What is the purpose of developing comprehensive datasets for LLMs-as-Judges?", "34261781-7fc3-4734-a537-9280170465c0": "How do comprehensive datasets contribute to the reliability and effectiveness of LLMs in their role as evaluators?", "0c0ce68c-20cc-4e48-9e17-800a70e1f691": "What are the main features of the HelpSteer and HelpSteer2 datasets that contribute to improving the alignment and usefulness of LLMs?", "1bd04c80-e957-4cff-9d15-e7884483897f": "Which sources contribute to the UltraFeedback dataset, and how many prompts does it contain?", "ed146216-7598-451d-b2f1-1f84b9b7b694": "What types of feedback does UltraFeedback provide for responses generated by different LLMs?", "245da9b9-e3c9-4fa9-ae64-158ac288bc85": "How does UltraFeedback contribute to the training of reward and critic models for LLMs?", "350ef013-9e08-40be-851d-a02d140877a7": "What is the primary function of AlpacaEval in evaluating chat-based large language models?", "7424642a-45f1-4ec0-8c04-df5accc06768": "How does Chatbot Arena differ from AlpacaEval in terms of evaluation methodology?", "c2ec2d85-1964-4a06-bbab-65bf5deb592c": "How does the platform use the Bradley-Terry model to rank LLMs and chatbots based on user votes?", "52193776-3336-4c1a-b30c-81e56ef99239": "What insights can be gained from analyzing over 1,000,000 user votes in open-domain dialogue interactions?", "450b3264-f46d-4ef9-9019-386524d4877f": "What methods does WildBench use to ensure unbiased assessments of LLM responses?", "516eb4fe-af07-44ad-9276-b7a0e4ea8ddf": "How does FLASK improve the interpretability and reliability of LLM evaluations?", "79880357-e8e1-4649-b9fd-f7ab01978139": "What domains are covered in the comprehensive evaluations mentioned in the context?", "a751cca1-50b7-4961-b50e-d9971b3e4564": "Which criteria are used to assess models during the evaluations?", "cb15d939-6baa-417d-858d-e256dcd68063": "What role do reward models and LLM-based judges play in ensuring alignment with human expectations?", "dfb7ec53-90ed-4927-9c09-d51bc7c1784f": "Which datasets are mentioned as addressing the challenge of alignment in reward models and LLM-based judges?", "29867087-10cf-4179-9383-69fafb7fbe23": "What are the main focus areas of RewardBench as described by Lambert et al. (2024)?", "19072f38-cf71-4ea3-a04d-fc5faef7d53e": "How does RM-Bench evaluate reward models differently compared to other benchmarks according to Liu et al. (2024b)?", "1ee6c530-16c1-49ca-8647-b1ca04d4df41": "What are the two components of JudgerBench as described by Cao et al., 2024a?", "f70954d8-6b45-4aa0-816d-f724c6917e7f": "How does JudgerBench incorporate human voting results to evaluate model performance?", "50a74ac1-887f-4255-b9c4-c89099962246": "What novel approach does JUDGEBENCH propose for evaluating LLM-based judges?", "a4556845-b642-4864-945a-3f6044cb2da9": "How does JUDGEBENCH address the limitations of existing benchmarks in assessing LLM capabilities?", "be87e68b-a44b-453d-9108-9210a9669762": "What types of tasks does MLLM-as-a-Judge benchmark cover for evaluating Multimodal LLMs?", "41a4d3a8-364a-49bd-8975-cfdfac621dc5": "How does MM-Eval contribute to the evaluation of multilingual capabilities in LLMs?", "36721c38-09a0-4033-badb-3e4f0e2be866": "What performance discrepancies does MM-Eval highlight in low-resource languages?", "8115fa22-b69f-4d53-99f2-80a5cb7ce698": "How many languages are included in the broader Language Resource subset mentioned in MM-Eval?", "53c3ec12-ff38-44ea-bb78-2db092ea3a1e": "What is the primary focus when evaluating LLMs-as-Judges models according to the given context?", "78c8649a-c9ea-4ccc-8815-1d2bf2961fe5": "Why is achieving high agreement with human ratings important in the evaluation of LLMs-as-Judges models?", "b6028a7a-c668-4205-979f-28bd42b97233": "What does accuracy measure in the context of evaluating an LLM's performance?", "e178d5e1-66d3-4c15-aabd-92748aa6d060": "How is accuracy calculated in classification tasks according to the given context?", "d041e473-0f2f-483a-9c5a-9e9d6027a6d8": "What does the Pearson Correlation Coefficient measure in the context of LLM evaluation scores?", "711cfac3-61f4-49a3-8a4f-178e36fafa6e": "Why might accuracy not fully capture the quality of a model when evaluating nuanced or continuous tasks?", "152782b7-803f-4c02-85cb-40b2688f53af": "What does the variable \\( r \\) represent in the given formula?", "bc8fe9d9-1a6d-47f5-afab-eca82f3a8524": "How are the terms \\( (x_i - \\bar{x}) \\) and \\( (y_i - \\bar{y}) \\) used in calculating \\( r \\)?", "a4d9114f-f8f6-4dc9-ab49-870f59255098": "What is the formula for the correlation coefficient \\( r \\) as given in the context?", "0b3fc2aa-612b-4c1e-9560-d81638179a14": "How are the terms \\(\\bar{x}\\) and \\(\\bar{y}\\) used in the calculation of the correlation coefficient \\( r \\)?", "86d93dac-23b0-444e-bd62-62ba836333da": "What do the variables \\( x_i \\) and \\( y_i \\) represent in the context of correlation analysis?", "3fd19c6a-ad20-4edb-8f3d-222ec2b7054f": "How does Spearman\u2019s Rank Correlation Coefficient differ from Pearson\u2019s correlation in terms of the data it compares?", "6d00dcce-77a0-4ea1-985b-132693a688a4": "What does the variable \\( \\rho \\) represent in the given formula?", "2cb74016-651c-4af6-8d72-340e2694f660": "How is the sum of the squared differences \\( \\sum d_i^2 \\) used in calculating \\( \\rho \\)?", "9759eb86-4201-4962-90de-d808f09f5584": "What does the variable \\( d_i \\) represent in the context of Spearman\u2019s correlation coefficient?", "29c4ea25-9b45-498c-82e0-00bc0455c26f": "Why is Spearman\u2019s \\( \\rho \\) considered more robust than Pearson\u2019s correlation in ranking-based evaluations?", "e2c7f7c6-4191-4603-be9b-d9f20d4d3d3a": "What does Kendall\u2019s Tau measure in the context of ranked lists?", "5f331fcc-993f-4722-bb7e-f1b436c805f6": "How is Kendall\u2019s Tau (\u03c4) mathematically defined according to the given formula?", "a2573f02-0e68-4d5c-be09-c0ae4f2c161f": "What do the variables C and D represent in the context of Kendall\u2019s \u03c4?", "e62eb1be-0bcf-4c4a-9647-43c9b246f146": "Why is Kendall\u2019s \u03c4 often preferred over Spearman\u2019s \u03c1 when evaluating rankings with many ties?", "cd98da12-1c40-4582-aada-5fe5129a2f29": "What does the variable \\( \\kappa \\) represent in the given formula?", "ea0e22fc-4072-456c-b87f-6dc9b590ee0e": "How are the terms \\( p_o \\) and \\( p_e \\) used to calculate \\( \\kappa \\) in the provided equation?", "5b33d8a0-b63f-4f51-acce-e18c790992f0": "What do the terms \\( p_o \\) and \\( p_e \\) represent in the context of Cohen\u2019s Kappa?", "aee42c7a-8fdd-4a5d-b61f-cb958b8b1cd1": "Why is Cohen\u2019s Kappa considered a more robust metric than simple accuracy in classification tasks?", "56472c05-63e8-4877-bada-a91ba9ff4c1b": "What does the Intraclass Correlation Coefficient (ICC) assess in the context of multiple evaluators?", "e7c7e090-3904-49f6-ae56-42e9a74ebf0d": "How is the ICC defined and what statistical model is it based on?", "4eced69e-fd56-4d26-ba01-a7e9322d75b7": "What type of metric is Pearson and what is its primary use case?", "1a2e7f80-6385-4487-8605-7457941a5a48": "Which metrics are considered robust to outliers according to the context?", "7dce7dcd-0683-4b49-9b5a-379d37e5853e": "What are the three key aspects of limitations discussed in the application of LLMs-as-judges?", "b6a8de79-93a7-4cb8-a133-8c7223273c9c": "How do the inherent characteristics of LLMs contribute to their limitations in effectiveness, reliability, and fairness?", "9065e6dc-546b-4b09-a1aa-70211c60fbea": "What is position bias and how does it affect judgment in sequences of input?", "e1314548-7018-449c-9a44-758f3bf18a0d": "How do social-related biases like authority bias and bandwagon-effect bias influence the responses of large language models?", "f1d2cc6b-1f87-4ff6-92a7-f7d204e47fc2": "What is Compassion-Fade Bias and how does it affect the judgments of large language models (LLMs)?", "986ff69e-18d2-4a00-aa4d-51d57b60cc0e": "How do Diversity Bias and Sentiment Bias differ in influencing the responses generated by LLMs?", "f82ed697-aad9-4a5c-a098-f2017173d6fb": "What is context bias and how can it affect the judgments produced by large language models (LLMs)?", "a2f2988a-e473-4823-a124-b769de445d87": "How does self-enhancement bias impact the objectivity of evaluations when a model judges its own outputs?", "d29e37be-8bed-43ee-956e-454f5621ff62": "What is Refinement-Aware Bias and how does it affect scoring variations during evaluation?", "dfd5ee36-1ec1-44b6-b972-424a9ec6d0ba": "How can Distraction Bias impact the quality of judgments?", "8868dd98-7ad9-4588-976e-b6b6e4bb25d7": "What causes large language models (LLMs) to inherit biases in their responses?", "74224a66-431d-452c-9341-6764504fcda8": "How can biases in training data impact the evaluation results of LLMs?", "dda5978c-28c4-4fdd-a0c9-5afec646b827": "What are the four groups of biases exhibited by LLMs-as-judges as categorized in the provided classification?", "4767274a-106e-4833-b9e2-80bfb406b5b1": "What aspects are covered in the detailed overview of each bias category mentioned in the context?", "fe2fa197-eba9-4251-8fed-cc3c05198a71": "What are Presentation-Related Biases in the context of LLMs?", "bebf98d6-791e-4417-8cab-15effd74c1ef": "Which two specific biases are introduced as examples of Presentation-Related Biases?", "9638f591-6a11-47db-8571-6dbfb675a05a": "What is position bias and in which contexts is it commonly observed?", "c73ff7db-faf2-40c6-929e-cfcd30193756": "How does the order of options influence human decision-making and machine learning model outcomes?", "40e7a568-a9cb-49b5-b71d-9226db1ee074": "What does position bias in LLMs-as-judges refer to?", "492cc77a-6a9b-46e3-9018-08627c0ea072": "How does position bias affect the selection of answers by LLMs when presented with multiple choices?", "02db77f3-725e-4fba-81a9-0ea70c1870a9": "What metrics does the LLMS (2025) framework introduce to investigate position bias in pairwise comparisons?", "9003f250-545c-4786-bf4f-b7edc2230786": "How do recent studies verify the agreement between LLM judgments and human preferences despite position biases?", "2d6f694c-7754-48a0-8b4b-1ee185a74ee9": "What is the naive approach proposed to mitigate position bias in LLM judgments?", "ab1d7e13-554b-4837-a722-68fbf07f3dd9": "How are inconsistent judgments identified and handled according to the naive approach?", "61b76ecc-0768-4e94-a714-1d7f8a64ed09": "What are the two categories of the swap-based debiasing method?", "aac36c30-74b3-48eb-a14a-5c60ba99977d": "How does the score-based method determine the final score for each candidate answer?", "3ea5ac7e-2019-4f7b-b9fb-19a1393b91a7": "What factor determines the conclusion of a tie according to the analysis mentioned?", "a45e26bc-d977-4ef2-97dc-0b5cb4b067be": "How does the size of the quality gap between candidate answers affect the impact of position bias?", "863e5f1f-2ec7-47dd-9383-f49985d3cdcb": "How does PORTIA utilize an alignment-based approach to improve the evaluation of candidate answers?", "bfce1756-9393-4938-ac4c-8a43c4df8802": "What are the benefits of presenting content in a balanced and aligned format when using PORTIA for answer evaluation?", "5397b275-1e83-488e-ae1e-22a9af579fbb": "How do discussion-based methods help reduce position bias in LLM-based evaluations?", "e105c762-f849-4871-a339-04867bd416a4": "What role does peer ranking and discussion play in improving the accuracy of LLM evaluations?", "466e43f4-089b-46e7-a963-bf26c331f23f": "What is verbosity bias and how does it affect the judgment of responses by humans or models?", "9cb8dacc-8c80-47cd-a4ee-e2acea430e8a": "Why might large language models (LLMs) prefer longer responses according to the concept of verbosity bias?", "7100ee06-27f8-4b50-b01d-71892a424127": "What technique does Khan et al. (2024) propose to reduce verbosity bias in LLMs-as-judges?", "778403ba-574f-46a9-9413-b8c93032732c": "How does the CALM framework by Ye et al. (2024b) help in assessing verbosity bias in LLMs-as-judges?", "60e9537c-956f-4545-b365-d6fac210e394": "What is the main goal of the contrastive judgments (Con-J) approach in training LLMs-as-judges?", "1233b853-c8af-4fc6-a131-da526ab65381": "How does the Con-J method help LLMs avoid verbosity bias in their assessments?", "14aed328-2a7a-4863-aef6-152732f7e37c": "What are some examples of social-related biases that can affect language models?", "5b2ecebb-2613-4ede-9607-217bf79f7393": "How do anonymization strategies or identity markers influence biases like Compassion-Fade Bias and Diversity Bias in language models?", "bd87f02e-c6f1-4b1e-a301-f9b9127d10b9": "What is authority bias in the context of LLMs-as-judges?", "c29424cf-ac7b-4fcf-b853-256dba32a68b": "How can authority bias affect the judgment outcomes of LLMs-as-judges?", "5535d248-d044-4e5e-bb0e-d9e3a203a73c": "What is one potential approach mentioned for mitigating authority bias in LLMs-as-judges?", "28b8816f-864c-4ee7-b216-c0dca3926e2b": "Why is further research necessary in developing methods to address authority bias in evaluative contexts?", "f5cc3ba5-7516-476f-95d0-a15d228920d2": "What is the bandwagon-effect bias in the context of LLMs-as-judges?", "511c322a-25a9-40dc-b3b3-c6029e75d240": "How does the bandwagon-effect bias affect the objectivity and fairness of judgments made by LLMs-as-judges?", "5e5cf0a7-e417-473b-99d4-b3fa3fad4a08": "What is one method mentioned to reduce the bandwagon-effect bias in evaluations?", "00e8ffcc-fe39-4bc9-885f-ef168056eee2": "Why is further exploration of debiasing strategies important for addressing bandwagon-effect bias?", "e04a96a8-ab3c-4eb8-9624-a42f77fd5649": "What is compassion-fade bias and how does it affect the judgments made by LLMs-as-judges?", "4790a2d7-5122-4957-aee8-e63dfe64e657": "How does anonymizing model names or using neutral identifiers influence evaluation outcomes according to the compassion-fade bias?", "89ff3b72-1656-4a2a-9060-0b9efcacc789": "How can standardizing judgment criteria help mitigate compassion-fade bias in evaluation prompts?", "fdfe32e9-0ad4-4473-8f1c-de3bb300cf06": "In what way do fairness-driven frameworks that address anonymization effects improve the reliability of LLMs-as-judges?", "a85c3038-e2d5-40ab-8a5e-79e8d2290f9b": "What is diversity bias in the context of LLMs-as-judges?", "897e0d07-ac1f-4704-9a62-87cae701ddb1": "Why is it important to address diversity bias in LLMs-as-judges?", "c34b3d26-7616-4e85-8f3c-d6fb6b723d31": "What are some examples of Content-Related Biases that can affect a large language model's responses?", "56ff98f8-5556-40cd-a4ff-33b2fed895f8": "How can Context Bias influence the outcomes generated by an LLM?", "099ece18-61c7-441d-8c43-593e629bf14d": "What is sentiment bias in the context of LLMs-as-judges?", "d8e4c0dd-0b61-4560-a270-bc0ee55b3c6e": "How can sentiment bias be addressed when evaluating responses from LLMs-as-judges?", "9c3d900e-e5af-454d-8351-364302cc5cf4": "What is token bias in large language models as described by Jiang et al. (2024) and others?", "2dced099-2d27-445f-9475-dd8e02bcc6d9": "How does the pre-training data contribute to token bias in LLMs?", "b70bfc26-f823-4e0d-a5c0-c4a88a513112": "What is contextual bias and how can it affect the judgments produced by large language models (LLMs) in different domains such as healthcare and finance?", "f6365c26-2694-4e16-b896-dbedc2328aa2": "How can the selection of contextual examples contribute to the introduction of bias in LLMs?", "8f314652-7216-42a9-9ef8-690aad699494": "What are some examples of cognitive-related biases exhibited by large language models (LLMs) as described in the context?", "15064b9a-8fe0-407a-a3d8-ff347d09d876": "How does Distraction Bias affect the information processing of LLMs according to the provided context?", "0d0b91ec-cf09-402b-be4f-9bb94db457a5": "What is overconfidence bias in the context of LLMs-as-judges?", "6ebaf4f9-a653-4d52-8a48-114ad595ed2b": "Why is overconfidence bias particularly concerning in evaluative contexts involving LLMs-as-judges?", "3f0fb8b3-4609-42da-aa52-a52af5955c8a": "How does Cascaded Selective Evaluation use Simulated Annotators to address overconfidence bias?", "71b7ee6b-22b2-4a56-b446-9644161d8776": "What role does in-context learning play in the Cascaded Selective Evaluation method proposed by Jung et al. (2024)?", "e78adad9-a4c4-4055-83fc-f68692ff1e75": "How does the adversarial debate mechanism help reduce overconfidence in large language models?", "69ebc4cb-2357-45d1-936e-ea1ab1426633": "In what way does structured debate improve the truthfulness of LLMs' judgments?", "4a0f0206-0ab9-4eb8-b7f9-9fa402cc240b": "What is self-enhancement bias and how does it manifest in the context of LLMs-as-judges?", "db043272-230c-407d-8737-a65fbc206373": "Why is self-enhancement bias particularly concerning in applications involving self-assessment or feedback generation by LLMs?", "26348364-bb0b-4dd5-97a9-9bb37af330aa": "What mechanisms does PRD introduce to address self-enhancement bias in large language models?", "c2489dd8-37b8-474d-b95c-7c114c20eae4": "How does the Peer Rank (PR) mechanism reduce the impact of self-enhancement bias among LLM reviewers?", "871e0c37-3041-49c0-9e51-75b65b1aa2de": "How do PR and PD help mitigate self-enhancement bias in model evaluations?", "b749e5b4-21a8-4fe3-8181-fb7167f87b99": "Why is focusing on content quality important in the multi-turn discussion described?", "f062c672-0553-42d3-a5bb-8c9812e30c7e": "What is the main purpose of the CALM framework proposed by Ye et al., 2024b?", "f1952ae0-d722-4c63-9fdb-73de31e97eeb": "According to CALM\u2019s findings, how can self-enhancement bias in LLMs-as-judges be effectively reduced?", "918776ff-cc43-41eb-ad98-8a3e87cbf312": "How does the Reference-Guided Verdict method help reduce self-enhancement bias in LLM evaluations?", "423a7d85-973e-45e9-bdd1-615cc8f8a527": "In what ways does integrating multiple LLMs contribute to producing less biased and more balanced judgments?", "7956fa90-c8ff-45a1-84cb-da7c59469dcf": "How does model diversity contribute to producing less biased and more balanced evaluations?", "ca9f7eae-27fa-463d-9644-29c6b28d80ea": "In what way do reference-guided criteria help combat self-enhancement bias?", "27a6461d-03c6-4e49-a490-d6fbd75e9e6c": "What is refinement-aware bias in the context of LLMs-as-judges?", "767d3388-b4e6-43a2-a411-4dcd10e68ad9": "How can the presentation of revision history affect the evaluation of responses by LLMs?", "df5b2b1c-db2f-4496-925f-66aca9ad5553": "What is one potential solution to address refinement-aware bias in LLMs-as-judges mentioned in the context?", "665e3018-12b6-4d96-8253-a2afb792b06f": "How does incorporating external feedback mechanisms help in reducing bias during judgment by LLMs?", "0c8a5114-0341-4bbc-8f33-54689287ca3a": "What is distraction bias in the context of LLMs-as-judges?", "de98497b-3373-4b60-9c11-3fa9ef04cf61": "How can irrelevant information, such as a meaningless statement, affect the evaluation outcomes of LLMs?", "a9a22024-09fd-4d3d-9b09-56e357a3c4c0": "What are some potential strategies mentioned to mitigate distraction bias in LLMs-as-judges?", "35eb45fe-e94b-475f-9e66-1f50a4e40646": "Why is further research needed in the context of addressing distraction bias in LLM evaluations?", "cc3c2027-a5ee-40a8-8bf4-4c1f8566cca2": "How can distraction bias be systematically addressed when using large language models as judges?", "a45e9977-9baa-48d8-a0c7-5fd38e1c1975": "What methods are effective in mitigating distraction bias in LLMs acting as evaluators?", "30fc6ec3-4db6-40ba-b520-0eed18368574": "What is fallacy-oversight bias in the context of LLMs-as-judges?", "9624b0c3-77c5-486a-b269-51f571b50e16": "How can fallacy-oversight bias affect the evaluation of responses by LLMs-as-judges?", "4ddf3df6-ffb8-46e8-8276-00bc7bfe7a86": "What are the main challenges posed by biases in LLMs-as-judges according to the context?", "4abf8425-a571-4f31-8474-0a3a8f4da739": "What areas should future research focus on to improve the performance of LLMs-as-judges?", "3ed3c522-c921-4c4f-8a87-4da8a0d9e078": "What general methodologies can be adapted to improve the performance of LLMs-as-judges?", "683bb531-c10f-49b4-ae85-e8d0f19ed716": "Why are tailored solutions necessary for addressing the challenges faced by LLMs-as-judges?", "dda12ecd-9ba7-49ec-a250-b302e67b8169": "What are adversarial attacks in the context of machine learning?", "a67964d7-f968-448a-8708-e6c24edcc0f1": "How do adversarial attacks impact the performance of AI models?", "50037143-12c9-4f28-8580-215ebe3f6cda": "What techniques do attackers use to deceive LLM judges in adversarial attacks?", "cedacc53-0188-4ff8-ae41-bd0a800176dd": "How can subtle modifications in input content affect the evaluation results of LLM judges?", "c7aca584-3fb8-44b4-a0c7-b2b1a9298b63": "How can small changes to input data affect the responses of large language models (LLMs)?", "eb268404-4932-446f-b02f-a2b4f442357e": "What are the potential consequences of attacks that manipulate input data for LLM evaluations?", "5751c8e8-5f19-4e4d-826c-4f8505c7e32e": "What are the three main categories of adversarial attacks on LLMs based on the manipulation level?", "1092ec67-46f3-4c2a-a621-d177a8d7ec8b": "How does the context differentiate between general adversarial attacks on LLMs and those specifically targeting LLMs-as-judges?", "251f0b12-a490-4ffc-8508-0de32d33f282": "What are some examples of character-level perturbations mentioned in the context that can deceive a model?", "8aa41edc-f0a8-4180-a704-a0e8378fd94b": "How do sentence-level modifications exploit a model\u2019s sensitivity according to the provided context?", "7815bc3b-142d-46ad-9b0b-7b714507e862": "What is the main focus of Structural and Semantic Distortions in the context of input manipulation?", "6bfb1f5f-4771-4fe9-8061-2e73251e377a": "How do semantic preservation with perturbations ensure minimal impact on meaning while altering model predictions?", "0f5beca9-59a0-4df3-8bb4-77bab510e9f6": "What role do gradient-based methods play in optimization-based attacks?", "b97f0f89-0d85-4a06-82a6-276d5578df48": "How do population-based optimization techniques generate adversarial examples in black-box settings?", "cdef5a47-03d8-4ab0-b027-ed3408b60235": "What vulnerabilities in LLMs are highlighted by adversarial attacks?", "bdc2f37f-3642-4e0b-bee8-c1f4d7748841": "Why is studying adversarial attacks on LLMs important for developing defense mechanisms?", "e6856da8-3547-49b7-993f-3ad90990d089": "What vulnerabilities have recent studies identified in LLMs-as-judges regarding adversarial attacks?", "f6c482a0-36c8-40d9-84e2-3a2fe08077c0": "How do malicious inputs affect automatic benchmarking systems like MT-Bench according to Zheng et al. and Doddapaneni et al.?", "f59b8844-3d2e-45eb-84f9-90b2d7cdd59c": "How do universal adversarial attacks affect the robustness of LLMs when used as judges according to Raina et al. (2024)?", "76074666-dabc-427c-bd67-1b089233b5d2": "What evidence suggests that universal attack phrases are transferable across different language models?", "97ee776f-41f3-4213-b7d2-d041187cf180": "What is the main purpose of the JudgeDeceiver attack introduced by Shi et al. (2024b)?", "df15f8a4-ff15-4fc2-a771-fce39e7def83": "How does JudgeDeceiver differ from handcrafted prompt injection methods in misleading LLMs-as-judges?", "1a510cea-77fb-45b0-bdb9-e2b5acf05da1": "What are the inherent weaknesses of LLMs-as-judges that make them vulnerable to adversarial manipulations?", "aae9d666-236a-4c41-a7e6-63a222af9025": "Why is it important to develop defense strategies against adversarial attacks on LLMs-as-judges as their use becomes more widespread?", "12335f24-e504-41fc-bd3d-b481a3a0b811": "What are some inherent weaknesses of LLMs that can affect their reliability as judges?", "9ca0e9fd-3357-404b-9d01-feb8ec85d85d": "How does the issue of knowledge recency impact the performance of LLMs?", "8baa4708-9cc3-4990-8a05-d2fc1312e19f": "What is a major limitation of large language models (LLMs) regarding the use of up-to-date information?", "8baa380f-1b89-46c3-8aef-4d75761d55fa": "Why is retraining LLMs on new data considered a challenging solution to keeping their knowledge current?", "57db514d-fba8-49b1-829e-dcba33296885": "How can a temporal disconnect affect the reliability of LLMs when used as judges in time-sensitive applications?", "9c465c7f-a6a5-44bd-b1c5-dfff38265207": "Why might an LLM-as-Judge incorrectly prefer an outdated response when evaluating answers about the COVID-19 pandemic?", "c416884c-bd5f-4411-a3fe-358210463892": "Why is addressing knowledge recency important in LLMs when they are used as judges?", "d0aa463e-7a5c-4883-91c5-55e7064d7bb0": "How can outdated knowledge affect the decision-making capabilities of LLMs acting as judges?", "2eb49a16-218e-4a04-b381-0e00ebc00311": "How do retrieval-augmented generation (RAG) methods help address the issue of knowledge recency in large language models?", "c063bdef-65e6-44c4-bc86-df9c999dfb45": "What role do periodic fine-tuning and continual learning frameworks play in maintaining the alignment of LLMs with the latest information?", "41e9d33d-1d9a-43a9-a900-335ee78e78c5": "What is the hallucination problem in large language models (LLMs)?", "62ef5a04-0ed8-4933-b092-8f3187d04b1f": "How can hallucination affect the reliability of LLMs when used as judges?", "11f7e747-2d86-4864-a3a1-233fa4f81ada": "How do fact-checking mechanisms help mitigate hallucination in the evaluation of LLMs-as-judges?", "b4f1de89-5d5a-4643-ab04-4ce8831b9fbb": "Why is it important to cross-verify the outputs of LLMs-as-judges with trusted databases and external knowledge sources?", "931f7fff-5957-430b-8c8a-a971cb5931ee": "What are some reasons why large language models (LLMs) may struggle with domain-specific knowledge according to the context?", "8e1220a1-22e2-4bfc-aed6-c74d0faabefe": "How can the lack of specialized knowledge in LLMs affect their performance in legal judgment tasks?", "7a16a42b-2213-438b-9dbd-631706f26091": "How do knowledge graphs enhance the performance of large language models in specialized domains?", "a6dac6cb-0373-4b04-bcaf-ccdb35f68713": "What role do retrieval-augmented generation (RAG) systems play in improving domain adaptation for LLMs?", "6c06cb85-8b6e-4eac-bebb-19a972625b2c": "What are some methods mentioned to address the inherent weaknesses of LLMs in order to improve their reliability as judges?", "20627d61-7955-42b0-98e4-dc4976703989": "What are the key goals for future work in developing more efficient and reliable LLM judges?", "ff03c09f-2e5c-4938-bbd9-2dcbe9fe7a54": "What methods are discussed for the automated construction of evaluation criteria in more efficient LLMs-as-judges?", "ff74c77c-fb41-4e2f-87fd-d355d92747a0": "How does the automated construction of evaluation tasks contribute to the efficiency of LLMs-as-judges?", "8ae5430d-83a7-4ea6-9cf6-93eeefbe0178": "What are the main limitations of current LLM judges in their evaluation process?", "25a5448a-a7db-4fe6-9e67-6baf42b5a471": "How could future LLM judges improve adaptability in their assessments?", "819aad8d-950a-416f-8af1-e53d7c9f7c4e": "What improvements does the configuration process bring to LLM judges?", "905bb57d-21cf-4b98-9aaf-f33be64057db": "How does the configuration process enhance the practicality and efficiency of LLM judges in real-world applications?", "9a4d6e48-79bb-421c-8b2b-b68df3d78394": "What are the main issues associated with existing static evaluation datasets for LLMs?", "e99f3b92-8019-470b-8623-d43678edfca7": "How could future LLM judges improve the evaluation process to enhance applicability and precision?", "7af6841c-87a4-4b54-9acb-ee5829c3af3d": "What limitations do existing LLM judges face in practical applications according to the context?", "fdac8d7f-9ae2-4027-90d1-04e21b9c12f7": "Why do existing LLM judges fall short in meeting the demands of real-world applications?", "3dde1d5b-528f-4015-8b54-172b9e98a4ea": "How could modular design principles help in creating scalable evaluation frameworks according to Xu et al. (2024a)?", "5a396c28-f165-4817-b909-72b75c4c69ea": "What are the benefits of using a modular approach in evaluation frameworks as mentioned in the context?", "33c86a55-7303-434f-91e1-37f065002e6f": "What challenges do existing LLM systems face when performing evaluation tasks in resource-constrained environments?", "126c7b59-b562-4119-897e-5ab3cc47d525": "How could future research improve the efficiency of evaluation processes for LLMs according to the context?", "7491ef69-db12-45a7-8b34-f78d0799b695": "What challenges does the multi-LLM evaluation paradigm present in terms of computational demands?", "c41dfdea-7d1f-458d-8984-3ebb962e2ca5": "How might future efforts address the resource requirements of multi-LLM evaluation systems?", "70c2f543-d999-44e0-b6be-ddd15f38d935": "How can the deep integration of reasoning and evaluation capabilities improve the effectiveness of LLM-as-Judge systems in complex tasks?", "3bc6e513-6a68-494c-8312-30658822beea": "In what way could LLM-as-Judge systems apply integrated reasoning and evaluation in legal scenarios to enhance case assessment?", "1078da0b-26aa-41d4-a891-20392228a9ec": "What are the limitations of using a single model in current LLMs-as-Judge systems?", "538b3248-741d-4a32-8218-6f6f47b78bcc": "How might collaborative multi-agent mechanisms improve the evaluation process in LLMs-as-Judge systems?", "10239924-f0b0-4f18-99ae-2709274611f3": "What is the benefit of using different models in judgment outcomes?", "3aef6273-e8ee-4073-a11d-219e47a11c24": "How do different models contribute to the stability and reliability of judgments?", "11afdf73-10e3-4b9e-a040-35268fbf0cf8": "What challenges do current LLMs-as-Judge systems face when handling tasks in specialized fields?", "e600620d-84d2-4442-a9fa-ec46df24902f": "Why do current LLMs-as-Judge systems struggle to remain effective as domain knowledge evolves?", "6972c0e7-c983-4ed9-9cdd-6c4a438ef1dd": "How can future LLMs-as-judges systems enhance their performance in specialized tasks according to the context?", "59fb4dea-826a-4da8-b6b7-c7c5579b6d83": "Why is it important for LLMs-as-judges systems to incorporate dynamic knowledge updating capabilities, especially in the legal domain?", "72ea7554-132f-44ee-b07c-c5f4c2666933": "What challenges do current LLMs-as-Judge systems face when transferring across different domains?", "7280b75b-a521-478a-840f-85e6fcace027": "Why is the transferability of LLMs-as-Judge systems across languages and fields considered limited?", "b4da0aad-3231-4265-828b-a22771b2a284": "How can cross-domain and cross-language transfer learning techniques improve the adaptability of large language models (LLMs) in diverse fields?", "9feea726-6f00-4056-8491-cafd27230cac": "In what way could evaluation capabilities developed in English be utilized to enhance performance in other languages such as German?", "892baa22-97a1-4bf5-a916-fb12baf274a8": "What are the limitations of current LLM-as-Judge systems in handling multimodal data?", "8a2596c4-69ee-4c04-9b06-fb8c5772e177": "Which key research areas are important for developing future LLM-as-Judge systems with cross-modal integration capabilities?", "db927832-4362-45b9-9496-412c3c376f41": "What methods can be used to enhance the interpretability of LLMs when they are employed as judges?", "308a5057-8520-4d72-93ac-4a75e6817d7d": "How does improving transparency contribute to the reliability of LLMs in judgment tasks?", "428f5dcb-056e-4107-997e-cca1c86d8c9b": "What are the main concerns regarding the transparency of current LLM-as-Judge systems in high-stakes domains?", "316dcacd-a350-41be-91ef-59c0426b4e94": "How might future research improve the interpretability and trustworthiness of LLM judges according to the provided context?", "586d4c7d-e311-416b-8e4d-ccc961618f9c": "What are some potential methods mentioned for mitigating bias in large language models?", "72b8d7f6-7583-49f3-8a8b-2c4c00fae825": "How can targeted approaches like adversarial debiasing training help ensure fairness in model outputs?", "7e8c97fd-8591-4ab1-82ef-9ab93e90a64a": "What challenges do LLMs face that affect their robustness in handling complex or uncertain texts?", "23c66141-82c6-4949-b534-16dcb81b0365": "What methods are suggested for future research to improve the robustness and reliability of LLMs in real-world environments?", "2e58e12f-787c-4b65-889a-9a8f500373d5": "What are the five dimensions across which the LLMs-as-Judges framework was systematically examined in the survey?", "cbd85adf-da13-4364-9ff9-00adce097d52": "What future directions does the survey suggest for advancing research in the field of LLM judges?", "ed988c00-2592-4e83-a79b-bc737fc6d12d": "What is the title of the work authored by Josh Achiam and colleagues in 2023?", "88acb9d2-0634-4dbf-bc36-1dafa5fcd08e": "Who are the authors of the 2024 paper on multi-agent workflows for synthetic preference optimization dataset generation?", "77ab2025-8ad0-46c8-ab21-f84c2a5e58a9": "What are the main contributions of the paper titled \"Self-rag: Learning to retrieve, generate, and critique through self-reflection\" as presented in arXiv:2310.11511 (2023)?", "9ba1e243-9cc4-4613-8117-c5b12b7c6311": "How does the work by Ashktorab et al. (2024) on aligning human and LLM judgments contribute to AI-assisted assessment strategies according to their arXiv preprint arXiv:2410.00873?", "35d21e4a-c638-4488-93e7-559622e1a989": "What is the main focus of the paper by Askell et al. (2021) titled \"A general language assistant as a laboratory for alignment\"?", "5404279e-b96d-4a2d-aa10-407a5955028a": "How do Babaei and Giudici (2024) apply GPT classifications in their research on credit lending?", "5e1c975f-524f-43ab-9247-670c0095ea58": "What is the title of the work authored by Yushi Bai et al. in 2024?", "0b699a03-0ae5-4a1e-a043-482d216a248b": "Which publication platform features the Qwen technical report by Jinze Bai et al. (2023)?", "cce08560-3e4a-4ef4-bf38-083ac98af4e4": "What is the focus of the dataset named Ms marco mentioned in the context?", "11281b7d-5bee-4574-a2ea-35ff6c1dccfb": "What methodology do Bandi and Harrasse (2024) use to evaluate large language models according to the context?", "904f5d3f-83c5-4d1e-a746-306350a3de83": "What are the main findings of the large-scale empirical study on using LLMs instead of human judges across 20 NLP evaluation tasks?", "fe8af4c2-635b-48cf-ae64-6ca2ab2fe676": "How does the concept of \"Graph of Thoughts\" contribute to solving elaborate problems with large language models as discussed by Besta et al. (2024)?", "ab88c19e-02d2-46c1-81f9-b95252780636": "What are the two model designs compared by Brake and Schaaf (2024) for clinical note generation?", "502d1d3d-89eb-419f-a2ad-04f46a444683": "How do Branch et al. (2022) evaluate the susceptibility of pre-trained language models?", "2a63814c-4e3e-4ee5-a8ea-c92f49fa0e29": "What is the main contribution of the CompassJudger-1 model introduced by Cao et al. (2024a)?", "ef0d2360-7996-4902-a329-99deb860f42b": "How do Cao et al. (2024b) propose to enhance reinforcement learning using language model critics?", "6060a800-71c8-43b4-8e41-400e12a7fd2a": "What is the main focus of the paper by Chan et al. (2023) titled \"Chateval: Towards better llm-based evaluators through multi-agent debate\"?", "aaca0b1e-18f1-417b-81cf-ea015cb45d4e": "What type of publication is the work by Yupeng Chang et al. (2024) on the evaluation of large language models?", "f2723858-4f70-4945-8101-31c5c2597450": "What are the main objectives of the study conducted by Chen et al. (2024b) on multimodal large language models as judges?", "fb975e03-cd3e-48b1-ab30-2842816f856e": "How does the research by Guiming Hardy Chen et al. (2024a) address judgment biases when comparing humans and large language models as judges?", "aea525af-8b81-45e6-991e-96a3b254951b": "What is the main focus of the paper titled \"An Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation\" by Chen et al. (2024e)?", "691da3ce-d629-434c-a73c-94bbaf4fe5d6": "How does the work by Jiefeng Chen et al. (2023c) propose to improve selective prediction in large language models?", "9c920f3c-dd3e-49d0-95f2-63a30ffc285f": "What is the main focus of the study conducted by Chen et al. (2024d) on large vision-language models?", "e743d4d5-eb5d-47c2-94a0-01e25d979081": "How do Chen et al. (2021) contribute to the evaluation of large language models?", "d1ee2d5e-f15c-4af7-bca7-bc91819a59f4": "What is the main focus of the paper \"Internet of agents: Weaving a web of heterogeneous agents for collaborative intelligence\" by Chen et al. (2024f)?", "a656c666-b029-4ee9-88a6-a5781ca26016": "According to Chen et al. (2023a), what capability is being taught to large language models?", "68379bc5-db1d-45a5-8c7e-64fa34666f7d": "What are the main findings of Chhun et al. (2022) regarding the evaluation of story generation using human criteria and automatic metrics?", "e6a67d5a-9396-4f6f-8abd-5548130e0529": "How do Chiang et al. (2024) describe the use of large language models as assignment evaluators in courses with over 1000 students?", "cb7324a6-f3a7-471b-b82d-10a2eb327647": "What is the main achievement of the Vicuna chatbot as described by Chiang et al. (2023)?", "7128a8fe-8aed-4384-b313-c3e22199d6b1": "What is the focus of the research conducted by Choi et al. (2024) in their paper titled \"Multi-News+\"?", "59368ac5-535b-4fed-a6d3-616baea70794": "What is the main focus of the research conducted by Chung et al. (2024) as mentioned in the context?", "26183633-b9a9-4370-be8e-e00d305f058b": "How is the Pearson correlation coefficient relevant to the work cited by Cohen et al. (2009) in the context?", "58aceb96-f0d5-437a-9e20-4714d0156406": "What are the main contributions of Craswell et al. in their overview of the TREC Deep Learning Tracks for 2021 and 2022?", "9e4ab15c-d391-462f-aa7c-35f11dc50313": "How do the publications by Craswell et al. in 2021 and 2022 differ in their coverage of the TREC Deep Learning Track?", "ac912cf0-e55a-42a4-bc82-1e4a479f4dbd": "What is the main focus of the paper titled \"ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback\" presented at the Forty-first International Conference on Machine Learning?", "9495238e-3a18-490d-ba75-6b4ea4106c0c": "How do Daynauth and Mars (2024) propose to mitigate token count bias in language model assessments according to their arXiv preprint?", "a166c47c-8226-4dcd-b51c-247d57c2a9a6": "What is the main focus of the paper titled \"PHUDGE: Phi-3 as Scalable Judge\" by Mahesh Deshwal and Apoorva Chawla (2024)?", "d6e516a6-312d-4566-9e9e-917501f85dac": "How do Dierickx et al. (2024) approach the use of large language models (LLMs) for fact-checking in their literature review?"}, "relevant_contexts": {"6612a33e-4f9f-4e15-b622-4eb44aaa982e": ["5c831b00-c7dc-4290-adcf-0bd37075ba8e"], "e56d00cc-381c-4076-bf14-35658f46195a": ["5c831b00-c7dc-4290-adcf-0bd37075ba8e"], "bc841e6c-afed-4775-a6c8-dab5db157044": ["814e7de6-ec3d-4d86-a282-1f81f7b9aa80"], "d0573f3a-7f60-4dec-a201-f8ff302b359a": ["814e7de6-ec3d-4d86-a282-1f81f7b9aa80"], "4ec76778-4556-4909-8394-920588f8e85a": ["647136cc-6ace-42d3-b060-98bd60e9657d"], "6f060a87-69a4-4e8a-a526-e73e8adf94c7": ["647136cc-6ace-42d3-b060-98bd60e9657d"], "129d1f15-0401-4190-a7be-2f940c859fc5": ["74183e6f-14d7-4767-8db3-12e052c233cb"], "d1770a9d-b37a-48f1-9b1c-c7359cc0ed18": ["74183e6f-14d7-4767-8db3-12e052c233cb"], "6743a51b-dfd3-4353-8439-e3324a640902": ["34aeeba9-369a-421c-a3c9-de9bf6b97fda"], "f951bf65-4db1-46c5-8108-4cf7e419a0aa": ["34aeeba9-369a-421c-a3c9-de9bf6b97fda"], "615cfcdb-f85e-4239-9526-3f270a32c7d3": ["f0077176-e258-4cec-91b8-183bbd5a910d"], "74bd2e91-d7a9-42fd-8f1f-c68a6ad4090c": ["f0077176-e258-4cec-91b8-183bbd5a910d"], "49effd7d-90b3-4f14-9703-247f903b51a5": ["f0b36adb-edf4-42fc-a03d-7e7012f8b897"], "ef154f9e-a089-4f9c-b17b-6cfcf6fdfff6": ["f0b36adb-edf4-42fc-a03d-7e7012f8b897"], "c9334af7-4d2d-4fcb-b416-4e2cefa99876": ["639ffa81-9a2a-4d3e-9f53-a4d767b1926e"], "498d7f7a-f1ca-4415-bd16-6dc8217ad2c5": ["639ffa81-9a2a-4d3e-9f53-a4d767b1926e"], "d00f080d-45ef-40a8-a27f-50fb90bce93c": ["74e8536f-4024-4b8d-ba1e-4a9712cd04ea"], "80477f6d-692e-48c9-8364-d679f7fb8479": ["74e8536f-4024-4b8d-ba1e-4a9712cd04ea"], "cb6602fc-8cb6-41db-b393-f389ce4034cc": ["39f27a87-69fb-4cc0-85bd-43fe8b1e0064"], "41057151-f121-4192-8b32-96db96e56604": ["39f27a87-69fb-4cc0-85bd-43fe8b1e0064"], "4f5a46e2-81aa-4a6d-8abb-e80a1b69d54e": ["9fcaddf2-21bd-4a47-a572-8cb168eafb96"], "3c428a05-2a14-4ab0-9c2f-d85bccb319b6": ["9fcaddf2-21bd-4a47-a572-8cb168eafb96"], "82925fa3-3643-4f28-97f6-d5d13f865a8e": ["0365dced-96d6-4bb8-9c53-7bc56febc39e"], "1dc96380-1de4-4924-9587-beb99b64591c": ["0365dced-96d6-4bb8-9c53-7bc56febc39e"], "a5218f28-1b1a-4fad-810e-0396c86c52f4": ["f38c5509-c6fa-4ce6-a1eb-50ab573e7c21"], "78d820ec-71c2-4999-9b81-2ecdebb46168": ["f38c5509-c6fa-4ce6-a1eb-50ab573e7c21"], "024f7a76-76a6-482e-b44e-98ae63826b5f": ["5d300c1c-0d8c-4b47-ae66-3f1f0e4eb982"], "1234b014-355c-40a2-849a-3f8fc52e94e7": ["5d300c1c-0d8c-4b47-ae66-3f1f0e4eb982"], "c8990614-4192-409c-ab3b-8e6cfe87c207": ["429537a1-271c-4227-ab37-7ef9889bc1c3"], "ef707913-5ea2-4d12-a25b-f3b5ccc0aa88": ["429537a1-271c-4227-ab37-7ef9889bc1c3"], "e01e4bdb-de2d-4259-bd70-d1da8de8017d": ["b909bb09-5283-4e7c-968f-932598a85b0f"], "275575c3-22ab-4191-897f-507c7705bc2f": ["b909bb09-5283-4e7c-968f-932598a85b0f"], "1efcfb36-fd9e-4af7-ac40-101384e255d2": ["ce9b6259-3b28-42a4-aafb-88e9b092902e"], "c4c557b6-4c45-41e7-8cd2-deae5af2f1f7": ["ce9b6259-3b28-42a4-aafb-88e9b092902e"], "83698269-6d17-49c3-bc05-529f8c6cf604": ["6beee3f6-9bcd-480d-b70b-40f1cdcbc4fc"], "579ca86f-023f-46ca-8506-c878838048b0": ["6beee3f6-9bcd-480d-b70b-40f1cdcbc4fc"], "c15b51d9-d350-4f83-b250-8cef06a8154e": ["6177fa25-8d22-4742-b3e4-93327506e9e3"], "f1bbd4a3-2f5c-4a2a-9975-fbba017c25a2": ["6177fa25-8d22-4742-b3e4-93327506e9e3"], "482c3e57-dbe6-493a-a97f-d0b4d1ee9f2a": ["35b991f6-2deb-4d10-807f-685a6bf9e905"], "e737b3a7-a58b-4a88-ac57-6481da0890a7": ["35b991f6-2deb-4d10-807f-685a6bf9e905"], "23653912-20b1-4112-88cd-8ce3a33c83b9": ["2cd13dca-1b4e-4488-8bd4-4e81640b7fa2"], "e050c6ab-ba80-4e4a-9b23-42098c257439": ["2cd13dca-1b4e-4488-8bd4-4e81640b7fa2"], "a788e79a-b91b-42da-a4ef-6b57354d088f": ["b16e4bde-05fd-488c-9baf-76d2a42bad73"], "f7e1bd7f-ce0e-447c-acf8-6b3fdae6a919": ["b16e4bde-05fd-488c-9baf-76d2a42bad73"], "6707dc3b-c0ec-4400-8763-e5ea8194e85b": ["9cfcdce5-c86a-4da5-89f2-0dd5b91ffeca"], "cd0f4653-f13c-47be-bdc9-89eca4e31db9": ["9cfcdce5-c86a-4da5-89f2-0dd5b91ffeca"], "884c9372-548e-40d9-a81b-e29f1cffa295": ["7ba861fd-f650-41e3-a4e5-9c4810213bbc"], "2537d39b-1736-441c-884c-93b77b759ba1": ["7ba861fd-f650-41e3-a4e5-9c4810213bbc"], "3334cca0-9ed6-4f24-abdf-9d68f32d1610": ["00548a73-db95-405c-86ce-13b4f68c619e"], "a071fa8e-d2f0-4a09-816b-a196bea52336": ["00548a73-db95-405c-86ce-13b4f68c619e"], "c5b48e9d-55e5-4945-b529-f22702ef0e07": ["b1bd14f3-3e1d-4e81-9255-65a181a82919"], "b85d6d8a-7769-42d9-8f10-3e4f132b317d": ["b1bd14f3-3e1d-4e81-9255-65a181a82919"], "cbf9920c-8fbf-4082-be3e-7601f4ae1dc0": ["0236b69e-12f3-4be5-999e-22d8319b3d0f"], "eb20ba6e-5db8-49e5-a065-f65023477b81": ["0236b69e-12f3-4be5-999e-22d8319b3d0f"], "9844e29e-6bea-4a62-abfc-ea01e5dc3b2a": ["cc4f140d-3853-4a26-b999-ece5f110186d"], "0a9843e5-2c92-4e2d-a121-21a42a1e4a07": ["cc4f140d-3853-4a26-b999-ece5f110186d"], "6d6a48a3-4a46-4551-9b25-7ccf4557ec79": ["0d53ef00-482d-4a57-bed9-39b8bcacce77"], "e55fa411-5b60-46b4-8095-2f75f4637aa8": ["0d53ef00-482d-4a57-bed9-39b8bcacce77"], "ea3a7026-ac10-48ab-a57a-a23f4ba9ec54": ["86bdf7fd-7c15-4070-8734-69515d8d6149"], "ba6cd7a5-3d44-4cd8-a8f6-08e113eb32bb": ["86bdf7fd-7c15-4070-8734-69515d8d6149"], "565eb1c8-f82b-4081-a62f-abbb0890c69f": ["9deb3bdc-9fad-4d63-aefc-d655a57e198f"], "68a80c8e-b984-490c-9451-2bb7807a1469": ["9deb3bdc-9fad-4d63-aefc-d655a57e198f"], "2d811bfd-64c5-40c0-9c7a-259d1fa23a8c": ["84ed9674-f703-4da1-a752-0db29c33183d"], "47e3cf26-aabd-465f-bf53-43b0cbbb0baa": ["84ed9674-f703-4da1-a752-0db29c33183d"], "f319bf68-fa2a-4b17-9437-4d05d8fe670c": ["65b89a97-d409-4c94-9e0b-443033d7dd36"], "8791f94c-e1e3-4da1-82c5-7c1b424b7cb5": ["65b89a97-d409-4c94-9e0b-443033d7dd36"], "bada7e86-9338-47d4-910e-84353f19e97d": ["ff3eaef0-3c2a-4f7e-a461-4e68463f7073"], "488e9927-37a9-46bd-9176-9694c70f0c31": ["ff3eaef0-3c2a-4f7e-a461-4e68463f7073"], "59e2a5bd-0e68-479b-925d-d4eea6e39ea6": ["7f90b1a4-3dce-4f0c-b32e-a216d940229e"], "1135dc3f-064d-4cec-a4fe-3ce3ecba4019": ["7f90b1a4-3dce-4f0c-b32e-a216d940229e"], "8a070389-de6a-4bd3-ad9b-3f3433716c45": ["5dbbadf2-fb8c-439d-94ed-d7b9d5f2814e"], "c2cbf044-8eeb-4859-bd01-30c3d7571586": ["5dbbadf2-fb8c-439d-94ed-d7b9d5f2814e"], "f99a9713-cc13-49a2-8a75-be44e1335bfd": ["140d27c8-e827-4165-9189-269d5b96f744"], "b2c8c8d2-c2b8-46ed-957e-887c57eb48e4": ["140d27c8-e827-4165-9189-269d5b96f744"], "dabbe08b-36e7-4ad2-a4a5-90cf132638c4": ["09752ebe-2625-47ef-914b-ab70f7b98b37"], "68664d87-4b94-4ca7-81b0-5f088b9192e0": ["09752ebe-2625-47ef-914b-ab70f7b98b37"], "de1293a5-895e-4c8d-82d3-ece90b470e43": ["4c1b9613-577b-4e92-b88c-a51590e51069"], "a5c49788-5821-4756-b41c-40a9de048900": ["4c1b9613-577b-4e92-b88c-a51590e51069"], "8ff197bd-5677-41d9-ab08-6424419cfd0c": ["aeaddd68-f4cf-4adb-9939-2495211ea537"], "c913fe69-e88a-4fd1-a37d-79a6922d40ec": ["aeaddd68-f4cf-4adb-9939-2495211ea537"], "2588afab-f37a-49ee-9fce-20584b157756": ["de8ac88a-f661-4a4d-935f-266748db582b"], "e08d4556-c7b0-4cce-834a-b68f41252ad5": ["de8ac88a-f661-4a4d-935f-266748db582b"], "1fc46ee4-3d82-4cf8-8d31-2f91a6804646": ["0d3614c8-a73e-459d-b8af-edfde80168e7"], "88c79617-dd16-4046-8d4e-a8912ffab8bd": ["0d3614c8-a73e-459d-b8af-edfde80168e7"], "774c2daf-4b60-45db-9374-03d75600b73c": ["7d56614f-d92f-4b77-8746-6bd44734b0d5"], "718bd332-bbbe-4763-8b23-fd6d60cb6ab6": ["7d56614f-d92f-4b77-8746-6bd44734b0d5"], "0986ec96-0593-494e-a32e-c772854774fd": ["644860d8-628e-4f24-a930-73a1930a709f"], "3f8d6208-5266-45e0-ab92-7b295a24c5a0": ["644860d8-628e-4f24-a930-73a1930a709f"], "c3531bd8-4d6b-4558-a910-d013c4c9794c": ["68644574-0607-44e0-9a14-e73915b58a22"], "1289347e-b104-4673-9ad7-b6b7f6d3974b": ["68644574-0607-44e0-9a14-e73915b58a22"], "c56f698d-7fb4-4c2d-a068-4a0664280574": ["a5d059a4-b420-4139-a551-61bec98192ce"], "c756c50d-913b-499d-a031-df63a895ae82": ["a5d059a4-b420-4139-a551-61bec98192ce"], "3ae8e29d-b77a-4716-9a66-ae4f45a59086": ["5a853c5a-bf31-4b51-a334-f3b4a2ea425d"], "ff242757-435a-4912-8f95-bc2dfa613456": ["5a853c5a-bf31-4b51-a334-f3b4a2ea425d"], "76d6a3b3-5b66-4492-8bd8-0fe06a8ae060": ["3bf43757-e79e-4f2f-992a-33c46e5b3638"], "d4b78bc0-0c02-45e4-b1b0-46ee1407f980": ["3bf43757-e79e-4f2f-992a-33c46e5b3638"], "8487a799-384f-481c-9881-637e8517f77e": ["ab201481-9c42-43c2-bb1b-d0f7e919e3a8"], "1ce0133a-dee0-46e2-9a04-d11fa5c4e423": ["ab201481-9c42-43c2-bb1b-d0f7e919e3a8"], "cb531d92-2d80-4989-bbfb-3d54cdc13c23": ["a34ab2f3-d86e-4a3a-b43f-075758f54b08"], "b416db1e-fb02-4156-add6-1d3fb0228466": ["a34ab2f3-d86e-4a3a-b43f-075758f54b08"], "08a75d4e-5e38-425f-a657-9cce809499bd": ["1183bdee-f14d-43ad-b527-ee4c698cfc5b"], "340711f2-12f4-4c65-8b42-a5b25c56e3d2": ["1183bdee-f14d-43ad-b527-ee4c698cfc5b"], "d4c4fbe5-ee74-4543-a4e6-c5f9f0a68aaa": ["32ae6495-af9f-4779-8ecd-e2170eb9a090"], "776c2c96-b70e-420e-8fae-30cab93c95bf": ["32ae6495-af9f-4779-8ecd-e2170eb9a090"], "79265b5a-9f86-48dd-8613-a8723fe05ccd": ["e177e84b-5fa3-47c0-b4c7-d1f08ad567d5"], "1a8200b8-8aff-4450-bb74-91f25e2ecafa": ["e177e84b-5fa3-47c0-b4c7-d1f08ad567d5"], "d0685267-f2d0-4d57-b652-6b2dc5d7263b": ["1e235644-c724-47d0-8d84-369e55f92142"], "68e5fbb1-16ee-4bb4-828a-7b4da60483ca": ["1e235644-c724-47d0-8d84-369e55f92142"], "a9ec91fe-8ecc-4155-a018-49ccb74b621f": ["e2abd54b-a3f5-49ba-ab1f-0d6d91d8626f"], "ecd647bb-0a96-4c67-b992-bd3b511b93d3": ["e2abd54b-a3f5-49ba-ab1f-0d6d91d8626f"], "a094a7cd-e6ba-49e2-8e34-a8e3d96db697": ["6d1918ef-26e1-4d08-9121-3a98504a93f6"], "fb3952df-91a1-4eb7-a4fe-2dffd694d914": ["6d1918ef-26e1-4d08-9121-3a98504a93f6"], "3f973a4e-79ff-4840-9bd8-2735ce78a8f4": ["1f73ae04-a46e-4ceb-a38c-19fce8702d29"], "a5e6dfa8-e5f7-454f-b9a9-9860a609b0e5": ["1f73ae04-a46e-4ceb-a38c-19fce8702d29"], "15c57a23-284e-4ad2-af57-d7be5165050a": ["66dee467-c747-4b01-95d0-7bcb2240e953"], "080de8c7-12b4-4bb7-8ebe-e2444d3ddf34": ["66dee467-c747-4b01-95d0-7bcb2240e953"], "7bca9f57-abec-4037-8d7e-f7c99fc2cce5": ["6e34ae22-1479-4f7e-a2d8-640e06350147"], "bd259dc8-768f-483a-accf-83bed92c9c44": ["6e34ae22-1479-4f7e-a2d8-640e06350147"], "2c97dc3b-7470-4242-afaf-1cf83b53ff6e": ["6d161b7e-ee7f-44ef-bf6e-332a7cb682ae"], "e8f95f28-3473-40c5-83c8-143bb76aac1a": ["6d161b7e-ee7f-44ef-bf6e-332a7cb682ae"], "6c8b172b-6d80-470e-b675-845b1b504cdd": ["96bdc4ab-4f58-401b-a698-f9dbd29e40e8"], "fe34eec0-4319-4cc6-85dc-93aab92d811e": ["96bdc4ab-4f58-401b-a698-f9dbd29e40e8"], "6a2a7685-4c9e-4a34-872c-22cc059c65b4": ["105e3a35-4b3c-4571-ae68-a31bbac67fc3"], "5123a6d6-113a-4af0-b4c4-f8f6ce6a742d": ["105e3a35-4b3c-4571-ae68-a31bbac67fc3"], "78d897db-d329-4a87-b09b-bd23a05a7a5a": ["95858828-03a1-4a50-a56f-926736148a52"], "2d304397-960e-4544-b3a5-575085aa0fc6": ["95858828-03a1-4a50-a56f-926736148a52"], "1c7e3bc2-8163-49f1-ac40-76e02e081ec8": ["cdbdf5b9-415e-49ae-9bd0-1ef86535f369"], "cb253d76-2360-4b49-ba23-5b2ef736452a": ["cdbdf5b9-415e-49ae-9bd0-1ef86535f369"], "2d9fad43-ddd0-4f83-b237-0f10f505ad6d": ["6c81ade6-c940-41a4-a4bc-8d66a427f204"], "b91eced4-6bb3-403e-b518-5cf667d5fa2e": ["6c81ade6-c940-41a4-a4bc-8d66a427f204"], "7cee07a4-9122-4049-a612-f9e41f45d6da": ["6c3faa81-a418-4af8-9327-4185a1422c07"], "1bb34796-e29e-4e24-b851-46b6980c3b5c": ["6c3faa81-a418-4af8-9327-4185a1422c07"], "b9ef00e2-32c0-4efc-a178-272bb8e46135": ["bca34828-d868-43fd-a8bf-f4cf1389325b"], "1744f3e8-f0a0-48f2-840e-77a11b0a420a": ["bca34828-d868-43fd-a8bf-f4cf1389325b"], "91549689-758b-4885-9375-ad9a33f88918": ["99d67b73-bfaf-4adb-87c8-dfd0f3c8aaa2"], "aea70b8b-cf08-4f9b-9542-1916d1e26e06": ["99d67b73-bfaf-4adb-87c8-dfd0f3c8aaa2"], "cfaf72cf-f0ca-46f6-9bf7-bd79c5e8408b": ["40baacdf-0a87-44c8-824d-a1081c88fdeb"], "bba2542e-fbb2-4ed9-aca1-da7cbd2a62a6": ["40baacdf-0a87-44c8-824d-a1081c88fdeb"], "aa9690fc-57bf-45cf-9790-e92b1d1c06e6": ["02260f23-46a0-49c6-bd38-5404e695bba4"], "c93488c7-a1ec-4f8b-b790-841576835554": ["02260f23-46a0-49c6-bd38-5404e695bba4"], "e8f1c227-6f26-40aa-8ef1-b756c1ad08de": ["176f31f9-a5a3-4443-8a00-f1fdb9da20db"], "30b446d3-4eb0-4514-a0dd-f153c4960e2d": ["176f31f9-a5a3-4443-8a00-f1fdb9da20db"], "3a525226-6fb3-478c-bd8e-8906798b8ca8": ["03118b9e-d481-464b-b50c-113647d7d625"], "c0625577-ade7-473c-852d-80f1a9320353": ["03118b9e-d481-464b-b50c-113647d7d625"], "0d90cecf-2ce2-410b-8414-c66d5793a4b7": ["dcac07f7-d854-4a98-bb08-ee4cd1eb03af"], "0d8c9001-24cb-4969-b36b-2e9b363be559": ["dcac07f7-d854-4a98-bb08-ee4cd1eb03af"], "5ac5e06d-8688-492b-ae45-2b39e5e51c3d": ["42ed9466-162b-477f-a298-d52f32bab6c9"], "5bb7546e-2f18-4dfc-a2dd-4c41b77f6685": ["42ed9466-162b-477f-a298-d52f32bab6c9"], "fcd7cdf2-af2e-4d19-a56c-5e97c1eda54c": ["356120ee-fe2d-4809-8639-353dacbc3587"], "e38d4782-20a8-4df9-8903-996e0c5e1f6c": ["356120ee-fe2d-4809-8639-353dacbc3587"], "5eaf0afb-5689-4a17-96d8-f123d3706382": ["346f4f12-9fb1-43b8-852e-b69edfb08548"], "9a789465-833f-4d7a-bb06-41ce5a395b80": ["346f4f12-9fb1-43b8-852e-b69edfb08548"], "158a198d-4f9d-4b06-a588-5c77d651dab9": ["94f94b3d-7b07-402a-a4cf-45c274bacc00"], "335faf2b-8087-43e5-b37f-424514bb7297": ["94f94b3d-7b07-402a-a4cf-45c274bacc00"], "d1a0e17f-a0a9-4c32-ac17-7f16ebfe26e1": ["24093628-6832-411b-8382-79cd86e6f97e"], "3941d085-7d21-425c-90d3-88f69587b8b4": ["24093628-6832-411b-8382-79cd86e6f97e"], "f7f58d87-195b-46a4-9220-77f92090b60b": ["36da4359-e2e0-4fa0-8b30-5a8999986959"], "9081af7d-e954-419e-b5ca-14c9448f0153": ["36da4359-e2e0-4fa0-8b30-5a8999986959"], "634f2932-248f-4175-9e58-893962f71ca4": ["d0905432-c78c-466b-af0b-c5fe0c021e36"], "b79ef8aa-0d00-48e2-8d13-0c191de6d48c": ["d0905432-c78c-466b-af0b-c5fe0c021e36"], "0a2d076d-2ef8-4667-a5f3-a3635ea0c7e3": ["d681af50-98ad-4d17-b95a-3768c00a2ebe"], "f38da9cb-8ea3-4bbf-b8e5-ff8904722ef8": ["d681af50-98ad-4d17-b95a-3768c00a2ebe"], "8f438a68-fb8f-4b2a-ac63-d1686deca389": ["f86b4bac-1923-40ce-9959-d0b8a42005de"], "7cef151c-eed4-4f66-9cad-1fff9936b7bd": ["f86b4bac-1923-40ce-9959-d0b8a42005de"], "256728c3-d715-49b3-ac43-d231f88c6ab1": ["da61b503-5529-4004-a333-9a9a9061d52e"], "b223c5b2-f231-470b-8c0f-e6ae7fd2acb0": ["da61b503-5529-4004-a333-9a9a9061d52e"], "a6560213-55e2-4d80-b6bc-4f9c0a8711f5": ["c1af1a5c-6836-4729-9c64-225668efe38f"], "c937e005-c042-4f3d-b542-8883cfd40cc3": ["c1af1a5c-6836-4729-9c64-225668efe38f"], "f6d43989-6fa3-4513-9bad-576f3d595863": ["636bc9c1-287f-4b4d-bc3f-3c195f1daf05"], "0474aa6a-dfff-4ceb-bd1f-2b5617f36dab": ["636bc9c1-287f-4b4d-bc3f-3c195f1daf05"], "177c8847-7d56-48b3-bef1-af323957be5d": ["d7566c58-77c4-4788-98de-823b02dc114b"], "70996d17-3084-4cbf-b503-d52f6d129787": ["d7566c58-77c4-4788-98de-823b02dc114b"], "289ad596-6154-420e-8903-cbd19b48b313": ["df0f42f1-a567-4028-83d2-f23f7aea0e5a"], "54433f32-17c6-4bf0-9267-01bc7234fcc6": ["df0f42f1-a567-4028-83d2-f23f7aea0e5a"], "75274291-5698-4ae8-b261-ebc72d443681": ["4f1b635c-5bf3-4893-a6fd-646bbcb5b3bd"], "918260c2-fa18-477f-a74d-b72852aa847a": ["4f1b635c-5bf3-4893-a6fd-646bbcb5b3bd"], "7360f4fd-23c4-4165-b400-203659dccfa6": ["50fcfa2b-5214-4596-9014-1a1b4c057da2"], "7cea5988-4563-4d33-9325-c49660b9e767": ["50fcfa2b-5214-4596-9014-1a1b4c057da2"], "4d2fae0e-c901-49f8-bd0f-42403199eda1": ["80d9d7dd-ba27-45b4-94f7-378c330d72f3"], "ebd4b5f3-9b59-45ab-86f8-774f42c33322": ["80d9d7dd-ba27-45b4-94f7-378c330d72f3"], "b3ca46d7-321f-47d9-9b23-f0642d4ec6ce": ["22c147ea-2296-4d29-918b-bb939d995b15"], "cd1dcfda-0c4d-4626-a782-665cdc9be51d": ["22c147ea-2296-4d29-918b-bb939d995b15"], "8eaa32a2-8b89-4937-9fa9-9ba3aa4bb142": ["4985cbbd-5c4f-4ad4-a733-85df57c61c2e"], "20f39bdd-9430-411e-bf79-d0da8963d7f1": ["4985cbbd-5c4f-4ad4-a733-85df57c61c2e"], "71c23d54-4519-4ef3-9269-0cd5e0a1d92d": ["3f31fc2f-7b03-427b-851e-38b91ddb05bf"], "04cfd980-6ca4-4070-b17d-fd35fcde62f5": ["3f31fc2f-7b03-427b-851e-38b91ddb05bf"], "d740bf18-2506-4b61-b5b4-e76d968af4b1": ["50641aec-722e-463c-9af9-b11329e50f0b"], "093a4a3a-dbcb-4626-b6bc-1b5ccbe835d8": ["50641aec-722e-463c-9af9-b11329e50f0b"], "ee70a9df-9739-46d9-ad66-86a7c3b7599d": ["1161220d-39f0-4db5-a2dd-39db4b1ee5f0"], "6515dfe0-47be-403e-b2ae-8ad85517b468": ["1161220d-39f0-4db5-a2dd-39db4b1ee5f0"], "bcc96150-9c93-4103-952a-40f7a20646c4": ["fe472712-d846-4950-af89-303191dc7901"], "b55b35dd-e357-4a7b-bf51-0c00fcbf1763": ["fe472712-d846-4950-af89-303191dc7901"], "567e0a56-5459-48c6-8ab5-d7ca35da516e": ["0d42b370-c957-4d18-9672-2900b830eaba"], "544dbbdc-cfca-4374-a38f-2d15df9cada4": ["0d42b370-c957-4d18-9672-2900b830eaba"], "93d0712d-574e-489e-972b-47a25e674984": ["938ca09a-b245-4fb7-b8c2-f7ac7f891e2f"], "f7a22cd1-113a-4f4c-b88b-23d541a03ebc": ["938ca09a-b245-4fb7-b8c2-f7ac7f891e2f"], "e214c7c2-1222-41a2-b4aa-e78a2c5aec4b": ["995f7fc9-2703-4d24-bdb3-86555ecaf0e4"], "866abe49-0ac3-4910-bda8-371a00914190": ["995f7fc9-2703-4d24-bdb3-86555ecaf0e4"], "8ea04020-83bb-49bf-b13f-7a91ad212525": ["95d27cee-5318-4d6f-99f2-2cd483a45d67"], "1ec70427-6c95-4b48-9097-d73ac2a083f8": ["95d27cee-5318-4d6f-99f2-2cd483a45d67"], "9d907393-5a7d-4830-8464-6f6e5fc99b98": ["3ff85a64-829b-4cd7-942e-9a77c30d11e2"], "8bd9add6-a51a-468f-bafb-454fcaeae457": ["3ff85a64-829b-4cd7-942e-9a77c30d11e2"], "75e453d7-bd6a-4680-94f8-d7eb73e2c8bc": ["6e93c080-8f7a-4b14-bc73-ba8d7d24c167"], "8fa55992-0293-4fac-8260-1bd5f6d5472c": ["6e93c080-8f7a-4b14-bc73-ba8d7d24c167"], "7fd2b6df-5097-4915-8dc1-0e144bce3a71": ["52102533-ec43-46bd-bfdd-10c77ae0b6b1"], "28760634-7038-4d54-9bc4-b548ee27d1f1": ["52102533-ec43-46bd-bfdd-10c77ae0b6b1"], "740d551c-0916-4c4a-80a2-41dcdf5c4613": ["316200a3-a584-4569-b0b2-d0c5cae669d4"], "2dee9fc8-53d3-4a28-b304-25865f0363cc": ["316200a3-a584-4569-b0b2-d0c5cae669d4"], "1e0282c9-1033-45d3-9918-e3c3424115df": ["e763e623-f294-4fcd-a4d5-7669459e4d84"], "f2ccf3ae-8a94-40c3-9e08-b672fd786110": ["e763e623-f294-4fcd-a4d5-7669459e4d84"], "c9dcc27d-49d9-4d2e-9c0f-b4cae5ac2451": ["f6a382ce-62ea-449b-9066-fd864664995b"], "f74186e1-1b4c-4d66-9ebb-8bce6605e743": ["f6a382ce-62ea-449b-9066-fd864664995b"], "b1909c26-6f21-4229-a747-b54133828c25": ["024822ec-199f-42a9-9b63-30d35fc817ca"], "0bf8aed5-b0b3-403e-9858-e92109aee9c6": ["024822ec-199f-42a9-9b63-30d35fc817ca"], "74a6d87c-bbfc-4539-b4ca-9bcd53a8fdbf": ["d3bffd95-900c-4465-9aa2-7b84465a400d"], "40cf331c-6f02-4dd2-b37e-39e33742fdb2": ["d3bffd95-900c-4465-9aa2-7b84465a400d"], "a57d6a5e-2b7a-45bc-8c91-077f5df75ba4": ["9f915464-7ebd-498c-839d-7659f332945e"], "9df9616c-6bea-45bd-969f-ff45234491d0": ["9f915464-7ebd-498c-839d-7659f332945e"], "0b797f9e-2b7c-4764-968f-3bc87c5a9f2c": ["02abfb88-9fc5-44de-a625-757ef1f34477"], "9b2e95bc-6bf8-4f0a-9af4-46317f34f9ef": ["02abfb88-9fc5-44de-a625-757ef1f34477"], "e4e115e8-ff6f-48c7-8da3-c21b96467c0a": ["d15d8973-0b4b-49ad-a520-551f61bc504c"], "c2627e56-b09e-4a3b-96ae-3b456664b411": ["d15d8973-0b4b-49ad-a520-551f61bc504c"], "dbcf62f6-a5b6-4a7e-9918-bc2c26828e61": ["a776d530-8f77-4c25-9b36-b809f57638b4"], "8316595a-749c-4020-8ead-cbd70f04eb04": ["a776d530-8f77-4c25-9b36-b809f57638b4"], "4e5bbe4c-b950-4ebc-bcd2-e432921ce48c": ["d4e7ff9a-6353-4f21-95f3-dede503ada22"], "b35a8f31-ef19-4f8b-a12c-1cd7b37f29d9": ["d4e7ff9a-6353-4f21-95f3-dede503ada22"], "2d0d24f3-fd00-44ad-91e7-dcf0669f6e18": ["b5204fed-23ad-4472-be11-2fc311f2b4e1"], "3e53ad48-70a1-46f7-ad63-0fd2a0cc2e7b": ["b5204fed-23ad-4472-be11-2fc311f2b4e1"], "53ff8a47-08f0-45a5-928a-369801d7e045": ["b49b072a-bc97-4c70-ad10-4d264203b92a"], "a6e2f8bc-cf27-44d8-ab5c-2f9a3784bf95": ["b49b072a-bc97-4c70-ad10-4d264203b92a"], "364d8764-a16e-4f67-8498-8903efb47ac9": ["ba6f185e-5027-4283-9964-14a64b5705eb"], "44ce44dc-061e-4e06-a79f-7ce59dc572a7": ["ba6f185e-5027-4283-9964-14a64b5705eb"], "32fb0a5d-31cc-4350-a38b-e169b8a81e87": ["78231644-741c-4c3b-aad4-1e0ed01bc845"], "4c4f410f-434c-4321-90e4-b91cd050593d": ["78231644-741c-4c3b-aad4-1e0ed01bc845"], "586f33e8-28fa-407d-8c08-d6f28ba1069c": ["8c45daf5-00b6-447b-8035-3e9f0bc99a15"], "13c35e57-3043-4223-8a22-2d08482e57da": ["8c45daf5-00b6-447b-8035-3e9f0bc99a15"], "7c47b8b7-ca21-489c-8965-dea51cc9c623": ["5423b83c-68ec-4676-8597-f4bbb130abe2"], "8858acb6-5c05-4218-a1fd-21a527c8ca85": ["5423b83c-68ec-4676-8597-f4bbb130abe2"], "c0812d2f-6b02-436f-8ffc-a99920f06b8a": ["2d005bdd-fa47-4e95-b7be-8880e3500bd1"], "4ccb0660-e4b7-47e2-85a7-3c28cba6046d": ["2d005bdd-fa47-4e95-b7be-8880e3500bd1"], "fb45ab41-7416-46cd-bc5c-11f86df9370d": ["c3447590-2bb6-46c1-aaf2-bf23b6463d68"], "33b8c86e-a873-43f1-9001-6a609ec274a1": ["c3447590-2bb6-46c1-aaf2-bf23b6463d68"], "c9fd5148-2c5e-4ba6-b6db-253ac39cb646": ["830449a3-0191-4ecc-a258-c338fcd00e8c"], "b7a979af-d87c-4977-a78f-6904c47af111": ["830449a3-0191-4ecc-a258-c338fcd00e8c"], "b308bb1b-c5d1-487c-9527-7e0e230d0385": ["21088631-e3d1-4d8c-b477-b4ecd4eccc4e"], "2d0e28db-91b7-4796-81bd-d42b59b4a2bf": ["21088631-e3d1-4d8c-b477-b4ecd4eccc4e"], "5c9cc342-c6cb-4e59-9ea1-38e8175c0d0d": ["9abc89db-acd0-4fd5-b609-49f8c2969f64"], "d4efd5bc-1364-4cf4-9aa3-176cebc9e6f3": ["9abc89db-acd0-4fd5-b609-49f8c2969f64"], "d53d3398-3f8c-4c10-9a37-e01e006e83ae": ["5c6fe855-984f-43e5-841a-899681f8beee"], "13fe8689-8802-4f44-a752-614f87c1c3c1": ["5c6fe855-984f-43e5-841a-899681f8beee"], "471a20f5-de5d-4a08-a951-644615e85580": ["e52bd279-ef80-42b5-ad29-93a9a821d6dd"], "601204b7-95d1-4ebc-b8d4-b253bb281838": ["e52bd279-ef80-42b5-ad29-93a9a821d6dd"], "ae782217-7305-40d6-843d-41e10b17b252": ["c28119df-7254-4948-8094-0781749c2511"], "d5c7e143-56b1-4dec-9d37-edf4dbde2fa2": ["c28119df-7254-4948-8094-0781749c2511"], "2dabdd33-5b0e-4d7a-b16f-045b010d5877": ["9f8916cc-9475-43af-be9c-bded92303496"], "3dfac66b-44bb-409d-be0c-80d942cccec3": ["9f8916cc-9475-43af-be9c-bded92303496"], "8b24aca0-ec35-4b64-a86b-74fc4d232d92": ["9fe78e51-b3d4-4bcd-aae4-3e9b7d55fe34"], "989047e8-b548-46c4-8e8d-7952dc1b2b7f": ["9fe78e51-b3d4-4bcd-aae4-3e9b7d55fe34"], "726d9606-accc-43d4-8d33-51c9647c69fb": ["1d2cdee9-33f4-457c-80c9-9d34aaae0ca5"], "b3ad944e-1bfd-4b1a-81b9-38ef6b4c803f": ["1d2cdee9-33f4-457c-80c9-9d34aaae0ca5"], "404e2320-b833-4fbd-93a5-ce39c834d81f": ["16669122-2f44-4659-a7b9-63581ee1b2e5"], "ffdb4bbb-8982-455c-9e8e-41e601db37fb": ["16669122-2f44-4659-a7b9-63581ee1b2e5"], "dc177dac-d77d-4b08-a043-745af077508c": ["57db0186-8c6f-4540-b0bb-2430d7fcefd8"], "27f2ea5e-3552-4a45-80bc-ba55dfc81df4": ["57db0186-8c6f-4540-b0bb-2430d7fcefd8"], "0ec16069-b401-4f35-ac41-e7d76b1fd9e7": ["2bfeb503-7c1d-46d2-9c5a-552d811d73cc"], "9080e99a-4724-4504-a934-cb216f3509a7": ["2bfeb503-7c1d-46d2-9c5a-552d811d73cc"], "e36b10c8-f31a-4be1-a7d6-ee5800732e44": ["2dad5d3b-d624-42ea-ab87-0991ae3708db"], "f8525da1-479e-49d6-9cf4-5211c518d05b": ["2dad5d3b-d624-42ea-ab87-0991ae3708db"], "d21eeb74-e0f2-44ae-8c14-4c0011661a32": ["42984b18-3d37-4f1f-8b31-62bb3060e5ce"], "8f9b35fc-ba72-455c-b551-37f37f0d1257": ["42984b18-3d37-4f1f-8b31-62bb3060e5ce"], "8ead70a8-48e5-4880-a02c-4fd4a2bddbc8": ["f04b9b9b-c67d-4891-96e8-a5a52a2f3c2a"], "4a564750-e506-4fa6-9035-69573e7bde6c": ["f04b9b9b-c67d-4891-96e8-a5a52a2f3c2a"], "b3bd4e00-dbe7-465a-af3e-e0e0b7da2f0a": ["1318e424-c54a-4d4f-be10-0553dca407e5"], "cbadb57d-ba6e-49d5-b9d2-25c14076f9a2": ["1318e424-c54a-4d4f-be10-0553dca407e5"], "d11d86b9-e0a6-4c17-aa69-e4912ba398dd": ["43c87fed-ce22-49f6-a7f0-1fed185b0567"], "165b1687-6e01-489c-a982-0875c402c585": ["43c87fed-ce22-49f6-a7f0-1fed185b0567"], "04e143d3-d9d0-44dd-9323-eb303cbca778": ["0aea360c-5590-43b1-8aad-a71573d1e187"], "755c99fb-6837-4514-be78-1c0bf20393a7": ["0aea360c-5590-43b1-8aad-a71573d1e187"], "53e2fdd1-3363-465c-ae70-683eab2b8e87": ["da755f86-8934-4355-b086-156085b7e640"], "bb2a7f2a-5c49-49e9-a791-00bf1b7de9ec": ["da755f86-8934-4355-b086-156085b7e640"], "173a9a00-e617-4f2d-8622-3308910f9f6c": ["3d3c8533-5e3f-424c-b974-dc106996e010"], "dfb7f8c9-f6b8-4ce6-93e9-58c4969ee645": ["3d3c8533-5e3f-424c-b974-dc106996e010"], "d12561dc-065e-465f-858e-559c910891af": ["774fdca2-4ffd-452b-b281-4186b165f56f"], "5de4d646-c6a7-474d-ac7b-b96ad48eb2c6": ["774fdca2-4ffd-452b-b281-4186b165f56f"], "e4a0b935-9fa9-4cc6-b57c-aec87e0e4805": ["dbe66a6c-2aa3-47e2-ad40-0a4e9d591058"], "e7f6f68d-0e88-4072-a05f-145e958035da": ["dbe66a6c-2aa3-47e2-ad40-0a4e9d591058"], "742fbb63-c789-4f05-8f62-88003a1b9ef8": ["5334dee8-9080-4fda-bf52-a85413383bd6"], "5148fe56-5021-4a8b-85d2-4caf6c13cb8a": ["5334dee8-9080-4fda-bf52-a85413383bd6"], "19cb2574-fefb-46fd-b982-bfa2330ab7a6": ["6911e32f-93a3-4528-91d4-2265af0ae9fe"], "bd12c39b-e8fd-4a4b-8763-81159fd2b0fb": ["6911e32f-93a3-4528-91d4-2265af0ae9fe"], "ff527326-bdbf-416a-a9f8-f2dfe167b658": ["e1a82daf-1ae5-4f1f-8ade-1e6ae536e1e8"], "4f4a4f08-0465-433c-ad02-f7c21c2f09b1": ["e1a82daf-1ae5-4f1f-8ade-1e6ae536e1e8"], "d0de0f78-5185-4f37-b945-78e4df7309ab": ["0a485a3e-5017-40ce-8057-3c8880399ed4"], "616e99a6-6a9f-4fa6-8f86-6bf6fd41c02e": ["0a485a3e-5017-40ce-8057-3c8880399ed4"], "c99c3b5f-b6a6-4cdc-ba86-1ac6ca1e4eb4": ["f7c3fd3c-1a3a-43d9-b021-fa6135a9f1fb"], "658d15a8-35b0-4cb7-8246-47d63732acd7": ["f7c3fd3c-1a3a-43d9-b021-fa6135a9f1fb"], "1e7a1c44-1a5e-4fcd-b83a-77123bde3ade": ["cae90d9f-da9f-4029-a071-b0f3c75c697f"], "389b8cd6-6265-4313-8a98-992df404269c": ["cae90d9f-da9f-4029-a071-b0f3c75c697f"], "8d87da82-9abb-4369-af65-997ec4cfcb52": ["d7967f08-08c2-4c40-8e31-183589d27ced"], "8b62230f-91bd-4ac8-a7f4-1eb4b38fcdcb": ["d7967f08-08c2-4c40-8e31-183589d27ced"], "46ac4d01-4680-454b-96f6-cf7c02d6b6d8": ["7066ff86-7ca7-466c-8a05-a732d9843f02"], "f33ea0dc-d5e6-48ae-9313-c1609df05e41": ["7066ff86-7ca7-466c-8a05-a732d9843f02"], "430174e7-5a13-4506-b82a-3925702f0d0c": ["89f5bb70-fd32-4932-949f-cbea6a68d77f"], "0f6597ce-48c3-4aef-8a95-7826369eb09f": ["89f5bb70-fd32-4932-949f-cbea6a68d77f"], "5f704986-e0af-435c-a0c6-71efd7e7ccc2": ["0bb45e9c-0a66-41a7-9b4f-5b457b11b4e5"], "788c0cdf-7d5a-479d-8898-d42463f9546a": ["0bb45e9c-0a66-41a7-9b4f-5b457b11b4e5"], "dd029f8e-6f23-4cb6-b5ef-f299db13aab2": ["8f5ad5e2-9dbe-4408-b78c-4a9b70773bd9"], "8bf00b5c-44f8-47cc-a322-10738c96c32e": ["8f5ad5e2-9dbe-4408-b78c-4a9b70773bd9"], "78075cd6-8c7b-4abc-8d63-d0ef94645ba7": ["5792f677-5cad-4c58-a91b-40f5ea01e1c4"], "c8420a50-c6db-4b81-9ae8-c25f60dc0d77": ["5792f677-5cad-4c58-a91b-40f5ea01e1c4"], "d5c1bd2f-a80d-4cd8-b426-7dcb0667cd3c": ["f190de5e-73c2-4dce-8579-79557869333e"], "6456fa39-7975-4360-9d6c-d6ddf672a83b": ["f190de5e-73c2-4dce-8579-79557869333e"], "be6ea7c0-9a4e-4cde-bf55-f1b873737178": ["532a7c54-4595-45b0-b35c-295176951d06"], "4e030707-3f3a-4f50-89e9-323023220aa1": ["532a7c54-4595-45b0-b35c-295176951d06"], "5f4262a0-2d7b-421b-a3ff-2f51833c1b4c": ["9eb45dfd-c5bf-4d6b-a84a-70f380690d56"], "4eec5904-f05a-41e6-b4ac-10d0de45ff6d": ["9eb45dfd-c5bf-4d6b-a84a-70f380690d56"], "10d544e9-2a1a-4a5a-9f20-de3cf74b7a36": ["bfa87940-ec4b-4a06-85a6-3f5a68f5cc0e"], "a165f551-89c2-4fe7-8aa1-fd0387e5434f": ["bfa87940-ec4b-4a06-85a6-3f5a68f5cc0e"], "75eadbeb-1f13-4d2e-8ee1-9f4e2e58159c": ["323f8410-bc1e-4a11-9e9a-42e2c67f822d"], "5c612444-e868-4080-9abe-57485b2a9d90": ["323f8410-bc1e-4a11-9e9a-42e2c67f822d"], "4b9445a1-f5ef-4932-ae0e-39a661300ddb": ["13ae3ae3-08e2-4268-8e62-d4486f6df36e"], "ad8e0480-0fa0-48ea-887b-eaaa401c95e7": ["13ae3ae3-08e2-4268-8e62-d4486f6df36e"], "ee6006a8-ef60-41cd-aaa0-010454a48e8a": ["1bfc2afa-599b-453a-a40f-75bcf8c0763a"], "086f56be-4cd8-4405-8a5e-c10dd112f97d": ["1bfc2afa-599b-453a-a40f-75bcf8c0763a"], "2f0f88ff-9f0b-4b73-9943-85978b3e00f1": ["05a3d0a7-de88-4be6-beae-67924292aedf"], "27ba1612-2d03-418b-a8c8-8d19520b1575": ["05a3d0a7-de88-4be6-beae-67924292aedf"], "c5f79245-fdc3-4deb-b97d-290b68cf1329": ["28f1a0e1-562b-4fcd-aaeb-428795c7f34f"], "9eda981d-2da9-4183-a559-81c9bbc91e33": ["28f1a0e1-562b-4fcd-aaeb-428795c7f34f"], "f92e7050-f57b-4ec3-b387-b59d74adb652": ["c5ca27dd-7e96-4d12-a14f-3af4f4eee176"], "83428902-100c-4b75-80da-b6637db6a461": ["c5ca27dd-7e96-4d12-a14f-3af4f4eee176"], "376cb100-cf50-4629-9ce0-9ad380a26826": ["78a89652-0548-440e-bec9-d3005aa88ac7"], "99a12c6e-b535-4812-8cdb-37f593491b89": ["78a89652-0548-440e-bec9-d3005aa88ac7"], "ce28e924-6d12-4551-ad40-737f179cb402": ["dc18b164-fffe-44b2-b74e-f9db7c2ebacb"], "3a32f0a9-ba59-4055-b659-96f4072c316c": ["dc18b164-fffe-44b2-b74e-f9db7c2ebacb"], "0fa1eabd-0113-4fc1-a0e8-0ace31885ff9": ["088df868-daf9-4fae-ae73-861ea5bb690f"], "9c8670fb-d554-426f-bc93-da518d0bd18b": ["088df868-daf9-4fae-ae73-861ea5bb690f"], "6876fb92-2658-4846-b2ab-697c83e9c2fa": ["b018149a-a434-46da-8577-a469ad6bcb63"], "c0aa4427-fea2-465c-b5d1-f989274144f3": ["b018149a-a434-46da-8577-a469ad6bcb63"], "6355a28d-2197-426e-8655-a61386d5f31b": ["0c19cf39-4942-4406-8bf2-85d42eebc993"], "446a7d6b-209e-4d15-a067-22e1895c920e": ["0c19cf39-4942-4406-8bf2-85d42eebc993"], "69d1deff-2013-433c-8de6-7a1670429685": ["3a1d8ee2-bae3-477d-b227-15ef1482c44a"], "6c6ccc85-6a96-44d3-84d6-03a82faa7b2c": ["3a1d8ee2-bae3-477d-b227-15ef1482c44a"], "dceaf995-9885-461a-97e4-6c0c1fd3c138": ["8b1fcfa2-946e-498d-a8de-8de53ab413cd"], "48d6f7d3-26ac-4c49-936f-9d7a7de6190e": ["8b1fcfa2-946e-498d-a8de-8de53ab413cd"], "2273b5e9-88f0-4c57-a2bc-9cd62cd6f4cb": ["c3693ee2-d2c7-44ba-bc2d-e38572141bf9"], "601c871b-9fb1-4c05-a50d-cdcf9172379e": ["c3693ee2-d2c7-44ba-bc2d-e38572141bf9"], "d76d70f5-a376-48fe-9a32-895e6c83f139": ["d6c5dd4e-41d4-4567-b89c-d3fa674cbb25"], "bf4f0887-b49f-41cd-a71c-64af97a19dad": ["d6c5dd4e-41d4-4567-b89c-d3fa674cbb25"], "96ad2744-111b-4fc7-86b4-5b3f057b0b3c": ["81df079a-9798-4743-a9d4-3e2432adbd5a"], "3da29286-6d4d-4e43-9ba4-da17ba5facac": ["81df079a-9798-4743-a9d4-3e2432adbd5a"], "0b504667-87a0-4366-ae77-5bd2f5eb28b7": ["537e0fe3-9ca5-485f-b4d7-b13585317bd5"], "6448e232-c698-4d38-a45e-91ae4d6cc452": ["537e0fe3-9ca5-485f-b4d7-b13585317bd5"], "5edaf5c9-75d5-4990-b4fc-7c496834e70a": ["04f8ec9d-65b0-4df4-ade1-b65b48efc07b"], "73481c62-564c-4853-814f-44ea0d0e4b1c": ["04f8ec9d-65b0-4df4-ade1-b65b48efc07b"], "1fd9ce4c-a2ba-4bf9-b3a4-4359da0b0b5b": ["f00bb9bf-4b70-437b-8a90-b37a0a6351ac"], "0f4a2a64-8ffa-4dce-aa9e-0e68c1cf9bc2": ["f00bb9bf-4b70-437b-8a90-b37a0a6351ac"], "ab249647-02cb-4f68-a948-c5109ef0f32f": ["45a082cf-57e6-4e1e-9f7a-9a984ed0877b"], "91e512e4-baf9-4501-b579-cc832d0087c6": ["45a082cf-57e6-4e1e-9f7a-9a984ed0877b"], "8bd25fc5-2432-4bf7-9b1d-307c90a0d06b": ["7c0b3bfb-8101-472f-9c56-8a8d40de6af1"], "4b49e1ed-e185-4bc8-8ab3-805607edbe34": ["7c0b3bfb-8101-472f-9c56-8a8d40de6af1"], "e6c7fcd2-9dd2-4349-8ce0-e553562bb531": ["87ed7253-2713-45cb-8750-1f2088286a57"], "ccd8073e-2626-4050-99c5-0ee31be3adb1": ["87ed7253-2713-45cb-8750-1f2088286a57"], "bb91d044-c074-4900-bad5-3aaa4360331c": ["5dee00a3-3603-466c-a0e6-b8c083e458a3"], "25b02628-60e3-4fd8-a8c8-856c647b8097": ["5dee00a3-3603-466c-a0e6-b8c083e458a3"], "80cbdbd3-d5f3-431d-a9ce-0c2167e2708c": ["dcb053b8-a3e9-446c-aec8-dd5aa9eebee6"], "09ead5b8-16b7-4b74-8fa1-52fa9d48d95e": ["dcb053b8-a3e9-446c-aec8-dd5aa9eebee6"], "46db5d9b-d49e-4f58-b5c6-ec82be26b6da": ["221210bf-54be-4bde-9b4d-6758822209e3"], "c058c324-bfda-420a-bc1f-fac2e3226bd7": ["221210bf-54be-4bde-9b4d-6758822209e3"], "845c053a-95a7-47b9-8398-058c973f3933": ["0d8e19f4-54fe-4183-98da-b9f3678d5cdc"], "e4056b12-d211-4577-a0be-1736a7e8fac6": ["0d8e19f4-54fe-4183-98da-b9f3678d5cdc"], "dafddd4f-7c00-421d-a643-7927a49985f7": ["e2b4ea07-01a5-4a7c-98f8-b5830a12058f"], "13d8d0d6-2c75-4fce-b24e-b431066e0f8e": ["e2b4ea07-01a5-4a7c-98f8-b5830a12058f"], "e35f1cef-ed53-4868-aa77-9c366e69b8a0": ["7ee26c75-8b73-4c6c-83be-72a851b86635"], "69453724-0f0d-4302-9497-e1b22b2e5474": ["7ee26c75-8b73-4c6c-83be-72a851b86635"], "ece6ffc1-205f-4a0d-9b0a-dd5f420e4568": ["28b76743-9fa1-462b-b172-e7acba50c6e5"], "c8568a75-8fd9-4747-b8b4-02c7b919b106": ["28b76743-9fa1-462b-b172-e7acba50c6e5"], "058b7ea3-d4df-48a4-ac23-0281da61e885": ["7ecdeb82-b51a-46bc-b4db-566d0b704cd5"], "2746ee62-cbfc-421c-9277-6a0e96ba43ef": ["7ecdeb82-b51a-46bc-b4db-566d0b704cd5"], "310a2e9b-a739-4d2a-82c6-d7a7bb4b844d": ["aa2fbd96-ee76-41dd-8c96-9381ca5d4884"], "3f532c99-11b9-4b34-ad08-e65d9ae7ef59": ["aa2fbd96-ee76-41dd-8c96-9381ca5d4884"], "c07af445-cc07-44bf-bd5c-509c4e9192df": ["90763949-ad14-4880-9c26-20a9bb81300c"], "fc1cb5a1-6c80-4a45-baf1-b0bf26a5dde4": ["90763949-ad14-4880-9c26-20a9bb81300c"], "f4aa7230-04b2-4f25-928d-a2be92dd6b63": ["9bcc42ec-7774-42ee-a57e-793dfb85bee0"], "a4b46362-ac8f-46df-8e12-2be148acea43": ["9bcc42ec-7774-42ee-a57e-793dfb85bee0"], "752a9928-d622-4cbf-9734-c8d9b5677caa": ["eb3c6f41-8d99-4984-b4a0-093132ea5982"], "a4f73f0d-c386-4b9f-8d1d-7b83cabae175": ["eb3c6f41-8d99-4984-b4a0-093132ea5982"], "d2e2ba9e-02e9-49f6-b92b-e100b94091df": ["da27cc26-7b3f-4a2c-8ea0-5c9ad646e003"], "12eabdeb-4904-4dfb-8e85-2c02d8a59283": ["da27cc26-7b3f-4a2c-8ea0-5c9ad646e003"], "dbbb8275-c007-46a1-ae2a-d302c46b95e6": ["0e1d87aa-f5b2-4058-8ff0-3a9f94fad13e"], "ccfc76f9-fc53-401d-a7a5-378d586c82db": ["0e1d87aa-f5b2-4058-8ff0-3a9f94fad13e"], "d6640e84-01e2-4b88-acc4-aa0ef66be16d": ["abede939-2834-48a9-9673-0d0e6e461a4b"], "8989d8eb-cfbf-4789-92b0-2d5131cad786": ["abede939-2834-48a9-9673-0d0e6e461a4b"], "e3852c0c-f494-452f-a3d6-f2f7fc9af9b3": ["3e46a035-94ad-4b14-a483-077259c0fd2d"], "137583ee-c8f6-4124-bf81-32bb7cb09621": ["3e46a035-94ad-4b14-a483-077259c0fd2d"], "f368d6d8-7118-40a3-ba1d-b8c2a85dd6b0": ["58d34680-b065-415a-8060-33b33603285e"], "4b99f5b5-8f13-41b2-9fe0-ca8e805a751c": ["58d34680-b065-415a-8060-33b33603285e"], "94dc1ae9-4abe-4cd8-a84f-9ae7aa3ba91b": ["a6e5227c-f2d1-4616-af21-cadb0d23b6c9"], "0e8ae25f-3371-487b-a97a-1a568a05ab06": ["a6e5227c-f2d1-4616-af21-cadb0d23b6c9"], "6b2599e7-760f-4584-ba72-ae8daeadb593": ["978b4f55-dac9-4d34-b44b-add468bf690a"], "be7fe999-816c-482a-8d65-aa898161e56a": ["978b4f55-dac9-4d34-b44b-add468bf690a"], "ab5f4e81-953e-4157-85a3-f04b7404e15b": ["fb0228b4-ced6-4d4a-a12d-9f173dc9f143"], "bd568974-81fe-4c3c-beb7-8bdf372a8d84": ["fb0228b4-ced6-4d4a-a12d-9f173dc9f143"], "8c4dd40b-b7c8-4677-b232-fe8c5e42df92": ["5d73a454-540a-4b78-a13e-ef4a7e934d6b"], "de48bd92-1ed5-458c-8d3f-2b81fe81df4a": ["5d73a454-540a-4b78-a13e-ef4a7e934d6b"], "9a98c566-3b24-4de8-b987-8013bbb4fe7e": ["00e0bb2e-3571-4a00-b31f-19ea45b08393"], "9edd08db-fe9d-4052-a7db-ce416f245535": ["00e0bb2e-3571-4a00-b31f-19ea45b08393"], "d6fe62bb-82fc-43b1-aad1-32daa672e08b": ["845d450c-b554-41e3-aafd-512e1e85f719"], "ea844ea8-f931-4aa0-87e7-ac1e49be3575": ["845d450c-b554-41e3-aafd-512e1e85f719"], "5181db5a-dbf5-4997-9285-4b1da51101dc": ["707983e7-d53a-4279-a4d1-e8b2e1a9b802"], "202ce89e-3311-4ec0-9431-28e4377b5f8b": ["707983e7-d53a-4279-a4d1-e8b2e1a9b802"], "848dffd1-b843-48c4-973f-216bbd6cf0e4": ["1afff266-c58b-4277-a2c2-7cda349c349e"], "2e9e4b03-31b1-4e8c-9f95-fa3f35244655": ["1afff266-c58b-4277-a2c2-7cda349c349e"], "cfcdc1f7-9d1a-4aea-acaa-b1e3014b89e3": ["021438a9-7b61-4129-8a65-e7df339e047c"], "d8577583-be9f-4caa-a375-ffc17c0528dd": ["021438a9-7b61-4129-8a65-e7df339e047c"], "4da22628-bce5-4f7e-9b69-704e0f55e2bb": ["cc82a082-6304-4d0b-938b-d63f4a2786c3"], "fcfd3b9b-c5df-4b8c-b218-5a2c4f0f382e": ["cc82a082-6304-4d0b-938b-d63f4a2786c3"], "cc60ef94-01ba-4a24-ac2e-a2c63146d12a": ["8c90f61c-4e72-4c08-91f0-8fb462795143"], "1f8ff266-2372-4138-b4ed-3ed7f62195e5": ["8c90f61c-4e72-4c08-91f0-8fb462795143"], "3c22a3c9-47e1-4f70-9fe6-e0de506176ae": ["b8b2673b-a2f9-4329-a235-d9224d6c33de"], "7e77c3cc-42b6-438b-acd1-3a959d3b1d7b": ["b8b2673b-a2f9-4329-a235-d9224d6c33de"], "d328a0d2-a4c0-4f82-bd4d-5233fbc0fdf0": ["0e24f8e2-6515-4737-b54e-dcf7ed397acf"], "14b6d582-668c-428e-b7f9-c2ba6da3d056": ["0e24f8e2-6515-4737-b54e-dcf7ed397acf"], "7e7765b6-0a23-4a9a-8e26-a4bab6daa8a7": ["041b59db-1ea6-4cde-8c8e-c4faed89fa43"], "139708f0-fa07-478e-a436-f1e8b6b95626": ["041b59db-1ea6-4cde-8c8e-c4faed89fa43"], "d0894454-903f-443c-8c9b-7df91f91b377": ["d5ee73c8-8acd-4330-9620-da2a0105bced"], "c4bb27b8-b81c-4415-afe7-8dd8ec2f552a": ["d5ee73c8-8acd-4330-9620-da2a0105bced"], "7e0c3e28-1655-4bc6-8d14-32d9dcd24a7f": ["f9e6492c-0367-49c4-be60-5cd07b437d2f"], "990b5a5c-6883-4633-b43c-c801ba936d6e": ["f9e6492c-0367-49c4-be60-5cd07b437d2f"], "4d7d1141-c49f-4531-b087-a8f92009121c": ["9345b1c1-2285-4605-96ec-076bf6dc2afe"], "55a49fd9-4a9f-48f2-a482-2554e08d305d": ["9345b1c1-2285-4605-96ec-076bf6dc2afe"], "a986540b-9457-4ff6-8ddf-3bdd0450542f": ["9c88caf6-46af-4bbc-9d43-94553556a33c"], "f95cecbf-0a17-4720-83cf-68fe54cc4b12": ["9c88caf6-46af-4bbc-9d43-94553556a33c"], "f4d28131-84e6-4dac-b128-725ab1af01b3": ["e3b1ce9a-15c8-4d75-a308-e66c236c0207"], "482a4559-df60-4c5c-a52c-0eaf05fc971f": ["e3b1ce9a-15c8-4d75-a308-e66c236c0207"], "2908d1fa-7932-4bcb-b57b-900577a0c365": ["4cb351c4-8f27-42e1-87f1-d89fa5453097"], "b0d08693-156b-456b-b2a2-c107d5b7cd84": ["4cb351c4-8f27-42e1-87f1-d89fa5453097"], "18e2d4ba-eab6-4670-b79a-c9e352aeb1a2": ["76597455-0cba-402e-b881-b4a21fffb63e"], "32e255ee-a360-4dd8-a51a-6203464a2f48": ["76597455-0cba-402e-b881-b4a21fffb63e"], "70c2223a-c14d-4d72-944b-38974b6d5b1f": ["ee68c7e7-d917-4dba-8239-3a883a2f8682"], "148e35d9-a3d7-4a01-9a48-a82514256631": ["ee68c7e7-d917-4dba-8239-3a883a2f8682"], "4a6ad75d-142d-49df-be16-ec7c09643ab2": ["5602e92a-014c-4e6c-9172-3d7947c13a8b"], "81901d6d-6079-4cf1-b192-6c213ee34530": ["5602e92a-014c-4e6c-9172-3d7947c13a8b"], "b4434c86-49c3-405b-baf4-5de74695b4d4": ["626ccc46-7957-4a03-b85a-474db380c8c6"], "f6102452-91ba-461e-9e48-4c5768804fc5": ["626ccc46-7957-4a03-b85a-474db380c8c6"], "2c1d42e9-3b9b-40cc-b837-b16c8096eea4": ["60376609-8fe3-48c6-b9c1-3801d6aa6b46"], "6949c62e-d251-4538-ba2b-c2bdc309b910": ["60376609-8fe3-48c6-b9c1-3801d6aa6b46"], "30c72483-ede4-4ac4-8e3d-e0ce11331f88": ["edc435e8-2eb8-4de9-b13f-00c956ac3d9d"], "0ac9cea0-274a-41e2-8752-3db06c915b5a": ["edc435e8-2eb8-4de9-b13f-00c956ac3d9d"], "4b9cd36e-be17-497d-a469-3ed5cd1e7fb2": ["3d93cddb-b9fa-4b89-b178-1b60704a2dcf"], "c4cb2208-a544-44ba-8c41-a296721dec23": ["3d93cddb-b9fa-4b89-b178-1b60704a2dcf"], "3248f589-422a-42f7-a7e4-29ed118a338f": ["9082df48-6fcf-4b64-868a-798a4438e9ed"], "9a814a65-bd2c-429c-bac2-7b11ec231da1": ["9082df48-6fcf-4b64-868a-798a4438e9ed"], "6bcc7349-f4ac-491d-989b-da3e580a48d5": ["77bab743-a9f6-43de-b14e-f65897a32587"], "f6b1dfa7-e3a8-47a8-b4d4-ace428b9e425": ["77bab743-a9f6-43de-b14e-f65897a32587"], "b4a90f75-3e2f-47dd-a8cd-20aaa040c5de": ["344d7640-42cf-4927-882d-022407170f5b"], "fbc2fed9-8a8f-46d6-a79e-a2b62c3f20f9": ["344d7640-42cf-4927-882d-022407170f5b"], "c8d936f1-053e-4b61-b177-b615e1dff214": ["de7d48a1-9d89-4fed-9d7d-7ca4cc909a73"], "f102af8c-d483-4d42-af05-8432a043c17b": ["de7d48a1-9d89-4fed-9d7d-7ca4cc909a73"], "c1101be1-de7f-48ac-836a-f4e65a46128a": ["e64d762a-1ddf-4d87-96d1-adea0be8f78d"], "f7125720-08c3-497c-9589-af787701db23": ["e64d762a-1ddf-4d87-96d1-adea0be8f78d"], "432a5b0f-667b-419d-97b1-8b7c7fd9af86": ["c6b4c06b-1542-4db3-b3ea-0f97cd974b26"], "ed2ed429-d365-4d00-8ed9-1d0e1b3a359c": ["c6b4c06b-1542-4db3-b3ea-0f97cd974b26"], "8a97e320-aea1-44b9-b34c-2928be1bfb6b": ["818609b0-01e9-4cb2-954a-b0a662747101"], "52a7053b-8c6e-4889-9a0b-070f8801f83b": ["818609b0-01e9-4cb2-954a-b0a662747101"], "0b934e65-4c55-428c-b5fa-402c987c5309": ["19242e3a-657a-411b-9a06-ba23097f2439"], "cbfc5395-27ec-43ec-b0db-714a47255e12": ["19242e3a-657a-411b-9a06-ba23097f2439"], "ef396fbc-d844-4747-9681-1da6dfccbb64": ["07264c64-bded-4e44-9e22-345e30d8580a"], "30c8ea02-59e0-431e-84a5-2fb1e1f08fac": ["07264c64-bded-4e44-9e22-345e30d8580a"], "15abe9ab-e9e6-4acf-bdde-ffb6fd6903f2": ["a02b95f8-b925-4914-a10f-6d39facfd820"], "53a9c5b2-27af-47e5-a92e-9c00d0e30d95": ["a02b95f8-b925-4914-a10f-6d39facfd820"], "1c75c4e2-9ed7-42d5-89e7-3a7e8a95903c": ["9f98a852-5cae-42fe-ad9b-fa46e3934c96"], "615369f7-4d29-4098-87eb-8c261647ca82": ["9f98a852-5cae-42fe-ad9b-fa46e3934c96"], "ea9b44ed-5e9a-41fb-b540-eb4f84072f07": ["8c80037a-14e2-4fb8-8c4e-fdbd0bc1be81"], "c8d25ef6-dc73-4b9d-84d5-01838cbf1c22": ["8c80037a-14e2-4fb8-8c4e-fdbd0bc1be81"], "d7129de1-bff7-4245-a632-dfe2f57bc447": ["6bde3175-2312-479a-a59b-826e3862aee6"], "e870a1a0-281d-4337-bcca-819d57d7f27b": ["6bde3175-2312-479a-a59b-826e3862aee6"], "26f0a6e1-f972-4cd2-8cb1-c15ee9ede9d5": ["601621d1-8606-47b4-a4b0-d00e7874a67b"], "13cf6d2e-5786-429a-b33a-944133c46a39": ["601621d1-8606-47b4-a4b0-d00e7874a67b"], "89cd6a7c-37f4-4b68-acd5-1d8e5d26806a": ["5643df79-ec7a-4e41-8248-9b8b142c5da0"], "e376bf0f-367a-484e-b423-8aa2bbe63fb8": ["5643df79-ec7a-4e41-8248-9b8b142c5da0"], "1a93645e-6a04-4c3d-ae1e-c6c00fc66cd6": ["bd20687b-34cd-475a-9304-74f044036470"], "45e5b050-47e6-4239-b544-ef57aa9838fe": ["bd20687b-34cd-475a-9304-74f044036470"], "bd531bce-72ff-4a55-a361-34e17fd0a40f": ["77fb0b64-a7b3-4ec1-a196-bf016f4f6c3c"], "81a268fd-64ab-4f3d-b295-bf07b2d8fbc9": ["77fb0b64-a7b3-4ec1-a196-bf016f4f6c3c"], "567e5c56-6f2b-41f0-93bf-b49c86669fe9": ["21d19286-0fe1-4179-9ed2-5253627810d4"], "dee2e12c-d0c6-4787-83cd-13a33fd5e56e": ["21d19286-0fe1-4179-9ed2-5253627810d4"], "74b7c60c-8ce1-479b-bec3-f00352a934cd": ["56ead149-c44b-4ffc-863d-21048d5bfeb6"], "e02b1532-5457-407e-b1fe-cac7fd9d18a9": ["56ead149-c44b-4ffc-863d-21048d5bfeb6"], "8690eabd-28ad-45bf-ac5a-445bb529d638": ["2daa7019-deaa-4b27-8324-40ca4e451d41"], "3f468737-dd7e-4799-8574-50f42485a800": ["2daa7019-deaa-4b27-8324-40ca4e451d41"], "20b10d67-8b24-49f8-a9a7-b27c640ec8b6": ["33373841-18d5-4fb4-af57-27370966775f"], "94586321-3e96-46d0-871c-36a3725d8380": ["33373841-18d5-4fb4-af57-27370966775f"], "d0e4f918-8fac-4662-b31b-22b6e294fd20": ["4c0af97e-e485-4cbf-a72c-7cfb56a18a07"], "13300178-e15d-4bc4-a781-d357a9948cfb": ["4c0af97e-e485-4cbf-a72c-7cfb56a18a07"], "3c17140a-2bb9-4e53-b439-08c349299fa4": ["3b1b0908-3a83-48eb-81d6-721a09f1296d"], "b727d493-38ea-4319-9065-e204c6dd2127": ["3b1b0908-3a83-48eb-81d6-721a09f1296d"], "e65fc91b-c6cf-4141-acf0-5300ba8def26": ["32244c54-3144-453d-9111-755447577020"], "8aa9b471-e025-4a49-a8eb-26f792a9acf0": ["32244c54-3144-453d-9111-755447577020"], "c0614aa1-b084-402e-a7f2-110fc7be112c": ["92e78bf5-c8bd-414f-87de-dac2a0651f5a"], "1fa35555-b67c-4fb1-918c-c8eabb7d1629": ["92e78bf5-c8bd-414f-87de-dac2a0651f5a"], "0de6caa4-532c-4e5c-846a-8cfebd004d45": ["e87c623e-533e-418a-b49a-e4d700de2e36"], "a9b61c73-744b-472c-94ad-feeb37c26fbf": ["e87c623e-533e-418a-b49a-e4d700de2e36"], "2183727d-eb13-4108-b797-5bf7b946b47b": ["88533952-345b-4e1b-b495-5534cd793cef"], "2213e9bb-79a4-43cc-ad51-b66ba9bbd9e5": ["88533952-345b-4e1b-b495-5534cd793cef"], "986a60fe-784c-49b6-897f-0f74c0e4b651": ["28410d68-56c6-4fb1-8676-9c527eec053c"], "b741fcb3-706e-4341-ad3c-2340811451db": ["28410d68-56c6-4fb1-8676-9c527eec053c"], "6ca99e18-10ad-4313-a835-927f454acabb": ["67011c04-5033-43ac-b089-19f2cfdbe177"], "893860e1-ca07-4b94-8968-a5f6c0202890": ["67011c04-5033-43ac-b089-19f2cfdbe177"], "d52ee58a-dbe9-4b9c-b266-af012f72604f": ["b13f3cc8-f374-4105-971a-ec5c142a82d4"], "2906c1eb-5944-4d49-a2b7-1c4e4b1bb787": ["b13f3cc8-f374-4105-971a-ec5c142a82d4"], "1379dc25-ba81-4ed9-8951-f7aa8f7f693a": ["26761e5e-4704-43b3-977f-cd050b3b2926"], "8f990e8f-93ef-4d60-9602-b1f848851b80": ["26761e5e-4704-43b3-977f-cd050b3b2926"], "ddf5cfa9-ca56-445e-bc2a-a3e41c4133a0": ["93a6ba0f-1260-49d5-89f8-25098cfea53b"], "9cf9d82e-dbf9-4ae6-8708-a9b970f9804e": ["93a6ba0f-1260-49d5-89f8-25098cfea53b"], "a44ba978-820e-49bd-bee8-5a7e6e5cd059": ["cf0ae808-5a4c-4639-b463-c6e741f7eb45"], "8d5dc1a4-d187-4b2e-bca2-a13c152f2d5f": ["cf0ae808-5a4c-4639-b463-c6e741f7eb45"], "2252bbdc-b843-4554-a825-5602908c5d25": ["2cc3003b-23f6-480c-8e05-3d794d3410ae"], "62e56246-6371-44a3-b5b3-26c2cf1c7fee": ["2cc3003b-23f6-480c-8e05-3d794d3410ae"], "75f20ae2-b624-47c5-8afd-ac6f77336b5b": ["6a1678e9-70fb-4c28-aa3c-aff056662057"], "2136e126-1c31-4a2e-9670-402aeb3ce097": ["6a1678e9-70fb-4c28-aa3c-aff056662057"], "cf7d4527-d1f3-4237-8180-0ec05f6d96a9": ["54dcc67c-e034-492f-a661-4af2e237b396"], "a607077d-3975-4120-9f02-18a42b1c5087": ["54dcc67c-e034-492f-a661-4af2e237b396"], "4e4ebc6d-2f5c-4234-8ecd-543315dead68": ["24c00e57-00b3-430d-9557-2eeb54d132fb"], "66de2991-fc28-4da3-bd8f-171c55ab5aa0": ["24c00e57-00b3-430d-9557-2eeb54d132fb"], "3b5c9429-04d9-4675-979d-da9be0132946": ["154dd056-8841-4f75-afbc-355453bfee01"], "d6d055e8-b136-48ee-a896-7b707c982dae": ["154dd056-8841-4f75-afbc-355453bfee01"], "9b676ead-609f-4ba4-a210-3dcd1fab8414": ["875cc12b-ad80-4f4e-a989-0ab2b2e1b7dd"], "c4322db8-996e-40f4-a459-9feb448934ad": ["875cc12b-ad80-4f4e-a989-0ab2b2e1b7dd"], "8d04571e-14bb-454d-8db6-da9a83bb4a8c": ["f58cde9d-872a-4e1c-9935-c495e97690be"], "bea369e6-85d0-45d0-82c5-f1ada4c81794": ["f58cde9d-872a-4e1c-9935-c495e97690be"], "52312c90-954a-4d70-a05f-3d71d57e3024": ["abf60b51-0585-499c-8eab-bc5e01091a46"], "89ce062a-ebbf-47e5-b10a-05bb2fe5bfb3": ["abf60b51-0585-499c-8eab-bc5e01091a46"], "ed7db102-d6d6-4c3c-8d76-9747a3eb7d66": ["b219ca9e-6fbd-4900-912e-f3c9691d8a37"], "0d4069c2-f158-4663-9318-24ca673292aa": ["b219ca9e-6fbd-4900-912e-f3c9691d8a37"], "53e45b56-6657-4a0a-8d42-ac9058610924": ["dc93e3dc-08ec-4348-8f31-1d24e5131abe"], "bb1f5ef0-a013-4d41-9a27-952a0d5a0a6e": ["dc93e3dc-08ec-4348-8f31-1d24e5131abe"], "f83bc992-f86a-44cd-851a-73c3cd03dfa5": ["c5a97819-57ae-4c69-9f3d-3b72dcf91978"], "1f43ec5e-7c8a-4b6a-b451-f20fb7f17903": ["c5a97819-57ae-4c69-9f3d-3b72dcf91978"], "b7df62fe-2c78-40cd-a174-0d2280971c79": ["7d909312-ce97-41cf-9960-ec573bb43ea2"], "9a48c5c4-41e0-42ed-97bc-e550ae26ed35": ["7d909312-ce97-41cf-9960-ec573bb43ea2"], "fcde8283-6252-4f38-805f-e1cb01b18ddb": ["b1bd943a-0d4b-43b3-b0dc-21f037af7fc2"], "bbbc8c50-6c12-4643-83af-8a9dcaeba194": ["b1bd943a-0d4b-43b3-b0dc-21f037af7fc2"], "885a1a00-c03e-4116-8fad-340c5ff1fbc9": ["aa1489d2-a931-404a-99ab-c74686a301df"], "44ee2f2f-ae07-4e67-ba92-ffb8f328f96a": ["aa1489d2-a931-404a-99ab-c74686a301df"], "f52e1bc7-3186-4959-b926-b33df33d3b46": ["d56b4ae9-61b4-4ef2-b303-de7dca969068"], "0e09e0ff-fb20-4e68-98f7-710d46402976": ["d56b4ae9-61b4-4ef2-b303-de7dca969068"], "6d802869-b47f-4857-aa7e-30b55ffa95b3": ["e3cab5b9-e8d3-45fc-930e-fcd631103977"], "8375a89a-aa7a-43f4-89a6-a8d976ccf958": ["e3cab5b9-e8d3-45fc-930e-fcd631103977"], "e98bb506-269e-45f7-be27-3bd9ac26acc8": ["14eff0b7-d9cc-4c07-93bc-77abbbc95a5a"], "816d3926-5247-4494-b2cd-539fb755e02c": ["14eff0b7-d9cc-4c07-93bc-77abbbc95a5a"], "544c074a-94fe-4c2a-bfd6-db1cfe3f0929": ["e29888eb-4898-401b-876c-ce527a4853ef"], "0c934f06-b3b8-43af-90bd-225e56dd9ccb": ["e29888eb-4898-401b-876c-ce527a4853ef"], "d0037d07-9898-47cd-af98-41378934bf9c": ["70c86512-4020-4c47-9647-f442c15e0a86"], "4179efcc-d74e-4b83-ae6b-18e26c250890": ["70c86512-4020-4c47-9647-f442c15e0a86"], "3df5db72-8962-44c6-a923-66ab42f125c2": ["2c9dde7d-2039-4a5b-8806-b47cd52a90c3"], "19bc90dc-645e-43cc-a55f-17be1a84aefa": ["2c9dde7d-2039-4a5b-8806-b47cd52a90c3"], "6a037e4b-cc19-493d-9569-542b05c6ddfd": ["79ee8ac8-3a4f-45e9-8d24-54dc9540f446"], "884b6899-f887-4824-bf67-217dbf391d62": ["79ee8ac8-3a4f-45e9-8d24-54dc9540f446"], "4f0cb9a7-d59e-4ce0-b097-afe6ec13d084": ["35cb9019-f457-4381-a1c0-c1dc5c8bd1fa"], "606a6ea4-226e-4357-929c-df027876256d": ["35cb9019-f457-4381-a1c0-c1dc5c8bd1fa"], "363ef18a-da05-4907-85e1-7d4e5159c043": ["78e2522d-e565-483e-868d-3651da03b7f8"], "e1ac292a-8dad-471e-8e53-9e15f5f94014": ["78e2522d-e565-483e-868d-3651da03b7f8"], "04b5c3d7-ba7d-4132-a640-56f5e1167f26": ["5e852bf5-d7d5-4006-a85f-3599a28ad53c"], "535678ed-16ea-4f8e-afcb-a35f0994547e": ["5e852bf5-d7d5-4006-a85f-3599a28ad53c"], "4c8de907-58b2-4988-a6aa-0753b38cb9d4": ["3f8ecbb1-f07e-4c88-a524-0790a600942e"], "18f35b0e-181e-49b1-a5bc-e0d10d41b20e": ["3f8ecbb1-f07e-4c88-a524-0790a600942e"], "ae15e5a1-26f7-453f-a484-9ec720fd680f": ["0f657622-2858-4871-acf4-c5d42a989c5e"], "86b92c86-4fa7-4add-b9ed-ede67ca2bc03": ["0f657622-2858-4871-acf4-c5d42a989c5e"], "b89d1ca0-3774-42a7-84fd-739764f00e48": ["758a8c77-111e-4e0e-9e32-a84364194843"], "9645bb9e-9c24-44fe-b7fa-795f233cb1a4": ["758a8c77-111e-4e0e-9e32-a84364194843"], "475e96e3-e055-47bb-96b7-ae07293bea15": ["c464674f-8456-41b5-a80d-267201c46861"], "3fe90947-390b-41c9-a940-f9689884a8cf": ["c464674f-8456-41b5-a80d-267201c46861"], "c02cabb6-7bda-4bad-b3c6-8792a337a782": ["5284b654-9e71-4126-8410-8cad99922c48"], "0dec50ed-647d-4163-9954-685d540d27ae": ["5284b654-9e71-4126-8410-8cad99922c48"], "d5757160-e2b4-4cdc-9284-8aecf7c0ac06": ["37653da7-9a11-4738-a079-2d662b941acd"], "b4a145c9-232c-4249-9f10-14ff7d1f8055": ["37653da7-9a11-4738-a079-2d662b941acd"], "e45ad7b4-c140-4741-ab3f-31bcfa74e7ae": ["ad72862b-79d8-492a-8ad1-92d298f52485"], "fcba6911-6781-49f3-9ae2-7d6d9ad57d9d": ["ad72862b-79d8-492a-8ad1-92d298f52485"], "035f5345-1013-4a88-b4ff-abe23b4aa195": ["ba0cd09a-ab10-41c2-abec-103ac71f47dc"], "7305ef4b-2ef7-40b6-834b-63125b0d2fba": ["ba0cd09a-ab10-41c2-abec-103ac71f47dc"], "11ed1dd8-1873-4aa6-9ac6-2627b4b2dad8": ["8351c118-2108-4856-9067-530d6b9239bf"], "97d5ac1c-d5d3-40d5-b28d-bbcc9f1429a8": ["8351c118-2108-4856-9067-530d6b9239bf"], "16eace87-73ee-4ea3-820b-b296e6c12896": ["583bcdb7-9be9-4238-b2cb-4b2a9212cf09"], "2ac77e66-69f6-4f86-ab83-8011450dda1a": ["583bcdb7-9be9-4238-b2cb-4b2a9212cf09"], "1c4d9ec0-fe63-4d24-91f5-f4f051f418a8": ["eb17b573-6a88-4456-993d-9a1ff341f7e9"], "7addc0ff-9d6f-4639-961f-0ad54903a400": ["eb17b573-6a88-4456-993d-9a1ff341f7e9"], "584b54e9-c41e-4ed8-8c2d-97471259dae6": ["71f3e1b3-2e7d-4259-8a12-3187ad2c0b12"], "e284e218-9c38-434e-b9b6-64cd2a9f0784": ["71f3e1b3-2e7d-4259-8a12-3187ad2c0b12"], "4096de6b-a417-4772-a121-14eb325cc919": ["f85fdfcf-afda-4b07-868e-4b5cc04e3e02"], "97ee37a3-3f3b-4f5d-9ab4-151d3e192d9f": ["f85fdfcf-afda-4b07-868e-4b5cc04e3e02"], "a1963da7-612a-4793-ad96-002bff5c3c99": ["5b4b6f87-aa41-4c40-9abe-7041c236a9b2"], "ddae77d8-2cee-4d8f-be56-87aa30367316": ["5b4b6f87-aa41-4c40-9abe-7041c236a9b2"], "a766baee-89ef-417b-a50d-18fd49459598": ["9bf4dae0-8a83-4613-ba1d-247f7bb065a7"], "e8f628c8-df2a-43ce-b228-1e8c11f9373c": ["9bf4dae0-8a83-4613-ba1d-247f7bb065a7"], "ec9fca3c-3150-4a18-80b8-1777c6b02611": ["8b42ce37-8549-4764-b6e4-d84481e5e1fc"], "ab96f981-02a6-4b86-a977-13b2e95f9a6c": ["8b42ce37-8549-4764-b6e4-d84481e5e1fc"], "fdef9d71-99e9-4f48-8436-141f7a8f798f": ["b77a99e0-53a8-4555-a681-d2b21d1bf555"], "800ed847-dfb5-4e7a-91f9-bd19a73002b2": ["b77a99e0-53a8-4555-a681-d2b21d1bf555"], "bd1e5e49-92fc-43c4-8b47-2f3fc240ec7f": ["5832c0fc-31ee-436b-8151-03dc7d012ca9"], "fc49c74e-0e44-4bd3-87fa-508fc8bdf5dd": ["5832c0fc-31ee-436b-8151-03dc7d012ca9"], "c885ac5f-dfa5-4d57-beb4-7fe24a75120c": ["59ef504a-63e3-421e-9394-2c4d20b457ab"], "067f0646-d29f-4f65-8cb4-500b9b8a3a8a": ["59ef504a-63e3-421e-9394-2c4d20b457ab"], "5fb796fa-fb7a-4260-8071-44f16da2185f": ["050013f2-963f-4591-b5d0-af4199856bf5"], "55151ca2-ab47-4efc-af31-eebed86f38b6": ["050013f2-963f-4591-b5d0-af4199856bf5"], "a3856391-4067-4b66-894a-eb799df4f644": ["9109755b-2984-4fed-ae91-3c95142aafaa"], "11f0e981-1fdb-47c1-8005-56a6d1b280d1": ["9109755b-2984-4fed-ae91-3c95142aafaa"], "40c04eef-0317-4fb1-9854-0f1972611dc3": ["44615b9a-8559-48fb-ad13-43c055627a57"], "059ab53d-9bd1-4925-809a-bbda5b3ced02": ["44615b9a-8559-48fb-ad13-43c055627a57"], "a74a69ce-d4cf-458b-91dc-60b5ade44e60": ["f7761dfd-0d85-43c9-a855-b33292a6ebf3"], "143154a0-6811-4f70-a272-d970a80008e7": ["f7761dfd-0d85-43c9-a855-b33292a6ebf3"], "6769c8fa-566a-4fa1-ab83-94fabda027e4": ["71f5a3b4-2208-4d0a-ac90-57815f01f0ac"], "28cd2bb0-3212-4f5d-945c-3e4ba114d0b4": ["71f5a3b4-2208-4d0a-ac90-57815f01f0ac"], "eed532b9-000d-42be-9abf-e44dd4d93534": ["a1e6d51c-ed66-43c3-af20-9d3378013c0e"], "b68a9cf9-1482-460a-a963-ad9903e91efc": ["a1e6d51c-ed66-43c3-af20-9d3378013c0e"], "edf5d4b1-499c-4577-8d5a-235b15db75b3": ["21b16653-2d6e-410f-8672-acfe47e9679f"], "4789d9c3-99a1-497c-afeb-0ffefd40aff5": ["21b16653-2d6e-410f-8672-acfe47e9679f"], "9cf6e257-cfda-4331-83cc-84b7b5d71921": ["9ac886f3-0190-46c5-9d05-6ff7c21e1897"], "b395610d-4650-4730-9ad0-0a2980a6c554": ["9ac886f3-0190-46c5-9d05-6ff7c21e1897"], "58f85e92-90f6-49f1-b031-68b9c5976860": ["3bca102c-090c-425d-8906-60ac119d7757"], "d5ac96ef-77da-488d-a173-86e9a919087a": ["3bca102c-090c-425d-8906-60ac119d7757"], "57485825-042d-4065-ad46-fe709185f348": ["38d5ea9c-96a3-499a-84e0-fec24b97e853"], "1fd19084-8272-4e88-8bd0-ebfb797c0fa0": ["38d5ea9c-96a3-499a-84e0-fec24b97e853"], "8b816452-923e-4235-a24b-82b679971a7f": ["5a017769-8dd9-4a6c-a5c1-3807232ac432"], "15cfdc98-c98a-4154-814f-61bb80e431b1": ["5a017769-8dd9-4a6c-a5c1-3807232ac432"], "86cce746-6821-4d06-835a-bac108635211": ["2c1735f3-ebcf-45e7-9591-5216a79eef1a"], "bcad99da-dff9-411a-8a5a-8249c64002df": ["2c1735f3-ebcf-45e7-9591-5216a79eef1a"], "ffdfe533-b251-42d0-9409-ec4ba925fb39": ["e653b082-0977-4b69-bbcb-df7da69ad259"], "610d7fa7-b913-45e0-8fac-cf1fd82e87f8": ["e653b082-0977-4b69-bbcb-df7da69ad259"], "36d9326d-a420-4655-84af-db39989133dc": ["e423baba-3dba-42f8-b8c3-f553453b3053"], "ecb0c72c-bfa0-44dd-8a9c-5ce1064542f9": ["e423baba-3dba-42f8-b8c3-f553453b3053"], "5cab75bd-99e8-4f0d-81cf-da817a0d8cc6": ["ca5d2cb0-3c7f-44c8-b319-1c9c8e5272fc"], "bc7b68e0-0f79-4984-b2be-830b5ec21422": ["ca5d2cb0-3c7f-44c8-b319-1c9c8e5272fc"], "9fbd1e8a-6bd0-4e02-967e-ab01d0cfc90e": ["e5d46b4e-1c38-4a04-b0be-bf7e040a658c"], "21273ad3-92a0-4cfb-a4f6-4c7e96da69bd": ["e5d46b4e-1c38-4a04-b0be-bf7e040a658c"], "3cd2c869-7011-481c-9504-166b63cd6060": ["0e271224-7d09-474c-bed8-052cc88133a4"], "b6e82d9d-4e43-4c4b-be98-52c1c094420d": ["0e271224-7d09-474c-bed8-052cc88133a4"], "c5fba027-c389-4c08-86d9-de4bd2db62c6": ["f774f4b3-7b20-4b4c-8531-f6499675e344"], "2d8907fe-4da1-403d-91cc-310fde86d07a": ["f774f4b3-7b20-4b4c-8531-f6499675e344"], "a3fd1c5c-6f95-48d0-bdcb-2486895d2e2a": ["15be8395-1afc-4fb1-973c-ce495170426b"], "37a53b5b-fbd1-49df-9a71-596ab7ef1e81": ["15be8395-1afc-4fb1-973c-ce495170426b"], "30e8b347-f13a-4ae5-ac3b-c277ca28be43": ["a635296b-7baf-4168-add8-3b3d6ed4e9a6"], "fabdb213-2d65-4835-a529-7d2476ea2536": ["a635296b-7baf-4168-add8-3b3d6ed4e9a6"], "73eaabbd-5bb4-43a0-9765-dfc709ee365b": ["0e7597b1-fed1-475f-b664-4005f3fc4ca2"], "5bb0382d-bccb-4b31-b226-423bc613caeb": ["0e7597b1-fed1-475f-b664-4005f3fc4ca2"], "b28eb5f6-a46e-4bdf-91c0-a436858450bb": ["23f91a32-e362-4b43-b11c-12b3c5fbebf5"], "d0bf0e71-c12d-45e2-8b09-2169c2cc542d": ["23f91a32-e362-4b43-b11c-12b3c5fbebf5"], "c131fdb5-3ee0-4dd6-b149-fc17a83e7295": ["a5ab3faa-5a60-40d1-8951-6fdc997e1d33"], "ea6864d0-1bfb-4bdf-92d8-50d553d5d01d": ["a5ab3faa-5a60-40d1-8951-6fdc997e1d33"], "fe1ca9eb-acae-4b31-b188-1933061ab417": ["77f095cd-def2-4c87-974f-c1c883959918"], "875120d9-eade-4f75-9396-1c17aaabb4bb": ["77f095cd-def2-4c87-974f-c1c883959918"], "b4844465-feaa-4d62-9556-ffe723a55661": ["9088952a-eb7e-4441-8ac4-6c940d7482ec"], "e32b98d3-8966-417d-9bd5-8e782ed2de78": ["9088952a-eb7e-4441-8ac4-6c940d7482ec"], "eedb6d23-3fa6-465c-a952-b256f27a695d": ["3b75c13c-6ac2-4dbb-988f-14204b379cef"], "f0a345f0-adc9-4324-917c-eb7cd1b8858b": ["3b75c13c-6ac2-4dbb-988f-14204b379cef"], "8a76d073-c790-4b2e-821a-892689bbf14c": ["f8adcc04-b13f-4271-8285-f29b09a77f7f"], "e5e65987-243c-4b5d-a252-0f4c7dc6571e": ["f8adcc04-b13f-4271-8285-f29b09a77f7f"], "02b4d63e-6490-42fd-bc78-c5b9070fc61f": ["207d9171-d14c-45ac-b9a3-5d2e58869be0"], "5dff3ad8-ff73-407a-94f9-4367d5ef51ec": ["207d9171-d14c-45ac-b9a3-5d2e58869be0"], "3e13a9ab-da6a-4153-906b-fc7343ddde39": ["66e5d1d0-5a91-486f-9bcd-74c3651278d1"], "614637ab-a73d-4780-abb2-aa6f5974cbd8": ["66e5d1d0-5a91-486f-9bcd-74c3651278d1"], "5addb4be-7f04-4428-aa20-0d4a594d10a0": ["63a237f5-9822-46e3-b2eb-62f4138ea8b4"], "21da9571-3ff5-411a-8c5f-468445daa2b9": ["63a237f5-9822-46e3-b2eb-62f4138ea8b4"], "11be9566-b418-4f29-8fbe-a02ed4102ea8": ["c489be35-182f-4889-a63d-e68c5ac21e91"], "e6659dd0-ba9c-4c86-be41-04c9b3a9de89": ["c489be35-182f-4889-a63d-e68c5ac21e91"], "215714cb-453a-4fd7-9639-b2ed1036ad32": ["73b5a481-33f4-4578-b3ee-5ebff6434c79"], "93bba5d7-a396-417d-97cf-9d06f3dbc8bd": ["73b5a481-33f4-4578-b3ee-5ebff6434c79"], "628a63d3-8dc4-46f5-af5f-0fa5c797b2fc": ["05e906d4-c11f-46bc-8fe2-da635f7b8077"], "999d4631-18d3-44c5-a73e-e15fe59d470b": ["05e906d4-c11f-46bc-8fe2-da635f7b8077"], "e6c3b0f8-a396-482b-9270-6286267a2ff7": ["69707767-f1cc-4151-9817-f55506e29994"], "c6ede956-5568-4da9-976c-d1aceb19166d": ["69707767-f1cc-4151-9817-f55506e29994"], "02add3dc-41f9-44a2-96d2-1e024e53fc2f": ["aa51b9c7-3687-4873-bc61-af53a3041538"], "311d57b6-e600-4212-9e91-6042ab252603": ["aa51b9c7-3687-4873-bc61-af53a3041538"], "df476a3c-199b-4146-a153-16c08d38e12e": ["21bed430-7f84-45df-bcb5-9c98ca192803"], "e453b866-beb9-42ae-9410-217e42bf3aac": ["21bed430-7f84-45df-bcb5-9c98ca192803"], "e3866cf4-d937-45fd-a273-192466ccaef8": ["d2b0d815-2236-417a-8cea-995f4e239d93"], "7b549ec4-3a27-4f36-a39a-606fa433ed7b": ["d2b0d815-2236-417a-8cea-995f4e239d93"], "65b8a85f-929d-4e42-9404-6a62be494353": ["05bbcde8-d512-40d8-9fad-82e9912a637c"], "1c5f4eb0-b282-4460-ac88-427bfdf14244": ["05bbcde8-d512-40d8-9fad-82e9912a637c"], "981f930c-3d6e-4990-83b1-e4ee13e5972a": ["8201fdbe-2e24-44a4-9005-c750ca576e14"], "2e60a2b5-f121-476a-a79f-45a6afac4b57": ["8201fdbe-2e24-44a4-9005-c750ca576e14"], "6e91b3b0-ea0a-4a33-82d8-67bf2ed8a193": ["50f16fd0-f1cd-4e38-9387-d375f24d0f7b"], "75cfb537-ee2f-47b5-a443-414e82b09f52": ["50f16fd0-f1cd-4e38-9387-d375f24d0f7b"], "77a4b388-6517-4eca-903a-58a46a3d20a1": ["ec92cc49-9a8d-4790-8bfb-33a54f5507ac"], "45c9f881-e00c-40f2-bea9-e680cc968490": ["ec92cc49-9a8d-4790-8bfb-33a54f5507ac"], "d36f1eac-d4a1-4bea-9364-5f8f12006939": ["91db738b-daba-4bc9-99ce-f51bdd1fdc0a"], "5a29c809-163f-4d04-ab63-aaf7b3aa7cb2": ["91db738b-daba-4bc9-99ce-f51bdd1fdc0a"], "e9a087f4-cb2d-4500-bcfd-d0e91f46e425": ["0c1a08bd-5019-49da-91b4-a64c755ad486"], "ed1492ff-da5b-4353-ba10-c4f4f2201c87": ["0c1a08bd-5019-49da-91b4-a64c755ad486"], "e50a4d23-d059-4dc6-b5e5-56ec29e771b9": ["1c5ae81d-86e9-4d49-a70e-214827e0f368"], "8f7ae0d7-8d23-4a6e-ba2d-e808b25a141e": ["1c5ae81d-86e9-4d49-a70e-214827e0f368"], "3b35b868-60f6-4c50-8caf-edaf66059f03": ["3658e8c0-330b-4ef9-b051-d9432ce7b423"], "d8d9a91b-b697-4cd0-ada6-ea1a57e403be": ["3658e8c0-330b-4ef9-b051-d9432ce7b423"], "5172d06e-eb01-42c6-804e-13af89548ec3": ["f3453376-a1fa-4b89-a8c1-6ea2bc0f9c8a"], "1a40a162-fdce-426a-905c-970599283051": ["f3453376-a1fa-4b89-a8c1-6ea2bc0f9c8a"], "3fbd2dbc-91de-4fcb-8afb-55a91652c710": ["ab8a75c6-5152-498c-bd18-a0f40ea1cc22"], "5b2c2213-0bf0-4111-82b5-9b0cb63c5b0b": ["ab8a75c6-5152-498c-bd18-a0f40ea1cc22"], "d0be9898-c6df-42ab-87c7-08ffbb839bc6": ["dc4bbb6d-488d-4d50-8266-aec71481de02"], "0e5e230f-922c-4eb3-a4f8-053484ea38f6": ["dc4bbb6d-488d-4d50-8266-aec71481de02"], "d2ecea54-5e2d-44eb-93a9-731ea99e47a4": ["b67dc10f-c2e0-4084-a7cf-868b411517e3"], "a1e171d7-6825-4e64-8925-396fefd9ae71": ["b67dc10f-c2e0-4084-a7cf-868b411517e3"], "51f1b6f4-5e8f-49f0-8e64-c784bdfbca3c": ["4ba4a76f-fca9-4c0b-9b7e-671fb6475e8f"], "99ba0000-cdf0-4058-85ef-8b5691384aad": ["4ba4a76f-fca9-4c0b-9b7e-671fb6475e8f"], "bff98321-794f-49bd-86e3-a532d53a074e": ["187e58d3-3c9b-476c-b487-bd7e6eb9e4d9"], "1e93aebe-e5c1-4af3-8d55-5de5cb5d79ae": ["187e58d3-3c9b-476c-b487-bd7e6eb9e4d9"], "156a27ca-4d60-46e6-8faf-2ba97581fb93": ["ade012f7-64f3-40d7-be01-7e147e2842ae"], "1ec81edd-a2da-4119-879a-3e9c2ecf8b97": ["ade012f7-64f3-40d7-be01-7e147e2842ae"], "cf385023-fae7-4ec6-a6e6-6a53f9678f09": ["525562ca-e99f-432e-8ad8-f0d9051b5416"], "6ee63bdf-b42a-49e0-95af-8340fdbedaa3": ["525562ca-e99f-432e-8ad8-f0d9051b5416"], "e9bb97d6-4274-46ef-9434-11608c6fc533": ["53de0d1e-7d71-47e4-86a5-8dc794b17d8e"], "2c8dda00-969f-4165-a989-282d05be6bf4": ["53de0d1e-7d71-47e4-86a5-8dc794b17d8e"], "4c1a7ffc-329b-4abc-8125-b1000e218b61": ["2babc226-629e-4011-a7b9-9a7865e480b0"], "4ea46be4-f1b7-47af-8e51-d392ee391904": ["2babc226-629e-4011-a7b9-9a7865e480b0"], "2e64c102-ede8-4299-875c-fd8f6309792e": ["257e193b-50b9-486a-8f5a-a80a50467abe"], "a08f4594-12d0-4c46-9755-792eba77d60b": ["257e193b-50b9-486a-8f5a-a80a50467abe"], "07ee477f-935c-4b33-a402-e0513e963e9d": ["aa192f4b-5d18-4568-a4a2-623b98bae8a9"], "7b672a8c-9a97-4a76-b587-7830553b34a2": ["aa192f4b-5d18-4568-a4a2-623b98bae8a9"], "862b2e4f-75ac-4707-8cbd-53f048ed3abc": ["8513b2d5-00ae-42f3-80af-8e881b3d9cec"], "0a1730cd-9e64-4be8-a4ae-32514d334c11": ["8513b2d5-00ae-42f3-80af-8e881b3d9cec"], "67feae3a-fc0b-47a6-8f7c-04d377d3bc33": ["2a6c3a5b-b635-432d-8127-06cf8d264f62"], "db365c25-751d-4d47-a7a9-af9472f7ed9f": ["2a6c3a5b-b635-432d-8127-06cf8d264f62"], "a7fec317-92d6-4866-b7c3-bcd353854553": ["1bd1aa91-43eb-4da2-95a6-0d94de7a914d"], "44c533ef-fb46-4e9b-a3fa-a1a5a899ca7a": ["1bd1aa91-43eb-4da2-95a6-0d94de7a914d"], "e9a781b7-2f32-4636-b9b4-1d7a4b1ecd58": ["f564a653-ed3c-4279-ae06-955525df3abf"], "7a5eff36-8789-40e9-ab53-3a41a01e4432": ["f564a653-ed3c-4279-ae06-955525df3abf"], "a7b957b3-e650-489d-9ea6-62cd331977af": ["d0f42152-d4e0-434c-a3b0-73598d452909"], "60f54333-5b72-4c5b-868b-b9d707274a4c": ["d0f42152-d4e0-434c-a3b0-73598d452909"], "055285fd-7720-4908-9dc0-be201d472214": ["51f5add2-53ea-4802-860a-3091c6409f51"], "3b4cc1c4-d815-4537-811d-bf5ceb45485c": ["51f5add2-53ea-4802-860a-3091c6409f51"], "e5d930c7-ec2d-4b7d-a76f-ad3f1c7c739a": ["7902ec38-a0ac-4784-80e2-f9a0bf237fc9"], "658bad98-976e-4a53-9687-bf1a820e0a3c": ["7902ec38-a0ac-4784-80e2-f9a0bf237fc9"], "4ae5ca0c-46c9-4025-9d1b-8355479230ff": ["646d15ac-c004-4c3c-8dff-255bef055a58"], "155f927e-bf11-445e-b654-f90065354046": ["646d15ac-c004-4c3c-8dff-255bef055a58"], "4409c2a6-ff80-4c92-8032-61bbd3b89fe4": ["9080a671-c947-47e8-ac5a-233215315cec"], "f4861293-8689-407c-a362-f6234e80ae22": ["9080a671-c947-47e8-ac5a-233215315cec"], "dcbf393c-ab29-4704-842d-7e508f11a43f": ["4cd7d65c-63c1-438d-b6b6-274808e7390f"], "b2bdea97-2f9c-4848-836f-7dcfe88d9fe0": ["4cd7d65c-63c1-438d-b6b6-274808e7390f"], "95d896d9-0ec7-4b78-b6a1-a79ae0d17a62": ["877c574c-2c30-477c-b8cb-3d39dbd2a687"], "43e778fb-5503-4ad1-aa61-ef780afa4b24": ["877c574c-2c30-477c-b8cb-3d39dbd2a687"], "3b805878-9e9d-455d-94f8-7c9930e49da5": ["c67b9fcc-cf42-4d0c-a2f9-5cf156c1bc98"], "ff86c3da-2610-463a-97cb-453b01d47d1b": ["c67b9fcc-cf42-4d0c-a2f9-5cf156c1bc98"], "95c86df9-eee1-444b-81c8-3047d88cf457": ["ff637ae5-5f8c-421d-a87a-bf928ca26a30"], "691d68e3-a808-4a34-b399-e38986233899": ["ff637ae5-5f8c-421d-a87a-bf928ca26a30"], "5ee298f8-39db-48fc-9a30-befa6df3954b": ["8d8d88dd-cc7f-4ace-bb9c-1c63a971e7bb"], "d73e4469-03ec-45e9-b9ff-654ae4324dde": ["8d8d88dd-cc7f-4ace-bb9c-1c63a971e7bb"], "f7ef97db-cba8-4218-ab35-33a226f5db4e": ["b668fa40-6592-4300-911e-dcc762c7e130"], "2cd13b7f-a3d8-4489-b83a-9c562378692d": ["b668fa40-6592-4300-911e-dcc762c7e130"], "05f768d0-ded1-4b7d-b92e-25ec38dfb621": ["ddbfc224-3adb-4b59-9003-33a064d4f4b6"], "9c4a4eb5-76f5-4dea-a1b1-5000062cc5d0": ["ddbfc224-3adb-4b59-9003-33a064d4f4b6"], "f97eb5ad-58f3-4c40-87ff-0dc922e3d7a4": ["185afd7e-fae3-4b61-9d68-b4e2a795109b"], "380e5367-b8e1-4ec1-8355-60787d1275ff": ["185afd7e-fae3-4b61-9d68-b4e2a795109b"], "c344a9d8-1191-4390-b2c3-d95754a31111": ["2b8ece9f-e5ff-4cb4-bd14-6664728789f1"], "6ede6812-0359-4868-979f-8755141b5394": ["2b8ece9f-e5ff-4cb4-bd14-6664728789f1"], "80debed3-6f43-482c-ac7d-050135c97332": ["18b95cba-56f8-47c6-9aca-263613a4627b"], "bfdce53b-3bfe-41b6-a7a7-49f7c2657a9e": ["18b95cba-56f8-47c6-9aca-263613a4627b"], "3ccdd1b1-576f-4a1a-b4a8-5a69e01afa14": ["a8cfb0fe-572a-4b0a-b47c-ca067e0ef1d7"], "7e164444-afbc-4b07-8f9c-0488910fc02e": ["a8cfb0fe-572a-4b0a-b47c-ca067e0ef1d7"], "6349b933-cc0e-42c7-b1bf-658f9e6161b3": ["47e68ab7-06bc-4110-9508-a1fdf6d498ec"], "fa2f298c-d1de-4f5e-875d-178835a69d8c": ["47e68ab7-06bc-4110-9508-a1fdf6d498ec"], "6f492b67-34d9-4032-bb3d-daf90a406ac3": ["71c88e9b-dc75-4994-81d0-ad718e4393ce"], "90f9d102-32c2-41c6-9cbb-c6da6da4cee7": ["71c88e9b-dc75-4994-81d0-ad718e4393ce"], "fa355fb3-e563-4372-adc5-5ff29358278e": ["45d2e516-48df-4ac3-81a3-3eeb7a55b9dc"], "4e33464c-6344-4247-acf6-77ae9f2e8083": ["45d2e516-48df-4ac3-81a3-3eeb7a55b9dc"], "c7976643-5832-4a69-8fd2-a1247c779b78": ["5ab4ce9f-50ad-494c-9458-c1a91433ebfa"], "209d9bba-88a0-4440-b8d3-2d348f482fdf": ["5ab4ce9f-50ad-494c-9458-c1a91433ebfa"], "c1206a83-05aa-4363-9e42-6f1cf3e56fb7": ["e7e4c7fa-9064-4f67-bbdb-93e9adc1bc27"], "46fd43bd-14fd-4fd7-b205-0971667458ef": ["e7e4c7fa-9064-4f67-bbdb-93e9adc1bc27"], "99866b18-e3e3-41dc-8ca6-446783e2a494": ["b89b1da1-c2df-4c75-93e5-789233bcd07b"], "8d000fb5-a71d-451b-96de-4801c096e1c4": ["b89b1da1-c2df-4c75-93e5-789233bcd07b"], "130150e4-54d5-4221-a883-886597207222": ["c98de2e3-0733-437d-a1b2-5bac282dfa9f"], "921ede14-0365-4ac3-8688-cb50be09e37f": ["c98de2e3-0733-437d-a1b2-5bac282dfa9f"], "0ec368bf-fafe-4616-92fc-ea5b5eea9483": ["5d3540c9-efae-45c0-9037-d56b52aebfd2"], "fddc7a44-d34a-4f97-a9a5-3ee2073ec011": ["5d3540c9-efae-45c0-9037-d56b52aebfd2"], "110a7e43-e3b6-4a46-a40e-653939be466a": ["a2eab9db-4cce-4797-bce5-43d4b37fa579"], "2c6076cc-8ccf-4457-bfad-c38b2ea16638": ["a2eab9db-4cce-4797-bce5-43d4b37fa579"], "31d027a6-436f-4c8b-92c9-c9f3e6382948": ["64e473dc-ae1a-4a74-aeff-e335219c69b1"], "72875e3a-489f-459c-adcb-414a358954d2": ["64e473dc-ae1a-4a74-aeff-e335219c69b1"], "3450bc38-0f3a-4254-b194-0288bd743b90": ["08427c60-eb63-4ee0-8afa-b44934294c4f"], "ecfeaca7-643e-4866-b8ba-f7483e6a3478": ["08427c60-eb63-4ee0-8afa-b44934294c4f"], "f287b17e-66cf-408e-8c77-2bdee7a9d6b1": ["e620ae0b-039e-42a3-803c-76c54b93b638"], "b5877ef4-4148-4d6c-81d4-46132c1c5fd6": ["e620ae0b-039e-42a3-803c-76c54b93b638"], "e6873844-6005-4f0b-bd0d-ac5836599a54": ["9f683436-662c-47f7-a4a9-799457abd39c"], "462c5fc6-9c22-4dc1-8c27-2016d11adaef": ["9f683436-662c-47f7-a4a9-799457abd39c"], "8e7c2ec2-540e-42be-ac0a-94deb35db279": ["9d086d5b-0034-484a-823a-f0783b607d1b"], "52f0d9cb-c4c8-4c5d-b49d-e7d501abc1a5": ["9d086d5b-0034-484a-823a-f0783b607d1b"], "f67cb6ed-39ac-44bb-a907-eeb5333c8807": ["5175e9b7-ab3e-4ef7-a555-f8ddd13ddae7"], "39d2520b-0e3e-45a9-8259-7e7281286067": ["5175e9b7-ab3e-4ef7-a555-f8ddd13ddae7"], "1231b987-0c9e-496f-b4ed-bf15a5273435": ["30cf7388-71a5-4655-b0e9-0aaa8e692352"], "5c5f3b33-7bfd-499d-b695-a6a04b871796": ["30cf7388-71a5-4655-b0e9-0aaa8e692352"], "28ad1071-db67-4376-ac8b-532600f5f569": ["3cf99d95-b227-4316-bcb2-8436bb3f5dfb"], "e38ef16d-0717-4f47-8d37-732c1df69579": ["3cf99d95-b227-4316-bcb2-8436bb3f5dfb"], "8f706c1c-ca12-4dfe-b24f-04f6ee2714bb": ["fb714e88-9244-4a05-801e-73bc09147111"], "e49813ba-d40d-4d4e-84f5-e1cf0bd7cc1d": ["fb714e88-9244-4a05-801e-73bc09147111"], "14a5a59c-0a04-4604-ba97-86f82b85ff73": ["b110c011-3c63-4f9a-9c9a-64ea4fdbe255"], "0d4a1103-132f-43b0-8b6e-7a062e43dc5c": ["b110c011-3c63-4f9a-9c9a-64ea4fdbe255"], "94446cb1-7659-4197-ba90-52342a467d74": ["6b28b0da-e17f-4cba-9937-b48c50bb7a30"], "b382e6c6-596d-420a-8206-61c24fd0c7c8": ["6b28b0da-e17f-4cba-9937-b48c50bb7a30"], "ca38827a-9e68-469d-a289-84dbf7154cc7": ["8dad49ac-7195-4cc6-b652-205254d7e136"], "86589b88-31df-433a-8728-583979f13ae6": ["8dad49ac-7195-4cc6-b652-205254d7e136"], "8cae3e59-29fd-4759-943d-f50145831131": ["2135b88c-648d-4abc-95ab-df68cdf358ad"], "f987bf07-c104-4765-8f9a-ab8750f8599c": ["2135b88c-648d-4abc-95ab-df68cdf358ad"], "9b652269-783b-4858-bbf3-7e0302f0a210": ["4960d73d-a9c2-4075-86d3-bb0542e216a6"], "a97f2c79-5fe7-4675-9f6a-b680212b7cd7": ["4960d73d-a9c2-4075-86d3-bb0542e216a6"], "b32f24e5-49c9-486b-9c34-58e3123fa75a": ["b3923a15-1d3a-4756-8bb1-76aa9afdfbfe"], "1fdc29f1-3b7a-41ae-b46d-be466453e0a0": ["b3923a15-1d3a-4756-8bb1-76aa9afdfbfe"], "71649ce9-6e8e-4e06-9077-8c005ab3f4b4": ["17b20f75-704c-4b60-8739-9c069b615bfa"], "4d3f9690-a983-4adc-87c4-642c435085d8": ["17b20f75-704c-4b60-8739-9c069b615bfa"], "de8b0072-44b7-4dda-9f23-44d48a786e46": ["9f0e2322-ff1d-4cd4-aa4a-bd2db245c74e"], "bf4cb7c3-5498-4847-a14e-d4bf4cd62849": ["9f0e2322-ff1d-4cd4-aa4a-bd2db245c74e"], "510ff174-cf3f-4ece-8115-427908097e80": ["1da43f09-62c7-4011-a0f6-88aa6f35f761"], "30a38d08-6c8c-4fe8-a9aa-decac638e4b8": ["1da43f09-62c7-4011-a0f6-88aa6f35f761"], "076c8b86-47f0-4b3b-bfba-bc5c164d6317": ["b16b38ad-a950-4303-8f74-924ef0c16b61"], "817ec9b1-08ed-4e3c-b883-ab6ea0bfed18": ["b16b38ad-a950-4303-8f74-924ef0c16b61"], "673d6085-6d86-4005-b8f1-75ccd2f9e032": ["b32a6b5b-a08f-4ca6-931b-e12bd130e7dc"], "aeae371b-2328-4f23-ace7-849a9842e0dd": ["b32a6b5b-a08f-4ca6-931b-e12bd130e7dc"], "5358b594-90f9-4866-9e62-3322c9e42a46": ["c6152016-2d06-4ec0-815b-670fa852ed9d"], "69971fb6-aba2-4b5c-b7f9-f4faf30ee46c": ["c6152016-2d06-4ec0-815b-670fa852ed9d"], "c8a2d1a1-6e61-4543-9614-f9927d460afa": ["55d1d8fc-9b9e-4ecf-b3cb-8cdc0699cf77"], "3fb77ca4-3006-4e36-befc-6f231498357a": ["55d1d8fc-9b9e-4ecf-b3cb-8cdc0699cf77"], "6960e83d-9e9a-4ae8-84e1-eb32d86ade3f": ["3505b143-1859-4ba0-b9ba-6aadd091ad4b"], "9a6b69f5-e08a-40c7-bfbd-05a3d8bb8cdc": ["3505b143-1859-4ba0-b9ba-6aadd091ad4b"], "aa720248-9b0e-4434-b80d-695c1e217f10": ["505523e4-2c89-4bbf-a1a9-9d01b0a5f99b"], "eb5c2d92-1c3a-4478-baab-7660a26a986a": ["505523e4-2c89-4bbf-a1a9-9d01b0a5f99b"], "40e645c9-1a4c-4b1d-9e1f-a342e4050c9d": ["3e2a007e-9292-45e7-997a-97a44a80cb78"], "649baa09-8ebc-4f2b-8f01-f3d047bcb8a7": ["3e2a007e-9292-45e7-997a-97a44a80cb78"], "086d5676-89e3-451d-81ae-815d24084b4e": ["bec87117-95c5-40d7-b81a-759caec9365b"], "295ea405-9ba4-4e69-82a5-73bd60f50306": ["bec87117-95c5-40d7-b81a-759caec9365b"], "3ed20e6f-3502-46e4-b66a-05e846fb290e": ["b511ce58-a85c-408d-816f-d29b88a8484f"], "72151c4c-a0f2-4cc6-ad8d-6bc4b89e423d": ["b511ce58-a85c-408d-816f-d29b88a8484f"], "17c3ee42-fd77-48a6-bcc1-3c6f87f4fa2c": ["b7fb5aeb-5a6f-4bfb-859e-5416a908e9e3"], "9ff0b2f1-9870-49ac-b576-423063ed2c48": ["b7fb5aeb-5a6f-4bfb-859e-5416a908e9e3"], "be925389-5c5b-4310-9cd3-28fdf473c43f": ["c41bcbd3-30b7-424c-8005-8e501ef1512a"], "c847b747-3259-4c3d-81a4-b7a113252e63": ["c41bcbd3-30b7-424c-8005-8e501ef1512a"], "c955ce3c-d16c-4acd-8d88-f63d33eade19": ["1b4c1bb8-93ed-452d-be10-fc455377084d"], "4d6e5ed0-82ee-409f-8bd5-078bc9ba59ff": ["1b4c1bb8-93ed-452d-be10-fc455377084d"], "370f8b69-c809-4cd2-a5ff-648a8b5dde49": ["d94e8c90-4405-471d-a597-3f17afb77473"], "1097d8e5-88c3-4127-80cd-3af6527a16fb": ["d94e8c90-4405-471d-a597-3f17afb77473"], "6e0fa94f-5b5f-404a-a304-0dbfbd2b1dfd": ["dd602890-5ff4-4c8d-aabb-93f710a2df2c"], "fbd1e2a3-a159-483b-9daa-a68aaa0ac412": ["dd602890-5ff4-4c8d-aabb-93f710a2df2c"], "a9946c3e-6092-407f-a798-81794cb67df7": ["f7967eb7-cea8-4208-adec-360cad8b5ad7"], "09b6b366-8f9b-4958-b019-6239a92c70f1": ["f7967eb7-cea8-4208-adec-360cad8b5ad7"], "75fc90af-1366-4cd7-a70b-185ee149e57d": ["9ebbadc2-3d68-4260-a1b8-8d48c39b4743"], "f551b460-4a11-4032-873b-94109f669ccc": ["9ebbadc2-3d68-4260-a1b8-8d48c39b4743"], "93e76e2c-2ce1-47dd-9595-1cb06998be1e": ["25eeec48-9aa9-4d1a-9bdb-4e436d957391"], "24cd256c-882f-4466-a9c1-107433afe2fb": ["25eeec48-9aa9-4d1a-9bdb-4e436d957391"], "41c9e079-993a-4c57-8a71-83228c1d2683": ["ae48837f-7b9e-43cf-8804-a89edc83f81b"], "3f4c1602-5134-4956-b899-01b908e3f6c2": ["ae48837f-7b9e-43cf-8804-a89edc83f81b"], "687c7205-dd75-4db8-94df-627b9e266092": ["3c7c9fa5-d428-4f21-820e-ca28739bc057"], "d7159312-e6f4-4b1f-b841-5d086d7c64e3": ["3c7c9fa5-d428-4f21-820e-ca28739bc057"], "b5e17291-55d0-4d1f-afd8-b9c95d21bce2": ["669b219c-3bec-4835-a3fe-deb49970036a"], "df622895-8d04-43a7-8890-dee5941257cb": ["669b219c-3bec-4835-a3fe-deb49970036a"], "6d452ac4-58cb-47dd-a4f5-23be2317bcc4": ["469127de-89db-4797-a1e1-106b49554fb3"], "7934daf9-439d-4fa1-8635-8d9c08d5e39b": ["469127de-89db-4797-a1e1-106b49554fb3"], "991ab183-4af9-4cb4-9e71-b4ecc7811bd0": ["f5ef1efa-22ac-4a31-ab45-569e59e9497a"], "41e952a4-23a9-45d0-a32f-99e747cb0af1": ["f5ef1efa-22ac-4a31-ab45-569e59e9497a"], "d9d42405-d1c0-4195-96d1-49a02d45c51b": ["00c8526d-829b-4c24-b7f3-dc4fd7959b77"], "7b3323ec-091f-4582-98d8-14080bb71fd5": ["00c8526d-829b-4c24-b7f3-dc4fd7959b77"], "328a3654-c460-4b1a-a563-6e82c202b7b5": ["be95a2a2-aebb-4ae2-977f-60a24f7161c6"], "4b674d60-54d8-47ca-892d-84141518254f": ["be95a2a2-aebb-4ae2-977f-60a24f7161c6"], "a447cab6-0bf3-4b3d-a910-54d16211e949": ["46fbdd80-f0be-44b5-8ba1-06db2e6e5470"], "70a1b9ae-7aef-47cc-9555-d6fc5b66735a": ["46fbdd80-f0be-44b5-8ba1-06db2e6e5470"], "59fc1585-8584-4f76-a2d3-7d2933a53929": ["e46c647e-a332-461a-8656-e1d21c0fa2cb"], "df3fec21-218c-4c6e-8a78-2eba212b71aa": ["e46c647e-a332-461a-8656-e1d21c0fa2cb"], "3a961e14-e27f-4313-a695-bd5125206f93": ["99c0742a-a224-4e13-8b7e-d788221170ca"], "8484d511-9b08-4bf7-8c89-6dc1bf530f4c": ["99c0742a-a224-4e13-8b7e-d788221170ca"], "0e899ed1-e842-4e55-96fb-2f8ea231aaf3": ["f7388864-ec3c-46e3-8236-31f6fa0842fc"], "42f49760-f504-4961-9066-bca2308e910c": ["f7388864-ec3c-46e3-8236-31f6fa0842fc"], "8a9d0a80-2537-4acf-8a26-dc23fb38b32d": ["a7eee4ec-72b1-4eb9-a1ac-d9c8eef5efd9"], "082cce7e-8bfb-45b1-a58a-5b3882c00691": ["a7eee4ec-72b1-4eb9-a1ac-d9c8eef5efd9"], "41281500-cc11-4734-b116-17c33b793baf": ["a954e08e-4d91-44a7-97ab-40316e4ebb38"], "fdf43c0b-77e3-4926-88e6-88ae19656c88": ["a954e08e-4d91-44a7-97ab-40316e4ebb38"], "175ccf4e-7a81-4bd5-ac29-95de8087c6d4": ["37074aa6-839c-4ad9-b3e8-c691d34ac431"], "14596261-86aa-459e-9d82-0c4f8ca1bab5": ["37074aa6-839c-4ad9-b3e8-c691d34ac431"], "eeb35cb0-5f93-4685-b348-a7d0badd30a8": ["db71458f-6c71-42e8-bfda-89708f83b7f8"], "27cd4dbf-8bc4-47fe-9b77-5d93ad2ef644": ["db71458f-6c71-42e8-bfda-89708f83b7f8"], "a2150aee-0fee-4141-8b30-5b151acb3afe": ["d2d5ad00-6a11-4d43-8a4d-914825752034"], "cb7446a9-f396-4845-8125-b20d067ebf0a": ["d2d5ad00-6a11-4d43-8a4d-914825752034"], "b485b215-07aa-4c84-afc4-1b77aa6cfaef": ["52f0f01e-8628-474a-a394-db44271c3f8e"], "dfbc012d-f75f-4710-a4db-387768ffb947": ["52f0f01e-8628-474a-a394-db44271c3f8e"], "2fc97456-b881-4109-8e22-5c59fc4b7c43": ["7b79639f-8db4-4030-92cc-aa430aebc39c"], "c32d14f2-8317-4332-bf26-1aece882f6b2": ["7b79639f-8db4-4030-92cc-aa430aebc39c"], "27cb7832-7697-451e-a562-9f304079e1dd": ["62872579-71c9-4ad9-83f0-e37a6d47a4a1"], "65aebc0b-eb32-4719-b6f8-bc9c298d1b37": ["62872579-71c9-4ad9-83f0-e37a6d47a4a1"], "0631cb68-8ba6-45bc-bb92-a86aef730c35": ["90d4c359-560c-4374-beca-9b01d31ee0bb"], "11bc41cd-f97c-414d-b4f1-602e5bed2cdc": ["90d4c359-560c-4374-beca-9b01d31ee0bb"], "02addd50-0d47-4ea1-9188-a4e640c1e07b": ["9bc6682d-ba91-466c-bc5a-e16cb8ce9043"], "16ea7acd-8f41-4ac6-a67b-b5466ffdb28d": ["9bc6682d-ba91-466c-bc5a-e16cb8ce9043"], "fd3a7263-6a26-4069-9f32-4354afb962ba": ["6ca78702-c65c-4e4b-9d95-2b86ff960332"], "0e0e863e-f23e-49bc-9054-780be264343e": ["6ca78702-c65c-4e4b-9d95-2b86ff960332"], "3c76f5e1-8483-43fd-a277-e03ce635e56c": ["2b86c335-4291-44df-bed1-a0a8ac8a2c72"], "c5179ea0-fa20-48db-86ab-3e20b3f99010": ["2b86c335-4291-44df-bed1-a0a8ac8a2c72"], "bc848f0c-f6bf-4dba-8fb5-51fc2fb47c26": ["137487af-2d01-4b77-8498-c30263a1eac1"], "7bbe5377-a88e-47d9-a247-783047afbe9c": ["137487af-2d01-4b77-8498-c30263a1eac1"], "07c17624-abd8-4b6c-bd7a-84c06ff82b86": ["ab564625-0f29-4802-bee8-c4be60f97ee3"], "7e23f612-ff93-47ea-b11e-4ec7e9cb809a": ["ab564625-0f29-4802-bee8-c4be60f97ee3"], "a73f872f-97b4-431a-ace7-1691f9e08ddf": ["30af432f-73fe-416d-8ef1-0bec5d523ccc"], "f9828d68-5ec1-43fb-bffa-eee1489578ef": ["30af432f-73fe-416d-8ef1-0bec5d523ccc"], "68f1bba2-e620-4461-a6dc-fbea8bb0ca6c": ["ea755269-8678-442e-a9c8-0c46031c6aaf"], "93a7885e-649b-47e6-8744-9f0609e7252c": ["ea755269-8678-442e-a9c8-0c46031c6aaf"], "d96859b5-78f7-41dd-9bc5-2deb8b117d12": ["b9c95be6-886f-44d1-9802-92dde18e7c28"], "76559ec9-946e-4b7c-8e6e-55e024b9788d": ["b9c95be6-886f-44d1-9802-92dde18e7c28"], "ec6c0f68-f844-4313-9882-c14fc02b2955": ["3d7e26fd-d22b-4711-9590-5f124d5055a2"], "74706422-bf68-4dd9-a6a1-b3a50d1b5974": ["3d7e26fd-d22b-4711-9590-5f124d5055a2"], "9a6bfa74-f489-4136-8153-247535f8eff1": ["2d7a7505-0450-41dd-b411-bc047429e16a"], "a97ad9fc-b69e-4b60-aa83-248ad55b914f": ["2d7a7505-0450-41dd-b411-bc047429e16a"], "c8b1ac7c-5303-4639-8352-b705e6ee23f8": ["6ea9e6c3-f08a-414a-9689-ebf7d1555953"], "db59606e-a910-4e3f-acf2-640101513857": ["6ea9e6c3-f08a-414a-9689-ebf7d1555953"], "5a31b42d-00db-477b-9d46-0631a9b90bc4": ["52fab6a3-3caf-4146-a9d6-bafc2ec445bb"], "cc331d52-69ce-4c23-8fb9-6da612896fe3": ["52fab6a3-3caf-4146-a9d6-bafc2ec445bb"], "49f384da-3dce-4bc8-92df-7698a43f2a1e": ["62941548-45c0-43ae-80a8-ae5ab945f6e4"], "1051ba44-0b77-47a6-9876-a3ca406fae28": ["62941548-45c0-43ae-80a8-ae5ab945f6e4"], "43e91d96-162d-41cb-a13d-03e9045079ba": ["6f7c4b10-ab23-497e-bed9-6f8dcf822745"], "b667ffbe-383a-4a4d-9699-d2fa0423980c": ["6f7c4b10-ab23-497e-bed9-6f8dcf822745"], "090d8c5c-3ee5-4339-9492-3d84eb0fb243": ["c0a70619-d442-4ac7-b34b-4128e88e5e60"], "cd68201d-ccd0-481b-9425-701731c55679": ["c0a70619-d442-4ac7-b34b-4128e88e5e60"], "398c1c4d-4636-40dd-8d4a-bba2b4b3ab7e": ["5f44bc21-5bda-4980-850b-b7e786380a32"], "5d6c177a-8fd6-4534-ad56-639b77f4130f": ["5f44bc21-5bda-4980-850b-b7e786380a32"], "a52ba0e7-1312-461f-a37e-82536552dc62": ["6c3336d0-dd06-47ee-91ad-1d0229322acf"], "6c97c0ee-2406-4207-a371-cb2fe47bc2c6": ["6c3336d0-dd06-47ee-91ad-1d0229322acf"], "cb84f40b-2d55-4a5f-914b-04165b2daca5": ["b5ade1fa-8fbe-46f0-8e9a-23054e0420bc"], "66a6c84b-a66e-4a0a-80d6-2d0d3206097c": ["b5ade1fa-8fbe-46f0-8e9a-23054e0420bc"], "0b0f13df-a595-42c8-8d98-4c9dd893f20d": ["ff3c4702-155b-453d-8f6e-a779cbcf65fb"], "08d5b906-7f21-4f20-b903-fbebd1b35129": ["ff3c4702-155b-453d-8f6e-a779cbcf65fb"], "a634ac46-6ffb-4425-b63f-20b1085c0616": ["eb793fbc-46dd-4799-a2a6-5c589802e15f"], "bb38782e-a182-4935-be9c-a04733a12b35": ["eb793fbc-46dd-4799-a2a6-5c589802e15f"], "8010103e-2ea3-47a6-b9f7-17f2be43116c": ["3453510b-a406-4873-87b6-4c55ac7da1f3"], "ca69096c-4d8c-4e0e-9c2b-e129bf464e0c": ["3453510b-a406-4873-87b6-4c55ac7da1f3"], "841412dc-dbbe-4246-9682-71634636d435": ["8465be0c-697d-4e0c-881f-f501e99ed9a3"], "8984a91d-5307-449d-84be-62db0e5fdffe": ["8465be0c-697d-4e0c-881f-f501e99ed9a3"], "a77e4024-4879-495c-9b04-2e63a3dfecdf": ["db0eb3a2-60d7-4af5-b808-14a48b00f9d3"], "58ea0138-a3f7-49e8-aaee-df3a6fe88d82": ["db0eb3a2-60d7-4af5-b808-14a48b00f9d3"], "1ddc67c7-2531-4827-9def-59109c426fb6": ["ee02bebf-0523-483a-993a-6da567ecd915"], "537cd948-b3ae-45b1-80a5-47a09b66fea2": ["ee02bebf-0523-483a-993a-6da567ecd915"], "82262be5-572a-43cc-8e62-cf172a6bc340": ["52405fd2-6b9a-4137-8d4d-7e9e6ecd4db7"], "ce81a6a6-a52f-4315-8463-41c2501ccf1e": ["52405fd2-6b9a-4137-8d4d-7e9e6ecd4db7"], "559c841d-1bd6-4216-8951-9cba0bc60769": ["8a8e20dc-9936-4e61-8766-5ab5d7542466"], "dadfcd8d-8811-4546-a3ab-8102a3dab837": ["8a8e20dc-9936-4e61-8766-5ab5d7542466"], "314d0ff4-590a-43ce-a522-2b08caaeb2b0": ["94d0dec6-0e49-4564-bc70-cb89b3e2935e"], "a72e5a4c-c76c-40f9-9c21-c38bb3e89b7a": ["94d0dec6-0e49-4564-bc70-cb89b3e2935e"], "76fd7169-9904-4a8e-b6cb-4cc833a8bbd0": ["4506adfd-9e4f-4a13-b511-f0170f92946b"], "bba7071c-12a4-4ed2-8226-6dd8ba3fc6bc": ["4506adfd-9e4f-4a13-b511-f0170f92946b"], "890b88a6-d3cd-4433-bc43-9eeadca28cfc": ["f6d2577e-a394-4b09-baea-88a939136116"], "b3c546ca-7ca3-47db-b343-7c143316e05d": ["f6d2577e-a394-4b09-baea-88a939136116"], "60d04203-a649-4a1a-96c2-cc0ff7e4d0ad": ["d57b8f3d-af10-4e6c-8c9f-3fc27afe1640"], "3bad7a62-74bb-43c9-93e9-703419df5376": ["d57b8f3d-af10-4e6c-8c9f-3fc27afe1640"], "e3b64b1f-c4c5-4c0d-b5ab-f6ed3afa6f98": ["bebd53dc-d449-4415-a3a0-2ea16ea6dcc3"], "eabf9c45-5ee4-4fd2-96a7-d681542ab4dd": ["bebd53dc-d449-4415-a3a0-2ea16ea6dcc3"], "7da15e82-39e8-456c-81d2-5ca4172f9094": ["9e53416e-7f20-40e9-b9f6-782128748ef8"], "ba64719c-790b-49a7-bbf7-7363a72c2e9f": ["9e53416e-7f20-40e9-b9f6-782128748ef8"], "30606e91-0b06-444e-bd25-9f71ffbbf1c1": ["03df9c96-4248-48c5-94b2-4dc42c5fe25f"], "0ff3c395-4a85-4478-8537-66c6c9b54dc8": ["03df9c96-4248-48c5-94b2-4dc42c5fe25f"], "fef92a88-463f-4b6e-88cc-3574a173d757": ["3ab46d61-53ae-4626-8c77-10e9a4deecea"], "b1151b8f-6bfe-436f-8119-c933bac0da50": ["3ab46d61-53ae-4626-8c77-10e9a4deecea"], "f9503905-c2c3-4716-b5ba-5cb5ae378e2f": ["1311e539-512a-46b4-ac21-2e716b6de8de"], "85fd73b4-bf61-45ea-994d-6d8b03e747f6": ["1311e539-512a-46b4-ac21-2e716b6de8de"], "619b8f1d-8513-4922-a2a4-56f7c89dbb3d": ["9a5369d8-3bf3-4bc8-a722-a6b41968edd6"], "fb2cb9fd-f94c-4c71-bda4-a7f399d2f5c4": ["9a5369d8-3bf3-4bc8-a722-a6b41968edd6"], "9ff35743-ac68-41ea-b2f1-791c9e9d3db3": ["ad287034-854a-4a89-a1df-59cfd666bd42"], "ae962f47-66e5-490a-8e24-26936a258915": ["ad287034-854a-4a89-a1df-59cfd666bd42"], "ab610945-6f23-419f-8c63-f2933911e526": ["257cf73e-add9-4031-8c3c-1650bc9a3411"], "c68a6214-8024-46ed-ae1b-2de95b30a14f": ["257cf73e-add9-4031-8c3c-1650bc9a3411"], "984bace7-0b05-40a9-aa82-db7559e79a8f": ["6ddfe620-4bb5-451d-aeae-1e468625ce2c"], "17b61b55-5465-4a51-84b2-06c065df191a": ["6ddfe620-4bb5-451d-aeae-1e468625ce2c"], "738c38af-6e35-43a0-8a54-1a7e421a3adc": ["82097155-5317-48fc-b07b-5be41385a77b"], "abd691d5-89fe-4f7d-974e-535ac47c2e1d": ["82097155-5317-48fc-b07b-5be41385a77b"], "ad4c34ed-264f-49dc-8188-074f399829e5": ["0b8da397-4dd0-489b-8ee9-82e6062cb704"], "31a0989b-48e2-462f-9950-66fdd1a619eb": ["0b8da397-4dd0-489b-8ee9-82e6062cb704"], "964a6e8f-062f-43a7-86a9-a9bc87533fa9": ["49cf468e-c35a-453c-bb12-87705828ad61"], "52e35ec8-e711-4868-b45d-507cd681edb2": ["49cf468e-c35a-453c-bb12-87705828ad61"], "2a329767-5f7b-46ad-8840-08dce0dcb337": ["9be78fe9-296a-4b60-a908-2d7628929763"], "23b2dbd8-69fc-4665-a109-19ab05260095": ["9be78fe9-296a-4b60-a908-2d7628929763"], "eb2382bc-58be-4141-8fcb-e28dfd93ac2c": ["ae159843-307d-4b90-b21f-8b593523023f"], "5227c401-6ad4-4e26-a593-8acc583d1a09": ["ae159843-307d-4b90-b21f-8b593523023f"], "7b07bb10-0235-4446-a815-5d3797b4944b": ["4a9f2084-d858-47ab-a56b-9eae523aa41d"], "a915766e-673e-45be-839b-23fef266f28d": ["4a9f2084-d858-47ab-a56b-9eae523aa41d"], "1b749412-d0d8-4cd3-9c2e-98aa54d19a8b": ["c01c1740-b4c7-4fb2-9a49-ec36bf5e3a77"], "6e5b4ead-a227-42dc-b04e-f8dc576da7f2": ["c01c1740-b4c7-4fb2-9a49-ec36bf5e3a77"], "f4371f74-55ca-4230-84fe-0783ea553d3b": ["62680352-30f3-4902-9379-ad6e15d2f543"], "4a89ff33-3616-4072-aa1b-2966fc747954": ["62680352-30f3-4902-9379-ad6e15d2f543"], "0bcba52f-716d-428a-9cbb-d2d02c5634c9": ["5899ed4b-8827-4ec8-a384-4e0c56837f95"], "8bbd3212-8486-4c6e-a237-a5a2d7366a05": ["5899ed4b-8827-4ec8-a384-4e0c56837f95"], "a50f8960-50ee-428d-b80f-63ec5a112310": ["3e724225-69df-4e90-a0bf-ce99189a8473"], "54ed23ad-2cda-42c9-8bd5-59c72084a8f1": ["3e724225-69df-4e90-a0bf-ce99189a8473"], "69d4e267-d4c3-4d46-9f18-5dbd6d818748": ["dd396ff3-76e2-4501-9df7-e808b845705b"], "58b9143f-1d4b-42ec-81d7-da6e8181aeba": ["dd396ff3-76e2-4501-9df7-e808b845705b"], "be5c8eec-deb3-4561-afa2-a57891d444f9": ["154b3d97-26ec-49fa-9c6c-334d04d95eb4"], "7ba42bc0-d2d2-4191-a77b-4c79b4fdcbb8": ["154b3d97-26ec-49fa-9c6c-334d04d95eb4"], "db16b379-6e98-47a1-9239-5180fb042c7d": ["49220757-4b8b-4b36-bf24-1379e8409fb8"], "e02e887b-64a6-4d07-aa41-cd499577f0f2": ["49220757-4b8b-4b36-bf24-1379e8409fb8"], "5a3a13f7-1770-491a-a4d1-afb0cd14a13f": ["49e2b4f7-c28e-455f-a012-2f6460eeaaf3"], "e2854152-645e-483f-8859-d3d403087372": ["49e2b4f7-c28e-455f-a012-2f6460eeaaf3"], "ce091045-27b7-40b5-b648-254c02b37b14": ["f92c5bb9-2931-4619-9221-44098fb9f465"], "feee5af9-dcbd-404c-941e-98a97d1a6f53": ["f92c5bb9-2931-4619-9221-44098fb9f465"], "a834a228-b45c-4d26-a925-c5139c3c817c": ["9c7f67c8-f578-4bf0-b295-4317554c1d08"], "a64beffd-340b-4d6c-8237-ea0fba743e8a": ["9c7f67c8-f578-4bf0-b295-4317554c1d08"], "a49b3a66-5fa6-4638-886c-a98190b8f087": ["50f722b0-3885-47a6-9e1e-021e05f3f083"], "4630e4c8-e1b8-4b9c-832d-e7cdf1325d18": ["50f722b0-3885-47a6-9e1e-021e05f3f083"], "c2274d54-17a0-4eaf-a97d-9f34e292f810": ["dc1017b1-ca60-48c7-9060-93161be02bbc"], "7dec4bb7-28ef-49d0-a274-bfe6fb0083d6": ["dc1017b1-ca60-48c7-9060-93161be02bbc"], "f0dbfbb5-8735-4fb0-a44c-3fdc60e8a77f": ["d4e69064-d25b-4f82-adb6-61f52e418c71"], "a4a7a28b-7754-4255-b6a1-9f35165d4054": ["d4e69064-d25b-4f82-adb6-61f52e418c71"], "d30ac259-6c87-4d30-9b54-d6f25a8aa041": ["47f68128-29c1-4c04-aab7-4cbfb48ff933"], "16385e44-fc48-40d7-b3e0-ed3a8f8f3f32": ["47f68128-29c1-4c04-aab7-4cbfb48ff933"], "a74ba28b-1321-419a-b046-1f8b9e719a1c": ["227f6a66-ca12-4f0f-84a2-9c02fb80e523"], "f4b96358-14ee-430f-b075-220cac01930c": ["227f6a66-ca12-4f0f-84a2-9c02fb80e523"], "3b8144f0-0134-4350-a335-1db64c9223c1": ["21e7fc97-f3f6-4d0a-a8b9-d77367dbf16a"], "de53d57a-28b8-486f-ab1b-cb4ede29c9a8": ["21e7fc97-f3f6-4d0a-a8b9-d77367dbf16a"], "7cd5ca1b-a8f4-4698-937f-18c575ae4dc8": ["7d6eb21b-99b4-47b6-9302-3aa15102742e"], "4ead2c8b-9bd4-4751-aa43-2e5474514184": ["7d6eb21b-99b4-47b6-9302-3aa15102742e"], "96496291-ab54-4324-bac2-63f7b0f9c005": ["93611f94-6558-4b2a-8a26-3e97d7ce920e"], "62504734-8216-4187-8712-a47fd06ef86a": ["93611f94-6558-4b2a-8a26-3e97d7ce920e"], "3e8f5335-ae14-472e-8104-0c18b551aed6": ["80ced9db-6594-4cbd-b7f8-b956afda5751"], "8c389671-4f09-40d9-860f-7a1809f3033a": ["80ced9db-6594-4cbd-b7f8-b956afda5751"], "cac1c7ee-a07d-43a8-9246-13a450f28c9f": ["5575f668-b7a7-4e3f-bc70-dd8987755600"], "510bc503-9ad4-44fe-95cf-5f782ce560b8": ["5575f668-b7a7-4e3f-bc70-dd8987755600"], "9b78a42e-9170-41fd-b72e-a88302b5a1c0": ["94833f0a-4973-4e83-a39b-2f777f8076c4"], "9fd622bb-c87d-4ea6-9819-28b2711d943d": ["94833f0a-4973-4e83-a39b-2f777f8076c4"], "d949c584-8543-475d-99a6-b0ec589cbb1b": ["64441995-17b5-4f60-9100-752e311fd4b5"], "d752d135-d444-4138-a68d-bebed082b751": ["64441995-17b5-4f60-9100-752e311fd4b5"], "22536de6-1b70-47d9-8911-285215b631f2": ["65002c2c-9170-4e2a-8454-913d78ba5a7f"], "66604244-f440-4b12-af66-dd9381c9adc9": ["65002c2c-9170-4e2a-8454-913d78ba5a7f"], "4c795439-efad-407e-a024-966c22498b7a": ["4a2d7b3b-b70c-4d5f-be98-452e014fb626"], "b0a0d048-f0e5-4683-9bc2-e8b314e6ec99": ["4a2d7b3b-b70c-4d5f-be98-452e014fb626"], "3d436c04-bb5e-4792-965f-0ced36f31223": ["ae73000d-452e-4a17-90af-2487a3ce6648"], "fb151dcb-b4c6-47b9-8d66-2fd94da98dc0": ["ae73000d-452e-4a17-90af-2487a3ce6648"], "04cb7505-8618-4423-b887-03cd6fd7e079": ["0eee3dfc-313b-4762-999d-fdaf987cec1f"], "056f5bb5-17c7-42a9-a899-0bb45ae74889": ["0eee3dfc-313b-4762-999d-fdaf987cec1f"], "27199a6e-fa43-42eb-aaf6-6dadfaba6b81": ["374ed34f-bfc2-43f7-9f22-2cd074166196"], "dbe51a96-6d02-4bdf-b4ef-59e7963eab55": ["374ed34f-bfc2-43f7-9f22-2cd074166196"], "51bd0ad8-9188-49ef-899f-61b7792d249d": ["27eded69-bc8c-4a5b-945f-0582abad68a4"], "d2b9dcd4-6177-4d44-9dcd-5e99c0cf6e4e": ["27eded69-bc8c-4a5b-945f-0582abad68a4"], "7c178293-944d-4a37-94e4-67baad919136": ["c171263a-d733-4b57-aa9a-7edfb093dc8e"], "27ff4e08-7c29-40dc-b795-a24436caec08": ["c171263a-d733-4b57-aa9a-7edfb093dc8e"], "cf14f59d-20c8-407f-a5f0-b20bafefb1cd": ["0b8ff81d-8116-4596-b343-e7289feff69f"], "d5c245b9-cdf4-46db-9ea7-efdafb296302": ["0b8ff81d-8116-4596-b343-e7289feff69f"], "c11b2de6-5b75-496e-bc91-821c2596a3a1": ["41c4badc-1d08-494e-b72a-06b95b87a1b2"], "efa9cb34-3023-40d2-9d70-2f5ab648fd5b": ["41c4badc-1d08-494e-b72a-06b95b87a1b2"], "9e8ff1b3-d2c8-477c-b350-4f75c583afa3": ["1a026a4f-53c8-44af-948f-8a505d4f1f56"], "1c2c51ba-d983-4b11-ba8e-d304a5695a38": ["1a026a4f-53c8-44af-948f-8a505d4f1f56"], "5299e82c-d702-4e69-82f2-ce1ac67d8623": ["b2c23485-d0cf-4045-9bd3-91485f2b7fdd"], "62b93675-c91c-4e9c-ab51-6e9ad8065640": ["b2c23485-d0cf-4045-9bd3-91485f2b7fdd"], "07b874f5-c91d-4df0-ba1d-3b5f2836dd23": ["5a3e9f9d-44cf-433d-9f7f-ba7e26dbff0d"], "e316bf9d-9bea-4fa5-a42f-8d425e697702": ["5a3e9f9d-44cf-433d-9f7f-ba7e26dbff0d"], "cad07ff3-f6d8-42f9-8123-20dc7dfb35bf": ["74a3854c-1c4f-496b-80ab-857c3a502384"], "f4d4a2f0-5af8-4946-8d56-a0ddb373e699": ["74a3854c-1c4f-496b-80ab-857c3a502384"], "e00552b6-81b9-413a-822a-8448a8293038": ["0db8bc66-7b08-4b51-bcad-8a1a7f217320"], "1ec814bc-fe99-49f0-908f-a8622f6e7a0d": ["0db8bc66-7b08-4b51-bcad-8a1a7f217320"], "7ef4bcce-128b-403a-8217-cb1c195481eb": ["3925b4d6-045a-4767-889c-adc0ddeff611"], "29b1c837-f577-4f38-bef5-252f787e97d5": ["3925b4d6-045a-4767-889c-adc0ddeff611"], "497cb6a9-ab5e-4c6c-a550-65e8a2a8d8b0": ["6c5937d4-c964-4701-9449-c674c6759618"], "f53473ea-2212-4be4-b8bb-55e524b98179": ["6c5937d4-c964-4701-9449-c674c6759618"], "e60a0149-c5fb-4991-96d5-38230d93443c": ["e5a7dfde-34f1-417c-b69b-ddae9da6b545"], "06085e20-8d12-47bd-8e42-ee456ebc1eec": ["e5a7dfde-34f1-417c-b69b-ddae9da6b545"], "f2f0dab7-714c-4dbc-9200-e5194959b2d4": ["0b050884-5ef9-454a-b3fb-007cbb57ac27"], "ff8fd2db-7783-4f27-a6fc-f38622568157": ["0b050884-5ef9-454a-b3fb-007cbb57ac27"], "93d64f82-77a0-4cf0-bf16-17a832366b74": ["a9c34cb2-2184-43f7-a93f-ce88d718d09a"], "df23ae12-b8f9-417b-a534-4b3c0f1a1b21": ["a9c34cb2-2184-43f7-a93f-ce88d718d09a"], "3b91f0c6-8d22-4bf8-b4ac-6ba4edf01123": ["80a26e83-2b8b-4dda-875b-5a7b35fe4f71"], "e80d724e-cd52-4636-ac91-6bc32df51cb0": ["80a26e83-2b8b-4dda-875b-5a7b35fe4f71"], "a57847d7-56dd-474a-be1a-942b6a3a042b": ["0bf93cab-a60a-4455-af9b-b31de2fecf8c"], "34261781-7fc3-4734-a537-9280170465c0": ["0bf93cab-a60a-4455-af9b-b31de2fecf8c"], "0c0ce68c-20cc-4e48-9e17-800a70e1f691": ["279c3e50-c2dc-42e6-8699-95ed10c1ac64"], "1bd04c80-e957-4cff-9d15-e7884483897f": ["279c3e50-c2dc-42e6-8699-95ed10c1ac64"], "ed146216-7598-451d-b2f1-1f84b9b7b694": ["c96120ad-3e16-4fd0-8954-2356212fe0a4"], "245da9b9-e3c9-4fa9-ae64-158ac288bc85": ["c96120ad-3e16-4fd0-8954-2356212fe0a4"], "350ef013-9e08-40be-851d-a02d140877a7": ["c363a336-d891-485b-982f-89a1b0bc7e18"], "7424642a-45f1-4ec0-8c04-df5accc06768": ["c363a336-d891-485b-982f-89a1b0bc7e18"], "c2ec2d85-1964-4a06-bbab-65bf5deb592c": ["37dfdbc6-43d4-4761-a1dc-dbe6e2971232"], "52193776-3336-4c1a-b30c-81e56ef99239": ["37dfdbc6-43d4-4761-a1dc-dbe6e2971232"], "450b3264-f46d-4ef9-9019-386524d4877f": ["e0751ff2-bfa2-42aa-bd70-743377505543"], "516eb4fe-af07-44ad-9276-b7a0e4ea8ddf": ["e0751ff2-bfa2-42aa-bd70-743377505543"], "79880357-e8e1-4649-b9fd-f7ab01978139": ["4b126ecd-26bb-420a-82ee-d78286845dcd"], "a751cca1-50b7-4961-b50e-d9971b3e4564": ["4b126ecd-26bb-420a-82ee-d78286845dcd"], "cb15d939-6baa-417d-858d-e256dcd68063": ["819e56fe-ae7e-4f75-b256-920b30b3e604"], "dfb7ec53-90ed-4927-9c09-d51bc7c1784f": ["819e56fe-ae7e-4f75-b256-920b30b3e604"], "29867087-10cf-4179-9383-69fafb7fbe23": ["08b38cd0-5994-4fa1-9ba9-cc722583133b"], "19072f38-cf71-4ea3-a04d-fc5faef7d53e": ["08b38cd0-5994-4fa1-9ba9-cc722583133b"], "1ee6c530-16c1-49ca-8647-b1ca04d4df41": ["6add79ac-d85c-4e2f-9ce7-7333ab4b7058"], "f70954d8-6b45-4aa0-816d-f724c6917e7f": ["6add79ac-d85c-4e2f-9ce7-7333ab4b7058"], "50a74ac1-887f-4255-b9c4-c89099962246": ["48f89ff8-02bd-494c-ab98-54b8c08ff226"], "a4556845-b642-4864-945a-3f6044cb2da9": ["48f89ff8-02bd-494c-ab98-54b8c08ff226"], "be87e68b-a44b-453d-9108-9210a9669762": ["c2b0bfb7-ebf4-48af-8526-bc99fa5e6fc8"], "41a4d3a8-364a-49bd-8975-cfdfac621dc5": ["c2b0bfb7-ebf4-48af-8526-bc99fa5e6fc8"], "36721c38-09a0-4033-badb-3e4f0e2be866": ["e438837b-fe3a-48f5-ac2e-3145adbd81bf"], "8115fa22-b69f-4d53-99f2-80a5cb7ce698": ["e438837b-fe3a-48f5-ac2e-3145adbd81bf"], "53c3ec12-ff38-44ea-bb78-2db092ea3a1e": ["e1dc4570-af27-4e44-a594-ba5c251e7149"], "78c8649a-c9ea-4ccc-8815-1d2bf2961fe5": ["e1dc4570-af27-4e44-a594-ba5c251e7149"], "b6028a7a-c668-4205-979f-28bd42b97233": ["953d2807-8a5c-45d8-bd7a-e29e6d1b184b"], "e178d5e1-66d3-4c15-aabd-92748aa6d060": ["953d2807-8a5c-45d8-bd7a-e29e6d1b184b"], "d041e473-0f2f-483a-9c5a-9e9d6027a6d8": ["6dbdf6cf-fd83-4e9a-b05e-95555bf4b6c9"], "711cfac3-61f4-49a3-8a4f-178e36fafa6e": ["6dbdf6cf-fd83-4e9a-b05e-95555bf4b6c9"], "152782b7-803f-4c02-85cb-40b2688f53af": ["a3584533-ced3-4dee-bcf9-e847718cb3f5"], "bc8fe9d9-1a6d-47f5-afab-eca82f3a8524": ["a3584533-ced3-4dee-bcf9-e847718cb3f5"], "a4d9114f-f8f6-4dc9-ab49-870f59255098": ["35321efb-e891-4cfd-913f-17edf49f51ac"], "0b3fc2aa-612b-4c1e-9560-d81638179a14": ["35321efb-e891-4cfd-913f-17edf49f51ac"], "86d93dac-23b0-444e-bd62-62ba836333da": ["63c2bf7a-93ca-4799-88b7-de33bea53452"], "3fd19c6a-ad20-4edb-8f3d-222ec2b7054f": ["63c2bf7a-93ca-4799-88b7-de33bea53452"], "6d00dcce-77a0-4ea1-985b-132693a688a4": ["dbf3da71-2e01-4117-a330-aec3c3305f6a"], "2cb74016-651c-4af6-8d72-340e2694f660": ["dbf3da71-2e01-4117-a330-aec3c3305f6a"], "9759eb86-4201-4962-90de-d808f09f5584": ["c8f8a6b6-cf66-4180-b66b-67a10d2fb706"], "29c4ea25-9b45-498c-82e0-00bc0455c26f": ["c8f8a6b6-cf66-4180-b66b-67a10d2fb706"], "e2c7f7c6-4191-4603-be9b-d9f20d4d3d3a": ["ee281701-6ecf-4774-80a1-69c8a6a06ef6"], "5f331fcc-993f-4722-bb7e-f1b436c805f6": ["ee281701-6ecf-4774-80a1-69c8a6a06ef6"], "a2573f02-0e68-4d5c-be09-c0ae4f2c161f": ["aca80990-85c8-46ee-a3a7-235d6fbea85c"], "e62eb1be-0bcf-4c4a-9647-43c9b246f146": ["aca80990-85c8-46ee-a3a7-235d6fbea85c"], "cd98da12-1c40-4582-aada-5fe5129a2f29": ["7318b191-ebc2-43fc-966d-fc5a8d40ba5f"], "ea0e22fc-4072-456c-b87f-6dc9b590ee0e": ["7318b191-ebc2-43fc-966d-fc5a8d40ba5f"], "5b33d8a0-b63f-4f51-acce-e18c790992f0": ["94f9d19e-190e-468f-a9e6-4d32239c612c"], "aee42c7a-8fdd-4a5d-b61f-cb958b8b1cd1": ["94f9d19e-190e-468f-a9e6-4d32239c612c"], "56472c05-63e8-4877-bada-a91ba9ff4c1b": ["1d062945-8a5f-481f-9d1c-01e59b3e9704"], "e7c7e090-3904-49f6-ae56-42e9a74ebf0d": ["1d062945-8a5f-481f-9d1c-01e59b3e9704"], "4eced69e-fd56-4d26-ba01-a7e9322d75b7": ["2946d9da-7431-4787-a0a3-37ad168aea57"], "1a2e7f80-6385-4487-8605-7457941a5a48": ["2946d9da-7431-4787-a0a3-37ad168aea57"], "7dce7dcd-0683-4b49-9b5a-379d37e5853e": ["f72b18cb-7e37-4247-b8b7-dabc9cfd1c2c"], "b6a8de79-93a7-4cb8-a133-8c7223273c9c": ["f72b18cb-7e37-4247-b8b7-dabc9cfd1c2c"], "9065e6dc-546b-4b09-a1aa-70211c60fbea": ["be2daffd-fbc1-4e39-a97f-dde4e6e3bb00"], "e1314548-7018-449c-9a44-758f3bf18a0d": ["be2daffd-fbc1-4e39-a97f-dde4e6e3bb00"], "f1d2cc6b-1f87-4ff6-92a7-f7d204e47fc2": ["bc634732-9300-422d-a06f-6ecdb0e97eb5"], "986ff69e-18d2-4a00-aa4d-51d57b60cc0e": ["bc634732-9300-422d-a06f-6ecdb0e97eb5"], "f82ed697-aad9-4a5c-a098-f2017173d6fb": ["1279e685-e0a6-49e9-b1a5-9de870ba5332"], "a2f2988a-e473-4823-a124-b769de445d87": ["1279e685-e0a6-49e9-b1a5-9de870ba5332"], "d29e37be-8bed-43ee-956e-454f5621ff62": ["c779b403-5ac1-4cf0-9f3d-476d7b3d16af"], "dfd5ee36-1ec1-44b6-b972-424a9ec6d0ba": ["c779b403-5ac1-4cf0-9f3d-476d7b3d16af"], "8868dd98-7ad9-4588-976e-b6b6e4bb25d7": ["992ec3d5-95d9-420c-b650-b95aaae00009"], "74224a66-431d-452c-9341-6764504fcda8": ["992ec3d5-95d9-420c-b650-b95aaae00009"], "dda5978c-28c4-4fdd-a0c9-5afec646b827": ["d40842db-4bbf-4fbe-89af-d83b15cf6e86"], "4767274a-106e-4833-b9e2-80bfb406b5b1": ["d40842db-4bbf-4fbe-89af-d83b15cf6e86"], "fe2fa197-eba9-4251-8fed-cc3c05198a71": ["e589ef41-039f-4855-94b0-125ad3fe9ba0"], "bebf98d6-791e-4417-8cab-15effd74c1ef": ["e589ef41-039f-4855-94b0-125ad3fe9ba0"], "9638f591-6a11-47db-8571-6dbfb675a05a": ["f0e1cf0a-a021-4f06-9ae1-4b948f3348f5"], "c73ff7db-faf2-40c6-929e-cfcd30193756": ["f0e1cf0a-a021-4f06-9ae1-4b948f3348f5"], "40e7a568-a9cb-49b5-b71d-9226db1ee074": ["9d1f9c02-cbf2-4a09-992a-e195d6fe6978"], "492cc77a-6a9b-46e3-9018-08627c0ea072": ["9d1f9c02-cbf2-4a09-992a-e195d6fe6978"], "02db77f3-725e-4fba-81a9-0ea70c1870a9": ["1da5023d-bdb6-497c-b93b-6ccc2c5be160"], "9003f250-545c-4786-bf4f-b7edc2230786": ["1da5023d-bdb6-497c-b93b-6ccc2c5be160"], "2d6f694c-7754-48a0-8b4b-1ee185a74ee9": ["5a32fb91-eb24-41cf-b69e-0893850cdff2"], "ab1d7e13-554b-4837-a722-68fbf07f3dd9": ["5a32fb91-eb24-41cf-b69e-0893850cdff2"], "61b76ecc-0768-4e94-a714-1d7f8a64ed09": ["95450b08-b676-4e68-bd58-8c37e9b02b8a"], "aac36c30-74b3-48eb-a14a-5c60ba99977d": ["95450b08-b676-4e68-bd58-8c37e9b02b8a"], "3ea5ac7e-2019-4f7b-b9fb-19a1393b91a7": ["419f7bee-e376-4f9c-bc2f-b177a49dca37"], "a45e26bc-d977-4ef2-97dc-0b5cb4b067be": ["419f7bee-e376-4f9c-bc2f-b177a49dca37"], "863e5f1f-2ec7-47dd-9383-f49985d3cdcb": ["a62ed3f5-f1b3-4dc4-97c7-222879f1e8bf"], "bfce1756-9393-4938-ac4c-8a43c4df8802": ["a62ed3f5-f1b3-4dc4-97c7-222879f1e8bf"], "5397b275-1e83-488e-ae1e-22a9af579fbb": ["ce6b582c-0bc8-41db-848a-f252399f4028"], "e105c762-f849-4871-a339-04867bd416a4": ["ce6b582c-0bc8-41db-848a-f252399f4028"], "466e43f4-089b-46e7-a963-bf26c331f23f": ["ec6098e4-5393-498d-9c19-e2078cd5eaf5"], "9cb8dacc-8c80-47cd-a4ee-e2acea430e8a": ["ec6098e4-5393-498d-9c19-e2078cd5eaf5"], "7100ee06-27f8-4b50-b01d-71892a424127": ["6bc34fb7-cbe7-4496-8d05-ee3e55e45b3b"], "778403ba-574f-46a9-9413-b8c93032732c": ["6bc34fb7-cbe7-4496-8d05-ee3e55e45b3b"], "60e9537c-956f-4545-b365-d6fac210e394": ["0f5260a0-12c9-4bdd-b6b8-b3b475d05be7"], "1233b853-c8af-4fc6-a131-da526ab65381": ["0f5260a0-12c9-4bdd-b6b8-b3b475d05be7"], "14aed328-2a7a-4863-aef6-152732f7e37c": ["86590aaa-d451-4f23-801b-e251cea00441"], "5b2ecebb-2613-4ede-9607-217bf79f7393": ["86590aaa-d451-4f23-801b-e251cea00441"], "bd87f02e-c6f1-4b1e-a301-f9b9127d10b9": ["64fe2866-8d47-4076-8dda-97a7bbdc856e"], "c29424cf-ac7b-4fcf-b853-256dba32a68b": ["64fe2866-8d47-4076-8dda-97a7bbdc856e"], "5535d248-d044-4e5e-bb0e-d9e3a203a73c": ["3ffcabaa-c3de-4069-9b4c-b328cd514225"], "28b8816f-864c-4ee7-b216-c0dca3926e2b": ["3ffcabaa-c3de-4069-9b4c-b328cd514225"], "f5cc3ba5-7516-476f-95d0-a15d228920d2": ["b765be94-adb1-45a2-9254-531cccb5e604"], "511c322a-25a9-40dc-b3b3-c6029e75d240": ["b765be94-adb1-45a2-9254-531cccb5e604"], "5e5cf0a7-e417-473b-99d4-b3fa3fad4a08": ["c760d27c-4114-4b8a-bad2-a74928737c89"], "00e8ffcc-fe39-4bc9-885f-ef168056eee2": ["c760d27c-4114-4b8a-bad2-a74928737c89"], "e04a96a8-ab3c-4eb8-9624-a42f77fd5649": ["44b19e29-7292-48d5-9e7e-2f3ce18e0aff"], "4790a2d7-5122-4957-aee8-e63dfe64e657": ["44b19e29-7292-48d5-9e7e-2f3ce18e0aff"], "89ff3b72-1656-4a2a-9060-0b9efcacc789": ["6becc746-3470-4d3c-887f-c790e7879246"], "fdfe32e9-0ad4-4473-8f1c-de3bb300cf06": ["6becc746-3470-4d3c-887f-c790e7879246"], "a85c3038-e2d5-40ab-8a5e-79e8d2290f9b": ["80330536-13b5-42c8-8df9-c7089f7e8dfa"], "897e0d07-ac1f-4704-9a62-87cae701ddb1": ["80330536-13b5-42c8-8df9-c7089f7e8dfa"], "c34b3d26-7616-4e85-8f3c-d6fb6b723d31": ["db637895-faed-458b-a57a-dd8aac7f4361"], "56ff98f8-5556-40cd-a4ff-33b2fed895f8": ["db637895-faed-458b-a57a-dd8aac7f4361"], "099ece18-61c7-441d-8c43-593e629bf14d": ["c9e5f39d-95c5-4c37-a847-edf222e633ef"], "d8e4c0dd-0b61-4560-a270-bc0ee55b3c6e": ["c9e5f39d-95c5-4c37-a847-edf222e633ef"], "9c3d900e-e5af-454d-8351-364302cc5cf4": ["4fe973ce-d7ff-4d20-a38e-6b0ad1b74486"], "2dced099-2d27-445f-9475-dd8e02bcc6d9": ["4fe973ce-d7ff-4d20-a38e-6b0ad1b74486"], "b70bfc26-f823-4e0d-a5c0-c4a88a513112": ["9d342ad1-4af1-47b2-b9f1-df606efe6f76"], "f6365c26-2694-4e16-b896-dbedc2328aa2": ["9d342ad1-4af1-47b2-b9f1-df606efe6f76"], "8f314652-7216-42a9-9ef8-690aad699494": ["c28c5c0e-1473-40f0-9340-9070cfe8184f"], "15064b9a-8fe0-407a-a3d8-ff347d09d876": ["c28c5c0e-1473-40f0-9340-9070cfe8184f"], "0d0b91ec-cf09-402b-be4f-9bb94db457a5": ["968412f3-c487-4221-a736-b343e7781f42"], "6ebaf4f9-a653-4d52-8a48-114ad595ed2b": ["968412f3-c487-4221-a736-b343e7781f42"], "3f0fb8b3-4609-42da-aa52-a52af5955c8a": ["94daa8be-a247-4615-b178-a161d3f624c5"], "71b7ee6b-22b2-4a56-b446-9644161d8776": ["94daa8be-a247-4615-b178-a161d3f624c5"], "e78adad9-a4c4-4055-83fc-f68692ff1e75": ["d27ff5c8-6730-4e0a-8773-0ed3d23aa8ce"], "69ebc4cb-2357-45d1-936e-ea1ab1426633": ["d27ff5c8-6730-4e0a-8773-0ed3d23aa8ce"], "4a0f0206-0ab9-4eb8-b7f9-9fa402cc240b": ["7bf72139-df7e-4724-84a9-44c69afef35f"], "db043272-230c-407d-8737-a65fbc206373": ["7bf72139-df7e-4724-84a9-44c69afef35f"], "26348364-bb0b-4dd5-97a9-9bb37af330aa": ["91c4d0d7-12a7-4877-90cb-514197a22613"], "c2489dd8-37b8-474d-b95c-7c114c20eae4": ["91c4d0d7-12a7-4877-90cb-514197a22613"], "871e0c37-3041-49c0-9e51-75b65b1aa2de": ["900b89d1-f85d-4774-8695-296543731d89"], "b749e5b4-21a8-4fe3-8181-fb7167f87b99": ["900b89d1-f85d-4774-8695-296543731d89"], "f062c672-0553-42d3-a5bb-8c9812e30c7e": ["294d76fb-50c2-4629-8adb-fd19c5c80db7"], "f1952ae0-d722-4c63-9fdb-73de31e97eeb": ["294d76fb-50c2-4629-8adb-fd19c5c80db7"], "918776ff-cc43-41eb-ad98-8a3e87cbf312": ["c2c245b0-4594-4acd-a455-7d5d3a4d2a2c"], "423a7d85-973e-45e9-bdd1-615cc8f8a527": ["c2c245b0-4594-4acd-a455-7d5d3a4d2a2c"], "7956fa90-c8ff-45a1-84cb-da7c59469dcf": ["c2dba223-e997-428b-a70e-6adf0ba830a3"], "ca9f7eae-27fa-463d-9644-29c6b28d80ea": ["c2dba223-e997-428b-a70e-6adf0ba830a3"], "27a6461d-03c6-4e49-a490-d6fbd75e9e6c": ["fbdef364-2c62-4bc9-a78c-6e9aed932885"], "767d3388-b4e6-43a2-a411-4dcd10e68ad9": ["fbdef364-2c62-4bc9-a78c-6e9aed932885"], "df5b2b1c-db2f-4496-925f-66aca9ad5553": ["83607e9a-2775-4ca8-b88e-56dcda4fc158"], "665e3018-12b6-4d96-8253-a2afb792b06f": ["83607e9a-2775-4ca8-b88e-56dcda4fc158"], "0c8a5114-0341-4bbc-8f33-54689287ca3a": ["c8434f58-edf8-429d-8a1c-2cccfe4f3c6a"], "de98497b-3373-4b60-9c11-3fa9ef04cf61": ["c8434f58-edf8-429d-8a1c-2cccfe4f3c6a"], "a9a22024-09fd-4d3d-9b09-56e357a3c4c0": ["25ccbcc8-bdb4-42b2-a128-463e487eb729"], "35eb45fe-e94b-475f-9e66-1f50a4e40646": ["25ccbcc8-bdb4-42b2-a128-463e487eb729"], "cc3c2027-a5ee-40a8-8bf4-4c1f8566cca2": ["f8760410-5191-4923-a44e-d34540e2ef7d"], "a45e9977-9baa-48d8-a0c7-5fd38e1c1975": ["f8760410-5191-4923-a44e-d34540e2ef7d"], "30fc6ec3-4db6-40ba-b520-0eed18368574": ["a4fd9402-84e9-4914-8f89-fe5800a136eb"], "9624b0c3-77c5-486a-b269-51f571b50e16": ["a4fd9402-84e9-4914-8f89-fe5800a136eb"], "4ddf3df6-ffb8-46e8-8276-00bc7bfe7a86": ["419ee15d-f7fc-4ab1-a601-eb3cd026277c"], "4abf8425-a571-4f31-8474-0a3a8f4da739": ["419ee15d-f7fc-4ab1-a601-eb3cd026277c"], "3ed3c522-c921-4c4f-8a87-4da8a0d9e078": ["7565dfb8-0769-4342-abd6-e4b17568e9e3"], "683bb531-c10f-49b4-ae85-e8d0f19ed716": ["7565dfb8-0769-4342-abd6-e4b17568e9e3"], "dda12ecd-9ba7-49ec-a250-b302e67b8169": ["8bdc6ba4-04b3-4c4f-847d-b3299103e032"], "a67964d7-f968-448a-8708-e6c24edcc0f1": ["8bdc6ba4-04b3-4c4f-847d-b3299103e032"], "50037143-12c9-4f28-8580-215ebe3f6cda": ["546b30ee-04e5-4568-9733-b2622e822fdb"], "cedacc53-0188-4ff8-ae41-bd0a800176dd": ["546b30ee-04e5-4568-9733-b2622e822fdb"], "c7aca584-3fb8-44b4-a0c7-b2b1a9298b63": ["e5aad670-0386-4a33-8514-ac4a8c1bbe28"], "eb268404-4932-446f-b02f-a2b4f442357e": ["e5aad670-0386-4a33-8514-ac4a8c1bbe28"], "5751c8e8-5f19-4e4d-826c-4f8505c7e32e": ["1d8e06e2-1854-4fb7-8e01-3f9653d2c302"], "1092ec67-46f3-4c2a-a621-d177a8d7ec8b": ["1d8e06e2-1854-4fb7-8e01-3f9653d2c302"], "251f0b12-a490-4ffc-8508-0de32d33f282": ["2545b10c-8376-4dfb-8ebf-437eddaadcae"], "8aa41edc-f0a8-4180-a704-a0e8378fd94b": ["2545b10c-8376-4dfb-8ebf-437eddaadcae"], "7815bc3b-142d-46ad-9b0b-7b714507e862": ["5f1b9574-0685-4d48-aad2-01825e4ec034"], "6bfb1f5f-4771-4fe9-8061-2e73251e377a": ["5f1b9574-0685-4d48-aad2-01825e4ec034"], "0f5beca9-59a0-4df3-8bb4-77bab510e9f6": ["60575f88-2308-4913-96e3-854fedd83d7e"], "b97f0f89-0d85-4a06-82a6-276d5578df48": ["60575f88-2308-4913-96e3-854fedd83d7e"], "cdef5a47-03d8-4ab0-b027-ed3408b60235": ["06183f93-8a1b-4a74-afe3-c7ee6d5cf1d6"], "bdc2f37f-3642-4e0b-bee8-c1f4d7748841": ["06183f93-8a1b-4a74-afe3-c7ee6d5cf1d6"], "e6856da8-3547-49b7-993f-3ad90990d089": ["66389d31-f140-4da9-a525-a8f5137a80c1"], "f6c482a0-36c8-40d9-84e2-3a2fe08077c0": ["66389d31-f140-4da9-a525-a8f5137a80c1"], "f59b8844-3d2e-45eb-84f9-90b2d7cdd59c": ["ddfb2071-9566-4e4e-8aad-bd495576a9bd"], "76074666-dabc-427c-bd67-1b089233b5d2": ["ddfb2071-9566-4e4e-8aad-bd495576a9bd"], "97ee776f-41f3-4213-b7d2-d041187cf180": ["b4fedfcf-8674-4c65-b410-bda8da9f8a76"], "df15f8a4-ff15-4fc2-a771-fce39e7def83": ["b4fedfcf-8674-4c65-b410-bda8da9f8a76"], "1a510cea-77fb-45b0-bdb9-e2b5acf05da1": ["d920cb93-a0bf-476a-b442-2c9f6d8c00df"], "aae9d666-236a-4c41-a7e6-63a222af9025": ["d920cb93-a0bf-476a-b442-2c9f6d8c00df"], "12335f24-e504-41fc-bd3d-b481a3a0b811": ["6021c705-dec5-4850-bb7c-d432a6f5221e"], "9ca0e9fd-3357-404b-9d01-feb8ec85d85d": ["6021c705-dec5-4850-bb7c-d432a6f5221e"], "8baa4708-9cc3-4990-8a05-d2fc1312e19f": ["149ef129-80c1-4bcb-a8ee-47a751fc5160"], "8baa380f-1b89-46c3-8aef-4d75761d55fa": ["149ef129-80c1-4bcb-a8ee-47a751fc5160"], "57db514d-fba8-49b1-829e-dcba33296885": ["e706091b-aa89-4706-9afe-ad0c02844f2e"], "9c465c7f-a6a5-44bd-b1c5-dfff38265207": ["e706091b-aa89-4706-9afe-ad0c02844f2e"], "c416884c-bd5f-4411-a3fe-358210463892": ["ac89ac07-9978-48f4-81d1-e2afef2a551a"], "d0aa463e-7a5c-4883-91c5-55e7064d7bb0": ["ac89ac07-9978-48f4-81d1-e2afef2a551a"], "2eb49a16-218e-4a04-b381-0e00ebc00311": ["191597f2-0deb-4f81-aac8-6aaddf5b04e2"], "c063bdef-65e6-44c4-bc86-df9c999dfb45": ["191597f2-0deb-4f81-aac8-6aaddf5b04e2"], "41e9d33d-1d9a-43a9-a900-335ee78e78c5": ["53756370-147d-4967-a184-ec91e6333273"], "62ef5a04-0ed8-4933-b092-8f3187d04b1f": ["53756370-147d-4967-a184-ec91e6333273"], "11f7e747-2d86-4864-a3a1-233fa4f81ada": ["56d7e3bb-f5df-4b74-84d9-32af3697b74f"], "b4f1de89-5d5a-4643-ab04-4ce8831b9fbb": ["56d7e3bb-f5df-4b74-84d9-32af3697b74f"], "931f7fff-5957-430b-8c8a-a971cb5931ee": ["040c79f3-405e-438e-8268-0eb0fbaed727"], "8e1220a1-22e2-4bfc-aed6-c74d0faabefe": ["040c79f3-405e-438e-8268-0eb0fbaed727"], "7a16a42b-2213-438b-9dbd-631706f26091": ["4caf2969-19f6-4b7b-a02a-40636663a69e"], "a6dac6cb-0373-4b04-bcaf-ccdb35f68713": ["4caf2969-19f6-4b7b-a02a-40636663a69e"], "6c06cb85-8b6e-4eac-bebb-19a972625b2c": ["c1bf72bd-5686-4ea8-a11b-3be77a70391a"], "20627d61-7955-42b0-98e4-dc4976703989": ["c1bf72bd-5686-4ea8-a11b-3be77a70391a"], "ff03c09f-2e5c-4938-bbd9-2dcbe9fe7a54": ["6facb05a-4b89-4f0d-b860-74fb6aca4021"], "ff74c77c-fb41-4e2f-87fd-d355d92747a0": ["6facb05a-4b89-4f0d-b860-74fb6aca4021"], "8ae5430d-83a7-4ea6-9cf6-93eeefbe0178": ["7f3fa0f3-81cf-42d5-a70f-531293dddd89"], "25a5448a-a7db-4fe6-9e67-6baf42b5a471": ["7f3fa0f3-81cf-42d5-a70f-531293dddd89"], "819aad8d-950a-416f-8af1-e53d7c9f7c4e": ["fb5e18e4-26db-4da5-842a-4d8640e9333b"], "905bb57d-21cf-4b98-9aaf-f33be64057db": ["fb5e18e4-26db-4da5-842a-4d8640e9333b"], "9a4d6e48-79bb-421c-8b2b-b68df3d78394": ["176cf018-5336-4c38-94de-c0ce0c2e3113"], "e99f3b92-8019-470b-8623-d43678edfca7": ["176cf018-5336-4c38-94de-c0ce0c2e3113"], "7af6841c-87a4-4b54-9acb-ee5829c3af3d": ["8fe5bbf8-6b00-4d71-a9a6-8a2de19e1f1e"], "fdac8d7f-9ae2-4027-90d1-04e21b9c12f7": ["8fe5bbf8-6b00-4d71-a9a6-8a2de19e1f1e"], "3dde1d5b-528f-4015-8b54-172b9e98a4ea": ["83f77e42-56a7-4d3d-aa22-773a58e0d132"], "5a396c28-f165-4817-b909-72b75c4c69ea": ["83f77e42-56a7-4d3d-aa22-773a58e0d132"], "33c86a55-7303-434f-91e1-37f065002e6f": ["a8518c41-e10f-4b25-a625-2a85ea70c97f"], "126c7b59-b562-4119-897e-5ab3cc47d525": ["a8518c41-e10f-4b25-a625-2a85ea70c97f"], "7491ef69-db12-45a7-8b34-f78d0799b695": ["ae85f326-3bd5-4071-b294-dc507a676dac"], "c41dfdea-7d1f-458d-8984-3ebb962e2ca5": ["ae85f326-3bd5-4071-b294-dc507a676dac"], "70c2f543-d999-44e0-b6be-ddd15f38d935": ["81656deb-f12a-403c-8c41-bb6607576200"], "3bc6e513-6a68-494c-8312-30658822beea": ["81656deb-f12a-403c-8c41-bb6607576200"], "1078da0b-26aa-41d4-a891-20392228a9ec": ["1edf66d4-0cea-496c-975c-dfe1ed7502c6"], "538b3248-741d-4a32-8218-6f6f47b78bcc": ["1edf66d4-0cea-496c-975c-dfe1ed7502c6"], "10239924-f0b0-4f18-99ae-2709274611f3": ["6ea30b8a-e0ba-4644-82e9-0a55d5720faa"], "3aef6273-e8ee-4073-a11d-219e47a11c24": ["6ea30b8a-e0ba-4644-82e9-0a55d5720faa"], "11afdf73-10e3-4b9e-a040-35268fbf0cf8": ["c6495d56-6b5b-4988-b968-4ffb32a59958"], "e600620d-84d2-4442-a9fa-ec46df24902f": ["c6495d56-6b5b-4988-b968-4ffb32a59958"], "6972c0e7-c983-4ed9-9cdd-6c4a438ef1dd": ["1b746a6a-d32e-4e3d-a2a8-609317218f64"], "59fb4dea-826a-4da8-b6b7-c7c5579b6d83": ["1b746a6a-d32e-4e3d-a2a8-609317218f64"], "72ea7554-132f-44ee-b07c-c5f4c2666933": ["5cc73ae3-1809-4e25-adcf-ea985f0a0708"], "7280b75b-a521-478a-840f-85e6fcace027": ["5cc73ae3-1809-4e25-adcf-ea985f0a0708"], "b4da0aad-3231-4265-828b-a22771b2a284": ["3d6c7af4-981a-4958-b975-408ec7e446c6"], "9feea726-6f00-4056-8491-cafd27230cac": ["3d6c7af4-981a-4958-b975-408ec7e446c6"], "892baa22-97a1-4bf5-a916-fb12baf274a8": ["2d9cfa39-0d0a-45a8-8154-9c7bac085dae"], "8a2596c4-69ee-4c04-9b06-fb8c5772e177": ["2d9cfa39-0d0a-45a8-8154-9c7bac085dae"], "db927832-4362-45b9-9496-412c3c376f41": ["58302961-14c6-4685-a91d-d7f14fef522b"], "308a5057-8520-4d72-93ac-4a75e6817d7d": ["58302961-14c6-4685-a91d-d7f14fef522b"], "428f5dcb-056e-4107-997e-cca1c86d8c9b": ["83e07b9b-79ce-4350-889a-58bb65b9e53b"], "316dcacd-a350-41be-91ef-59c0426b4e94": ["83e07b9b-79ce-4350-889a-58bb65b9e53b"], "586d4c7d-e311-416b-8e4d-ccc961618f9c": ["a32b445f-7be5-404f-b6ec-6a2662f49705"], "72b8d7f6-7583-49f3-8a8b-2c4c00fae825": ["a32b445f-7be5-404f-b6ec-6a2662f49705"], "7e8c97fd-8591-4ab1-82ef-9ab93e90a64a": ["cb2fd25d-bc26-4baa-a8b9-2aae4c42e572"], "23c66141-82c6-4949-b534-16dcb81b0365": ["cb2fd25d-bc26-4baa-a8b9-2aae4c42e572"], "2e58e12f-787c-4b65-889a-9a8f500373d5": ["3edf4dc2-c639-4b40-9d68-3291c00a0917"], "cbd85adf-da13-4364-9ff9-00adce097d52": ["3edf4dc2-c639-4b40-9d68-3291c00a0917"], "ed988c00-2592-4e83-a79b-bc737fc6d12d": ["18c84214-cd2a-4cd2-8c6f-0a9ad7400141"], "88acb9d2-0634-4dbf-bc36-1dafa5fcd08e": ["18c84214-cd2a-4cd2-8c6f-0a9ad7400141"], "77ab2025-8ad0-46c8-ab21-f84c2a5e58a9": ["120ba925-fefe-428c-8ca4-3b799e86f6bb"], "9ba1e243-9cc4-4613-8117-c5b12b7c6311": ["120ba925-fefe-428c-8ca4-3b799e86f6bb"], "35d21e4a-c638-4488-93e7-559622e1a989": ["28f00ab2-161f-4702-800f-c385de1c6a47"], "5404279e-b96d-4a2d-aa10-407a5955028a": ["28f00ab2-161f-4702-800f-c385de1c6a47"], "5e1c975f-524f-43ab-9247-670c0095ea58": ["cdcfef42-d57a-4e8d-a757-ee5dd8c60ed5"], "0b699a03-0ae5-4a1e-a043-482d216a248b": ["cdcfef42-d57a-4e8d-a757-ee5dd8c60ed5"], "cce08560-3e4a-4ef4-bf38-083ac98af4e4": ["378c5ac2-54af-4178-a6aa-a03286b229cc"], "11281b7d-5bee-4574-a2ea-35ff6c1dccfb": ["378c5ac2-54af-4178-a6aa-a03286b229cc"], "904f5d3f-83c5-4d1e-a746-306350a3de83": ["59d27793-c918-4d10-a7a7-c9f421ddbf25"], "fe8af4c2-635b-48cf-ae64-6ca2ab2fe676": ["59d27793-c918-4d10-a7a7-c9f421ddbf25"], "ab88c19e-02d2-46c1-81f9-b95252780636": ["509e72aa-96cd-40c8-86a6-8a013b7cedc4"], "502d1d3d-89eb-419f-a2ad-04f46a444683": ["509e72aa-96cd-40c8-86a6-8a013b7cedc4"], "2a63814c-4e3e-4ee5-a8ea-c92f49fa0e29": ["209758fa-9ac6-4634-9796-492ff1f6e7ff"], "ef0d2360-7996-4902-a329-99deb860f42b": ["209758fa-9ac6-4634-9796-492ff1f6e7ff"], "6060a800-71c8-43b4-8e41-400e12a7fd2a": ["a7e6833b-d179-4deb-abc0-1472a691a7a3"], "aaca0b1e-18f1-417b-81cf-ea015cb45d4e": ["a7e6833b-d179-4deb-abc0-1472a691a7a3"], "f2723858-4f70-4945-8101-31c5c2597450": ["bcf72fe9-9032-41aa-924f-6489518f535e"], "fb975e03-cd3e-48b1-ab30-2842816f856e": ["bcf72fe9-9032-41aa-924f-6489518f535e"], "aea525af-8b81-45e6-991e-96a3b254951b": ["50ed275f-ae2c-4764-a08e-4b4036e8d256"], "691da3ce-d629-434c-a73c-94bbaf4fe5d6": ["50ed275f-ae2c-4764-a08e-4b4036e8d256"], "9c920f3c-dd3e-49d0-95f2-63a30ffc285f": ["842f517a-76a2-445f-9800-5406a9fe4dff"], "e743d4d5-eb5d-47c2-94a0-01e25d979081": ["842f517a-76a2-445f-9800-5406a9fe4dff"], "d1ee2d5e-f15c-4af7-bca7-bc91819a59f4": ["e1c103e1-300c-486d-9768-e7fcc2b9f77b"], "a656c666-b029-4ee9-88a6-a5781ca26016": ["e1c103e1-300c-486d-9768-e7fcc2b9f77b"], "68379bc5-db1d-45a5-8c7e-64fa34666f7d": ["e270a88e-8798-4bca-944c-c8510c0344fb"], "e6a67d5a-9396-4f6f-8abd-5548130e0529": ["e270a88e-8798-4bca-944c-c8510c0344fb"], "cb7324a6-f3a7-471b-b82d-10a2eb327647": ["193e179b-fd79-49ad-9634-d6072283147d"], "7128a8fe-8aed-4384-b313-c3e22199d6b1": ["193e179b-fd79-49ad-9634-d6072283147d"], "59368ac5-535b-4fed-a6d3-616baea70794": ["36a605cf-00d1-4fe9-bc33-13be0bba5f75"], "26183633-b9a9-4370-be8e-e00d305f058b": ["36a605cf-00d1-4fe9-bc33-13be0bba5f75"], "58aceb96-f0d5-437a-9e20-4714d0156406": ["18e9c3c9-b1a9-43a1-ba89-ead9eba0ee78"], "9e4ab15c-d391-462f-aa7c-35f11dc50313": ["18e9c3c9-b1a9-43a1-ba89-ead9eba0ee78"], "ac912cf0-e55a-42a4-bc82-1e4a479f4dbd": ["a90410f7-e0f4-4aac-a4ee-ef239859712a"], "9495238e-3a18-490d-ba75-6b4ea4106c0c": ["a90410f7-e0f4-4aac-a4ee-ef239859712a"], "a166c47c-8226-4dcd-b51c-247d57c2a9a6": ["e075c35f-0971-482f-913b-b3716ac37eb3"], "d6e516a6-312d-4566-9e9e-917501f85dac": ["e075c35f-0971-482f-913b-b3716ac37eb3"]}, "corpus": {"5c831b00-c7dc-4290-adcf-0bd37075ba8e": "A Survey of Useful LLM Evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 Introduction\n\n1.1 Artificial Intelligence and Large Language Model\n1.2 Why Evaluating LLMs is Important\n1.3 The Roadmap of Useful LLMs\n1.4 Study Overview\n\n\n\n2 Core Ability Evaluation\n\n\n2.1 Reasoning\n\n2.1.1 Logical Reasoning\n2.1.2 Mathematical Reasoning\n2.1.3 Commonsense Reasoning\n2.1.4 Multi-hop Reasoning\n2.1.5 Structured Data Reasoning\n\n\n\n2.2 Societal Impact\n\n\n2.2.1 Safety\n\nContent Safety\nSecurity\nEthical Consideration\n\n\n\n2.2.2 Truthfulness\n\nHallucination\nBias Mitigation\n\n\n\n\n\n2.3 Domain Knowledge\n\n2.3.1 Finance\n2.3.2 Legislation\n2.3.3 Psychology\n2.3.4 Medicine\n2.3.5 Education\n\n\n\n\n\n3 Agent Evaluation\n\n3.1 Planning", "814e7de6-ec3d-4d86-a282-1f81f7b9aa80": "3 Agent Evaluation\n\n3.1 Planning\n\n3.2 Application Scenarios\n\n\n3.2.1 Web Grounding\n\nSearch Engine\nOnlineshopping\n\n\n3.2.2 Code Generation\n3.2.3 Database Queries\n3.2.4 API Calls\n3.2.5 Tool Creation\n3.2.6 Robotic Navigation\n3.2.7 Robotic Manipulation\n\n\n3.3 Benchmark\n\n\n\n4 Future Directions\n\n4.1 Dynamic Evaluation\n4.2 LLMs as Evaluators\n4.3 Root Cause Analysis\n4.4 Fine-grained LLM Agent Evaluation\n4.5 Robot Benchmark Development\n\n\n5 Conclusion\n\n\n\n\n\nA Survey of Useful LLM Evaluation", "647136cc-6ace-42d3-b060-98bd60e9657d": "A Survey of Useful LLM Evaluation\n\n\n\nJi-Lun Peng\u2217\u2003Sijia Cheng\u2217\u2003Egil Diau\u2217\u2003Yung-Yu Shih\u2217\u2003\nPo-Heng Chen\u2217\u2003Yen-Ting Lin\u2003Yun-Nung Chen\nNational Taiwan University, Taipei, Taiwan \n{b09207002, r11922184, r12922a03, r12944007, r11922044}@ntu.edu.tw\n{ytl, y.v.chen}ieee.org", "74183e6f-14d7-4767-8db3-12e052c233cb": "Abstract", "34aeeba9-369a-421c-a3c9-de9bf6b97fda": "LLMs have gotten attention across various research domains due to their exceptional performance on a wide range of complex tasks. Therefore, refined methods to evaluate the capabilities of LLMs are needed to determine the tasks and responsibility they should undertake. Our study mainly discussed how LLMs, as useful tools, should be effectively assessed. We proposed the two-stage framework: from \u201ccore ability\u201d to \u201cagent\u201d, clearly explaining how LLMs can be applied based on their specific capabilities, along with the evaluation methods in each stage. Core ability refers to the capabilities that LLMs need in order to generate high-quality natural language texts. After confirming LLMs possess", "f0077176-e258-4cec-91b8-183bbd5a910d": "language texts. After confirming LLMs possess core ability, they can solve real-world and complex tasks as agent. In the \"core ability\" stage, we discussed the reasoning ability, societal impact, and domain knowledge of LLMs. In the \u201cagent\u201d stage, we demonstrated embodied action, planning, and tool learning of LLMs agent applications. Finally, we examined the challenges currently confronting the evaluation methods for LLMs, as well as the directions for future development.111https://github.com/MiuLab/EvalLLM-Survey", "f0b36adb-edf4-42fc-a03d-7e7012f8b897": "**footnotetext: Equal contribution.", "639ffa81-9a2a-4d3e-9f53-a4d767b1926e": "A Survey of Useful LLM Evaluation\n\n\n\n\n\n\nJi-Lun Peng\u2217\u2003Sijia Cheng\u2217\u2003Egil Diau\u2217\u2003Yung-Yu Shih\u2217\n\nPo-Heng Chen\u2217\u2003Yen-Ting Lin\u2003Yun-Nung Chen\n\nNational Taiwan University, Taipei, Taiwan\n\n{b09207002, r11922184, r12922a03, r12944007, r11922044}@ntu.edu.tw\n\n{ytl, y.v.chen}ieee.org\n\n\n\n\n\n\n\n1 Introduction\n\nFigure 1: The two-stage framework of our LLMs evaluation.\n\n\n\n1.1 Artificial Intelligence and Large Language Model", "74e8536f-4024-4b8d-ba1e-4a9712cd04ea": "Artificial intelligence (AI) simulates human behavior to complete multiple tasks needing human intelligence. The first models of AI tried to simulate the function of a single neuron with feedforward, simple input-output functions Muthukrishnan et\u00a0al. (2020). As time has progressed, a variety of machine learning (ML) and deep learning (DL) models have been developed. They are not only capable of identifying patterns from vast amounts of data, but they can also make predictions, and even handle unstructured data such as text, images, and audio. Recently, the Transformer architecture Vaswani et\u00a0al. (2017) was proposed, allowing word embeddings to be context-dependent, and model training to be", "39f27a87-69fb-4cc0-85bd-43fe8b1e0064": "to be context-dependent, and model training to be scaled up Min et\u00a0al. (2023). Therefore, researchers gradually increased the parameters in pre-trained language models trying to reach better performance. Using the Generative Pre-trained Transformer (GPT) series as an illustration, the progression in complexity and models\u2019 capability is marked by a significant increase in the number of parameters: GPT-1 Radford et\u00a0al. (2018) has 117 million parameters, GPT-2 Radford et\u00a0al. (2019) expands this to 1.5 billion parameters, and GPT-3 Mann et\u00a0al. (2020) further escalates to 175 billion parameters. Moreover, GPT-4 released by OpenAI with much larger model size could accept image and text inputs and", "9fcaddf2-21bd-4a47-a572-8cb168eafb96": "model size could accept image and text inputs and produce text outputs, and exhibited human-level performance on various professional and academic benchmarks Achiam et\u00a0al. (2023). The models mentioned above, due to their tremendous size, are referred to as LLMs. They have gotten attention across various research domains due to their exceptional performance on a wide range of complex tasks.", "0365dced-96d6-4bb8-9c53-7bc56febc39e": "1.2 Why Evaluating LLMs is Important", "f38c5509-c6fa-4ce6-a1eb-50ab573e7c21": "The early works testing model\u2019s intelligence referred to as the Turing Test, raising the question of whether machines could imitate human intelligence and made people fail to differentiate Pinar\u00a0Saygin et\u00a0al. (2000). Evaluating AI is vital as it helps us gauge the real-world capabilities and limitations of AI systems. As AI technologies improve, particularly in areas like software testing and structural engineering, they can sometimes perform better than humans. However, we need clear benchmarks to make sure these technologies are both reliable and effective Salehi and Burgue\u00f1o (2018). With the rapid evolution of LLMs, refined methods to evaluate the capabilities of LLMs is needed to", "5d300c1c-0d8c-4b47-ae66-3f1f0e4eb982": "to evaluate the capabilities of LLMs is needed to determine the tasks and responsibility they should undertake. Because LLMs exhibit a broad spectrum of capabilities beyond the specific task they are trained for: predicting the next words of human-written texts Nolfi (2023), such as formal linguistic competence Mahowald et\u00a0al. (2023), factual knowledge Petroni et\u00a0al. (2019), and even theory of mind skills Kosinski (2023), we should design benchmarks or evaluation methods specific to each task or domain. In current benchmarks, the comprehensive abilities of LLMs are automatically evaluated through tasks spanning multiple domains such as HELM Liang et\u00a0al. (2022) and BIG-Bench Srivastava", "429537a1-271c-4227-ab37-7ef9889bc1c3": "HELM Liang et\u00a0al. (2022) and BIG-Bench Srivastava et\u00a0al. (2022), or by generating human feedback automatically like AlpacaFarm Dubois et\u00a0al. (2024) and MT-bench Zheng et\u00a0al. (2024). However, when LLMs are required to perform specific tasks, the existence of evaluation methods tailored to those tasks becomes potential. This allows for a comparison of different models\u2019 capabilities under identical tasks to select the best performer. In this study, we categorizes LLMs\u2019 distinct abilities, systematically reviews existing evaluation methods under each category, and discusses how LLMs, as \"useful\" tools, should be effectively assessed.", "b909bb09-5283-4e7c-968f-932598a85b0f": "1.3 The Roadmap of Useful LLMs\n\nTo determine whether LLMs are capable to become useful tools, we should split LLMs\u2019 capabilities into \"core ability\" and \"agent\", and discuss them respectively. Core ability refers to the capabilities that LLMs need in order to generate high-quality natural language texts, which are the foundation of performing complex behaviors.", "ce9b6259-3b28-42a4-aafb-88e9b092902e": "Firstly, LLMs must possess the capability for reasoning, as during interactions with humans, they are required to deduce arguments step by step to engage in effective discussion. Furthermore, the societal impact of LLMs needs significant attention, for LLMs must be perceived as safe and trustworthy for humans to believe in and actively use them. Lastly, LLMs should have knowledge across various domains, and they can assist humans in solving problems occurring withing diverse fields.", "6beee3f6-9bcd-480d-b70b-40f1cdcbc4fc": "Upon confirming that LLMs possess these core abilities, we can utilize LLMs to perform complex behaviors to deal with real-world problems, which we define as agent. For instance, LLMs agents can perform planning, generating an explicit deliberation process that chooses and organizes actions by anticipating their expected outcomes Ghallab et\u00a0al. (2004). Then, LLMs agent can solve tasks in various scenarios such as using tools, creating tools, navigating embodied robots, and so on.", "6177fa25-8d22-4742-b3e4-93327506e9e3": "Even though LLMs can display the aforementioned capabilities, comprehensive evaluation methods are necessary to ensure that LLMs achieve as satisfactory level of performance in executing each task. Existing papers on LLM evaluation methods, including Guo et\u00a0al. (2023) and Chang et\u00a0al. (2023), provide a thorough review of evaluation approaches for various aspects of LLMs, yet no study has offered a phased framework to explore the usability of LLMs. Hence, this paper proposes a two-stage framework to examine whether LLMs are sufficiently useful tools (Figure\u00a01).\n\n\n\n\n1.4 Study Overview", "35b991f6-2deb-4d10-807f-685a6bf9e905": "1.4 Study Overview\n\nIn this study, we first introduce the evaluation methods of the core ability of LLMs (Figure\u00a02), including Reasoning with 5 subsections, Societal Impact with 2 subsections, and Domain Knowledge with 5 subsections. Then, for LLMs agent (Figure\u00a03), we introduce evaluation methods of the agent application of LLMs, including Planning, Application Scenarios with 7 subsections, and Benchmark. In these subsections, we present applications of LLMs, evaluation methods, and datasets. Lastly, we give our point of view on the usability of LLMs and suggest future directions and challenges in evaluating LLMs.\n\n\nThe contributions of this paper are as follows:\n\n\n(1)", "2cd13dca-1b4e-4488-8bd4-4e81640b7fa2": "(1)\n\nWe provide a two-stage framework: from core ability to agent to examine whether LLMs are sufficiently useful tools.\n\n\n\n(2)\n\nIn each section, we elucidate the applications of LLMs pertaining to the specific capability, along with the evaluation methods. Furthermore, we provide an analysis of the current performance levels of LLMs in these domains.\n\n\n\n(3)\n\nWe examine the challenges currently confronting the evaluation methods for LLMs, as well as the directions for future development.\n\n\n\n\n\n\n\n\n2 Core Ability Evaluation\n\n\n{forest}", "b16e4bde-05fd-488c-9baf-76d2a42bad73": "for tree=\ngrow=east,\nreversed=true,\nanchor=base west,\nparent anchor=east,\nchild anchor=west,\nbase=left,\nfont=,\nrectangle,\ndraw,\nrounded corners,align=left,\ninner xsep=4pt,\ninner ysep=1pt,\n,\nwhere level=1font=,fill=pink!50,\nwhere level=2font=,fill=green!10,\nwhere level=3font=,fill=gray!20,\n[Core Ability Evaluation\n(Sec.\u00a02),fill=yellow!20,font=[Reasoning\n(Sec.\u00a02.1)\n[Logical Reasoning\n[Weston et\u00a0al. (2015),  Bhagavatula et\u00a0al. (2019)]\n]\n[Mathematical Reasoning\n[Cobbe et\u00a0al. (2021), Hendrycks et\u00a0al. (2021)]\n]\n[Commonsense Reasoning\n[Talmor et\u00a0al. (2018), Mihaylov et\u00a0al. (2018)]\n]\n[Multi-hop Reasoning\n[Geva et\u00a0al. (2021), Yang et\u00a0al. (2018)]\n]\n[Structured Data Reasoning", "9cfcdce5-c86a-4da5-89f2-0dd5b91ffeca": "]\n[Structured Data Reasoning\n[Chen et\u00a0al. (2020), Zhang et\u00a0al. (2018)]\n]\n]\n[Societal Impact\n(Sec.\u00a02.2)\n[Safety[Lin et\u00a0al. (2023), Kim et\u00a0al. (2024b), Yuan et\u00a0al. (2024), Scherrer et\u00a0al. (2023)]\n]\n[Truthfulness\n[Jiang et\u00a0al. (2024), Zhang et\u00a0al. (2024b), Hort et\u00a0al. (2021), Zhang et\u00a0al. (2023)]\n]\n]\n[Domain Knowledge\n(Sec.\u00a02.3)\n[Finance\n[Wu et\u00a0al. (2023), Xie et\u00a0al. (2023), Li et\u00a0al. (2023b)]\n]\n[Legislation\n[Blair-Stanek et\u00a0al. (2023), Engel and Mcadams (2024), Liga and Robaldo (2023), Deroy et\u00a0al. (2023)\n]\n]\n[Psychology\n[Lu et\u00a0al. (2024), Demszky et\u00a0al. (2023), Demszky et\u00a0al. (2023)\n]\n]\n[Medicine", "7ba861fd-f650-41e3-a4e5-9c4810213bbc": "]\n]\n[Medicine\n[Agrawal et\u00a0al. (2022), Sharma and Thakur (2023), Benoit (2023), Kumar (2023), Thirunavukarasu et\u00a0al. (2023)\n]\n]\n[Education\n[Abdelghani et\u00a0al. (2023), Jia et\u00a0al. (2021), Menick et\u00a0al. (2022), Dijkstra et\u00a0al. (2022), Kasneci et\u00a0al. (2023)\n]\n]\n]\n]", "00548a73-db95-405c-86ce-13b4f68c619e": "Figure 2: The overview of core ability evaluation.\n\n\nThe evaluation of core abilities in LLMs thoroughly examines their linguistic capabilities across three essential dimensions: reasoning, societal impact, and domain-specific knowledge.\nThis essential evaluation emphasizes LLMs\u2019 proficiency in complex cognitive reasoning processes in Section\u00a02.1, their commitment to truthfulness and safety standards in Section\u00a02.2, and their adeptness in applying specialized knowledge across a wide range of domains in Section\u00a02.3.", "b1bd14f3-3e1d-4e81-9255-65a181a82919": "By confirming that LLMs possess these core abilities, we recognize the potential of these skills to evolve into more complex behaviors.\nThis development emphasizes the adaptability and scalability of LLMs as tools for advanced applications, indicating that the focus will be on enhancing these foundational abilities further in the future.\n\n\n\n2.1 Reasoning", "0236b69e-12f3-4be5-999e-22d8319b3d0f": "2.1 Reasoning\n\nProficiency in reasoning empowers both humans and machines to make well-founded decisions, derive logical conclusions, and adeptly tackle problems.\nRecent research\u00a0(Huang and Chang, 2023; Sun et\u00a0al., 2024) has increasingly emphasized the augmentation of reasoning capacities in LLMs, aiming to attain human-level or even surpass human-level reasoning prowess within specialized domains.\nIn this section, our attention is directed towards evaluating the various reasoning abilities of LLMs.\nThe reasoning task can be categorized into the following groups:\nlogical reasoning, mathematical reasoning, commonsense reasoning, multi-hop reasoning and structured data reasoning.", "cc4f140d-3853-4a26-b999-ece5f110186d": "2.1.1 Logical Reasoning\n\n\n\n\n\n\n\nType\n\n\n\n\nExample Source\n\n\n\n\nInput\n\n\n\n\nanswer\n\n\n\n\n\n\n\n\nInductive Reasoning\n\n\n\n\nbAbI-15\u00a0(Weston et\u00a0al., 2015)\n\n\n\n\nSheep are afraid of wolves.\nCats are afraid of dogs.\nMice are afraid of cats.\nGertrude is a sheep.\nWhat is Gertrude afraid of?\n\n\n\n\nwolves\n\n\n\n\n\n\nDeductive Reasoning\n\n\n\n\nbAbI-16\u00a0(Weston et\u00a0al., 2015)\n\n\n\n\nLily is a swan.\nLily is white.\nBernhard is green.\nGreg is a swan.\nWhat color is Greg?\n\n\n\n\nwhite\n\n\n\n\n\n\nAbductive Reasoning\n\n\n\n\n\u03b1\ud835\udefc\\alphaitalic_\u03b1-NLI\u00a0(Bhagavatula et\u00a0al., 2019)\n\n\n\n\nobs1: I walked into my math class.\nobs2: I ended up failing.\nhyp1: I saw the string by the door.\nhyp2: I didn\u2019t study for the test.\n\n\n\n\nhyp2", "0d53ef00-482d-4a57-bed9-39b8bcacce77": "hyp2\n\n\n\n\n\n\nTable 1: Examples for different types of logical reasoning.", "86bdf7fd-7c15-4070-8734-69515d8d6149": "Based on concepts from philosophy and logic, logical reasoning can further be divided to three different types:\n1) Inductive reasoning involves inferring general conclusions based on observed patterns or regularities in specific instances. bAbI-15\u00a0(Weston et\u00a0al., 2015) and EntailmentBank\u00a0(Dalvi et\u00a0al., 2021) are common benchmarks for inductive reasoing.\n2) Deductive reasoning is the process of deriving necessary conclusions based on known premises and logical rules. bAbI-16\u00a0(Weston et\u00a0al., 2015) is an common benchmark for testing deductive reasoning.", "9deb3bdc-9fad-4d63-aefc-d655a57e198f": "3) abductive reasoning is a form of reasoning where possible explanations or hypotheses are inferred based on given observations and known information. \u03b1\ud835\udefc\\alphaitalic_\u03b1-NLI, \u03b1\ud835\udefc\\alphaitalic_\u03b1-NLG\u00a0(Bhagavatula et\u00a0al., 2019) and AbductiveRules\u00a0(Young et\u00a0al., 2022) are several benchmarks for abductive reasoning.\nTable\u00a01 show several examples of each type of logical reasoning task.", "84ed9674-f703-4da1-a752-0db29c33183d": "Xu et\u00a0al. (2023a) is a comprehensive study on logical reasoning in several LLMs including text-davinci-003, ChatGPT and BARD. They found that BARD perform best generally among three models and ChatGPT performs worse in deductive and inductive settings. Besides, they also show that ChatGPT falls short in generation tasks since it is tailored for chatting. Han et\u00a0al. (2023) and Liu et\u00a0al. (2023) include GPT-4 in their evaluation and found that its performance qualitatively matches that of humans in some scenarios.\n\n\n\n\n2.1.2 Mathematical Reasoning", "65b89a97-d409-4c94-9e0b-443033d7dd36": "2.1.2 Mathematical Reasoning\n\nMathematical reasoning necessitates models to grasp and manipulate mathematical concepts across diverse scenarios.\nFor example, the problem may request model to perform arithmetic operations and manipulating abstract symbols to attain an accurate numerical outcome.\nNotable examples include GSM8K\u00a0(Cobbe et\u00a0al., 2021) and MATH\u00a0(Hendrycks et\u00a0al., 2021).", "ff3eaef0-3c2a-4f7e-a461-4e68463f7073": "Stolfo et\u00a0al. (2023) found that instruction-tuned LLM have a remarkable improvement in both sensitivity and robustness on mathematical problem compared to non-instruction-tuned models.\nYuan et\u00a0al. (2023) compare the arithemtic capability of 13 models on each operation types and found that GPT-4 is the only model that have excellent performance in every of them.\n\n\n\n\n2.1.3 Commonsense Reasoning", "7f90b1a4-3dce-4f0c-b32e-a216d940229e": "Commonsense reasoning entails the capacity to grasp and apply fundamental knowledge about the world.\nIt\u2019s essential for machines to reach a level of comprehension and interaction comparable to human cognition.\nMoreover, commonsense cognition is pivotal in various reasoning processes such as causal detection, spatial and temporal understanding, among others.\nTypically, commonsense reasoning tasks are structured as multiple-choice or true/false problem, which contain questions that require model to apply commonsense knowledge to answer.", "5dbbadf2-fb8c-439d-94ed-d7b9d5f2814e": "For instance, the problem may ask \"Where do you put your grapes just before checking out?\", and the model should select the correct answer, which is \"grocery cart.\"\nThe CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) consist questions with complex semantics that require prior knowledge to answer.\nSimilarly, OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018) contains elementary-level questions designed to assess understanding of basic scientific facts and their application in novel scenarios.", "140d27c8-e827-4165-9189-269d5b96f744": "Bang et\u00a0al. (2023) shows that ChatGPT has commonsense reasoning capability over several commonsence benchmark over general knowledge\u00a0(Talmor et\u00a0al., 2018) and physical concepts\u00a0(Bisk et\u00a0al., 2020; Wang et\u00a0al., 2018).\nBian et\u00a0al. (2024) shows that instruction tuning models have superior performance on several commonsense QA dataset including CommonsenseQA\u00a0(Talmor et\u00a0al., 2018) and OpenBookQA\u00a0(Mihaylov et\u00a0al., 2018), which illustrates that commonsense ability can be improved by with human alignment.\n\n\n\n\n2.1.4 Multi-hop Reasoning", "09752ebe-2625-47ef-914b-ab70f7b98b37": "The multi-hop reasoning tasks necessitate models to engage in sequential reasoning steps to derive answers.\nIt serves as a prominent assessment for LLMs, evaluating their capability to analyze questions and solve them through a step-by-step decomposition process akin to human-level ability.\nThe process can be viewed as an amalgamation of diverse reasoning ability, as each step may necessitate the application of one or more of the reasoning tasks discussed earlier.\nFor instance, the question might be, \u2019Was the director of \u2019Interstellar\u2019 born in Paris?\u2019\nIn this case, the models must first identify the director of the movie and then ascertain their birthplace.", "4c1b9613-577b-4e92-b88c-a51590e51069": "StrategyQA\u00a0(Geva et\u00a0al., 2021) requires models to generate several implicit reasoning steps to devise a strategy leading to a final decision for the question.\nHotpotQA\u00a0(Yang et\u00a0al., 2018) is requires finding and reasoning over multiple supporting documents to formulate responses. Its questions are diverse and not confined by any pre-existing knowledge bases.\nHoVer\u00a0(Jiang et\u00a0al., 2020) requires models to gather facts from multiple Wikipedia articles which are related to a claim and determine if these facts substantiate the claim.", "aeaddd68-f4cf-4adb-9939-2495211ea537": "Zheng et\u00a0al. (2023b) discovered that ChatGPT fails to deliver reliable and accurate answers on HotpotQA. Their further analysis indicates that this failure can stem from various factors, with factual correctness being the most critical. Addressing this issue, they underscore the significance of knowledge memorization and recall for LLMs.\n\n\n\n\n2.1.5 Structured Data Reasoning", "de8ac88a-f661-4a4d-935f-266748db582b": "2.1.5 Structured Data Reasoning\n\nThe aforementioned reasoning tasks have primarily concentrated on scenarios involving purely plain text data.\nIn contrast, structured data, characterized by specific formats like tables, knowledge graphs, and databases, presents greater challenges for machine comprehension and reasoning.\nTo perform structured data reasoning, models must be able to understand the format of the data, analyze the information it contains, and generate answers to questions related to that data.", "0d3614c8-a73e-459d-b8af-edfde80168e7": "HybridQA\u00a0(Chen et\u00a0al., 2020) integrates questions aligned with Wikipedia tables and multiple free-form corpora linked with entities from the table. The model is required to aggregate both tabular and textual information to generate answers.\nMetaQA\u00a0(Zhang et\u00a0al., 2018) comprises question-answer pairs within the movie domain and offers a knowledge graph (KG) to facilitate information retrieval. The models are tasked with conducting multi-hop reasoning on the KG and accommodating potential mismatches between KG entities and the question in order to derive answers.", "7d56614f-d92f-4b77-8746-6bd44734b0d5": "Spider Realistic\u00a0(Deng et\u00a0al., 2020) presents a SQL-based QA dataset, necessitating models to engage in text-to-SQL generation. Specifically, models must accurately identify textual references to columns and values and map them to the provided database schema.", "644860d8-628e-4f24-a930-73a1930a709f": "Gao et\u00a0al., 2023 conducted a comprehensive investigation into the text-to-SQL task across multiple LLMs, employing various prompt engineering methods.\nFurthermore, they performed fine-tuning experiments on open-source models.\nHowever, their findings revealed that even after fine-tuning, the performance of these models still lags behind proprietary models with zero-shot evaluation.\n\n\n\n\n\n2.2 Societal Impact", "68644574-0607-44e0-9a14-e73915b58a22": "LLMs have become crucial elements in modern society, significantly influencing various fields.\nWith their remarkable abilities in text generation and comprehension, LLMs are reshaping our interactions with information.\nTherefore, it is essential to understand the implications of LLMs.\nBy exploring these dimensions, we aim to comprehend the broader societal impacts of LLMs.\nOur goal is to simplify complex concepts into accessible insights, improving our ability to evaluate LLMs effectively.\nThis discussion explores the societal impacts of LLMs, focusing on two critical aspects: Safety and Trustworthiness.", "a5d059a4-b420-4139-a551-61bec98192ce": "Through exploring these dimensions, we aim to understand the broader societal implications of LLMs.", "5a853c5a-bf31-4b51-a334-f3b4a2ea425d": "2.2.1 Safety\n\nIn this section, we explore essential safety mechanisms required to protect users when interacting with LLMs. Ensuring that these models generate only safe content is crucial, Oviedo-Trespalacios et\u00a0al. (2023) found that ChatGPT sometimes made incorrect or harmful statements, emphasizing the need for expert verification. We address safety concerns by categorizing them into three main areas:\nThis section explores essential concerns related to the safety of LLMs, including Content Safety, Security, and Ethical Consideration.\n\n\nContent Safety", "3bf43757-e79e-4f2f-992a-33c46e5b3638": "Content Safety\n\nAs LLMs and generative AI become more prevalent, the associated content safety risks also escalate.\nBenchmarks offer critical insights into these risks.\nToxicChat Lin et\u00a0al. (2023), based on real user queries from an open-source chatbot, emphasizes the unique challenges of detecting toxicity in user-AI conversations.\nThe Open AI Moderation Dataset Markov et\u00a0al. (2023) provides a comprehensive approach to identifying undesired content in real-world applications.", "ab201481-9c42-43c2-bb1b-d0f7e919e3a8": "AEGISSAFETYDATASET Ghosh et\u00a0al. (2024), with around 26,000 human-LLM interaction instances annotated by humans, deepens the understanding of content safety issues.\nThe AI Safety Benchmark v0.5 Vidgen et\u00a0al. (2024), created by the MLCommons AI Safety Working Group, focuses on evaluating LLM safety.\nSALAD-Bench Li et\u00a0al. (2024a), designed to estimate LLMs, includes evaluations of attack and defense methods.\nSafetyBench (Scherrer et\u00a0al., 2023), a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple-choice questions spanning seven distinct categories of safety concerns.", "a34ab2f3-d86e-4a3a-b43f-075758f54b08": "CValues (Xu et\u00a0al., 2023b), the first Chinese human values evaluation benchmark to measure the alignment ability of LLMs in terms of both safety and responsibility criteria.\nKCDD (Kim et\u00a0al., 2024a) contains 22,249 dialogues generated by crowd workers, designed to simulate offline scenarios. This dataset categorizes dialogues into four criminal classes that align with international legal standards.\nBeaverTails (Ji et\u00a0al., 2023) introduces a novel \"QA moderation\" strategy to test models\u2019 safety alignment, offering a fresh perspective distinct from conventional content moderation approaches.", "1183bdee-f14d-43ad-b527-ee4c698cfc5b": "Additionally, it is crucial to ensure that LLMs do not produce adult content accessible by minors (Cifuentes et\u00a0al., 2022; Karamizadeh et\u00a0al., 2023), mitigate any harmful content that could affect children, guarantee that outputs do not encourage illegal activities (Nayerifard et\u00a0al., 2023; Casino et\u00a0al., 2022), and avoid the generation of content that could incite violence.\nIn this section, benchmarks and datasets play a vital role in evaluating the safety alignment of LLMs.\nBy providing annotated data that highlights harmful or inappropriate content, these resources enable researchers to develop and refine algorithms for content moderation and safety enforcement.\n\n\n\nSecurity", "32ae6495-af9f-4779-8ecd-e2170eb9a090": "This section reviews a collection of papers that focus on the dual aspects of enhancing data privacy practices and strengthening the resilience of LLMs against adversarial threats.\nStaab et\u00a0al. (2023) discusses the ability of LLMs to infer personal attributes such as location, income, and gender from seemingly innocuous text inputs, using a dataset derived from actual Reddit profiles to demonstrate significant privacy risks. The discussion extends with Kim et\u00a0al. (2024b) introducing ProPILE, a probing tool that enables data subjects to detect potential PII leakage in services based on LLMs.", "e177e84b-5fa3-47c0-b4c7-d1f08ad567d5": "Das et\u00a0al. (2024) examines these vulnerabilities in depth, highlighting the urgent need for improved security protocols and the exploration of effective defenses, while Yan et\u00a0al. (2024a) focuses on clarifying the data privacy concerns associated with LLMs.\nMoreover, Carlini et\u00a0al. (2023) and Yao et\u00a0al. (2024) emphasize the significant privacy risks posed by LLMs, particularly through their tendency to memorize and reproduce parts of their training data verbatim.", "1e235644-c724-47d0-8d84-369e55f92142": "On the resilience against adversarial attacks, Yip et\u00a0al. (2024) introduces a framework that quantifies the resilience of applications against prompt inject attacks using innovative techniques for robust and interoperable evaluations.\nLiu et\u00a0al. (2024b); Jin et\u00a0al. (2024) both proposes for the use of gradient-based method to enhance the evaluation of adversarial resilience in LLM. These methodologies emphasize a critical shift towards more sophisticated and reliable assessments of adversarial threat landscapes in LLMs.", "e2abd54b-a3f5-49ba-ab1f-0d6d91d8626f": "RigorLLM Yuan et\u00a0al. (2024), a framework employing techniques like energy-based data generation and minimax optimization to enhance the moderation of harmful content and improve resilience against complex adversarial attacks.\nInjecAgent Zhan et\u00a0al. (2024), a benchmark specifically designed to assess the vulnerability of tool-integrated LLM agents to indirect prompt injection attacks, demonstrating significant susceptibility in commonly used LLM agents.", "6d1918ef-26e1-4d08-9121-3a98504a93f6": "Ethical Consideration\n\nEthical evaluation in sensitive areas such as medical ethics Balas et\u00a0al. (2024) and moral decision-making Scherrer et\u00a0al. (2023) is challenging.\nThese studies are important for enhancing the ethical functioning of LLMs, ensuring their responsible application in real-world settings.\nIn the following sections, we will introduce some existing studies that address ethical considerations in LLM evaluation and their approaches to improving ethical standards.", "1f73ae04-a46e-4ceb-a38c-19fce8702d29": "Sorensen et\u00a0al. (2024) emphasizes value pluralism in decision-making. They introduce ValuePrism, a dataset of 218k values, rights, and duties connected to 31k human-written situations, used to build Value Kaleidoscope (Kaleido), a model that explains and assesses human values. Kaleido\u2019s outputs are preferred over GPT-4, showing more accuracy and broader coverage.", "66dee467-c747-4b01-95d0-7bcb2240e953": "Duan et\u00a0al. (2024) explores LLMs\u2019 ethical values using Moral Foundation Theory, proposing DeNEVIL, a prompt generation algorithm that reveals value inclinations. They create MoralPrompt, a dataset of 2,397 prompts, and benchmark LLMs\u2019 intrinsic values, finding many models misaligned. They develop VILMO, an alignment method enhancing value compliance, applicable to both black-box and open-source models.", "6e34ae22-1479-4f7e-a2d8-640e06350147": "In the MoralChoice datasets Scherrer et\u00a0al. (2023), one example of a moral dilemma involves a scenario where the respondent is a driver approaching a pedestrian crossing the street. The possible responses are: (1) Slowing down and waiting for the pedestrian, which reflects a responsible and ethical decision. (2) Accelerating and hitting the pedestrian, which represents an irresponsible and unethical choice. This dilemma illustrates the types of decision-making scenarios used to evaluate ethical behavior in realistic settings.", "6d161b7e-ee7f-44ef-bf6e-332a7cb682ae": "Scherrer et\u00a0al. (2023) introduces a novel statistical method to examine the moral beliefs of LLMs and quantifies how likely LLMs are to make decisions in various moral scenarios, analyzing their responses across 680 high-ambiguity and 687 low-ambiguity dilemmas. The findings indicate that LLMs generally align with common sense in straightforward situations but exhibit notable uncertainty in more ambiguous contexts. This research provides insights into LLMs\u2019 decision-making tendencies and their ability to mirror human moral judgments in ethical situations.\n\n\n\n\n\n2.2.2 Truthfulness", "96bdc4ab-4f58-401b-a698-f9dbd29e40e8": "2.2.2 Truthfulness\n\nEvaluating the reliability of LLMs necessitates ensuring the truthfulness of their outputs.\nTurpin et\u00a0al. (2023) demonstrate that Chain-of-Thought (CoT) explanations can systematically misrepresent the true reasoning behind a model\u2019s predictions.\nKhan et\u00a0al. (2024) points out that as LLMs grow more complex, possibly surpassing human experts, the evaluation dynamic might shift, raising the question of whether simpler models can effectively assess more advanced ones. This scenario underscores the ongoing importance of truthfulness in LLM outputs, reflecting the evolving challenges in model evaluation.", "105e3a35-4b3c-4571-ae68-a31bbac67fc3": "As trustworthiness becomes a key priority, researchers have implemented various evaluation strategies to ensure model reliability.\nThis section details strategies to reinforce the trustworthiness of LLM outputs.\nBesides the widely known TruthfulQA benchmark Lin et\u00a0al. (2022) , we also focus on the following topics: Hallucination, Bias Mitigation.\n\n\nHallucination\n\nHallucinations in LLMs, where models generate factually incorrect or fabricated content, pose significant challenges to their trustworthiness and reliability.", "95858828-03a1-4a50-a56f-926736148a52": "Techniques such as HaluEval 2.0 Jiang et\u00a0al. (2024)\nand HalluCode Liu et\u00a0al. (2024a) benchmarks have been developed for effective hallucination detection.\nOther methods include FEWL Wei et\u00a0al. (2024), which measures hallucinations without gold-standard answers by leveraging multiple LLM responses,\nand TofuEval Tang et\u00a0al. (2024), which evaluates hallucinations in dialogue summarization with detailed error taxonomy.\nSelf-Alignment for Factuality Zhang et\u00a0al. (2024b) uses self-evaluation to improve factual accuracy within LLMs.", "cdbdf5b9-415e-49ae-9bd0-1ef86535f369": "The LLM-free multi-dimensional benchmark AMBER Wang et\u00a0al. (2024a) allows for the evaluation of both generative and discriminative tasks, including various types of hallucinations, through a low-cost and efficient evaluation pipeline. This benchmark facilitates a comprehensive evaluation and detailed analysis of mainstream MLLMs like GPT-4V, also providing guidelines for mitigating hallucinations.", "6c81ade6-c940-41a4-a4bc-8d66a427f204": "Feldman et\u00a0al. (2023) helps recognize and flag instances when LLMs operate outside their domain knowledge, ensuring that users receive accurate information.\nThis method significantly reduces hallucinations when context accompanies question prompts, achieving a high effectiveness in eliminating hallucinations through tag evaluation.", "6c3faa81-a418-4af8-9327-4185a1422c07": "Yang et\u00a0al. (2023) introduces a self-check approach for detecting factual errors in LLMs during critical tasks, using reverse validation in a zero-resource setting. The PHD benchmark, designed for detecting hallucinations at the passage level and annotated by humans, enhances the evaluation of detection methods and surpasses existing approaches in efficiency and accuracy.", "bca34828-d868-43fd-a8bf-f4cf1389325b": "Bias Mitigation\n\nA range of studies address the issue of bias in the evaluation and operation of LLMs, emphasizing the need to diminish these biases to improve both quality and reliability.", "99d67b73-bfaf-4adb-87c8-dfd0f3c8aaa2": "Here are some general bias benchmarks.\nBBQ Parrish et\u00a0al. (2021) is a dataset of question sets constructed by the authors that highlight attested social biases against people belonging to protected classes along nine social dimensions relevant for U.S. English-speaking contexts.\nBIAS Vermetten et\u00a0al. (2022) is a novel behavior-based benchmark designed to detect structural bias per dimension and across dimension-based on 39 statistical tests.\nRecLLM Zhang et\u00a0al. (2023) investigates fairness in LLM-based recommendations, presenting the FaiRLLM benchmark to evaluate biases towards sensitive user attributes.", "40baacdf-0a87-44c8-824d-a1081c88fdeb": "MERS Wu and Aji (2023) introduced assesses machine-generated text on multiple dimensions, including factual accuracy and linguistic quality, to specifically target and reduce biases that favor incorrect factual content in LLM evaluations.", "02260f23-46a0-49c6-bd38-5404e695bba4": "Below are specific bias benchmarks relevant to distinct sectors.\nIn the financial sector, Daniel et\u00a0al. (2008) tackles the \"look-ahead benchmark bias\" in the evaluation of investment managers, which identifies significant discrepancies in performance metrics due to timing differences in benchmark composition. This finding stresses the need for precise benchmarking methods to avoid overstated performance assessments.\nHort et\u00a0al. (2021) uses a model behavior mutation approach for benchmarking ML bias mitigation methods. Although the results indicate that many methods struggle to effectively balance fairness and accuracy, they underline the need for more robust strategies in bias mitigation.", "176f31f9-a5a3-4443-8a00-f1fdb9da20db": "Wessel et\u00a0al. (2023) introduces the Media Bias Identification Benchmark (MBIB), a comprehensive framework that integrates various types of media biases, enhancing the effectiveness of detection techniques and promoting a more unified and effective approach to bias evaluation in media content.", "03118b9e-d481-464b-b50c-113647d7d625": "2.3 Domain Knowledge\n\nAs LLMs demonstrate their capabilities in reasoning and safety, experts have begun to explore the knowledge of LLMs in various domains. They utilize LLMs to complete specific tasks, making these models useful assistants. In this section, we delve into five domains: Finance, Legislation, Psychology, Medicine, and Education, introducing the applications, evaluation methods, and discussing the direction and limitations of LLMs in each domain.\n\n\n\n2.3.1 Finance", "dcac07f7-d854-4a98-bb08-ee4cd1eb03af": "The application of LLMs in Finance field developed relatively earlier. A few models were even designed specifically for financial use, such as FinBERT Liu et\u00a0al. (2021b), XuanYuan 2.0 Zhang and Yang (2023), and BloombergGPT Wu et\u00a0al. (2023). BloombergGPT is a 50 billion parameter language model that is trained on a wide range of financial data. From the validation process of BloombergGPT, we can understand the evaluation methods of financial LLMs. Wu et\u00a0al. (2023) evaluated BloombergGPT on two broad categories of tasks: finance-specific and general purpose. Regarding the finance-specific tasks, FPB Malo et\u00a0al. (2014), FiQA SA Maia et\u00a0al. (2018), Headline Sinha and Khandait (2021), NER", "42ed9466-162b-477f-a298-d52f32bab6c9": "(2018), Headline Sinha and Khandait (2021), NER Alvarado et\u00a0al. (2015), and ConvFinQA Chen et\u00a0al. (2022) were used. They also used social media and news as aspect-specific sentiment analysis dataset, and compared BloombergGPT response with financial experts\u2019 annotation. Regarding the general purpose tasks, standard LLM benchmarks were utilized for evaluation, such as BIG-bench Hard Suzgun et\u00a0al. (2022), and several datasets about Knowledge Assessments, Reading Comprehension, and Linguistic Tasks. Conditionally, Xie et\u00a0al. (2023) proposed PIXIU, a framework including the financial LLM based on fine-tuning LLaMA, a instruction data with 136K data samples to support the finetuning, and an", "356120ee-fe2d-4809-8639-353dacbc3587": "data samples to support the finetuning, and an evaluation benchmark with 5 tasks and 9 datasets, giving LLMs in financial area a benchmark to assess their ability. When mentioning LLMs for financial use, Li et\u00a0al. (2023b) argued that two major challenges are the production of disinformation and the manifestation of biases, such as racial, gender, and religious biases, in LLMs. Also, the primary challenge in evaluation was incorporating domain knowledge from financial experts to validate the model\u2019s performance based on financial NLP tasks Lee et\u00a0al. (2024).", "346f4f12-9fb1-43b8-852e-b69edfb08548": "2.3.2 Legislation", "94f94b3d-7b07-402a-a4cf-45c274bacc00": "LLMs\u2019 ability in legislation area has also attracted attention because GPT-4 scored approximately 297 points on the uniform bar examination, passing the threshold for all jurisdiction Katz et\u00a0al. (2024). Various tasks such as statutory reasoning, term interpretation, and legal rule classification were performed by LLMs, and their performance were also evaluated. Blair-Stanek et\u00a0al. (2023) evaluated the performance of GPT-3 in statutory reasoning with SARA dataset Holzenberger et\u00a0al. (2020). they found that GPT-3 only reached 78% accuracy in zero-shot condition, showing that GPT-3 couldn\u2019t handle basic legal work because statutes in the dataset were far less complex than real statutes. Engel", "24093628-6832-411b-8382-79cd86e6f97e": "were far less complex than real statutes. Engel and Mcadams (2024) asked Chat 3.5 Turbo whether the statutory term \u201cvehicle\u201d includes a list of candidate objects to assessment LLMs\u2019 understanding of statutory meaning. They found that Chat 3.5 Turbo give the similar result to 2,800 English speakers\u2019 response Tobia (2020). Liga and Robaldo (2023) found that GPT-3 is capable to recognize the difference between obligation rules, permission rules and constitutive rules with LegalDocML Palmirani and Vitali (2011) and LegalRuleML Athan et\u00a0al. (2013) dataset. Whether LLMs possess sufficient capability to be applied in the professional legal field, The investigation indicates that pre-trained LLMs", "36da4359-e2e0-4fa0-8b30-5a8999986959": "The investigation indicates that pre-trained LLMs are not yet ready for fully automatic deployment for case judgement summarization because inconsistent or hallucinated information has been found in the generated abstractive summaries Deroy et\u00a0al. (2023).", "d0905432-c78c-466b-af0b-c5fe0c021e36": "2.3.3 Psychology", "d681af50-98ad-4d17-b95a-3768c00a2ebe": "Human language data is important and valuable in every subdomain in psychology. Because LLMs have the capability to understand and utilize multiple language, emotion detection and psychological measurement can be done by LLMs. Plenty of researches evaluated whether LLMs could complete these tasks with enough quality.Rathje et\u00a0al. (2023) tested whether different versions of GPT (3.5 Turbo, 4, and 4 Turbo) can detect sentiment, discrete emotions, offensiveness, and moral foundations in text across 12 languages. They found that LLMs outperformed existing English-language dictionary analysis at detecting psychological constructs as judged by manual annotators. Lu et\u00a0al. (2024) evaluated", "f86b4bac-1923-40ce-9959-d0b8a42005de": "by manual annotators. Lu et\u00a0al. (2024) evaluated GPT-4V\u2019s performance in 5 crucial abilities for affective computing tasks. They used DISFA dataset Mavadati et\u00a0al. (2013) to assess GPT-4V\u2019s ability to action unit detection, RAF-DB dataset Shan and Deng (2018) for facial expression and compound emotion recognition Du et\u00a0al. (2014), CASME2 dataset Yan et\u00a0al. (2014) for Micro-expression Recognition Zhao et\u00a0al. (2023), and iMiGUE dataset Liu et\u00a0al. (2021a) for Micro-gesture Recognition. The results showed that GPT-4V could give satisfactory answers to action unit, compound emotion and Micro-gesture test samples, but failed to answer facial expression and Micro-expression test samples correctly.", "da61b503-5529-4004-a333-9a9a9061d52e": "and Micro-expression test samples correctly. Regarding psychological measurement, Demszky et\u00a0al. (2023) proposed 2 methods to evaluate the effects of features on human thought and behaviour: 1) Expert evaluation means trained research assistants and LLMs score the same texts for particular psychological construct, and then compute agreement between their scores. 2) Impact evaluation means assessing the effect before and after the manipulation. For instance, Karinshak et\u00a0al. (2023) used impact evaluation to measure participants\u2019 attitude to GPT-3-generated pro-vaccination messages. Demszky et\u00a0al. (2023) additionally proposed that in assessing the capability of LLMs for psychological tasks,", "c1af1a5c-6836-4729-9c64-225668efe38f": "the capability of LLMs for psychological tasks, initial assessment could be conducted using expert evaluation for a manipulation check or a measure of construct validity. Subsequently, text aligning with expert evaluations might be utilized in an impact evaluation study that attempts to measure the intended effects on third-party participants, similar to assessing predictive or external validity.", "636bc9c1-287f-4b4d-bc3f-3c195f1daf05": "2.3.4 Medicine", "d7566c58-77c4-4788-98de-823b02dc114b": "As ChatGPT was able to pass the United States Medical Licensing Exam (USMLE) Kung et\u00a0al. (2023) without additionally training, LLMs were noticed in medical area. Previous researches focused on exploring LLMs\u2019 potential in clinical work and research Thirunavukarasu et\u00a0al. (2023). Agrawal et\u00a0al. (2022) introduced dataset from manual reannotation of the CASI dataset Moon et\u00a0al. (2014) for benchmarking few-shot clinical information extraction, and showed that GPT-3 outperform existing baseline of this task. Sharma and Thakur (2023) demonstrated ChatGPT can help researchers design new drugs and optimize the pharmacokinetics and pharmacodynamics of new drugs. Benoit (2023) showed when presented", "df0f42f1-a567-4028-83d2-f23f7aea0e5a": "of new drugs. Benoit (2023) showed when presented with 45 simplified standardized vignettes Semigran et\u00a0al. (2015), ChatGPT identified illnesses with 75.6% first-pass diagnostic accuracy and 57.8% triage accuracy, which performed similarly to physicians\u2019 72.1% on the same set of 45 vignettes. However, when writing academic clinical paper, current LLMs cannot meet ICMJE authorship criteria because they cannot understand the role of authors or take responsibility for the paper Zielinski et\u00a0al. (2023). Also, Kumar (2023) assess the ChatGPT\u2019s utility for academic writing in biomedical domain, showing that although the content of the response were systematic, precise and original, it lacked", "4f1b635c-5bf3-4893-a6fd-646bbcb5b3bd": "were systematic, precise and original, it lacked quality and depth of academic writing. In summary, plenty of deployment of LLM applications in medical area is not currently feasible and need to have deeper evaluation. clinicians and researchers will remain responsible for delivering optimal knowledge and care Thirunavukarasu et\u00a0al. (2023).", "50fcfa2b-5214-4596-9014-1a1b4c057da2": "2.3.5 Education", "80d9d7dd-ba27-45b4-94f7-378c330d72f3": "The conversational and knowledgeable features of LLMs make the applications of LLMs in education possible. Current evaluation methods of LLMs in education field can be generally divided into two categories: 1) Human annotation means that experts directly score the material generated by LLMs or annotate unlabeled data from external datasets or online websites to create an evaluation dataset. Abdelghani et\u00a0al. (2023) used GPT-3 for generating linguistic and semantic cues that can help children formulate divergent questions. They have 2 experts to evaluate the quality of the linguistic and semantic cues generated. Jia et\u00a0al. (2021) had fluent English speakers to annotated data from a", "22c147ea-2296-4d29-918b-bb939d995b15": "fluent English speakers to annotated data from a peer-assessment platform, Expertiza and make sure enough inter-annotator agreement to test the accuracy of the BERT model for evaluating peer assessments. Menick et\u00a0al. (2022). evaluated their Self-Supported Question Answering model by asking paid contractors to assess model samples from Natural Questions Kwiatkowski et\u00a0al. (2019) and ELI5 Fan et\u00a0al. (2019) datasets. 2) Metrics and models means that traditional metrics or trained model are utilized to assess the material generated by LLMs automatically. Dijkstra et\u00a0al. (2022) proposed EduQuiz, an end-to-end quiz generator based", "4985cbbd-5c4f-4ad4-a733-85df57c61c2e": "on a GPT-3 model, able to generate a complete multiple-choice question, with the correct and distractor answers. They used BLEU-4 Papineni et\u00a0al. (2002), ROUGE-L Lin (2004), and METEOR Banerjee and Lavie (2005) metrics to compared prediction and ground truth instances. Raina and Gales (2022) use the RACE++ dataset Liang et\u00a0al. (2019) to train a deep learning", "3f31fc2f-7b03-427b-851e-38b91ddb05bf": "model to explicitly class a multiple-choice question in the complexity levels of easy, medium and hard, which could make the process of assessing multiple-choice question generation automatic. After the overall review, Kasneci et\u00a0al. (2023) concluded integrating LLMs into the educational area offers considerable benefits, such as enhancing student learning experiences and assisting teachers, but this integration must adhere to strict requirements concerning privacy, security, environmental sustainability, regulation, and ethics. Additionally, it should be accompanied by continuous human oversight, guidance, and the application of critical thinking.", "50641aec-722e-463c-9af9-b11329e50f0b": "3 Agent Evaluation\n\n\n{forest}", "1161220d-39f0-4db5-a2dd-39db4b1ee5f0": "for tree=\ngrow=east,\nreversed=true,\nanchor=base west,\nparent anchor=east,\nchild anchor=west,\nbase=left,\nfont=,\nrectangle,\ndraw,\nrounded corners,align=left,\ninner xsep=4pt,\ninner ysep=1pt,\n,\nwhere level=1font=,fill=pink!50,\nwhere level=2font=,fill=green!10,\nwhere level=3font=,fill=gray!20,\n[Agent Evaluation\n(Sec.\u00a03),fill=yellow!20,font=[Planning\n(Sec.\u00a03.1)\n[Song et\u00a0al. (2023a), Huang et\u00a0al. (2022b), Yao et\u00a0al. (2023b), Shinn et\u00a0al. (2023),fill=gray!20]\n]\n[Application Scenarios\n(Sec.\u00a03.2)\n[Web Grounding\n[Nakano et\u00a0al. (2022), Qin et\u00a0al. (2023a), Yao et\u00a0al. (2023a)\n]\n]\n[Code Generation\n[Liang et\u00a0al. (2023),\nZhang et\u00a0al. (2024a)\n]\n]\n[Database Queries\n[Hu et\u00a0al. (2023)]\n]\n[API Calls", "fe472712-d846-4950-af89-303191dc7901": "[Database Queries\n[Hu et\u00a0al. (2023)]\n]\n[API Calls\n[Li et\u00a0al. (2023a),\nQin et\u00a0al. (2023b),\nYan et\u00a0al. (2024b)\n]\n]\n[Tool Creation\n[Cai et\u00a0al. (2024),\nQian et\u00a0al. (2023)\n]\n]\n[Robotic Navigation\n[Shah et\u00a0al. (2022), Zhou et\u00a0al. (2023a), Zheng et\u00a0al. (2023a)\n]\n]\n[Robotic Manipulation\n[Huang et\u00a0al. (2023), Yu et\u00a0al. (2023)\n]\n]\n]\n[Benchmark\n(Sec.\u00a03.3)\n[Ruan et\u00a0al. (2023), Li et\u00a0al. (2023a), Tang et\u00a0al. (2023),fill=gray!20]\n]\n]", "0d42b370-c957-4d18-9672-2900b830eaba": "Figure 3: The overview of agent evaluation.\n\n\nBuilding upon LLM\u2019s core abilities, there has been a growing research area that employs LLMs as central controllers to construct autonomous agents to obtain human-like decision-making capabilities Wang et\u00a0al. (2024b). \nIn this section, we\u2019ll first discuss the methods used to assess LLM agents\u2019 capabilities of planning. And also introduce the evaluation based on various application scenarios.\nEach subsection will provide detailed insights into the applications of LLMs, the methodologies used for evaluation, and the datasets employed.\n\n\n\n3.1 Planning", "938ca09a-b245-4fb7-b8c2-f7ac7f891e2f": "3.1 Planning\n\nPlanning by an agent involves the strategic formulation and execution of actions or steps to achieve specific goals or outcomes within a given environment, typically using algorithms or models to predict and decide the best course of action.", "995f7fc9-2703-4d24-bdb3-86555ecaf0e4": "Facing the challenge of executing complex tasks that require decomposition into simpler subtasks, robot planning empowers robots to autonomously identify and execute actions towards achieving specific goals, taking into account their surroundings and objectives. In this context, several innovative approaches, such as Huang et\u00a0al. (2022a); Singh et\u00a0al. (2023); Song et\u00a0al. (2023a) harness the extensive commonsense knowledge available through LLMs, enabling these models to efficiently segment tasks into manageable subtasks. The Inner Monologue Huang et\u00a0al. (2022b) system utilizes LLMs for dynamic planning in robotic tasks by integrating continuous natural language feedback. Similarly, SayPlan", "95d27cee-5318-4d6f-99f2-2cd483a45d67": "natural language feedback. Similarly, SayPlan Rana et\u00a0al. (2023) enhances task planning capabilities of LLMs by grounding them with 3D Scene Graphs to facilitate extensive environmental interactions. These methods are evaluated across virtual environments, embodied agents, and physical robots. Moreover, several works like DEPS Wang et\u00a0al. (2023b), AdaPlanner Sun et\u00a0al. (2023), and Robots That Ask For Help Ren et\u00a0al. (2023), introduce dynamic elements of interactive re-planning, adaptive strategies, and the ability to seek assistance when faced with uncertainties. These developments are pivotal for the practical application and effectiveness of robotics in real-world settings, illustrating a", "3ff85a64-829b-4cd7-942e-9a77c30d11e2": "robotics in real-world settings, illustrating a significant stride towards more adaptable and intelligent robotic systems. They are evaluated in increasingly complex situations that closely mirror real-life conditions.", "6e93c080-8f7a-4b14-bc73-ba8d7d24c167": "An LLM-based agent employs LLMs to analyze and generate human-like text, aiding in decision-making and strategic planning by processing vast amounts of information quickly and accurately. React Yao et\u00a0al. (2023b) presents a paradigm that synergistically blends reasoning and action within language models, enhancing performance and interpretability across various decision-making tasks, as evidenced by benchmarks in ALFWorld and WebShop. Reflexion Shinn et\u00a0al. (2023) introduces a groundbreaking framework that employs verbal feedback for reinforcement learning, enabling language agents to refine their skills through self-reflection without updating model weights. This method is evaluated across", "52102533-ec43-46bd-bfdd-10c77ae0b6b1": "model weights. This method is evaluated across diverse decision-making, reasoning, and programming tasks, demonstrating marked enhancements over traditional approaches in environments such as AlfWorld, HotPotQA, and HumanEval. SelfCheck Miao et\u00a0al. (2023) offers a zero-shot mechanism that empowers LLMs to autonomously verify their multi-step reasoning in math problem-solving, which significantly boosts accuracy on benchmarks including GSM8K, MathQA, and MATH by filtering out low-confidence solutions.", "316200a3-a584-4569-b0b2-d0c5cae669d4": "3.2 Application Scenarios\n\n\n3.2.1 Web Grounding\n\nIn this section, we focus on LLMs performing tasks in web environments. We categorize the evaluation methods based on tasks.\n\n\nSearch Engine\n\nWebGPT Nakano et\u00a0al. (2022) developed a text-based web-browsing environment, enabling interaction with a fine-tuned language model to generate more faithful outputs. Evaluation of WebGPT models is conducted through three main approaches: comparison with answers authored by human demonstrators on a held-out set of questions, comparison with the highest-voted answers from the ELI5 dataset, and evaluation using the TruthfulQA dataset.", "e763e623-f294-4fcd-a4d5-7669459e4d84": "WebCPM Qin et\u00a0al. (2023a) employs tool learning to enable models to answer long-form questions through web searches. Its evaluation encompasses four sub-tasks: action prediction, search query generation, supporting fact extraction, and information synthesis, with each task independently assessed using Micro-F1 and Macro-F1 for action prediction and Rouge-L for other three tasks including text generation. In holistic evaluation, eight annotators manually compare the model-generated answers based on human preference.\n\n\n\nOnlineshopping", "f6a382ce-62ea-449b-9066-fd864664995b": "Onlineshopping\n\nWebShop Yao et\u00a0al. (2023a) introduces a benchmark for assessing LLM-based agents\u2019 abilities in product search and retrieval. Their dataset, comprising 12,087 instructions, is divided into 10,587 for training, 1,000 for development, and 500 for testing, with human shopping paths recorded for each instance. Evaluation metrics include task score and success rate, revealing that humans outperform LLMs across all measures.\n\n\n\n\n\n3.2.2 Code Generation", "024822ec-199f-42a9-9b63-30d35fc817ca": "3.2.2 Code Generation\n\nTo enable nuanced control in robots for complex real-world tasks, the Code as Policies Liang et\u00a0al. (2023) paradigm uses LLMs to generate policy code for spatial reasoning and adapting to new instructions. The code quality is assessed with HumanEval and RoboCodeGen. RoboCodeGen, a benchmark with 37 function generation tasks, focuses on spatial and geometric reasoning and control, supports third-party libraries like NumPy, lacks documentation strings and type hints, and permits undefined functions for hierarchical code generation. The evaluation metric is the pass rate of generated code that passes manually written unit tests.", "d3bffd95-900c-4465-9aa2-7b84465a400d": "The CODEAGENTBENCH benchmark Zhang et\u00a0al. (2024a) is designed to evaluate LLMs in real-world repo-level code generation tasks. It provides comprehensive input information, such as documentation, code dependencies, and runtime environment details, challenging LLMs to produce accurate and well-integrated code solutions.\n\n\n\n\n3.2.3 Database Queries", "9f915464-7ebd-498c-839d-7659f332945e": "3.2.3 Database Queries\n\nIntegrating external databases or knowledge bases allows agents to access specific domain information, resulting in more realistic actions. For example, ChatDB Hu et\u00a0al. (2023) uses SQL statements to query databases, enabling logical actions by the agents. They created a dataset of 70 records from fruit shop management logs for evaluation. The experiment clearly demonstrates that ChatDB outperforms ChatGPT with significantly higher accuracy.\n\n\n\n\n3.2.4 API Calls", "02abfb88-9fc5-44de-a625-757ef1f34477": "3.2.4 API Calls\n\nLLM agents can also enhance their capabilities by calling APIs. API-Bank, as introduced by Li et\u00a0al. (2023a), provides a specialized benchmark to evaluate tool-augmented LLM performance. This benchmark includes 53 standard API tools, a detailed workflow for tool-augmented LLMs, and a dataset with 264 annotated dialogues. Evaluation metrics involve accuracy of API calls and ROUGE-L for post-call responses, with task planning efficacy measured by the successful completion of planned tasks through model-driven API calls.", "d15d8973-0b4b-49ad-a520-551f61bc504c": "Qin et\u00a0al. (2023b) undertake a scholarly inquiry into the utilization of tool learning within contemporary Language Models (LLMs), delving into both their effectiveness and limitations. They evaluate 18 representative tools across six tasks using existing datasets and extend their study to 12 additional tasks, such as slide-making, AI painting, and 3D model construction. They augment user queries generated by ChatGPT and manually assess the success rates of these operations.", "a776d530-8f77-4c25-9b36-b809f57638b4": "The Berkeley Function-Calling Leaderboard (BFCL) Yan et\u00a0al. (2024b) evaluates LLMs on function processing, syntax tree analysis, and function execution across various scenarios. It features an interactive comparison tool and a dataset covering fields like Mathematics, Sports, and Finance. Evaluations include Simple, Multiple, and Parallel Function tests. BFCL aids the integration of LLMs into platforms like Langchain and AutoGPT, providing detailed analyses on cost and latency for models like GPT-4.\n\n\n\n\n3.2.5 Tool Creation", "d4e7ff9a-6353-4f21-95f3-dede503ada22": "The usage of tools is contingent upon the accessibility of external tools Schick et\u00a0al. (2023). Recently, efforts have been made to employ LLM as a tool creator in order to generate tools that can be utilized for diverse requests(Ruan et\u00a0al. (2023)).", "b5204fed-23ad-4472-be11-2fc311f2b4e1": "LATM Cai et\u00a0al. (2024) utilizes GPT-4 to develop tools, demonstrating that more cost-effective models can achieve comparable performance to larger models in these applications. They employ six datasets from various domains: logic reasoning, object tracking, Dyck language, word sequencing, the Chinese remainder theorem, and meeting scheduling. The first five datasets are sourced from BigBench Srivastava et\u00a0al. (2023), while the meeting scheduling task is specifically designed to showcase the model\u2019s real-world utility.", "b49b072a-bc97-4c70-ad10-4d264203b92a": "CREATOR Qian et\u00a0al. (2023) evaluates LLMs\u2019 ability to create tools using the Creation Challenge dataset, which includes 2,000 novel and challenging problems that existing tools or code packages cannot adequately solve. Evaluations demonstrate that ChatGPT\u2019s tool-making performance improves with additional hints, achieving up to 75.5% accuracy, highlighting the importance of tool creation in enhancing LLM problem-solving capabilities.", "ba6f185e-5027-4283-9964-14a64b5705eb": "3.2.6 Robotic Navigation\n\nNavigation by an embodied agent involves the autonomous movement and decision-making of a robotic or virtual entity within a physical or simulated environment, using sensors and algorithms to perceive surroundings, plan routes, and accomplish navigational tasks.", "78231644-741c-4c3b-aad4-1e0ed01bc845": "LM-Nav Shah et\u00a0al. (2022) proposed a system for robotic navigation that utilizes LLM, VLM, visual navigation model (VNM), and robotic navigation\u2014enabling a robot to navigate complex environments using natural language instructions without needing specific training data annotated with language descriptions. They benchmark on 20 queries, in environments of varying difficulty, corresponding to a total combined length of over 6 km. LFG Shah et\u00a0al. (2023) leverages language models as heuristics to enhance planning algorithms, guiding robots through unfamiliar environments using semantic cues from natural language descriptions. They evaluate navigational performance on ObjectNav.", "8c45daf5-00b6-447b-8035-3e9f0bc99a15": "NavGPT Zhou et\u00a0al. (2023a) utilizes LLMs to perform explicit reasoning and planning. This approach incorporates textual descriptions of visual observations, navigation history, and potential future paths to enhance navigation tasks. Following this, the NaviLLM model Zheng et\u00a0al. (2023a) emerges as a versatile solution for embodied navigation. It adeptly tailors LLMs to manage a wide spectrum of embodied navigation challenges by employing schema-based instructions that transform disparate tasks into unified generative modeling problems. The performance of these models is rigorously assessed using vision-language navigation (VLN) benchmarks, such as R2R, Reverie, CVDN, and SOON.", "5423b83c-68ec-4676-8597-f4bbb130abe2": "3.2.7 Robotic Manipulation\n\nManipulation involves the use of embodied agent to interact with and manipulate physical objects in their environment, enabling tasks ranging from simple pick-and-place operations to complex assembly processes.", "2d005bdd-fa47-4e95-b7be-8880e3500bd1": "VoxPoser Huang et\u00a0al. (2023) presents an innovative approach where the key novelty is the use of LLMs not just for understanding natural language instructions, but crucially, for generating code that interacts with VLMs to create detailed 3D value maps. These maps guide robotic actions, bridging the gap between abstract instructions and physical execution. They directly evaluate the result on the success rate of robot manipulation tasks. L2R Yu et\u00a0al. (2023) presents a method for translating language instructions into reward functions using LLMs that robots can optimize to execute specific tasks, demonstrating this approach with a variety of complex locomotion and manipulation tasks in", "c3447590-2bb6-46c1-aaf2-bf23b6463d68": "of complex locomotion and manipulation tasks in simulated environments.", "830449a3-0191-4ecc-a258-c338fcd00e8c": "3.3 Benchmark\n\n\n\n\n\n\n\nBenchmark\n\n\n\n\nDescription\n\n\n\n\n\n\n\n\nAPIBench (Patil et\u00a0al., 2023)\n\n\n\n\nAn evaluation system with 73 API tools, 314 annotated tool-use dialogues with 753 API calls, and a training set containing 1,888 tool-use dialogues from 2,138 APIs across 1,000 domains\n\n\n\n\n\n\nToolEval (Qin et\u00a0al., 2023c)\n\n\n\n\nconstructed automatically using ChatGPT, includes a collection of 16,464 real-world RESTful APIs across 49 categories, with diverse instructions and solution paths generated for both single-tool and multi-tool scenarios.\n\n\n\n\n\n\nToolAlpaca (Tang et\u00a0al., 2023)\n\n\n\n\ncontaining 3,938 instances from over 400 real-world tool APIs across 50 categories\n\n\n\n\n\n\nRestBench (Song et\u00a0al., 2023b)", "21088631-e3d1-4d8c-b477-b4ecd4eccc4e": "RestBench (Song et\u00a0al., 2023b)\n\n\n\n\nhuman-annotated dataset comprising two real-world scenarios (TMDB movie database and Spotify music player) with 54 and 40 commonly used APIs respectively, annotated with 10 instruction-solution pairs for development and 157 pairs (100 for TMDB, 57 for Spotify) for testing\n\n\n\n\n\n\nWebArena (Zhou et\u00a0al., 2023b)\n\n\n\n\nA realistic and reproducible web environment featuring four fully operational web applications (e-commerce, discussion forums, collaborative development, and content management) with 812 long-horizon tasks\n\n\n\n\n\n\nMIND2WEB (Deng et\u00a0al., 2023)", "9abc89db-acd0-4fd5-b609-49f8c2969f64": "MIND2WEB (Deng et\u00a0al., 2023)\n\n\n\n\nover 2,000 tasks from 137 real-world websites across 31 domains with crowdsourced action sequences, enabling the creation of agents that handle diverse, complex web interactions\n\n\n\n\n\n\nTable 2: Benchmarks for Agent Evaluation", "5c6fe855-984f-43e5-841a-899681f8beee": "Table 2: Benchmarks for Agent Evaluation\n\n\nThe evaluation LLMs\u2019 capability on tool manipulation primarily revolves around assessing the efficacy of a single tool, gauging its impact on downstream tasks using established benchmarks, as discussed previously. However, an increasing number of researchers are shifting their focus towards scenarios that involve the combined use of multiple tools to evaluate the performance of LLMs trained with tool learning. This approach ensures a more comprehensive and diverse appraisal of the model\u2019s abilities and constraints across various tool sets.", "e52bd279-ef80-42b5-ad29-93a9a821d6dd": "APIBench Patil et\u00a0al. (2023) assembles a comprehensive API corpus from major hubs like HuggingFace, TorchHub, and TensorHub, including all API calls from TorchHub and TensorHub and the top 20 most downloaded models from each HuggingFace task category. Using Self-Instruct Wang et\u00a0al. (2023a), they create 10 synthetic user prompts per API to evaluate LLMs for functional correctness and hallucination issues.\n\n\nToolBench, developed by Xu et\u00a0al. (2023c), evaluates LLMs\u2019 generalization and advanced reasoning skills across various tool-based tasks. It integrates existing and newly collected datasets, featuring eight tasks with about 100 test cases each.", "c28119df-7254-4948-8094-0781749c2511": "Based on ToolBench, ToolLLM Qin et\u00a0al. (2023c) introduces ToolEval, an automatic evaluator resembling a leaderboard. ToolEval uses two metrics: pass rate, which measures the proportion of successfully completed instructions within limited attempts, and win rate, which compares performance against ChatGPT. This evaluation method combines automatic and manual assessments while using ChatGPT-generated solutions as a benchmark, reducing potential human biases and unfairness.", "9f8916cc-9475-43af-be9c-bded92303496": "ToolAlpaca Tang et\u00a0al. (2023) expands the evaluation framework to encompass real-world scenarios. Using a training set of 426 tool uses, the study evaluates ten new tools across 100 evaluation instances. Following the ReAct style Yao et\u00a0al. (2023b), tool usage is integrated during text generation, with human reviewers assessing program accuracy and overall correctness.", "9fe78e51-b3d4-4bcd-aae4-3e9b7d55fe34": "RestBench Song et\u00a0al. (2023b) explores real-world user instructions using APIs, focusing on TMDB movie database and Spotify music player scenarios. It filters 54 and 40 commonly used APIs respectively, constructing OpenAPI specifications. Integrating RestGPT, which links LLMs with RESTful APIs, it follows standard web service protocols. RestBench evaluates performance with human-annotated instructions and gold solution paths, demonstrating RestGPT\u2019s effectiveness in complex tasks and advancing towards Artificial General Intelligence (AGI).", "1d2cdee9-33f4-457c-80c9-9d34aaae0ca5": "WebArena (Zhou et\u00a0al. (2023b)) offers an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Its purpose is to evaluate agents in an end-to-end fashion and determine the accuracy of their completed tasks.", "16669122-2f44-4659-a7b9-63581ee1b2e5": "MIND2WEB (Deng et\u00a0al. (2023)), is the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. MIND2WEB boasts a collection of over 2,000 tasks curated from 137 websites that span 31 different domains, replacing the oversimplified simulation environments commonly found in other datasets with a realm of real-world websites.\n\n\n\n\n\n4 Future Directions", "57db0186-8c6f-4540-b0bb-2430d7fcefd8": "The rapid advancements in the capabilities and application areas of LLMs have enabled them to replace other tools in a short time, significantly enhancing people\u2019s lives. However, the progress in evaluation methodologies has not kept pace with the expansion of LLM capabilities, often making it challenging to find benchmarks that fully match current tasks. There is substantial room for improvement in current evaluation methods to assess LLMs\u2019 performance in various tasks more accurately and provide a basis for decision-making. Consequently, we propose five future directions for developing evaluation methods. We expect these improvements will make LLMs a more \"useful\" presence in the eyes of", "2bfeb503-7c1d-46d2-9c5a-552d811d73cc": "make LLMs a more \"useful\" presence in the eyes of the public.", "2dad5d3b-d624-42ea-ab87-0991ae3708db": "4.1 Dynamic Evaluation\n\nCurrent benchmarks are mostly static and do not change once they are created. However, unchanging benchmarks can present two problems when used for evaluation. Firstly, factual knowledge in the real world changes over time. For example, the presidency may change every four years, necessitating that datasets for evaluating the factual knowledge of LLMs also be updated over time and ideally updated automatically to ensure that the information provided by LLMs is accurate and contemporary.", "42984b18-3d37-4f1f-8b31-62bb3060e5ce": "Secondly, as LLM models expand, data from the datasets might leak and become part of the training data for LLMs, at which point these datasets no longer function as effective evaluative tools. Therefore, the evaluation questions within the datasets must be capable of being automatically replaced and updated. For example, the framework proposed by Wang et\u00a0al. (2024c) can manipulate the context or question of original instances, reframing new evolving instances with high confidence that dynamically extends existing benchmarks. Such advancements would ensure that benchmarks can consistently measure the capabilities of LLMs as they progress.\n\n\n\n\n4.2 LLMs as Evaluators", "f04b9b9b-c67d-4891-96e8-a5a52a2f3c2a": "Many datasets currently require human annotators to label each question\u2019s answer, a process that is both time-consuming and prone to errors. Therefore, employing LLMs as evaluators represents a promising direction for development. LLMs can simulate a scorer by reading text and providing ratings, allowing us to avoid designing new benchmarks for every task. Instead, we can leverage the broad capabilities of LLMs to act as scorers across various tasks. Li et\u00a0al. (2024b) has reviewed the current methods of using LLMs as scorers and has also identified potential issues, such as a preference for content generated by the same model or specific biases in evaluation order. In the future, we can", "1318e424-c54a-4d4f-be10-0553dca407e5": "biases in evaluation order. In the future, we can gradually address the biases inherent in LLMs as evaluators. In that case, we can enhance the rapid development of LLM applications while enabling them to self-assess, thus eliminating the need for additional dataset design.", "43c87fed-ce22-49f6-a7f0-1fed185b0567": "4.3 Root Cause Analysis", "0aea360c-5590-43b1-8aad-a71573d1e187": "The evaluation methods we mentioned earlier primarily rely on assessing LLMs\u2019 outputs. For instance, we pose questions to LLMs and evaluate them based on the accuracy of their responses. This evaluation approach allows us to quickly gauge the extent of a model\u2019s capabilities in various aspects and understand what it can help us accomplish. However, by solely examining the model\u2019s output, we cannot identify the root cause of why the model produces a particular response. When the model answers correctly, we cannot ascertain whether it genuinely possesses the corresponding ability or if it has simply encountered similar questions before and memorized the answers. Similarly, when the model\u2019s", "da755f86-8934-4355-b086-156085b7e640": "the answers. Similarly, when the model\u2019s response does not meet expectations, it is also challenging to determine why the model made an error. Therefore, we propose that future evaluation methods should include analyzing the root cause of model predictions. This will enable us to better analyze LLMs, facilitating the development of more useful LLMs in the future.", "3d3c8533-5e3f-424c-b974-dc106996e010": "4.4 Fine-grained LLM Agent Evaluation\n\nExisting benchmarks mostly rely on the final completion status of tasks, lacking fine-grained step-wise evaluations. Additionally, while current research focuses more on agents\u2019 capabilities in executing tasks within limited environments such as online-shopping, environmental feedback is often rule-based, simplistic, and distant from real-world scenarios. A potential future direction is to leverage high-intelligence models like LLM to design more realistic evaluation environments.\n\n\n\n\n4.5 Robot Benchmark Development", "774fdca2-4ffd-452b-b281-4186b165f56f": "Recent research in robotics primarily emphasizes the use of simulation environments to facilitate the transition to real-world applications. These environments are pivotal in enhancing the generalization capabilities of robots across various conditions. There is an increasing need to develop large-scale benchmarks, comparable to ImageNet in the field of computer vision, to rigorously assess these generalization abilities. Moreover, to accurately simulate real-world scenarios, it is essential to integrate specific tasks that mirror actual conditions. Additionally, the concept of a digital twin represents another promising avenue for evaluating robots in both simulated and real-world", "dbe66a6c-2aa3-47e2-ad40-0a4e9d591058": "robots in both simulated and real-world settings. Given the substantial disparities that still exist in computer vision when testing out-of-domain data, employing digital twins and similar methodologies could significantly reduce the sim-2-real gap, thereby enabling a more focused approach on evaluate models capabilities.", "5334dee8-9080-4fda-bf52-a85413383bd6": "Furthermore, detailed evaluations of other aspects, such as the sim-to-real gap, robustness against adversarial perturbations, human-robot collaboration, and multi-robot coordination, remain critical for deploying robots effectively in real-world scenarios. Lastly, as deep learning continues to demonstrate success with extensive data training, evaluating robot foundational models like RT-2 and PaLM-E will also be essential for advancing our understanding and application of robotics in complex environments.\n\n\n\n\n\n5 Conclusion", "6911e32f-93a3-4528-91d4-2265af0ae9fe": "Because of the inexplicability of LLMs, we need various evaluation methods to understand their capabilities, and this is the driving force behind the progress of LLMs. This study introduced the two-stage framework: from core ability to agent to evaluate the usability of LLMs. We reviewed applications, benchmarks, and evaluation methods in each section, aiming to elucidate the advantages and limitations of current LLM development. Lastly, we proposed several directions for the advancement of LLMs evaluation methods aimed at making future evaluations of LLMs more flexible, automated, and capable of identifying the root causes of issues. We look forward to future research making LLMs a more", "e1a82daf-1ae5-4f1f-8ade-1e6ae536e1e8": "forward to future research making LLMs a more useful tool for aiding human society.", "0a485a3e-5017-40ce-8057-3c8880399ed4": "Acknowledgements\n\n\nReferences\n\n\nAbdelghani et\u00a0al. (2023)\n\nRania Abdelghani, Yen-Hsiang Wang, Xingdi Yuan, Tong Wang, Pauline Lucas, H\u00e9l\u00e8ne Sauz\u00e9on, and Pierre-Yves Oudeyer. 2023.\n\n\nGpt-3-driven pedagogical agents to train children\u2019s curious question-asking skills.\n\n\nInternational Journal of Artificial Intelligence in Education, pages 1\u201336.\n\n\n\n\nAchiam et\u00a0al. (2023)\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia\u00a0Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et\u00a0al. 2023.\n\n\nGpt-4 technical report.\n\n\narXiv preprint arXiv:2303.08774.\n\n\n\n\nAgrawal et\u00a0al. (2022)", "f7c3fd3c-1a3a-43d9-b021-fa6135a9f1fb": "Agrawal et\u00a0al. (2022)\n\nMonica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim, and David Sontag. 2022.\n\n\nLarge language models are few-shot clinical information extractors.\n\n\narXiv preprint arXiv:2205.12689.\n\n\n\n\nAlvarado et\u00a0al. (2015)\n\nJulio Cesar\u00a0Salinas Alvarado, Karin Verspoor, and Timothy Baldwin. 2015.\n\n\nDomain adaption of named entity recognition to support credit risk assessment.\n\n\nIn Proceedings of the Australasian Language Technology Association Workshop 2015, pages 84\u201390.\n\n\n\n\nAthan et\u00a0al. (2013)\n\nTara Athan, Harold Boley, Guido Governatori, Monica Palmirani, Adrian Paschke, and Adam Wyner. 2013.\n\n\nOasis legalruleml.", "cae90d9f-da9f-4029-a071-b0f3c75c697f": "Oasis legalruleml.\n\n\nIn proceedings of the fourteenth international conference on artificial intelligence and law, pages 3\u201312.\n\n\n\n\nBalas et\u00a0al. (2024)\n\nMichael Balas, Jordan\u00a0Joseph Wadden, Philip\u00a0C H\u00e9bert, Eric Mathison, Marika\u00a0D Warren, Victoria Seavilleklein, Daniel Wyzynski, Alison Callahan, Sean\u00a0A Crawford, Parnian Arjmand, et\u00a0al. 2024.\n\n\nExploring the potential utility of ai large language models for medical ethics: an expert panel evaluation of gpt-4.\n\n\nJournal of Medical Ethics, 50(2):90\u201396.\n\n\n\n\nBanerjee and Lavie (2005)\n\nSatanjeev Banerjee and Alon Lavie. 2005.\n\n\nMeteor: An automatic metric for mt evaluation with improved correlation with human judgments.", "d7967f08-08c2-4c40-8e31-183589d27ced": "In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65\u201372.\n\n\n\n\nBang et\u00a0al. (2023)\n\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet\u00a0V. Do, Yan Xu, and Pascale Fung. 2023.\n\n\nA multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.\n\n\n\n\nBenoit (2023)\n\nJames\u00a0RA Benoit. 2023.\n\n\nChatgpt for clinical vignette generation, revision, and evaluation.\n\n\nMedRxiv, pages 2023\u201302.\n\n\n\n\nBhagavatula et\u00a0al. (2019)", "7066ff86-7ca7-466c-8a05-a732d9843f02": "Bhagavatula et\u00a0al. (2019)\n\nChandra Bhagavatula, Ronan\u00a0Le Bras, Chaitanya Malaviya, Keisuke Sakaguchi, Ari Holtzman, Hannah Rashkin, Doug Downey, Scott Wen-tau Yih, and Yejin Choi. 2019.\n\n\nAbductive commonsense reasoning.\n\n\narXiv preprint arXiv:1908.05739.\n\n\n\n\nBian et\u00a0al. (2024)\n\nNing Bian, Xianpei Han, Le\u00a0Sun, Hongyu Lin, Yaojie Lu, Ben He, Shanshan Jiang, and Bin Dong. 2024.\n\n\nChatgpt is a knowledgeable but inexperienced solver: An investigation of commonsense problem in large language models.\n\n\n\n\nBisk et\u00a0al. (2020)\n\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et\u00a0al. 2020.\n\n\nPiqa: Reasoning about physical commonsense in natural language.", "89f5bb70-fd32-4932-949f-cbea6a68d77f": "In Proceedings of the AAAI conference on artificial intelligence, volume\u00a034, pages 7432\u20137439.\n\n\n\n\nBlair-Stanek et\u00a0al. (2023)\n\nAndrew Blair-Stanek, Nils Holzenberger, and Benjamin Van\u00a0Durme. 2023.\n\n\nCan gpt-3 perform statutory reasoning?\n\n\nIn Proceedings of the Nineteenth International Conference on Artificial Intelligence and Law, pages 22\u201331.\n\n\n\n\nCai et\u00a0al. (2024)\n\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2024.\n\n\nLarge language models as tool makers.\n\n\n\n\nCarlini et\u00a0al. (2023)\n\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023.\n\n\nQuantifying memorization across neural language models.", "0bb45e9c-0a66-41a7-9b4f-5b457b11b4e5": "Casino et\u00a0al. (2022)\n\nFran Casino, Thomas\u00a0K Dasaklis, Georgios\u00a0P Spathoulas, Marios Anagnostopoulos, Amrita Ghosal, Istvan Borocz, Agusti Solanas, Mauro Conti, and Constantinos Patsakis. 2022.\n\n\nResearch trends, challenges, and emerging topics in digital forensics: A review of reviews.\n\n\nIEEE Access, 10:25464\u201325493.\n\n\n\n\nChang et\u00a0al. (2023)\n\nYupeng Chang, Xu\u00a0Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et\u00a0al. 2023.\n\n\nA survey on evaluation of large language models.\n\n\nACM Transactions on Intelligent Systems and Technology.\n\n\n\n\nChen et\u00a0al. (2020)\n\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Wang. 2020.", "8f5ad5e2-9dbe-4408-b78c-4a9b70773bd9": "Hybridqa: A dataset of multi-hop question answering over tabular and textual data.\n\n\narXiv preprint arXiv:2004.07347.\n\n\n\n\nChen et\u00a0al. (2022)\n\nZhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William\u00a0Yang Wang. 2022.\n\n\nConvfinqa: Exploring the chain of numerical reasoning in conversational finance question answering.\n\n\narXiv preprint arXiv:2210.03849.\n\n\n\n\nCifuentes et\u00a0al. (2022)\n\nJenny Cifuentes, Ana\u00a0Lucila Sandoval\u00a0Orozco, and Luis\u00a0Javier Garcia\u00a0Villalba. 2022.\n\n\nA survey of artificial intelligence strategies for automatic detection of sexually explicit videos.\n\n\nMultimedia Tools and Applications, 81(3):3205\u20133222.\n\n\n\n\nCobbe et\u00a0al. (2021)", "5792f677-5cad-4c58-a91b-40f5ea01e1c4": "Cobbe et\u00a0al. (2021)\n\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et\u00a0al. 2021.\n\n\nTraining verifiers to solve math word problems.\n\n\narXiv preprint arXiv:2110.14168.\n\n\n\n\nDalvi et\u00a0al. (2021)\n\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah Smith, Leighanna Pipatanangkura, and Peter Clark. 2021.\n\n\nExplaining answers with entailment trees.\n\n\narXiv preprint arXiv:2104.08661.\n\n\n\n\nDaniel et\u00a0al. (2008)\n\nGilles Daniel, Didier Sornette, and Peter Wohrmann. 2008.\n\n\nLook-ahead benchmark bias in portfolio performance evaluation.\n\n\narXiv preprint arXiv:0810.1922.", "f190de5e-73c2-4dce-8579-79557869333e": "arXiv preprint arXiv:0810.1922.\n\n\n\n\nDas et\u00a0al. (2024)\n\nBadhan\u00a0Chandra Das, M\u00a0Hadi Amini, and Yanzhao Wu. 2024.\n\n\nSecurity and privacy challenges of large language models: A survey.\n\n\narXiv preprint arXiv:2402.00888.\n\n\n\n\nDemszky et\u00a0al. (2023)\n\nDorottya Demszky, Diyi Yang, David\u00a0S Yeager, Christopher\u00a0J Bryan, Margarett Clapper, Susannah Chandhok, Johannes\u00a0C Eichstaedt, Cameron Hecht, Jeremy Jamieson, Meghann Johnson, et\u00a0al. 2023.\n\n\nUsing large language models in psychology.\n\n\nNature Reviews Psychology, 2(11):688\u2013701.\n\n\n\n\nDeng et\u00a0al. (2020)\n\nXiang Deng, Ahmed\u00a0Hassan Awadallah, Christopher Meek, Oleksandr Polozov, Huan Sun, and Matthew Richardson. 2020.", "532a7c54-4595-45b0-b35c-295176951d06": "Structure-grounded pretraining for text-to-sql.\n\n\narXiv preprint arXiv:2010.12773.\n\n\n\n\nDeng et\u00a0al. (2023)\n\nXiang Deng, Yu\u00a0Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, and Yu\u00a0Su. 2023.\n\n\nMind2web: Towards a generalist agent for the web.\n\n\n\n\nDeroy et\u00a0al. (2023)\n\nAniket Deroy, Kripabandhu Ghosh, and Saptarshi Ghosh. 2023.\n\n\nHow ready are pre-trained abstractive models and llms for legal case judgement summarization?\n\n\narXiv preprint arXiv:2306.01248.\n\n\n\n\nDijkstra et\u00a0al. (2022)\n\nRamon Dijkstra, Z\u00fclk\u00fcf Gen\u00e7, Subhradeep Kayal, Jaap Kamps, et\u00a0al. 2022.\n\n\nReading comprehension quiz generation using generative pre-trained transformers.\n\n\nIn iTextbooks@ AIED, pages 4\u201317.", "9eb45dfd-c5bf-4d6b-a84a-70f380690d56": "In iTextbooks@ AIED, pages 4\u201317.\n\n\n\n\nDu et\u00a0al. (2014)\n\nShichuan Du, Yong Tao, and Aleix\u00a0M Martinez. 2014.\n\n\nCompound facial expressions of emotion.\n\n\nProceedings of the national academy of sciences, 111(15):E1454\u2013E1462.\n\n\n\n\nDuan et\u00a0al. (2024)\n\nShitong Duan, Xiaoyuan Yi, Peng Zhang, Tun Lu, Xing Xie, and Ning Gu. 2024.\n\n\nDenevil: Towards deciphering and navigating the ethical values of large language models via instruction learning.\n\n\n\n\nDubois et\u00a0al. (2024)\n\nYann Dubois, Chen\u00a0Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\u00a0S Liang, and Tatsunori\u00a0B Hashimoto. 2024.\n\n\nAlpacafarm: A simulation framework for methods that learn from human feedback.", "bfa87940-ec4b-4a06-85a6-3f5a68f5cc0e": "Advances in Neural Information Processing Systems, 36.\n\n\n\n\nEngel and Mcadams (2024)\n\nChristoph Engel and Richard\u00a0H Mcadams. 2024.\n\n\nAsking gpt for the ordinary meaning of statutory terms.\n\n\nMPI Collective Goods Discussion Paper, (2024/5).\n\n\n\n\nFan et\u00a0al. (2019)\n\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. 2019.\n\n\nEli5: Long form question answering.\n\n\narXiv preprint arXiv:1907.09190.\n\n\n\n\nFeldman et\u00a0al. (2023)\n\nPhilip Feldman, James\u00a0R. Foulds, and Shimei Pan. 2023.\n\n\nTrapping llm hallucinations using tagged context prompts.\n\n\n\n\nGao et\u00a0al. (2023)\n\nDawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, and Jingren Zhou. 2023.", "323f8410-bc1e-4a11-9e9a-42e2c67f822d": "Text-to-sql empowered by large language models: A benchmark evaluation.\n\n\narXiv preprint arXiv:2308.15363.\n\n\n\n\nGeva et\u00a0al. (2021)\n\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021.\n\n\nDid aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.\n\n\nTransactions of the Association for Computational Linguistics, 9:346\u2013361.\n\n\n\n\nGhallab et\u00a0al. (2004)\n\nMalik Ghallab, Dana Nau, and Paolo Traverso. 2004.\n\n\nAutomated Planning: theory and practice.\n\n\nElsevier.\n\n\n\n\nGhosh et\u00a0al. (2024)\n\nShaona Ghosh, Prasoon Varshney, Erick Galinkin, and Christopher Parisien. 2024.", "13ae3ae3-08e2-4268-8e62-d4486f6df36e": "Aegis: Online adaptive ai content safety moderation with ensemble of llm experts.\n\n\n\n\nGuo et\u00a0al. (2023)\n\nZishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong, et\u00a0al. 2023.\n\n\nEvaluating large language models: A comprehensive survey.\n\n\narXiv preprint arXiv:2310.19736.\n\n\n\n\nHan et\u00a0al. (2023)\n\nSimon\u00a0J. Han, Keith Ransom, Andrew Perfors, and Charles Kemp. 2023.\n\n\nInductive reasoning in humans and large language models.\n\n\n\n\nHendrycks et\u00a0al. (2021)\n\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021.\n\n\nMeasuring mathematical problem solving with the math dataset.", "1bfc2afa-599b-453a-a40f-75bcf8c0763a": "arXiv preprint arXiv:2103.03874.\n\n\n\n\nHolzenberger et\u00a0al. (2020)\n\nNils Holzenberger, Andrew Blair-Stanek, and Benjamin Van\u00a0Durme. 2020.\n\n\nA dataset for statutory reasoning in tax law entailment and question answering.\n\n\narXiv preprint arXiv:2005.05257.\n\n\n\n\nHort et\u00a0al. (2021)\n\nMax Hort, Jie\u00a0M Zhang, Federica Sarro, and Mark Harman. 2021.\n\n\nFairea: A model behaviour mutation approach to benchmarking bias mitigation methods.\n\n\nIn Proceedings of the 29th ACM joint meeting on european software engineering conference and symposium on the foundations of software engineering, pages 994\u20131006.\n\n\n\n\nHu et\u00a0al. (2023)\n\nChenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, and Hang Zhao. 2023.", "05a3d0a7-de88-4be6-beae-67924292aedf": "Chatdb: Augmenting llms with databases as their symbolic memory.\n\n\n\n\nHuang and Chang (2023)\n\nJie Huang and Kevin Chen-Chuan Chang. 2023.\n\n\nTowards reasoning in large language models: A survey.\n\n\n\n\nHuang et\u00a0al. (2022a)\n\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022a.\n\n\nLanguage models as zero-shot planners: Extracting actionable knowledge for embodied agents.\n\n\nIn International Conference on Machine Learning, pages 9118\u20139147. PMLR.\n\n\n\n\nHuang et\u00a0al. (2023)\n\nWenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li\u00a0Fei-Fei. 2023.\n\n\nVoxposer: Composable 3d value maps for robotic manipulation with language models.\n\n\narXiv preprint arXiv:2307.05973.", "28f1a0e1-562b-4fcd-aaeb-428795c7f34f": "arXiv preprint arXiv:2307.05973.\n\n\n\n\nHuang et\u00a0al. (2022b)\n\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson, Igor Mordatch, Yevgen Chebotar, Pierre Sermanet, Noah Brown, Tomas Jackson, Linda Luu, Sergey Levine, Karol Hausman, and Brian Ichter. 2022b.\n\n\nInner monologue: Embodied reasoning through planning with language models.\n\n\nIn arXiv preprint arXiv:2207.05608.\n\n\n\n\nJi et\u00a0al. (2023)\n\nJiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce\u00a0Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2023.\n\n\nBeavertails: Towards improved safety alignment of llm via a human-preference dataset.\n\n\n\n\nJia et\u00a0al. (2021)", "c5ca27dd-7e96-4d12-a14f-3af4f4eee176": "Jia et\u00a0al. (2021)\n\nQinjin Jia, Jialin Cui, Yunkai Xiao, Chengyuan Liu, Parvez Rashid, and Edward\u00a0F Gehringer. 2021.\n\n\nAll-in-one: Multi-task learning bert models for evaluating peer assessments.\n\n\narXiv preprint arXiv:2110.03895.\n\n\n\n\nJiang et\u00a0al. (2024)\n\nChaoya Jiang, Wei Ye, Mengfan Dong, Hongrui Jia, Haiyang Xu, Ming Yan, Ji\u00a0Zhang, and Shikun Zhang. 2024.\n\n\nHal-eval: A universal and fine-grained hallucination evaluation framework for large vision language models.\n\n\narXiv preprint arXiv:2402.15721.\n\n\n\n\nJiang et\u00a0al. (2020)\n\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020.", "78a89652-0548-440e-bec9-d3005aa88ac7": "Hover: A dataset for many-hop fact extraction and claim verification.\n\n\n\n\nJin et\u00a0al. (2024)\n\nMingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong Zhang, Yongfeng Zhang, et\u00a0al. 2024.\n\n\nAttackeval: How to evaluate the effectiveness of jailbreak attacking on large language models.\n\n\narXiv preprint arXiv:2401.09002.\n\n\n\n\nKaramizadeh et\u00a0al. (2023)\n\nSasan Karamizadeh, Saman Shojae\u00a0Chaeikar, and Alireza Jolfaei. 2023.\n\n\nAdult content image recognition by boltzmann machine limited and deep learning.\n\n\nEvolutionary Intelligence, 16(4):1185\u20131194.\n\n\n\n\nKarinshak et\u00a0al. (2023)\n\nElise Karinshak, Sunny\u00a0Xun Liu, Joon\u00a0Sung Park, and Jeffrey\u00a0T Hancock. 2023.", "dc18b164-fffe-44b2-b74e-f9db7c2ebacb": "Working with ai to persuade: Examining a large language model\u2019s ability to generate pro-vaccination messages.\n\n\nProceedings of the ACM on Human-Computer Interaction, 7(CSCW1):1\u201329.\n\n\n\n\nKasneci et\u00a0al. (2023)\n\nEnkelejda Kasneci, Kathrin Se\u00dfler, Stefan K\u00fcchemann, Maria Bannert, Daryna Dementieva, Frank Fischer, Urs Gasser, Georg Groh, Stephan G\u00fcnnemann, Eyke H\u00fcllermeier, et\u00a0al. 2023.\n\n\nChatgpt for good? on opportunities and challenges of large language models for education.\n\n\nLearning and individual differences, 103:102274.\n\n\n\n\nKatz et\u00a0al. (2024)\n\nDaniel\u00a0Martin Katz, Michael\u00a0James Bommarito, Shang Gao, and Pablo Arredondo. 2024.\n\n\nGpt-4 passes the bar exam.", "088df868-daf9-4fae-ae73-861ea5bb690f": "Gpt-4 passes the bar exam.\n\n\nPhilosophical Transactions of the Royal Society A, 382(2270):20230254.\n\n\n\n\nKhan et\u00a0al. (2024)\n\nAkbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel\u00a0R Bowman, Tim Rockt\u00e4schel, and Ethan Perez. 2024.\n\n\nDebating with more persuasive llms leads to more truthful answers.\n\n\narXiv preprint arXiv:2402.06782.\n\n\n\n\nKim et\u00a0al. (2024a)\n\nMinju Kim, Heuiyeen Yeen, and Myoung-Wan Koo. 2024a.\n\n\nTowards context-based violence detection: A korean crime dialogue dataset.\n\n\nIn Findings of the Association for Computational Linguistics: EACL 2024, pages 603\u2013623.\n\n\n\n\nKim et\u00a0al. (2024b)", "b018149a-a434-46da-8577-a469ad6bcb63": "Kim et\u00a0al. (2024b)\n\nSiwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong\u00a0Joon Oh. 2024b.\n\n\nPropile: Probing privacy leakage in large language models.\n\n\nAdvances in Neural Information Processing Systems, 36.\n\n\n\n\nKosinski (2023)\n\nMichal Kosinski. 2023.\n\n\nTheory of mind may have spontaneously emerged in large language models.\n\n\narXiv preprint arXiv:2302.02083, 4:169.\n\n\n\n\nKumar (2023)\n\nArun\u00a0HS Kumar. 2023.\n\n\nAnalysis of chatgpt tool to assess the potential of its utility for academic writing in biomedical domain.\n\n\nBiology, Engineering, Medicine and Science Reports, 9(1):24\u201330.\n\n\n\n\nKung et\u00a0al. (2023)", "0c19cf39-4942-4406-8bf2-85d42eebc993": "Kung et\u00a0al. (2023)\n\nTiffany\u00a0H Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De\u00a0Leon, Camille Elepa\u00f1o, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, et\u00a0al. 2023.\n\n\nPerformance of chatgpt on usmle: potential for ai-assisted medical education using large language models.\n\n\nPLoS digital health, 2(2):e0000198.\n\n\n\n\nKwiatkowski et\u00a0al. (2019)\n\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et\u00a0al. 2019.\n\n\nNatural questions: a benchmark for question answering research.\n\n\nTransactions of the Association for Computational Linguistics, 7:453\u2013466.", "3a1d8ee2-bae3-477d-b227-15ef1482c44a": "Lee et\u00a0al. (2024)\n\nJean Lee, Nicholas Stevens, Soyeon\u00a0Caren Han, and Minseok Song. 2024.\n\n\nA survey of large language models in finance (finllms).\n\n\narXiv preprint arXiv:2402.02315.\n\n\n\n\nLi et\u00a0al. (2024a)\n\nLijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu\u00a0Qiao, and Jing Shao. 2024a.\n\n\nSalad-bench: A hierarchical and comprehensive safety benchmark for large language models.\n\n\n\n\nLi et\u00a0al. (2023a)\n\nMinghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023a.\n\n\nApi-bank: A comprehensive benchmark for tool-augmented llms.\n\n\n\n\nLi et\u00a0al. (2023b)\n\nYinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023b.", "8b1fcfa2-946e-498d-a8de-8de53ab413cd": "Large language models in finance: A survey.\n\n\nIn Proceedings of the Fourth ACM International Conference on AI in Finance, pages 374\u2013382.\n\n\n\n\nLi et\u00a0al. (2024b)\n\nZhen Li, Xiaohan Xu, Tao Shen, Can Xu, Jia-Chen Gu, and Chongyang Tao. 2024b.\n\n\nLeveraging large language models for nlg evaluation: A survey.\n\n\narXiv preprint arXiv:2401.07103.\n\n\n\n\nLiang et\u00a0al. (2023)\n\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2023.\n\n\nCode as policies: Language model programs for embodied control.\n\n\nIn 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 9493\u20139500. IEEE.\n\n\n\n\nLiang et\u00a0al. (2022)", "c3693ee2-d2c7-44ba-bc2d-e38572141bf9": "Liang et\u00a0al. (2022)\n\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et\u00a0al. 2022.\n\n\nHolistic evaluation of language models.\n\n\narXiv preprint arXiv:2211.09110.\n\n\n\n\nLiang et\u00a0al. (2019)\n\nYichan Liang, Jianheng Li, and Jian Yin. 2019.\n\n\nA new multi-choice reading comprehension dataset for curriculum learning.\n\n\nIn Asian Conference on Machine Learning, pages 742\u2013757. PMLR.\n\n\n\n\nLiga and Robaldo (2023)\n\nDavide Liga and Livio Robaldo. 2023.\n\n\nFine-tuning gpt-3 for legal rule classification.\n\n\nComputer Law & Security Review, 51:105864.\n\n\n\n\nLin (2004)\n\nChin-Yew Lin. 2004.", "d6c5dd4e-41d4-4567-b89c-d3fa674cbb25": "Lin (2004)\n\nChin-Yew Lin. 2004.\n\n\nRouge: A package for automatic evaluation of summaries.\n\n\nIn Text summarization branches out, pages 74\u201381.\n\n\n\n\nLin et\u00a0al. (2022)\n\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\n\n\nTruthfulqa: Measuring how models mimic human falsehoods.\n\n\n\n\nLin et\u00a0al. (2023)\n\nZi\u00a0Lin, Zihan Wang, Yongqi Tong, Yangkun Wang, Yuxin Guo, Yujia Wang, and Jingbo Shang. 2023.\n\n\nToxicchat: Unveiling hidden challenges of toxicity detection in real-world user-ai conversation.\n\n\n\n\nLiu et\u00a0al. (2024a)\n\nFang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang, Zhen Yang, and Li\u00a0Zhang. 2024a.\n\n\nExploring and evaluating hallucinations in llm-powered code generation.", "81df079a-9798-4743-a9d4-3e2432adbd5a": "arXiv preprint arXiv:2404.00971.\n\n\n\n\nLiu et\u00a0al. (2023)\n\nHanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. 2023.\n\n\nEvaluating the logical reasoning ability of chatgpt and gpt-4.\n\n\n\n\nLiu et\u00a0al. (2024b)\n\nXiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, and Chaowei Xiao. 2024b.\n\n\nAutomatic and universal prompt injection attacks against large language models.\n\n\narXiv preprint arXiv:2403.04957.\n\n\n\n\nLiu et\u00a0al. (2021a)\n\nXin Liu, Henglin Shi, Haoyu Chen, Zitong Yu, Xiaobai Li, and Guoying Zhao. 2021a.\n\n\nimigue: An identity-free video dataset for micro-gesture understanding and emotion analysis.", "537e0fe3-9ca5-485f-b4d7-b13585317bd5": "In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10631\u201310642.\n\n\n\n\nLiu et\u00a0al. (2021b)\n\nZhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li, and Jun Zhao. 2021b.\n\n\nFinbert: A pre-trained financial language representation model for financial text mining.\n\n\nIn Proceedings of the twenty-ninth international conference on international joint conferences on artificial intelligence, pages 4513\u20134519.\n\n\n\n\nLu et\u00a0al. (2024)\n\nHao Lu, Xuesong Niu, Jiyao Wang, Yin Wang, Qingyong Hu, Jiaqi Tang, Yuting Zhang, Kaishen Yuan, Bin Huang, Zitong Yu, et\u00a0al. 2024.\n\n\nGpt as psychologist? preliminary evaluations for gpt-4v on visual affective computing.", "04f8ec9d-65b0-4df4-ade1-b65b48efc07b": "arXiv preprint arXiv:2403.05916.\n\n\n\n\nMahowald et\u00a0al. (2023)\n\nKyle Mahowald, Anna\u00a0A Ivanova, Idan\u00a0A Blank, Nancy Kanwisher, Joshua\u00a0B Tenenbaum, and Evelina Fedorenko. 2023.\n\n\nDissociating language and thought in large language models: a cognitive perspective.\n\n\narXiv preprint arXiv:2301.06627.\n\n\n\n\nMaia et\u00a0al. (2018)\n\nMacedo Maia, Siegfried Handschuh, Andr\u00e9 Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018.\n\n\nWww\u201918 open challenge: financial opinion mining and question answering.\n\n\nIn Companion proceedings of the the web conference 2018, pages 1941\u20131942.\n\n\n\n\nMalo et\u00a0al. (2014)\n\nPekka Malo, Ankur Sinha, Pekka Korhonen, Jyrki Wallenius, and Pyry Takala. 2014.", "f00bb9bf-4b70-437b-8a90-b37a0a6351ac": "Good debt or bad debt: Detecting semantic orientations in economic texts.\n\n\nJournal of the Association for Information Science and Technology, 65(4):782\u2013796.\n\n\n\n\nMann et\u00a0al. (2020)\n\nBen Mann, N\u00a0Ryder, M\u00a0Subbiah, J\u00a0Kaplan, P\u00a0Dhariwal, A\u00a0Neelakantan, P\u00a0Shyam, G\u00a0Sastry, A\u00a0Askell, S\u00a0Agarwal, et\u00a0al. 2020.\n\n\nLanguage models are few-shot learners.\n\n\narXiv preprint arXiv:2005.14165.\n\n\n\n\nMarkov et\u00a0al. (2023)\n\nTodor Markov, Chong Zhang, Sandhini Agarwal, Tyna Eloundou, Teddy Lee, Steven Adler, Angela Jiang, and Lilian Weng. 2023.\n\n\nA holistic approach to undesired content detection in the real world.\n\n\n\n\nMavadati et\u00a0al. (2013)", "45a082cf-57e6-4e1e-9f7a-9a984ed0877b": "Mavadati et\u00a0al. (2013)\n\nS\u00a0Mohammad Mavadati, Mohammad\u00a0H Mahoor, Kevin Bartlett, Philip Trinh, and Jeffrey\u00a0F Cohn. 2013.\n\n\nDisfa: A spontaneous facial action intensity database.\n\n\nIEEE Transactions on Affective Computing, 4(2):151\u2013160.\n\n\n\n\nMenick et\u00a0al. (2022)\n\nJacob Menick, Maja Trebacz, Vladimir Mikulik, John Aslanides, Francis Song, Martin Chadwick, Mia Glaese, Susannah Young, Lucy Campbell-Gillingam, Geoffrey Irving, et\u00a0al. 2022.\n\n\nTeaching language models to support answers with verified quotes. arxiv.\n\n\n\n\nMiao et\u00a0al. (2023)\n\nNing Miao, Yee\u00a0Whye Teh, and Tom Rainforth. 2023.\n\n\nSelfcheck: Using llms to zero-shot check their own step-by-step reasoning.", "7c0b3bfb-8101-472f-9c56-8a8d40de6af1": "arXiv preprint arXiv:2308.00436.\n\n\n\n\nMihaylov et\u00a0al. (2018)\n\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018.\n\n\nCan a suit of armor conduct electricity? a new dataset for open book question answering.\n\n\nIn EMNLP.\n\n\n\n\nMin et\u00a0al. (2023)\n\nBonan Min, Hayley Ross, Elior Sulem, Amir Pouran\u00a0Ben Veyseh, Thien\u00a0Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. 2023.\n\n\nRecent advances in natural language processing via large pre-trained language models: A survey.\n\n\nACM Computing Surveys, 56(2):1\u201340.\n\n\n\n\nMoon et\u00a0al. (2014)\n\nSungrim Moon, Serguei Pakhomov, Nathan Liu, James\u00a0O Ryan, and Genevieve\u00a0B Melton. 2014.", "87ed7253-2713-45cb-8750-1f2088286a57": "A sense inventory for clinical abbreviations and acronyms created using clinical notes and medical dictionary resources.\n\n\nJournal of the American Medical Informatics Association, 21(2):299\u2013307.\n\n\n\n\nMuthukrishnan et\u00a0al. (2020)\n\nNikesh Muthukrishnan, Farhad Maleki, Katie Ovens, Caroline Reinhold, Behzad Forghani, Reza Forghani, et\u00a0al. 2020.\n\n\nBrief history of artificial intelligence.\n\n\nNeuroimaging Clinics of North America, 30(4):393\u2013399.\n\n\n\n\nNakano et\u00a0al. (2022)", "5dee00a3-3603-466c-a0e6-b8c083e458a3": "Nakano et\u00a0al. (2022)\n\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu\u00a0Jiang, Karl Cobbe, Tyna Eloundou, Gretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. 2022.\n\n\nWebgpt: Browser-assisted question-answering with human feedback.\n\n\n\n\nNayerifard et\u00a0al. (2023)\n\nTahereh Nayerifard, Haleh Amintoosi, Abbas\u00a0Ghaemi Bafghi, and Ali Dehghantanha. 2023.\n\n\nMachine learning in digital forensics: a systematic literature review.\n\n\narXiv preprint arXiv:2306.04965.\n\n\n\n\nNolfi (2023)\n\nStefano Nolfi. 2023.\n\n\nOn the unexpected abilities of large language models.", "dcb053b8-a3e9-446c-aec8-dd5aa9eebee6": "arXiv preprint arXiv:2308.09720.\n\n\n\n\nOviedo-Trespalacios et\u00a0al. (2023)\n\nOscar Oviedo-Trespalacios, Amy\u00a0E Peden, Thomas Cole-Hunter, Arianna Costantini, Milad Haghani, JE\u00a0Rod, Sage Kelly, Helma Torkamaan, Amina Tariq, James David\u00a0Albert Newton, et\u00a0al. 2023.\n\n\nThe risks of using chatgpt to obtain common safety-related information and advice.\n\n\nSafety science, 167:106244.\n\n\n\n\nPalmirani and Vitali (2011)\n\nMonica Palmirani and Fabio Vitali. 2011.\n\n\nAkoma-ntoso for legal documents.\n\n\nLegislative XML for the Semantic Web: Principles, Models, Standards for Document Management, pages 75\u2013100.\n\n\n\n\nPapineni et\u00a0al. (2002)\n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.", "221210bf-54be-4bde-9b4d-6758822209e3": "Bleu: a method for automatic evaluation of machine translation.\n\n\nIn Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\n\n\n\n\nParrish et\u00a0al. (2021)\n\nAlicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thompson, Phu\u00a0Mon Htut, and Samuel\u00a0R Bowman. 2021.\n\n\nBbq: A hand-built bias benchmark for question answering.\n\n\narXiv preprint arXiv:2110.08193.\n\n\n\n\nPatil et\u00a0al. (2023)\n\nShishir\u00a0G. Patil, Tianjun Zhang, Xin Wang, and Joseph\u00a0E. Gonzalez. 2023.\n\n\nGorilla: Large language model connected with massive apis.\n\n\n\n\nPetroni et\u00a0al. (2019)", "0d8e19f4-54fe-4183-98da-b9f3678d5cdc": "Petroni et\u00a0al. (2019)\n\nFabio Petroni, Tim Rockt\u00e4schel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander\u00a0H Miller, and Sebastian Riedel. 2019.\n\n\nLanguage models as knowledge bases?\n\n\narXiv preprint arXiv:1909.01066.\n\n\n\n\nPinar\u00a0Saygin et\u00a0al. (2000)\n\nAyse Pinar\u00a0Saygin, Ilyas Cicekli, and Varol Akman. 2000.\n\n\nTuring test: 50 years later.\n\n\nMinds and machines, 10(4):463\u2013518.\n\n\n\n\nQian et\u00a0al. (2023)\n\nCheng Qian, Chi Han, Yi\u00a0R. Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. 2023.\n\n\nCreator: Tool creation for disentangling abstract and concrete reasoning of large language models.\n\n\n\n\nQin et\u00a0al. (2023a)", "e2b4ea07-01a5-4a7c-98f8-b5830a12058f": "Qin et\u00a0al. (2023a)\n\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu\u00a0Han, Ning Ding, Huadong Wang, Ruobing Xie, Fanchao Qi, Zhiyuan Liu, Maosong Sun, and Jie Zhou. 2023a.\n\n\nWebcpm: Interactive web search for chinese long-form question answering.\n\n\n\n\nQin et\u00a0al. (2023b)", "7ee26c75-8b73-4c6c-83be-72a851b86635": "Qin et\u00a0al. (2023b)\n\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Yufei Huang, Chaojun Xiao, Chi Han, Yi\u00a0Ren Fung, Yusheng Su, Huadong Wang, Cheng Qian, Runchu Tian, Kunlun Zhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen Zhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi, Yuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong, Yaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan, Xu\u00a0Han, Xian Sun, Dahai Li, Jason Phang, Cheng Yang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and Maosong Sun. 2023b.\n\n\nTool learning with foundation models.\n\n\n\n\nQin et\u00a0al. (2023c)", "28b76743-9fa1-462b-b172-e7acba50c6e5": "Qin et\u00a0al. (2023c)\n\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023c.\n\n\nToolllm: Facilitating large language models to master 16000+ real-world apis.\n\n\n\n\nRadford et\u00a0al. (2018)\n\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et\u00a0al. 2018.\n\n\nImproving language understanding by generative pre-training.\n\n\n\n\nRadford et\u00a0al. (2019)\n\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et\u00a0al. 2019.\n\n\nLanguage models are unsupervised multitask learners.", "7ecdeb82-b51a-46bc-b4db-566d0b704cd5": "OpenAI blog, 1(8):9.\n\n\n\n\nRaina and Gales (2022)\n\nVatsal Raina and Mark Gales. 2022.\n\n\nMultiple-choice question generation: Towards an automated assessment framework.\n\n\narXiv preprint arXiv:2209.11830.\n\n\n\n\nRana et\u00a0al. (2023)\n\nKrishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian Reid, and Niko Suenderhauf. 2023.\n\n\nSayplan: Grounding large language models using 3d scene graphs for scalable task planning.\n\n\nIn 7th Annual Conference on Robot Learning.\n\n\n\n\nRathje et\u00a0al. (2023)\n\nSteve Rathje, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire Robertson, and Jay\u00a0J Van\u00a0Bavel. 2023.\n\n\nGpt is an effective tool for multilingual psychological text analysis.\n\n\n\n\nRen et\u00a0al. (2023)", "aa2fbd96-ee76-41dd-8c96-9381ca5d4884": "Ren et\u00a0al. (2023)\n\nAllen\u00a0Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, et\u00a0al. 2023.\n\n\nRobots that ask for help: Uncertainty alignment for large language model planners.\n\n\narXiv preprint arXiv:2307.01928.\n\n\n\n\nRuan et\u00a0al. (2023)\n\nJingqing Ruan, Yihong Chen, Bin Zhang, Zhiwei Xu, Tianpeng Bao, Guoqing Du, Shiwei Shi, Hangyu Mao, Ziyue Li, Xingyu Zeng, and Rui Zhao. 2023.\n\n\nTptu: Large language model-based ai agents for task planning and tool usage.\n\n\n\n\nSalehi and Burgue\u00f1o (2018)\n\nHadi Salehi and Rigoberto Burgue\u00f1o. 2018.\n\n\nEmerging artificial intelligence methods in structural engineering.", "90763949-ad14-4880-9c26-20a9bb81300c": "Engineering structures, 171:170\u2013189.\n\n\n\n\nScherrer et\u00a0al. (2023)\n\nNino Scherrer, Claudia Shi, Amir Feder, and David\u00a0M. Blei. 2023.\n\n\nEvaluating the moral beliefs encoded in llms.\n\n\n\n\nSchick et\u00a0al. (2023)\n\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023.\n\n\nToolformer: Language models can teach themselves to use tools.\n\n\n\n\nSemigran et\u00a0al. (2015)\n\nHannah\u00a0L Semigran, Jeffrey\u00a0A Linder, Courtney Gidengil, and Ateev Mehrotra. 2015.\n\n\nEvaluation of symptom checkers for self diagnosis and triage: audit study.\n\n\nbmj, 351.\n\n\n\n\nShah et\u00a0al. (2023)", "9bcc42ec-7774-42ee-a57e-793dfb85bee0": "bmj, 351.\n\n\n\n\nShah et\u00a0al. (2023)\n\nDhruv Shah, Michael\u00a0Robert Equi, B\u0142a\u017cej Osi\u0144ski, Fei Xia, Brian Ichter, and Sergey Levine. 2023.\n\n\nNavigation with large language models: Semantic guesswork as a heuristic for planning.\n\n\nIn Conference on Robot Learning, pages 2683\u20132699. PMLR.\n\n\n\n\nShah et\u00a0al. (2022)\n\nDhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. 2022.\n\n\nLM-nav: Robotic navigation with large pre-trained models of language, vision, and action.\n\n\nIn 6th Annual Conference on Robot Learning.\n\n\n\n\nShan and Deng (2018)\n\nLi\u00a0Shan and Weihong Deng. 2018.\n\n\nReliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition.", "eb3c6f41-8d99-4984-b4a0-093132ea5982": "IEEE Transactions on Image Processing, 28(1):356\u2013370.\n\n\n\n\nSharma and Thakur (2023)\n\nGaurav Sharma and Abhishek Thakur. 2023.\n\n\nChatgpt in drug discovery.\n\n\n\n\nShinn et\u00a0al. (2023)\n\nNoah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023.\n\n\nReflexion: Language agents with verbal reinforcement learning.\n\n\n\n\nSingh et\u00a0al. (2023)\n\nIshika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2023.\n\n\nProgprompt: Generating situated robot task plans using large language models.\n\n\nIn 2023 IEEE International Conference on Robotics and Automation (ICRA), pages 11523\u201311530. IEEE.", "da27cc26-7b3f-4a2c-8ea0-5c9ad646e003": "Sinha and Khandait (2021)\n\nAnkur Sinha and Tanmay Khandait. 2021.\n\n\nImpact of news on the commodity market: Dataset and results.\n\n\nIn Advances in Information and Communication: Proceedings of the 2021 Future of Information and Communication Conference (FICC), Volume 2, pages 589\u2013601. Springer.\n\n\n\n\nSong et\u00a0al. (2023a)\n\nChan\u00a0Hee Song, Jiaman Wu, Clayton Washington, Brian\u00a0M. Sadler, Wei-Lun Chao, and Yu\u00a0Su. 2023a.\n\n\nLlm-planner: Few-shot grounded planning for embodied agents with large language models.\n\n\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV).\n\n\n\n\nSong et\u00a0al. (2023b)", "0e1d87aa-f5b2-4058-8ff0-3a9f94fad13e": "Song et\u00a0al. (2023b)\n\nYifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke\u00a0Wang, Rong Yao, Ye\u00a0Tian, and Sujian Li. 2023b.\n\n\nRestgpt: Connecting large language models with real-world restful apis.\n\n\n\n\nSorensen et\u00a0al. (2024)\n\nTaylor Sorensen, Liwei Jiang, Jena\u00a0D Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, et\u00a0al. 2024.\n\n\nValue kaleidoscope: Engaging ai with pluralistic human values, rights, and duties.\n\n\nIn Proceedings of the AAAI Conference on Artificial Intelligence, volume\u00a038, pages 19937\u201319947.\n\n\n\n\nSrivastava et\u00a0al. (2022)", "abede939-2834-48a9-9673-0d0e6e461a4b": "Srivastava et\u00a0al. (2022)\n\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal\u00a0Md Shoeb, Abubakar Abid, Adam Fisch, Adam\u00a0R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et\u00a0al. 2022.\n\n\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models.\n\n\narXiv preprint arXiv:2206.04615.\n\n\n\n\nSrivastava et\u00a0al. (2023)", "3e46a035-94ad-4b14-a483-077259c0fd2d": "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal\u00a0Md Shoeb, Abubakar Abid, Adam Fisch, Adam\u00a0R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander\u00a0W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman\u00a0S. Iyer, Anders Andreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlm\u00fcller, Andrew Dai, Andrew La, Andrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul", "58d34680-b065-415a-8060-33b33603285e": "Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla Karaka\u015f, B.\u00a0Ryan Roberts, Bao\u00a0Sheng Loe, Barret Zoph, Bart\u0142omiej Bojanowski, Batuhan \u00d6zyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci, Bill\u00a0Yuchen Lin, Blake Howald, Bryan", "a6e5227c-f2d1-4616-af21-cadb0d23b6c9": "Orinion, Cameron Diao, Cameron Dour, Catherine Stinson, Cedrick Argueta, C\u00e9sar\u00a0Ferri Ram\u00edrez, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-Burch, Chris Waites, Christian Voigt, Christopher\u00a0D. Manning, Christopher Potts, Cindy Ramirez, Clara\u00a0E. Rivera, Clemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel\u00a0Mosegu\u00ed Gonz\u00e1lez, Danielle Perszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David Drakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz Yuret, Derek Chen,", "978b4f55-dac9-4d34-b44b-add468bf690a": "Emelin, Denis Kleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri\u00a0Coelho Mollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin\u00a0Dogus Cubuk, Elad Segal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang,", "fb0228b4-ced6-4d4a-a12d-9f173dc9f143": "Ethan\u00a0A. Chi, Ethan Dyer, Ethan Jerzak, Ethan Kim, Eunice\u00a0Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar, Fernando Mart\u00ednez-Plumed, Francesca Happ\u00e9, Francois Chollet, Frieda Rong, Gaurav Mishra, Genta\u00a0Indra Winata, Gerard de\u00a0Melo, Germ\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo Jaimovitch-L\u00f3pez, Gregor Betz, Guy Gur-Ari, Hana Galijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich Sch\u00fctze, Hiromu Yakura, Hongming Zhang, Hugh\u00a0Mee Wong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime\u00a0Fern\u00e1ndez Fisac, James\u00a0B. Simon, James Koppel,", "5d73a454-540a-4b78-a13e-ef4a7e934d6b": "Fisac, James\u00a0B. Simon, James Koppel, James Zheng, James Zou, Jan Koco\u0144, Jana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason Phang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy Kim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan", "00e0bb2e-3571-4a00-b31f-19ea45b08393": "Waweru, John Burden, John Miller, John\u00a0U. Balis, Jonathan Batchelder, Jonathan Berant, J\u00f6rg Frohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones, Joshua\u00a0B. Tenenbaum, Joshua\u00a0S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh\u00a0D. Dhole, Kevin Gimpel, Kevin Omondi, Kory Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li\u00a0Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis\u00a0Oliveros Col\u00f3n, Luke Metz, L\u00fctfi\u00a0Kerem", "845d450c-b554-41e3-aafd-512e1e85f719": "He, Luis\u00a0Oliveros Col\u00f3n, Luke Metz, L\u00fctfi\u00a0Kerem \u015eenel, Maarten Bosma, Maarten Sap, Maartje ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan, Marco Marelli, Marco Maru, Maria Jose\u00a0Ram\u00edrez Quintana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis, Martin Potthast, Matthew\u00a0L. Leavitt, Matthias Hagen, M\u00e1ty\u00e1s Schubert, Medina\u00a0Orduna", "707983e7-d53a-4279-a4d1-e8b2e1a9b802": "Baitemirova, Melody Arnaud, Melvin McElrath, Michael\u00a0A. Yee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Micha\u0142 Sw\u0119drowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac Suzgun, Mitch Walker, Mo\u00a0Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini, Mukund\u00a0Varma T, Nanyun Peng, Nathan\u00a0A. Chi, Nayeon Lee, Neta Gur-Ari Krakover, Nicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas Deckers, Niklas Muennighoff, Nitish\u00a0Shirish Keskar, Niveditha\u00a0S. Iyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans, Pablo Antonio\u00a0Moreno", "1afff266-c58b-4277-a2c2-7cda349c349e": "Omer Levy, Owain Evans, Pablo Antonio\u00a0Moreno Casares, Parth Doshi, Pascale Fung, Paul\u00a0Pu Liang, Paul Vicol, Pegah Alipoormolabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter Eckersley, Phu\u00a0Mon Htut, Pinyu Hwang, Piotr Mi\u0142kowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing Lyu, Qinlang Chen, Rabin Banjade, Rachel\u00a0Etta Rudolph, Raefer Gabriel, Rahel", "021438a9-7b61-4129-8a65-e7df339e047c": "Habacker, Ramon Risco, Rapha\u00ebl Milli\u00e8re, Rhythm Garg, Richard Barnes, Rif\u00a0A. Saurous, Riku Arakawa, Robbe Raymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan LeBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan Salakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif\u00a0M. Mohammad, Sajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel\u00a0R. Bowman, Samuel\u00a0S. Schoenholz, Sanghyun Han, Sanjeev Kwatra, Sarah\u00a0A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima", "cc82a082-6304-4d0b-938b-d63f4a2786c3": "Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang\u00a0Shane Gu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima, Debnath, Siamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha\u00a0Priscilla Makini, Soo-Hwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon, Stella Biderman, Stephanie Lin, Stephen", "8c90f61c-4e72-4c08-91f0-8fb462795143": "Prasad, Steven\u00a0T. Piantadosi, Stuart\u00a0M. Shieber, Summer Misherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu, Tariq Ali, Tatsu Hashimoto, Te-Lin Wu, Th\u00e9o Desbordes, Theodore Rothschild, Thomas Phan, Tianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Demberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh, Vinay\u00a0Uday Prabhu, Vishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout Vossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu", "b8b2673b-a2f9-4329-a235-d9224d6c33de": "Yadollah Yaghoobzadeh, Yair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu\u00a0Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang, Zijie\u00a0J. Wang, Zirui Wang, and Ziyi Wu. 2023.", "0e24f8e2-6515-4737-b54e-dcf7ed397acf": "Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.\n\n\n\n\nStaab et\u00a0al. (2023)\n\nRobin Staab, Mark Vero, Mislav Balunovi\u0107, and Martin Vechev. 2023.\n\n\nBeyond memorization: Violating privacy via inference with large language models.\n\n\narXiv preprint arXiv:2310.07298.\n\n\n\n\nStolfo et\u00a0al. (2023)\n\nAlessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Sch\u00f6lkopf, and Mrinmaya Sachan. 2023.\n\n\nA causal framework to quantify the robustness of mathematical reasoning with language models.\n\n\n\n\nSun et\u00a0al. (2023)\n\nHaotian Sun, Yuchen Zhuang, Lingkai Kong, Bo\u00a0Dai, and Chao Zhang. 2023.\n\n\nAdaplanner: Adaptive planning from feedback with language models.", "041b59db-1ea6-4cde-8c8e-c4faed89fa43": "In Advances in Neural Information Processing Systems, volume\u00a036, pages 58202\u201358245. Curran Associates, Inc.\n\n\n\n\nSun et\u00a0al. (2024)\n\nJiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, Yue Wu, Wenhai Wang, Junsong Chen, Zhangyue Yin, Xiaozhe Ren, Jie Fu, Junxian He, Wu\u00a0Yuan, Qi\u00a0Liu, Xihui Liu, Yu\u00a0Li, Hao Dong, Yu\u00a0Cheng, Ming Zhang, Pheng\u00a0Ann Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Ji-Rong Wen, Xipeng Qiu, Yike Guo, Hui Xiong, Qun Liu, and Zhenguo Li. 2024.\n\n\nA survey of reasoning with foundation models.\n\n\n\n\nSuzgun et\u00a0al. (2022)", "d5ee73c8-8acd-4330-9620-da2a0105bced": "Suzgun et\u00a0al. (2022)\n\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi\u00a0Tay, Hyung\u00a0Won Chung, Aakanksha Chowdhery, Quoc\u00a0V Le, Ed\u00a0H Chi, Denny Zhou, et\u00a0al. 2022.\n\n\nChallenging big-bench tasks and whether chain-of-thought can solve them.\n\n\narXiv preprint arXiv:2210.09261.\n\n\n\n\nTalmor et\u00a0al. (2018)\n\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018.\n\n\nCommonsenseqa: A question answering challenge targeting commonsense knowledge.\n\n\narXiv preprint arXiv:1811.00937.\n\n\n\n\nTang et\u00a0al. (2024)", "f9e6492c-0367-49c4-be60-5cd07b437d2f": "Tang et\u00a0al. (2024)\n\nLiyan Tang, Igor Shalyminov, Amy\u00a0Wing mei Wong, Jon Burnsky, Jake\u00a0W. Vincent, Yu\u2019an Yang, Siffi Singh, Song Feng, Hwanjun Song, Hang Su, Lijia Sun, Yi\u00a0Zhang, Saab Mansour, and Kathleen McKeown. 2024.\n\n\nTofueval: Evaluating hallucinations of llms on topic-focused dialogue summarization.\n\n\n\n\nTang et\u00a0al. (2023)\n\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le\u00a0Sun. 2023.\n\n\nToolalpaca: Generalized tool learning for language models with 3000 simulated cases.\n\n\n\n\nThirunavukarasu et\u00a0al. (2023)\n\nArun\u00a0James Thirunavukarasu, Darren Shu\u00a0Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting\u00a0Fang Tan, and Daniel Shu\u00a0Wei Ting. 2023.", "9345b1c1-2285-4605-96ec-076bf6dc2afe": "Large language models in medicine.\n\n\nNature medicine, 29(8):1930\u20131940.\n\n\n\n\nTobia (2020)\n\nKevin\u00a0P Tobia. 2020.\n\n\nTesting ordinary meaning.\n\n\nHarv. L. Rev., 134:726.\n\n\n\n\nTurpin et\u00a0al. (2023)\n\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel\u00a0R. Bowman. 2023.\n\n\nLanguage models don\u2019t always say what they think: Unfaithful explanations in chain-of-thought prompting.\n\n\n\n\nVaswani et\u00a0al. (2017)\n\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan\u00a0N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017.\n\n\nAttention is all you need.\n\n\nAdvances in neural information processing systems, 30.\n\n\n\n\nVermetten et\u00a0al. (2022)", "9c88caf6-46af-4bbc-9d43-94553556a33c": "Vermetten et\u00a0al. (2022)\n\nDiederick Vermetten, Bas van Stein, Fabio Caraffini, Leandro\u00a0L Minku, and Anna\u00a0V Kononova. 2022.\n\n\nBias: A toolbox for benchmarking structural bias in the continuous domain.\n\n\nIEEE Transactions on Evolutionary Computation, 26(6):1380\u20131393.\n\n\n\n\nVidgen et\u00a0al. (2024)", "e3b1ce9a-15c8-4d75-a308-e66c236c0207": "Bertie Vidgen, Adarsh Agrawal, Ahmed\u00a0M. Ahmed, Victor Akinwande, Namir Al-Nuaimi, Najla Alfaraj, Elie Alhajjar, Lora Aroyo, Trupti Bavalatti, Borhane Blili-Hamelin, Kurt Bollacker, Rishi Bomassani, Marisa\u00a0Ferrara Boston, Sim\u00e9on Campos, Kal Chakra, Canyu Chen, Cody Coleman, Zacharie\u00a0Delpierre Coudert, Leon Derczynski, Debojyoti Dutta, Ian Eisenberg, James Ezick, Heather Frase, Brian Fuller, Ram Gandikota, Agasthya Gangavarapu, Ananya Gangavarapu, James Gealy, Rajat Ghosh, James Goel, Usman Gohar, Sujata Goswami, Scott\u00a0A. Hale, Wiebke Hutiri, Joseph\u00a0Marvin Imperial, Surgan Jandial, Nick Judd, Felix Juefei-Xu, Foutse Khomh, Bhavya Kailkhura, Hannah\u00a0Rose Kirk, Kevin Klyman, Chris Knotz, Michael", "4cb351c4-8f27-42e1-87f1-d89fa5453097": "Kirk, Kevin Klyman, Chris Knotz, Michael Kuchnik, Shachi\u00a0H. Kumar, Chris Lengerich, Bo\u00a0Li, Zeyi Liao, Eileen\u00a0Peters Long, Victor Lu, Yifan Mai, Priyanka\u00a0Mary Mammen, Kelvin Manyeki, Sean McGregor, Virendra Mehta, Shafee Mohammed, Emanuel Moss, Lama Nachman, Dinesh\u00a0Jinenhally Naganna, Amin Nikanjam, Besmira Nushi, Luis Oala, Iftach Orr, Alicia Parrish, Cigdem", "76597455-0cba-402e-b881-b4a21fffb63e": "Patlak, William Pietri, Forough Poursabzi-Sangdeh, Eleonora Presani, Fabrizio Puletti, Paul R\u00f6ttger, Saurav Sahay, Tim Santos, Nino Scherrer, Alice\u00a0Schoenauer Sebag, Patrick Schramowski, Abolfazl Shahbazi, Vin Sharma, Xudong Shen, Vamsi Sistla, Leonard Tang, Davide Testuggine, Vithursan Thangarasa, Elizabeth\u00a0Anne Watkins, Rebecca Weiss, Chris Welty, Tyler Wilbers, Adina Williams, Carole-Jean Wu, Poonam Yadav, Xianjun Yang, Yi\u00a0Zeng, Wenhui Zhang, Fedor Zhdanov, Jiacheng Zhu, Percy Liang, Peter Mattson, and Joaquin Vanschoren. 2024.", "ee68c7e7-d917-4dba-8239-3a883a2f8682": "Introducing v0.5 of the ai safety benchmark from mlcommons.\n\n\n\n\nWang et\u00a0al. (2024a)\n\nJunyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji\u00a0Zhang, and Jitao Sang. 2024a.\n\n\nAmber: An llm-free multi-dimensional benchmark for mllms hallucination evaluation.\n\n\n\n\nWang et\u00a0al. (2024b)\n\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu\u00a0Chen, Yankai Lin, Wayne\u00a0Xin Zhao, Zhewei Wei, and Jirong Wen. 2024b.\n\n\nA survey on large language model based autonomous agents.\n\n\nFrontiers of Computer Science, 18(6).\n\n\n\n\nWang et\u00a0al. (2024c)", "5602e92a-014c-4e6c-9172-3d7947c13a8b": "Wang et\u00a0al. (2024c)\n\nSiyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei, and Xuanjing Huang. 2024c.\n\n\nBenchmark self-evolving: A multi-agent framework for dynamic llm evaluation.\n\n\narXiv preprint arXiv:2402.11443.\n\n\n\n\nWang et\u00a0al. (2018)\n\nSu\u00a0Wang, Greg Durrett, and Katrin Erk. 2018.\n\n\nModeling semantic plausibility by injecting world knowledge.\n\n\narXiv preprint arXiv:1804.00619.\n\n\n\n\nWang et\u00a0al. (2023a)\n\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah\u00a0A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2023a.\n\n\nSelf-instruct: Aligning language models with self-generated instructions.\n\n\n\n\nWang et\u00a0al. (2023b)", "626ccc46-7957-4a03-b85a-474db380c8c6": "Wang et\u00a0al. (2023b)\n\nZihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, and Yitao Liang. 2023b.\n\n\nDescribe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents.\n\n\narXiv preprint arXiv:2302.01560.\n\n\n\n\nWei et\u00a0al. (2024)\n\nJiaheng Wei, Yuanshun Yao, Jean-Francois Ton, Hongyi Guo, Andrew Estornell, and Yang Liu. 2024.\n\n\nMeasuring and reducing llm hallucination without gold-standard answers via expertise-weighting.\n\n\narXiv preprint arXiv:2402.10412.\n\n\n\n\nWessel et\u00a0al. (2023)\n\nMartin Wessel, Tom\u00e1s Horych, Terry Ruas, Akiko Aizawa, Bela Gipp, and Timo Spinde. 2023.", "60376609-8fe3-48c6-b9c1-3801d6aa6b46": "Introducing mbib-the first media bias identification benchmark task and dataset collection.\n\n\nIn Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 2765\u20132774.\n\n\n\n\nWeston et\u00a0al. (2015)\n\nJason Weston, Antoine Bordes, Sumit Chopra, Alexander\u00a0M Rush, Bart Van\u00a0Merri\u00ebnboer, Armand Joulin, and Tomas Mikolov. 2015.\n\n\nTowards ai-complete question answering: A set of prerequisite toy tasks.\n\n\narXiv preprint arXiv:1502.05698.\n\n\n\n\nWu and Aji (2023)\n\nMinghao Wu and Alham\u00a0Fikri Aji. 2023.\n\n\nStyle over substance: Evaluation biases for large language models.\n\n\n\n\nWu et\u00a0al. (2023)", "edc435e8-2eb8-4de9-b13f-00c956ac3d9d": "Wu et\u00a0al. (2023)\n\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023.\n\n\nBloomberggpt: A large language model for finance.\n\n\narXiv preprint arXiv:2303.17564.\n\n\n\n\nXie et\u00a0al. (2023)\n\nQianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023.\n\n\nPixiu: A large language model, instruction data and evaluation benchmark for finance.\n\n\narXiv preprint arXiv:2306.05443.\n\n\n\n\nXu et\u00a0al. (2023a)\n\nFangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria. 2023a.", "3d93cddb-b9fa-4b89-b178-1b60704a2dcf": "Are large language models really good logical reasoners? a comprehensive evaluation and beyond.\n\n\n\n\nXu et\u00a0al. (2023b)\n\nGuohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, Ji\u00a0Zhang, Chao Peng, Fei Huang, and Jingren Zhou. 2023b.\n\n\nCvalues: Measuring the values of chinese large language models from safety to responsibility.\n\n\n\n\nXu et\u00a0al. (2023c)\n\nQiantong Xu, Fenglu Hong, Bo\u00a0Li, Changran Hu, Zhengyu Chen, and Jian Zhang. 2023c.\n\n\nOn the tool manipulation capability of open-source large language models.\n\n\n\n\nYan et\u00a0al. (2024a)\n\nBiwei Yan, Kun Li, Minghui Xu, Yueyan Dong, Yue Zhang, Zhaochun Ren, and Xiuzhen Cheng. 2024a.", "9082df48-6fcf-4b64-868a-798a4438e9ed": "On protecting the data privacy of large language models (llms): A survey.\n\n\n\n\nYan et\u00a0al. (2024b)\n\nFanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir\u00a0G. Patil, Ion Stoica, and Joseph\u00a0E. Gonzalez. 2024b.\n\n\nBerkeley function calling leaderboard.\n\n\n\n\nYan et\u00a0al. (2014)\n\nWen-Jing Yan, Xiaobai Li, Su-Jing Wang, Guoying Zhao, Yong-Jin Liu, Yu-Hsin Chen, and Xiaolan Fu. 2014.\n\n\nCasme ii: An improved spontaneous micro-expression database and the baseline evaluation.\n\n\nPloS one, 9(1):e86041.\n\n\n\n\nYang et\u00a0al. (2023)\n\nShiping Yang, Renliang Sun, and Xiaojun Wan. 2023.\n\n\nA new benchmark and reverse validation method for passage-level hallucination detection.\n\n\n\n\nYang et\u00a0al. (2018)", "77bab743-a9f6-43de-b14e-f65897a32587": "Yang et\u00a0al. (2018)\n\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William\u00a0W Cohen, Ruslan Salakhutdinov, and Christopher\u00a0D Manning. 2018.\n\n\nHotpotqa: A dataset for diverse, explainable multi-hop question answering.\n\n\narXiv preprint arXiv:1809.09600.\n\n\n\n\nYao et\u00a0al. (2023a)\n\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2023a.\n\n\nWebshop: Towards scalable real-world web interaction with grounded language agents.\n\n\n\n\nYao et\u00a0al. (2023b)\n\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023b.\n\n\nReact: Synergizing reasoning and acting in language models.\n\n\n\n\nYao et\u00a0al. (2024)", "344d7640-42cf-4927-882d-022407170f5b": "Yao et\u00a0al. (2024)\n\nYifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang. 2024.\n\n\nA survey on large language model (llm) security and privacy: The good, the bad, and the ugly.\n\n\nHigh-Confidence Computing, page 100211.\n\n\n\n\nYip et\u00a0al. (2024)\n\nDaniel\u00a0Wankit Yip, Aysan Esmradi, and Chun\u00a0Fai Chan. 2024.\n\n\nA novel evaluation framework for assessing resilience against prompt injection attacks in large language models.\n\n\n\n\nYoung et\u00a0al. (2022)\n\nNathan Young, Qiming Bao, Joshua Bensemann, and Michael Witbrock. 2022.\n\n\nAbductionrules: Training transformers to explain unexpected inputs.\n\n\narXiv preprint arXiv:2203.12186.\n\n\n\n\nYu et\u00a0al. (2023)", "de7d48a1-9d89-4fed-9d7d-7ca4cc909a73": "Yu et\u00a0al. (2023)\n\nWenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez\u00a0Arenas, Hao-Tien Lewis\u00a0Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, Brian Ichter, Ted Xiao, Peng Xu, Andy Zeng, Tingnan Zhang, Nicolas Heess, Dorsa Sadigh, Jie Tan, Yuval Tassa, and Fei Xia. 2023.\n\n\nLanguage to rewards for robotic skill synthesis.\n\n\nArxiv preprint arXiv:2306.08647.\n\n\n\n\nYuan et\u00a0al. (2023)\n\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. 2023.\n\n\nHow well do large language models perform in arithmetic tasks?\n\n\n\nYuan et\u00a0al. (2024)\n\nZhuowen Yuan, Zidi Xiong, Yi\u00a0Zeng, Ning Yu, Ruoxi Jia, Dawn Song, and Bo\u00a0Li. 2024.", "e64d762a-1ddf-4d87-96d1-adea0be8f78d": "Rigorllm: Resilient guardrails for large language models against undesired content.\n\n\n\n\nZhan et\u00a0al. (2024)\n\nQiusi Zhan, Zhixiang Liang, Zifan Ying, and Daniel Kang. 2024.\n\n\nInjecagent: Benchmarking indirect prompt injections in tool-integrated large language model agents.\n\n\narXiv preprint arXiv:2403.02691.\n\n\n\n\nZhang et\u00a0al. (2023)\n\nJizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023.\n\n\nIs chatgpt fair for recommendation? evaluating fairness in large language model recommendation.\n\n\nIn Proceedings of the 17th ACM Conference on Recommender Systems, RecSys \u201923. ACM.\n\n\n\n\nZhang et\u00a0al. (2024a)\n\nKechi Zhang, Jia Li, Ge\u00a0Li, Xianjie Shi, and Zhi Jin. 2024a.", "c6b4c06b-1542-4db3-b3ea-0f97cd974b26": "Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges.\n\n\n\n\nZhang et\u00a0al. (2024b)\n\nXiaoying Zhang, Baolin Peng, Ye\u00a0Tian, Jingyan Zhou, Lifeng Jin, Linfeng Song, Haitao Mi, and Helen Meng. 2024b.\n\n\nSelf-alignment for factuality: Mitigating hallucinations in llms via self-evaluation.\n\n\narXiv preprint arXiv:2402.09267.\n\n\n\n\nZhang and Yang (2023)\n\nXuanyu Zhang and Qing Yang. 2023.\n\n\nXuanyuan 2.0: A large chinese financial chat model with hundreds of billions parameters.\n\n\nIn Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pages 4435\u20134439.\n\n\n\n\nZhang et\u00a0al. (2018)", "818609b0-01e9-4cb2-954a-b0a662747101": "Zhang et\u00a0al. (2018)\n\nYuyu Zhang, Hanjun Dai, Zornitsa Kozareva, Alexander\u00a0J Smola, and Le\u00a0Song. 2018.\n\n\nVariational reasoning for question answering with knowledge graph.\n\n\nIn AAAI.\n\n\n\n\nZhao et\u00a0al. (2023)\n\nGuoying Zhao, Xiaobai Li, Yante Li, and Matti Pietik\u00e4inen. 2023.\n\n\nFacial micro-expressions: an overview.\n\n\nProceedings of the IEEE.\n\n\n\n\nZheng et\u00a0al. (2023a)\n\nDuo Zheng, Shijia Huang, Lin Zhao, Yiwu Zhong, and Liwei Wang. 2023a.\n\n\nTowards learning a generalist model for embodied navigation.\n\n\n\n\nZheng et\u00a0al. (2024)\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi\u00a0Lin, Zhuohan Li, Dacheng Li, Eric Xing, et\u00a0al. 2024.", "19242e3a-657a-411b-9a06-ba23097f2439": "Judging llm-as-a-judge with mt-bench and chatbot arena.\n\n\nAdvances in Neural Information Processing Systems, 36.\n\n\n\n\nZheng et\u00a0al. (2023b)\n\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang. 2023b.\n\n\nWhy does chatgpt fall short in providing truthful answers?\n\n\n\nZhou et\u00a0al. (2023a)\n\nGengze Zhou, Yicong Hong, and Qi\u00a0Wu. 2023a.\n\n\nNavgpt: Explicit reasoning in vision-and-language navigation with large language models.\n\n\narXiv preprint arXiv:2305.16986.\n\n\n\n\nZhou et\u00a0al. (2023b)\n\nShuyan Zhou, Frank\u00a0F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2023b.", "07264c64-bded-4e44-9e22-345e30d8580a": "Webarena: A realistic web environment for building autonomous agents.\n\n\n\n\nZielinski et\u00a0al. (2023)\n\nChris Zielinski, Margaret Winker, Rakesh Aggarwal, Lorraine Ferris, Markus Heinemann, Jose\u00a0Florencio Lape\u00f1a\u00a0Jr, Sanjay Pai, Edsel Ing, Leslie Citrome, et\u00a0al. 2023.\n\n\nWame recommendations on chatgpt and chatbots in relation to scholarly publications.\n\n\n\n\n\n\n\n\n\nGenerated  on Mon Jun  3 02:19:28 2024 by LaTeXML", "a02b95f8-b925-4914-a10f-6d39facfd820": "LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 Introduction\n\n2 PRELIMINARIES\n\n2.1 Evaluation Function E\ud835\udc38Eitalic_E\n\n2.2 Evaluation Input\n\n2.2.1 Evaluation Type \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T\n2.2.2 Evaluation Criteria \ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C.\n2.2.3 Evaluation References \u211b\u211b\\mathcal{R}caligraphic_R.\n\n\n2.3 Evaluation Output\n\n\n\n3 Functionality\n\n\n3.1 Performance Evaluation\n\n3.1.1 Responses Evaluation\n3.1.2 Model Evaluation\n\n\n\n3.2 Model Enhancement\n\n3.2.1 Reward Modeling During Training\n3.2.2 Acting as Verifier During Inference\n3.2.3 Feedback for Refinement\n\n\n\n3.3 Data Construction\n\n3.3.1 Data Annotation\n3.3.2 Data Synthesize\n\n\n\n\n\n4 Methodology", "9f98a852-5cae-42fe-ad9b-fa46e3934c96": "4 Methodology\n\n\n4.1 Single-LLM System\n\n4.1.1 Prompt-based\n4.1.2 Tuning-based\n4.1.3 Post-processing\n\n\n\n4.2 Multi-LLM System\n\n4.2.1 Communication\n4.2.2 Aggregation\n\n\n4.3 Human-AI Collaboration System\n\n\n\n5 Application\n\n5.1 General\n5.2 Multimodal\n5.3 Medical\n5.4 Legal\n5.5 Financial\n5.6 Education\n5.7 Information Retrieval\n\n5.8 Others\n\n5.8.1 Soft Engineering\n5.8.2 Biology\n5.8.3 Social Science\n\n\n\n\n\n6 Meta-evaluation\n\n\n6.1 Benchmarks\n\n6.1.1 Code Generation\n6.1.2 Machine Translation\n6.1.3 Text Summarization\n6.1.4 Dialogue Generation\n6.1.5 Automatic Story Generation\n6.1.6 Values Alignment\n6.1.7 Recommendation\n6.1.8 Search\n6.1.9 Comprehensive Data\n\n\n\n6.2 Metric", "8c80037a-14e2-4fb8-8c4e-fdbd0bc1be81": "6.2 Metric\n\n6.2.1 Accuracy\n6.2.2 Pearson Correlation Coefficient\n6.2.3 Spearman\u2019s Rank Correlation Coefficient\n6.2.4 Kendall\u2019s Tau\n6.2.5 Cohen\u2019s Kappa\n6.2.6 Intraclass Correlation Coefficient (ICC)\n\n\n\n\n\n7 Limitation\n\n\n7.1 Biases\n\n7.1.1 Presentation-Related Biases\n7.1.2 Social-Related Biases\n7.1.3 Content-Related Biases\n7.1.4 Cognitive-Related Biases\n\n\n\n7.2 Adversarial Attacks\n\n7.2.1 Adversarial Attacks on LLMs\n7.2.2 Adversarial Attacks on LLMs-as-judges\n\n\n\n7.3 Inherent Weaknesses\n\n7.3.1 Knowledge Recency\n7.3.2 Hallucination\n7.3.3 Domain-Specific Knowledge Gaps\n\n\n\n\n\n8 Future Work\n\n\n8.1 More Efficient LLMs-as-Judges", "6bde3175-2312-479a-a59b-826e3862aee6": "8.1 More Efficient LLMs-as-Judges\n\n8.1.1 Automated Construction of Evaluation Criteria and Tasks\n8.1.2 Scalable Evaluation Systems\n8.1.3 Accelerating Evaluation Processes\n\n\n\n8.2 More Effective LLMs-as-Judges\n\n8.2.1 Integration of Reasoning and Judge Capabilities\n8.2.2 Establishing a Collective Judgment Mechanism\n8.2.3 Enhancing Domain Knowledge\n8.2.4 Cross-Domain and Cross-Language Transferability\n8.2.5 Multimodal Integration Evaluation\n\n\n\n8.3 More Reliable LLMs-as-Judges\n\n8.3.1 Enhancing Interpretability and Transparency\n8.3.2 Mitigating Bias and Ensuring Fairness\n8.3.3 Enhancing Robustness\n\n\n\n\n9 Conclusion\n\n\n\n\n\nLLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods", "601621d1-8606-47b4-a4b0-d00e7874a67b": "Haitao Li\n\nliht22@mails.tsinghua.edu.cn\n\nDepartment of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua UniversityBeijingChina\n\n,\u00a0\nQian Dong\n\ndq22@mails.tsinghua.edu.cn\n\nDepartment of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua UniversityBeijingChina\n\n,\u00a0\nJunjie Chen\n\nchenjj826@gmail.com\n\nDepartment of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua UniversityBeijingChina\n\n,\u00a0\nHuixue Su\n\nsuhuixue@ruc.edu.cn\n\nGaoling School of Artificial Intelligence, Renmin University of ChinaBeijingChina\n\n,\u00a0\nYujia Zhou\n\nsuhuixue@ruc.edu.cn", "5643df79-ec7a-4e41-8248-9b8b142c5da0": ",\u00a0\nYujia Zhou\n\nsuhuixue@ruc.edu.cn\n\nDepartment of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua UniversityBeijingChina\n\n,\u00a0\nQingyao Ai\n\naiqy@tsinghua.edu.cn\n\nDepartment of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua UniversityBeijingChina\n\n,\u00a0\nZiyi Ye\n\nyeziyi1998@gmail.com\n\nDepartment of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua UniversityBeijingChina\n\n\u00a0and\u00a0\nYiqun Liu\n\nyiqunliu@tsinghua.edu.cn\n\nDepartment of Computer Science and Technology, Institute for Internet Judiciary, Tsinghua UniversityBeijingChina", "bd20687b-34cd-475a-9304-74f044036470": "Abstract.\nThe rapid advancement of Large Language Models (LLMs) has driven their expanding application across various fields.\nOne of the most promising applications is their role as evaluators based on natural language responses, referred to as \u201cLLMs-as-judges\u201d.\nThis framework has attracted growing attention from both academia and industry due to their excellent effectiveness, ability to generalize across tasks, and interpretability in the form of natural language.\nThis paper presents a comprehensive survey of the LLMs-as-judges paradigm from five key perspectives: Functionality, Methodology, Applications, Meta-evaluation, and Limitations.", "77fb0b64-a7b3-4ec1-a196-bf016f4f6c3c": "We begin by providing a systematic definition of LLMs-as-Judges and introduce their functionality\u00a0(Why use LLM judges?).\nThen we address methodology to construct an evaluation system with LLMs\u00a0(How to use LLM judges?).\nAdditionally, we investigate the potential domains for their application (Where to use LLM judges?) and discuss methods for evaluating them in various contexts\u00a0(How to evaluate LLM judges?).\nFinally, we provide a detailed analysis of the limitations of LLM judges and discuss potential future directions.", "21d19286-0fe1-4179-9ed2-5253627810d4": "Through a structured and comprehensive analysis, we aim aims to provide insights on the development and application of LLMs-as-judges in both research and practice. We will continue to maintain the relevant resource list at https://github.com/CSHaitao/Awesome-LLMs-as-Judges.", "56ead149-c44b-4ffc-863d-21048d5bfeb6": "Large Language Models, Evaluation, LLMs-as-Judges\n\n\n\n1. Introduction", "2daa7019-deaa-4b27-8324-40ca4e451d41": "Studies on evaluation methods have long been a key force in guiding the development of modern Artificial Intelligence (AI)\u00a0(Chang et\u00a0al., 2024).\nAI researchers have continuously sought to measure and validate the intelligence of AI models through various tasks\u00a0(Chang et\u00a0al., 2024; Guo et\u00a0al., 2023).\nIn the mid-20th century, AI evaluation primarily centered on assessing algorithm performance in specific tasks, such as logical reasoning and numerical computation\u00a0(Nilsson, 2014).\nTraditional machine learning tasks like classification and regression often use programmable and statistical metrics, including accuracy, precision, and recall.", "33373841-18d5-4fb4-af57-27370966775f": "With the emergence of deep learning, the complexity of AI systems grew rapidly, prompting a shift in evaluation standards\u00a0(LeCun et\u00a0al., 2015).\nThe evaluation of AI has expanded from pre-defined, programmable machine metrics to more flexible, robust evaluators for solving complex, realistic tasks.\nA typical example is the Turing Test\u00a0(French, 2000; Turing, 2009), which determines whether an AI model can exhibit human-like intelligent behavior through dialogue with humans.\nThe Turing Test provides a fundamental guideline in the evaluation of AI models, especially on AI models\u2019 intelligence in flexible and realistic environments.", "4c0af97e-e485-4cbf-a72c-7cfb56a18a07": "Recently, the emergence of Large Language Models (LLMs) and generative AI serves as a new milestone in the evolution of AI evaluation.\nLLMs exhibit remarkable generalization and adaptability, showcasing strong transfer capabilities across previously unseen tasks and diverse domains\u00a0(Achiam et\u00a0al., 2023; Bai et\u00a0al., 2023). However, their powerful capabilities also present new challenges for evaluation.\nDue to the highly generative and open-ended nature of their outputs, standardized metrics are often insufficient for a comprehensive evaluation.", "3b1b0908-3a83-48eb-81d6-721a09f1296d": "For example, in natural language generation (NLG) tasks, traditional metrics like BLEU\u00a0(Papineni et\u00a0al., 2002) and ROUGE\u00a0(Lin, 2004) often fail to capture key aspects such as text fluency, logical coherence, and creativity. Moreover, modern AI evaluation extends beyond task performance and must account for the ability to address complex, dynamic problems in real-world scenarios, including robustness, fairness, and interpretability.", "32244c54-3144-453d-9111-755447577020": "Human annotations, frequently regarded as the \u201cground truth,\u201d can offer comprehensive insights and valuable feedback. By gathering responses from experts or users, researchers can gain a deeper understanding of a model\u2019s performance, practicality, and potential risks. However, collecting them are typically time-consuming and resource-intensive, making it challenging to scale up for large-scale evaluation.", "92e78bf5-c8bd-414f-87de-dac2a0651f5a": "In this context, a new paradigm has emerged to replace humans and statistical metrics with LLMs in evaluation, referred to as LLMs-as-judges\u00a0(Ashktorab et\u00a0al., 2024; Tseng et\u00a0al., 2024; Bavaresco et\u00a0al., 2024, 2024).\nCompared to traditional evaluation methods, LLMs-as-judges show significant strengths.\nFirst, LLM judges can adjust their evaluation criteria based on the specific task context, rather than relying on a fixed set of metrics, making the evaluation process more flexible and refined.", "e87c623e-533e-418a-b49a-e4d700de2e36": "Second, LLM judges can generate interpretive evaluations, offering more comprehensive feedback on model performance and enabling researchers to gain deeper insights into the evaluater\u2019s strengths and weaknesses.\nFinally, LLM judges offer a scalable and reproducible alternative to human evaluation, significantly reducing the costs and time associated with human involvement.", "88533952-345b-4e1b-b495-5534cd793cef": "Despite its great potential and significant advantages, LLMs-as-judges also face several critical challenges.\nFor example, the evaluation results of LLMs are often influenced by the prompt template, which can lead to biased or inconsistent assessments\u00a0(Xu et\u00a0al., 2023a).\nConsidering that LLMs are trained on extensive text corpus, they may also inherit various implicit biases, impacting the fairness and reliability of their assessments\u00a0(Ye et\u00a0al., 2024b).\nMoreover, distinct tasks and domains require specific evaluation criteria, making it difficult for LLMs to adapt their standards dynamically to specific contexts.", "28410d68-56c6-4fb1-8676-9c527eec053c": "Considering the vast potential of this field,\nthis survey aims to systematically review and analyze the current state and key challenges of the LLMs-as-judges.\nAs shown in Figures 1 and 2, we discuss existing research across five key perspectives: 1) Functionality: Why use LLM judges, 2) Methodology: How to use LLM judges, 3) Application: Where to use LLM judges, 4) Meta-evaluation: How to evaluate LLM judges and 5) Limitation: Existing problems of LLM judges. We explore the key challenges confronting LLMs-as-judges and hope to provide a clearer guideline for their future development.\n\n\nIn summary, the main contributions of this paper are as follows:\n\n\n\n\n(1)", "67011c04-5033-43ac-b089-19f2cfdbe177": "(1)\n\nComprehensive and Timely Survey: We present the extensive survey on the emerging paradigm of LLMs-as-judges, systematically reviewing the current state of research and developments in this field. By examining LLMs as performance evaluators based on their generated natural language, we highlight the unique role of LLMs in shaping the future of AI evaluation.\n\n\n\n(2)", "b13f3cc8-f374-4105-971a-ec5c142a82d4": "(2)\n\nSystematic Analysis Across Five Key Perspectives: We organize our survey around five critical aspects: Functionality, Methodology, Application, Meta-evaluation, and Limitation. This structured approach allows for a nuanced understanding of how and why LLMs are utilized as evaluators, their practical implementations, and reliability concerns.\n\n\n\n(3)", "26761e5e-4704-43b3-977f-cd050b3b2926": "(3)\n\nCurrent Challenges and Future Research Directions: We discuss the existing challenges for adopting LLMs-as-judges, highlighting potential research opportunities and directions while offering a forward-looking perspective on the future development of this paradigm, encouraging researchers to delve deeper into this exciting area. We also provide an open-source repository at https://github.com/CSHaitao/Awesome-LLMs-as-Judges, with the goal of fostering a collaborative community and advancing best practices in this area.", "93a6ba0f-1260-49d5-89f8-25098cfea53b": "The organization of this paper is as follows. In Section (\u00a72), we provide the formal definition of LLMs-as-judges. Then, Section (\u00a73) reviews existing work from the perspective of \u201cWhy use LLM judges\u201d. Following that, Section (\u00a74) covers \u201cHow to use LLM judges\u201d, summarizing the current technical developments in LLMs-as-judges. Section (\u00a75) discusses \u201cWhere to use LLM judges\u201d, focusing on their application domains. In Section (\u00a76), we review the metrics and benchmarks used for evaluating LLMs-as-judges. Section (\u00a77) discusses the limitations and challenges of LLM judges. We discuss major future work in Sections (\u00a78) and (\u00a79) to conclude the paper.", "cf0ae808-5a4c-4639-b463-c6e741f7eb45": "{forest}\nforked edges,\nfor tree=\ngrow=east,\nreversed=true,\nanchor=base west,\nparent anchor=east,\nchild anchor=west,\nbase=left,\nfont=,\nrectangle,\ndraw=hidden-draw,\nrounded corners,\nalign=left,\nminimum width=4em,\nedge+=darkgray, line width=1pt,\ns sep=3pt,\ninner xsep=2pt,\ninner ysep=3pt,\nver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,\n,\nwhere level=1text width=6.4em,font=,,\nwhere level=2text width=6.4em,font=,,\nwhere level=3text width=6.4em,font=,,\nwhere level=4text width=6.4em,font=,,\n[LLMs-as-Judges, ver\n[FUNCTIONALITY(\u00a73)\n[Performance \nEvaluation (\u00a73.1)\n[Responses \nEvaluation(\u00a73.1.1)", "2cc3003b-23f6-480c-8e05-3d794d3410ae": "Evaluation (\u00a73.1)\n[Responses \nEvaluation(\u00a73.1.1)\n[LLM-Eval\u00a0(Lin and Chen, 2023), Wang et al.\u00a0(Wang et\u00a0al., 2024c), Zhou et al.\u00a0(Zhou et\u00a0al., 2024a), ARES\u00a0(Saad-Falcon et\u00a0al., 2023), SELF-RAG\u00a0(Asai et\u00a0al., 2023), Lei et al.\u00a0(Lei et\u00a0al., 2024), leaf, text width=41em]]\n[Model \nEvaluation (\u00a73.1.2)\n[Auto-Arena\u00a0(Zhao et\u00a0al., 2024b; Luo et\u00a0al., 2024), LMExam\u00a0(Bai et\u00a0al., 2024), KIEval\u00a0(Yu et\u00a0al., 2024), leaf, text width=41em]]]\n[Model \nEnhancement (\u00a73.2)\n[Reward Modeling \nDuring Training \n(\u00a73.2.1)", "6a1678e9-70fb-4c28-aa3c-aff056662057": "[Reward Modeling \nDuring Training \n(\u00a73.2.1)\n[SRLMs\u00a0(Yuan et\u00a0al., 2024), OAIF\u00a0(Guo et\u00a0al., 2024), RLAIF\u00a0(Lee et\u00a0al., 2023), RELC\u00a0(Cao et\u00a0al., 2024b), CREAM\u00a0(Wang et\u00a0al., 2024d), CGPO\u00a0(Xu et\u00a0al., 2024a), Chen et al.\u00a0(Chen et\u00a0al., 2023c), leaf, text width=41em]]\n[Acting as Verifier \nDuring Inference \n(\u00a73.2.2)\n[Best-of-N sampling\u00a0(Jinnai et\u00a0al., 2024; Sun et\u00a0al., 2024), ToT\u00a0(Yao et\u00a0al., 2024), GoT\u00a0(Besta et\u00a0al., 2024), Lightman et al.\u00a0(Lightman et\u00a0al., 2023), SE-GBS\u00a0(Xie et\u00a0al., 2024a), REPS\u00a0(Kawabata and Sugawara, 2024), Musolesi et al.\u00a0(Musolesi, 2024), leaf, text width=41em]]\n[Feedback for \nRefinement (\u00a73.2.3)", "54dcc67c-e034-492f-a661-4af2e237b396": "[Feedback for \nRefinement (\u00a73.2.3)\n[SELF-REFINE\u00a0(Madaan et\u00a0al., 2024), SELF-DEBUGGING\u00a0(Chen et\u00a0al., 2023a), REFINER\u00a0(Paul et\u00a0al., 2023), Xu et al.\u00a0(Xu et\u00a0al., 2023c), Self-Correct\u00a0(Huang et\u00a0al., 2023; Tyen et\u00a0al., 2023), Valmeekam et al.\u00a0(Valmeekam et\u00a0al., 2023), leaf, text width=41em]]]\n[Data \nConstruction (\u00a73.3)\n[Data \nAnnotation (\u00a73.3.1)\n[He et al.\u00a0(He et\u00a0al., 2024a), Gilardi et al.\u00a0(Gilardi et\u00a0al., 2023), T\u00f6rnberg et al.\u00a0(T\u00f6rnberg, 2023), FullAnno\u00a0(Hao et\u00a0al., 2024), Latif et al.\u00a0(Latif et\u00a0al., 2023), AnnoLLM\u00a0(He et\u00a0al., 2023a), LLMAAA\u00a0(Zhang et\u00a0al., 2023a), leaf, text width=41em]]\n[Data \nSynthesize (\u00a73.3.2)", "24c00e57-00b3-430d-9557-2eeb54d132fb": "[Data \nSynthesize (\u00a73.3.2)\n[SELFEE\u00a0(Ye et\u00a0al., 2023a), SynPO\u00a0(Dong et\u00a0al., 2024a), Arif et al.\u00a0(Arif et\u00a0al., 2024), SELF-INSTRUCT\u00a0(Wang et\u00a0al., 2022), Evol-Instruct\u00a0(Xu et\u00a0al., 2023d; Zeng et\u00a0al., 2024), STaR\u00a0(Zelikman et\u00a0al., 2024), Mendoncca et al.\u00a0(Mendon\u00e7a et\u00a0al., 2024), \nReSTEM\u00a0(Singh et\u00a0al., 2023), Kim et al.\u00a0(Kim et\u00a0al., 2024a), leaf, text width=41em]]]]\n[\nMETHODOLOGY (\u00a74)\n[\nSingle-LLM (\u00a74.1)\n[\nPrompt-based (\u00a74.1.1)\n[\nIn-Context Learning\n[\nGPTScore\u00a0(Fu et\u00a0al., 2023), LLM-EVAL\u00a0(Lin and Chen, 2023), TALEC\u00a0(Zhang et\u00a0al., 2024c), Jain et al.\u00a0(Jain et\u00a0al., 2023), ALLURE\u00a0(Hasanbeig et\u00a0al., 2023), Song et al.\u00a0(Song et\u00a0al., 2024b), leaf, text width=33em\n]\n]\n[\nStep-by-step\n[", "154dd056-8841-4f75-afbc-355453bfee01": "]\n]\n[\nStep-by-step\n[\nChain-of-Thought (CoT)\u00a0(Wei et\u00a0al., 2022; Kotonya et\u00a0al., 2023), G-EVAL\u00a0(Liu et\u00a0al., 2023a), ICE-Score\u00a0(Zhuo, 2023), ProtocoLLM\u00a0(Yi et\u00a0al., 2024), Chiang et al.\u00a0(Chiang and Lee, 2023), \nFineSurE\u00a0(Song et\u00a0al., 2024a), leaf, text width=33em\n]\n]\n[\nDefinition \nAugmentation\n[\nAUTOCALIBRATE\u00a0(Liu et\u00a0al., 2023d), PORTIA\u00a0(Li et\u00a0al., 2023d), SALC\u00a0(Gupta et\u00a0al., 2024), LLM-as-a-personalized-judge\u00a0(Dong et\u00a0al., 2024b), BiasAlert\u00a0(Fan et\u00a0al., 2024), \nChen et al.\u00a0(Chen et\u00a0al., 2024c), leaf, text width=33em\n]\n]\n[\nMulti-turn \nOptimization\n[", "875cc12b-ad80-4f4e-a989-0ab2b2e1b7dd": "]\n]\n[\nMulti-turn \nOptimization\n[\nACTIVE-CRITIC\u00a0(Xu et\u00a0al., 2024b), AUTOCALIBRATE\u00a0(Liu et\u00a0al., 2023d), Auto-Arena\u00a0(Zhao et\u00a0al., 2024b; Luo et\u00a0al., 2024), LMExam\u00a0(Bai et\u00a0al., 2024), KIEval\u00a0(Yu et\u00a0al., 2024), leaf, text width=33em\n]\n]]\n[\nTuning-based (\u00a74.1.2)\n[\nScore-based Tuning\n[\nChen et al.\u00a0(Chen et\u00a0al., 2023c), AttrScore\u00a0(Yue et\u00a0al., 2023b), PHUDGE\u00a0(Deshwal and Chawla, 2024), ECT\u00a0(Wang et\u00a0al., 2023e), SELF-J\u00a0(Ye and Ng, 2024), SorryBench\u00a0(Xie et\u00a0al., 2024b), TIGERScore\u00a0(Jiang et\u00a0al., 2023b), \nFENCE\u00a0(Xie et\u00a0al., 2024d), ARES\u00a0(Saad-Falcon et\u00a0al., 2023), leaf, text width=33em\n]\n]\n[\nPreference-based \nLearning\n[", "f58cde9d-872a-4e1c-9935-c495e97690be": "]\n]\n[\nPreference-based \nLearning\n[\nMeta-Rewarding\u00a0(Wu et\u00a0al., 2024b), Con-J\u00a0(Ye et\u00a0al., 2024a), JudgeLM\u00a0(Zhu et\u00a0al., 2023), INSTRUCTSCORE\u00a0(Xu et\u00a0al., 2023e), AUTO-J\u00a0(Li et\u00a0al., 2023c), Shepherd\u00a0(Wang et\u00a0al., 2023c), \nX-EVAL\u00a0(Liu et\u00a0al., 2023c), Themis\u00a0(Hu et\u00a0al., 2024a), CritiqueLLM\u00a0(Ke et\u00a0al., 2024), FedEval-LLM\u00a0(He et\u00a0al., 2024b), PandaLM\u00a0(Wang et\u00a0al., 2023d), Self-Taught\u00a0(Wang et\u00a0al., 2024e), \nFLAMe\u00a0(Vu et\u00a0al., 2024), Self-Rationalization\u00a0(Trivedi et\u00a0al., 2024a), CompassJudger-1\u00a0(Cao et\u00a0al., 2024a), Zhou et al.\u00a0(Zhou et\u00a0al., 2024c), HALU-J\u00a0(Wang et\u00a0al., 2024b),", "abf60b51-0585-499c-8eab-bc5e01091a46": "PROMETHEUS\u00a0(Kim et\u00a0al., 2023), PROMETHEUS 2\u00a0(Kim et\u00a0al., 2024b), PROMETHEUS-VISION\u00a0(Lee et\u00a0al., 2024a), LLaVA-Critic\u00a0(Xiong et\u00a0al., 2024), leaf, text width=33em\n]\n]\n]\n[\nPost-processing(\u00a74.1.3)\n[\nProbability \nCalibration\n[\nDaynauth et al.\u00a0(Daynauth and Mars, 2024), ProbDiff\u00a0(Xia et\u00a0al., 2024b), PoE\u00a0(Liusie et\u00a0al., 2024), CRISPR\u00a0(Yang et\u00a0al., 2024), leaf, text width=33em\n]\n]\n[\nText Reprocessing\n[\nSottana et al.\u00a0(Sottana et\u00a0al., 2023), AUTO-J\u00a0(Li et\u00a0al., 2023c), Yan et al.\u00a0(Yan et\u00a0al., 2024a), Tessler et al.\u00a0(Tessler et\u00a0al., 2024), REVISEVAL\u00a0(Zhang et\u00a0al., 2024b)Ren et al.\u00a0(Ren et\u00a0al., 2023), \nOpen-LLM-Leaderboard\u00a0(Myrzakhan et\u00a0al., 2024), leaf, text width=33em\n]\n]\n]]\n[\nMulti-LLM (\u00a74.2)\n[", "b219ca9e-6fbd-4900-912e-f3c9691d8a37": "]\n]\n]]\n[\nMulti-LLM (\u00a74.2)\n[\nCommunication(\u00a74.2.1)\n[\nCooperation\n[\nWideDeep\u00a0(Zhang et\u00a0al., 2023b), Xu et al.\u00a0(Xu et\u00a0al., 2023c), ABSEval\u00a0(Liang et\u00a0al., 2024b), leaf, text width=33em\n]\n]\n[\nCompetition\n[\nOwens et al.\u00a0(Owens et\u00a0al., 2024), Auto-Arena\u00a0(Zhao et\u00a0al., 2024b), Bandi et al.\u00a0(Bandi and Harrasse, 2024), Moniri et al.\u00a0(Moniri et\u00a0al., 2024), ChatEval\u00a0(Chan et\u00a0al., 2023), PRD\u00a0(Li et\u00a0al., 2023b), leaf, text width=33em\n]\n]\n]\n[\nAggregation (\u00a74.2.2)\n[\nBadshah et al.\u00a0(Badshah and Sajjad, 2024), PoLL\u00a0(Verga et\u00a0al., 2024), Language-Model-as-an-Examiner\u00a0(Bai et\u00a0al., 2024), MULTI-NEWS+\u00a0(Choi et\u00a0al., 2024), PiCO\u00a0(Ning et\u00a0al., 2024), PRE\u00a0(Chu et\u00a0al., 2024), Chen et al. \u00a0(Chen et\u00a0al., 2024e),", "dc93e3dc-08ec-4348-8f31-1d24e5131abe": "Zhang et al.\u00a0(Zhang et\u00a0al., 2024a), \nAIME\u00a0(Patel et\u00a0al., 2024), HD-EVAL\u00a0(Liu et\u00a0al., 2024a), Gao et al.\u00a0(Gao et\u00a0al., 2024), GED\u00a0(Hu et\u00a0al., 2024c), Fusion-Eval\u00a0(Shu et\u00a0al., 2024), Jung et al.\u00a0(Jung et\u00a0al., 2024), CascadedEval\u00a0(Huang et\u00a0al., 2024a), leaf, text width=41em\n]\n]]\n[\nHuman-AI \nCollaboration (\u00a74.3)\n[\nCOEVAL(Li et\u00a0al., 2023a), EvalGen(Shankar et\u00a0al., 2024), EvaluLLM(Pan et\u00a0al., 2024a), LLM TAs(Chiang et\u00a0al., 2024), Wang et al.\u00a0(Wang et\u00a0al., 2023b), leaf, text width=49em\n]\n]\n]\n[APPLICATION (\u00a75)\n[\nGeneral (\u00a75.1)\n[\nDialogue Generation\u00a0(Li et\u00a0al., 2017), Summarization\u00a0(Narayan et\u00a0al., 2018), Translation\u00a0(Feng et\u00a0al., 2024), Fusion-Eval\u00a0(Shu et\u00a0al., 2024), leaf, text width=49em\n]\n]\n[", "c5a97819-57ae-4c69-9f3d-3b72dcf91978": "]\n]\n[\nMultimodal (\u00a75.2)\n[\nLLaVA-Critic\u00a0(Xiong et\u00a0al., 2024), Chen et al.\u00a0(Chen et\u00a0al., 2024b), Latif et al.\u00a0(Latif et\u00a0al., 2023), self-reward\u00a0(Zhou et\u00a0al., 2024b; Deng et\u00a0al., 2024),CODA-LM\u00a0(Chen et\u00a0al., 2024d), leaf, text width=49em\n]\n]\n[\nMedical (\u00a75.3)\n[\nXie et al.\u00a0(Xie et\u00a0al., 2024c), Brake et al.\u00a0(Brake and Schaaf, 2024), Krolik et al.\u00a0(Krolik et\u00a0al., 2024), Li et al.\u00a0(Li et\u00a0al., 2024c), Medical Reasoning\u00a0(Jeong et\u00a0al., 2024), leaf, text width=49em\n]\n]\n[\nLegal (\u00a75.4)\n[\nYue et al.\u00a0(Yue et\u00a0al., 2023a),Ryu et al.\u00a0(Ryu et\u00a0al., 2023), Raju et al.\u00a0(Raju et\u00a0al., 2024), Ma et al.\u00a0(Ma et\u00a0al., 2024), leaf, text width=49em\n]\n]\n[\nFinancial (\u00a75.5)\n[", "7d909312-ce97-41cf-9960-ec573bb43ea2": "]\n]\n[\nFinancial (\u00a75.5)\n[\nFinMA\u00a0(Xie et\u00a0al., 2023), Babaei et al.\u00a0(Babaei and Giudici, 2024), Son et al.\u00a0(Son et\u00a0al., 2024a), leaf, text width=49em\n]\n]\n[\nEducation (\u00a75.6)\n[\nLLM TA\u00a0(Chiang et\u00a0al., 2024), Wang et al.\u00a0(Wang et\u00a0al., 2024c), Song et al.\u00a0(Song et\u00a0al., 2024c), Zhou et al.\u00a0(Zhou et\u00a0al., 2024a), Xia et al.\u00a0(Xia et\u00a0al., 2024a), Debatrix\u00a0(Liang et\u00a0al., 2024a), leaf, text width=49em\n]\n]\n[\nInformation Retrieval \n(\u00a75.7)\n[\nRahmani et al.\u00a0(Rahmani et\u00a0al., 2024), JudgeRank\u00a0(Niu et\u00a0al., 2024), Zhang et al.\u00a0(Zhang et\u00a0al., 2024a), Soboroff et al. \u00a0(Soboroff, 2024), ARES\u00a0(Saad-Falcon et\u00a0al., 2023), leaf, text width=49em\n]\n]\n[\nOthers (\u00a75.8)\n[", "b1bd943a-0d4b-43b3-b0dc-21f037af7fc2": "]\n]\n[\nOthers (\u00a75.8)\n[\nCode\u00a0(Patel et\u00a0al., 2024; Weyssow et\u00a0al., 2024), Kumar et al.\u00a0(Kumar et\u00a0al., 2024), Hijazi et al.\u00a0(Hijazi et\u00a0al., 2024), Tessler et al.\u00a0(Tessler et\u00a0al., 2024), Sotopia\u00a0(Zhou et\u00a0al., 2023b), leaf, text width=49em\n]\n]]\n[META-EVALUATION \n(\u00a76)\n[Benchmarks (\u00a76.1)\n[\nCode Generation \n(\u00a76.1.1)\n[\nHumanEval\u00a0(Chen et\u00a0al., 2021), SWEBench\u00a0(Jimenez et\u00a0al., 2023), DevAI\u00a0(Zhuge et\u00a0al., 2024), CrossCodeEval\u00a0(Ding et\u00a0al., 2024), CodeUltraFeedback\u00a0(Weyssow et\u00a0al., 2024), leaf, text width=41em\n]\n]\n[\nMachine Translation \n(\u00a76.1.2)\n[\nFreitag et al.\u00a0(Freitag et\u00a0al., 2021b), Literary Translation Comparisons\u00a0(Karpinska and Iyyer, 2023), MQM\u00a0(Freitag et\u00a0al., 2021a), leaf, text width=41em\n]\n]\n[", "aa1489d2-a931-404a-99ab-c74686a301df": "]\n]\n[\nText Summarization \n(\u00a76.1.3)\n[\nSummEval\u00a0(Fabbri et\u00a0al., 2021), FRANK\u00a0(Pagnoni et\u00a0al., 2021), OpinsummEval\u00a0(Shen and Wan, 2023), leaf, text width=41em\n]\n]\n[\nDialogue Generation \n(\u00a76.1.4)\n[\nTopical-Chat\u00a0(Gopalakrishnan et\u00a0al., 2023), PERSONA-CHAT\u00a0(Zhang, 2018), Mehri et al.\u00a0(Mehri and Eskenazi, 2020), DSTC10 Track 5\u00a0(Yoshino et\u00a0al., 2023; Zhang et\u00a0al., 2021), leaf, text width=41em\n]\n]\n[\nAutomatic Story \nGeneration (\u00a76.1.5)\n[\nHANNA\u00a0(Chhun et\u00a0al., 2022), MANS\u00a0(Guan et\u00a0al., 2021), OpenMEVA\u00a0(Guan et\u00a0al., 2021), StoryER\u00a0(Chen et\u00a0al., 2023b), PERSER\u00a0(Wang et\u00a0al., 2023e), leaf, text width=41em\n]\n]\n[\nValues Alignment \n(\u00a76.1.6)\n[", "d56b4ae9-61b4-4ef2-b303-de7dca969068": "]\n]\n[\nValues Alignment \n(\u00a76.1.6)\n[\nPKU-SafeRLHF\u00a0(Ji et\u00a0al., 2024), HHH\u00a0(Askell et\u00a0al., 2021), CVALUES\u00a0(Xu et\u00a0al., 2023b), leaf, text width=41em\n]\n]\n[\nRecommendation \n(\u00a76.1.7)\n[\nMovieLens\u00a0(Harper and Konstan, 2015), Zhang et al.\u00a0(Zhang et\u00a0al., 2024a), Yelp\u00a0(Asghar, 2016), leaf, text width=41em\n]\n]\n[\nSearch (\u00a76.1.8)\n[\nTREC Deep Learning Track\u00a0(Lawrie et\u00a0al., 2024), MS MARCO v2 collection\u00a0(Bajaj et\u00a0al., 2016), LeCaRDv2\u00a0(Li et\u00a0al., 2024d), leaf, text width=41em\n]\n]\n[\nComprehensive Data \n(\u00a76.1.9)\n[", "e3cab5b9-e8d3-45fc-930e-fcd631103977": "]\n]\n[\nComprehensive Data \n(\u00a76.1.9)\n[\nHelpSteer\u00a0(Wang et\u00a0al., 2023a), HelpSteer2\u00a0(Wang et\u00a0al., 2024a), UltraFeedback\u00a0(Cui et\u00a0al., 2024), UltraChat\u00a0(Ding et\u00a0al., 2023), ShareGPT\u00a0(Chiang et\u00a0al., 2023), TruthfulQA\u00a0(Lin et\u00a0al., 2021), AlpacaEval\u00a0(Dubois et\u00a0al., 2024), \nChatbot Arena\u00a0(Zheng et\u00a0al., 2023a), MT-Bench\u00a0(Zheng et\u00a0al., 2023a), \nWildBench\u00a0(Lin et\u00a0al., 2024), FLASK\u00a0(Ye et\u00a0al., 2023b), RewardBench\u00a0(Lambert et\u00a0al., 2024), RM-Bench\u00a0(Liu et\u00a0al., 2024b), \nJudgeBench\u00a0(Tan et\u00a0al., 2024), \nMLLM-as-a-Judge\u00a0(Chen et\u00a0al., 2024b), MM-Eval\u00a0(Son et\u00a0al., 2024b), leaf, text width=41em\n]\n]]\n[Metric (\u00a76.2)", "14eff0b7-d9cc-4c07-93bc-77abbbc95a5a": "]\n]]\n[Metric (\u00a76.2)\n[Accuracy, Pearson\u00a0(Cohen et\u00a0al., 2009), Spearman\u00a0(Sedgwick, 2014), Kendall\u2019s Tau\u00a0(Sen, 1968), Cohen\u2019s Kappa\u00a0(Warrens, 2015), ICC\u00a0(Bartko, 1966), leaf, text width=49em ]]]]", "e29888eb-4898-401b-876c-ce527a4853ef": "Figure 1. Taxonomy of LLMs-as-judges in functionality, methodology, application, meta-evaluation.", "70c86512-4020-4c47-9647-f442c15e0a86": "{forest}\nforked edges,\nfor tree=\ngrow=east,\nreversed=true,\nanchor=base west,\nparent anchor=east,\nchild anchor=west,\nbase=left,\nfont=,\nrectangle,\ndraw=hidden-draw,\nrounded corners,\nalign=left,\nminimum width=4em,\nedge+=darkgray, line width=1pt,\ns sep=3pt,\ninner xsep=2pt,\ninner ysep=3pt,\nver/.style=rotate=90, child anchor=north, parent anchor=south, anchor=center,\n,\nwhere level=1text width=6.4em,font=,,\nwhere level=2text width=6.4em,font=,,\nwhere level=3text width=6.4em,font=,,\nwhere level=4text width=6.4em,font=,,\n[LLMs-as-judges, ver\n[LIMITATION (\u00a77)\n[Biases (\u00a77.1)\n[Presentation-Related \n(\u00a77.1.1)", "2c9dde7d-2039-4a5b-8806-b47cd52a90c3": "[Biases (\u00a77.1)\n[Presentation-Related \n(\u00a77.1.1)\n[Position bias\u00a0(Blunch, 1984; Raghubir and Valenzuela, 2006; Ko et\u00a0al., 2020; Wang et\u00a0al., 2018; LLMS, 2025; Zheng et\u00a0al., 2023a; Chen et\u00a0al., 2024a; Wang et\u00a0al., 2023b; Li et\u00a0al., 2023c; Zheng et\u00a0al., 2023b; Raina et\u00a0al., 2024; Hou et\u00a0al., 2024; Li et\u00a0al., 2023d, b; Khan et\u00a0al., 2024; Zhou et\u00a0al., 2023a; Li et\u00a0al., 2024a; Shi et\u00a0al., 2024a; Stureborg et\u00a0al., 2024; Zhao et\u00a0al., 2024a), Verbosity bias\u00a0(Nasrabadi, 2024; Ye et\u00a0al., 2024b, a), leaf, text width=41em] ]\n[Social-Related (\u00a77.1.2)", "79ee8ac8-3a4f-45e9-8d24-54dc9540f446": "[Social-Related (\u00a77.1.2)\n[Authority bias\u00a0(Chen et\u00a0al., 2024a; Ye et\u00a0al., 2024b; Zhao et\u00a0al., 2023a), Bandwagon-effect bias\u00a0(Koo et\u00a0al., 2023; Ye et\u00a0al., 2024b), Compassion-fade bias\u00a0(Koo et\u00a0al., 2023; Ye et\u00a0al., 2024b), Diversity bias\u00a0(Chen et\u00a0al., 2024a; Ye et\u00a0al., 2024b), leaf, text width=41em] ]\n[Content-Related \n(\u00a77.1.3)\n[Sentiment bias\u00a0(Ye et\u00a0al., 2024b), Token Bias\u00a0(Jiang et\u00a0al., 2024; Li et\u00a0al., 2024a; Pezeshkpour and Hruschka, 2023; Raina et\u00a0al., 2024), Contextual Bias\u00a0(Poulain et\u00a0al., 2024; Zhou et\u00a0al., 2024d, 2023a; Fei et\u00a0al., 2023; Zhao et\u00a0al., 2021; Han et\u00a0al., 2022), leaf, text width=41em] ]\n[Cognitive-Related \n(\u00a77.1.4)", "35cb9019-f457-4381-a1c0-c1dc5c8bd1fa": "[Cognitive-Related \n(\u00a77.1.4)\n[Overconfidence bias\u00a0(Khan et\u00a0al., 2024; Jung et\u00a0al., 2024), Self-enhancement bias\u00a0(Liu et\u00a0al., 2023a; Zheng et\u00a0al., 2023a; Li et\u00a0al., 2023b; Liu et\u00a0al., 2023a; Brown, 1986; Ye et\u00a0al., 2024b; Badshah and Sajjad, 2024), Refinement-aware bias\u00a0(Ye et\u00a0al., 2024b; Xu et\u00a0al., 2024c)\nDistraction bias\u00a0(Ye et\u00a0al., 2024b; Koo et\u00a0al., 2023; Shi et\u00a0al., 2023), Fallacy-oversight bias\u00a0(Chen et\u00a0al., 2024a; Ye et\u00a0al., 2024b), leaf, text width=41em] ] ]\n[Adversarial Attacks \n(\u00a77.2)\n[Adversarial Attacks \non LLMs (\u00a77.2.1)", "78e2522d-e565-483e-868d-3651da03b7f8": "(\u00a77.2)\n[Adversarial Attacks \non LLMs (\u00a77.2.1)\n[Text-Level Manipulations\u00a0(Ebrahimi et\u00a0al., 2017; Jiang et\u00a0al., 2023a; Branch et\u00a0al., 2022; Perez and Ribeiro, 2022), Structural and Semantic Distortions\u00a0(Xu et\u00a0al., 2023a), Optimization-Based Attacks\u00a0(Sun, 2020; Sun et\u00a0al., 2020; Lee et\u00a0al., 2022), leaf, text width=41em] ]\n[Adversarial Attacks \non LLMs-as-Judges \n(\u00a77.2.2)\n[Zheng et al.\u00a0(Zheng et\u00a0al., 2024), Doddapaneni et al.\u00a0(Doddapaneni et\u00a0al., 2024), MT-Bench\u00a0(Zheng et\u00a0al., 2023a), Raina et al.\u00a0(Raina et\u00a0al., 2024), Shi et al.\u00a0(Shi et\u00a0al., 2024b), leaf, text width=41em] ] ]\n[Inherent Weaknesses \n(\u00a77.3)\n[Knowledge Recency \n(\u00a77.3.1)", "5e852bf5-d7d5-4006-a85f-3599a28ad53c": "(\u00a77.3)\n[Knowledge Recency \n(\u00a77.3.1)\n[Zhao et al. \u00a0(Zhao et\u00a0al., 2023b), Luo et al.\u00a0(Luo et\u00a0al., 2023), Gao et al.\u00a0(Gao et\u00a0al., 2023), Lewis et al.\u00a0(Lewis et\u00a0al., 2020), Wu et al.\u00a0(Wu et\u00a0al., 2024a), Dierickx et al. \u00a0(Dierickx et\u00a0al., 2024), leaf, text width=41em]]\n[Hallucination (\u00a77.3.2)\n[Dierickx et al.\u00a0(Dierickx et\u00a0al., 2024), Ji et al.\u00a0(Ji et\u00a0al., 2023), Tonmoy et al.\u00a0(Tonmoy et\u00a0al., 2024), leaf, text width=41em]]\n[Domain-Specific \nKnowledge Gaps \n(\u00a77.3.3)\n[Feng et al.\u00a0(Feng et\u00a0al., 2023), Pan et al.\u00a0(Pan et\u00a0al., 2024b), Gao et al.\u00a0(Gao et\u00a0al., 2023), Szymanski et al.\u00a0(Szymanski et\u00a0al., 2024), Dorner et al.\u00a0(Dorner et\u00a0al., 2024), leaf, text width=41em]] ]]\n[\nFUTURE WORK \n(\u00a78)\n[", "3f8ecbb1-f07e-4c88-a524-0790a600942e": "[\nFUTURE WORK \n(\u00a78)\n[\nMore Efficient (\u00a78.1)\n[\nAutomated Construction of Evaluation Criteria and Tasks\u00a0(Bai et\u00a0al., 2024; Zhao et\u00a0al., 2024b; Yu et\u00a0al., 2024; Zhang et\u00a0al., 2024c; Wang et\u00a0al., 2024f), Scalable Evaluation Systems\u00a0(Xu et\u00a0al., 2024a), Accelerating Evaluation Processes\u00a0(Lee et\u00a0al., 2024b; Liu et\u00a0al., 2024c; Chen et\u00a0al., 2024f), leaf, text width=49em\n]\n]\n[\nMore Effective (\u00a78.2)\n[\nIntegration of Reasoning and Judge Capabilities\u00a0(Zhuo, 2023; Yi et\u00a0al., 2024; Stephan et\u00a0al., 2024), Establishing a Collective Judgment Mechanism\u00a0(Chan et\u00a0al., 2023; Chu et\u00a0al., 2024), Enhancing Domain Knowledge\u00a0(Raju et\u00a0al., 2024),", "0f657622-2858-4871-acf4-c5d42a989c5e": "Cross-Domain and Cross-Language Transferability\u00a0(Son et\u00a0al., 2024b; Hada et\u00a0al., 2023; Watts et\u00a0al., 2024), Multimodal Integration Evaluation\u00a0(Chen et\u00a0al., 2024b), leaf, text width=49em\n]\n]\n[\nMore Reliable (\u00a78.3)\n[\nEnhancing Interpretability and Transparency\u00a0(Liu et\u00a0al., 2024a), \nMitigating Bias and Ensuring Fairness\u00a0(Li et\u00a0al., 2024a), \nEnhancing Robustness\u00a0(Shi et\u00a0al., 2024b; Elangovan et\u00a0al., 2024), leaf, text width=49em\n]]]]", "758a8c77-111e-4e0e-9e32-a84364194843": "Figure 2. Taxonomy of LLMs-as-judges in limitation and future work.\n\n\n\n\n2. PRELIMINARIES\n\nIn this section, we will provide a formal definition of LLMs-as-judges, aiming to encompass all current evaluation paradigms and methods, thereby offering readers a clear and thorough understanding. Figure 3 presents an overview of the LLMs-as-judges system.", "c464674f-8456-41b5-a80d-267201c46861": "The LLMs-as-judges paradigm is a flexible and powerful evaluation framework where LLMs are employed as evaluative tools, responsible for assessing the quality, relevance, and effectiveness of generated outputs based on defined evaluation criteria. This framework leverages the extensive knowledge and deep contextual understanding of LLMs, enabling it to flexibly adapt to various tasks in NLP and machine learning.\nWe formalize the input-output structure of the LLMs-as-Judges paradigm, unifying various evaluation scenarios into a unified perspective.\nSpecifically, the evaluation process can be defined as follows:\n\n\n(1)", "5284b654-9e71-4126-8410-8cad99922c48": "(1)\n\n(\ud835\udcb4,\u2130,\u2131)=E\u2062(\ud835\udcaf,\ud835\udc9e,\ud835\udcb3,\u211b)\ud835\udcb4\u2130\u2131\ud835\udc38\ud835\udcaf\ud835\udc9e\ud835\udcb3\u211b(\\mathcal{Y},\\mathcal{E},\\mathcal{F})=E(\\mathcal{T},\\mathcal{C},\\mathcal{X},%\n\\mathcal{R})( caligraphic_Y , caligraphic_E , caligraphic_F ) = italic_E ( caligraphic_T , caligraphic_C , caligraphic_X , caligraphic_R )", "37653da7-9a11-4738-a079-2d662b941acd": "where E\ud835\udc38Eitalic_E is the evaluation function, taking the evaluation type \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T, evaluation criteria \ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C, evaluation item \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X and optional references \u211b\u211b\\mathcal{R}caligraphic_R as input. Based on these inputs, the LLM can produces three outputs: evaluation result \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y, explanation \u2130\u2130\\mathcal{E}caligraphic_E and feedback \u2131\u2131\\mathcal{F}caligraphic_F.", "ad72862b-79d8-492a-8ad1-92d298f52485": "Different input-output configurations correspond to distinct methods and objectives. This unified formulation brings together diverse evaluation paradigms, offering a structured framework for categorizing and understanding various approaches within LLMs-as-judges.", "ba0cd09a-ab10-41c2-abec-103ac71f47dc": "2.1. Evaluation Function E\ud835\udc38Eitalic_E\n\n\nThe evaluation function E\ud835\udc38Eitalic_E in the context of LLMs-as-judges can be categorized into three primary configurations: Single-LLM systems, Multi-LLM systems, and Hybrid systems that combine LLMs with human evaluators. Each of these configurations serves distinct purposes, offers different advantages, and faces unique challenges.\n\n\n\n\n\u2022", "8351c118-2108-4856-9067-530d6b9239bf": "\u2022\n\nSingle-LLM Evaluation System\u00a0(Lin and Chen, 2023; Zhang et\u00a0al., 2024c; Liu et\u00a0al., 2023a):  A single LLM evaluation system relies on a single model to perform the evaluation tasks. It is simple to deploy and scale, making it efficient for tasks that don\u2019t require specialized evaluation. However, its flexibility is limited, as it may struggle with tasks that demand specialized knowledge or reasoning over complex inputs. Additionally, if not properly trained, a single model may introduce biases, leading to inaccurate evaluations.\n\n\n\n\u2022", "583bcdb7-9be9-4238-b2cb-4b2a9212cf09": "Multi-LLM Evaluation Systems\u00a0(Chu et\u00a0al., 2024; Chan et\u00a0al., 2023; Li et\u00a0al., 2023b):  A Multi-LLM evaluation system combines multiple models that work together to perform evaluation tasks. These models may interact through various mechanisms, such as collaboration, or competition, to refine their outputs and achieve more accurate results. By leveraging the strengths of different models, a multi-model system can cover a broader range of evaluation criteria and provide a more comprehensive assessment. However, this comes at a higher computational cost and requires more resources, making deployment and maintenance more challenging, particularly for large-scale tasks.", "eb17b573-6a88-4456-993d-9a1ff341f7e9": "Moreover, while cooperation between models often enhances evaluation results, the methods through which these models achieve consensus or resolve differences remain key areas of ongoing exploration.", "71f3e1b3-2e7d-4259-8a12-3187ad2c0b12": "\u2022", "f85fdfcf-afda-4b07-868e-4b5cc04e3e02": "Human-AI Collaboration System\u00a0(Li et\u00a0al., 2023a; Wang et\u00a0al., 2023b; Shankar et\u00a0al., 2024):  In this system, LLMs work alongside human evaluators, combining the efficiency of automated evaluation with the nuanced judgment of human expertise. This configuration allows human evaluators to mitigate potential biases in the LLM\u2019s output and provide subjective insights into complex evaluation tasks. While this system offers greater reliability and depth, it comes with challenges in coordinating between the models and humans, ensuring consistent evaluation standards, and integrating feedback. Additionally, the inclusion of human evaluators increases both the cost and time required for the", "5b4b6f87-aa41-4c40-9abe-7041c236a9b2": "increases both the cost and time required for the evaluation process, making it less scalable than purely model-based systems.", "9bf4dae0-8a83-4613-ba1d-247f7bb065a7": "2.2. Evaluation Input\n\nIn the LLMs-as-judges paradigm, in addition to the evaluation item \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X, LLM judges typically receive three other types of inputs: Evaluation Type \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T , Evaluation Criteria \ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C, and Evaluation References \u211b\u211b\\mathcal{R}caligraphic_R. The following provides a detailed explanation:\n\n\n\n2.2.1. Evaluation Type \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T\n\n\nThe Evaluation Type \ud835\udcaf\ud835\udcaf\\mathcal{T}caligraphic_T defines the specific evaluation mode, determining how the evaluation will be conducted. It typically includes three approaches: pointwise, pairwise, and listwise evaluation.\n\n\n\n\n\u2022", "8b42ce37-8549-4764-b6e4-d84481e5e1fc": "\u2022\n\nPointwise Evaluation\u00a0(Wang et\u00a0al., 2023e; Kim et\u00a0al., 2023; Ye and Ng, 2024): This method evaluates each candidate item individually based on the specified criteria.\nFor example, in a text summarization task, the LLM might evaluate each generated summary separately, assigning a score based on factors like informativeness, coherence, and conciseness. Although pointwise evaluation is simple and easy to apply, it may fail to capture the relative quality differences between candidates and can be influenced by biases arising from evaluating items in isolation.\n\n\n\n\u2022", "b77a99e0-53a8-4555-a681-d2b21d1bf555": "\u2022\n\nPairwise Evaluation\u00a0(Saad-Falcon et\u00a0al., 2023; Cao et\u00a0al., 2024a; He et\u00a0al., 2024b; Hu et\u00a0al., 2024b): This method involves directly comparing two candidate items to determine which one performs better according to the specified criteria. It is commonly used in preference-based tasks. For example, given two summaries of a news article, the LLM may be asked to decide which summary is more coherent or informative. Pairwise evaluation closely mirrors human decision-making processes by focusing on relative preferences rather than assigning absolute scores. This approach is especially effective when the differences between outputs are subtle and difficult to quantify.\n\n\n\n\u2022", "5832c0fc-31ee-436b-8151-03dc7d012ca9": "\u2022\n\nListwise Evaluation\u00a0(Zhuang et\u00a0al., 2024; Yan et\u00a0al., 2024b; Hou et\u00a0al., 2024; Niu et\u00a0al., 2024):\nThis method is designed to collectively evaluate the entire list of candidate items, evaluating and ranking them based on the specific criteria. It is often applied in ranking tasks, such as document retrieval in search engines, where the objective is to determine the relevance of the documents in relation to a user query. Listwise evaluation takes into account the interactions between multiple candidates, making it well-suited for applications that require holistic analysis.", "59ef504a-63e3-421e-9394-2c4d20b457ab": "In general, these three evaluation modes are not entirely independent. pointwise scores can be aggregated to create pairwise comparisons or used to construct a ranked list. Similarly, pairwise preferences can be organized into a complete ranking list for listwise analysis.", "050013f2-963f-4591-b5d0-af4199856bf5": "However, these transformations are not always reliable within the LLMs-as-judges framework\u00a0(Liu et\u00a0al., 2024c). For example, in pointwise evaluation, output A\ud835\udc34Aitalic_A may receive a score of 5, while output B\ud835\udc35Bitalic_B receives a score of 4, yet, a direct pairwise comparison might not consistently yield A>B\ud835\udc34\ud835\udc35A>Bitalic_A > italic_B due to potential bias. Additionally, LLM judges do not always satisfy transitivity in their judgments. For instance, given pairwise preferences where zi>zjsubscript\ud835\udc67\ud835\udc56subscript\ud835\udc67\ud835\udc57z_{i}>z_{j}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT and zj>zksubscript\ud835\udc67\ud835\udc57subscript\ud835\udc67\ud835\udc58z_{j}>z_{k}italic_z", "9109755b-2984-4fed-ae91-3c95142aafaa": "zj>zksubscript\ud835\udc67\ud835\udc57subscript\ud835\udc67\ud835\udc58z_{j}>z_{k}italic_z start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT > italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT , the LLM may not necessarily yield zi>zksubscript\ud835\udc67\ud835\udc56subscript\ud835\udc67\ud835\udc58z_{i}>z_{k}italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT > italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT. These inconsistencies contribute to concerns about the reliability and trustworthiness of the LLM-as-Judge framework, which we will discuss in detail in Section (\u00a77).", "44615b9a-8559-48fb-ad13-43c055627a57": "Figure 3. Overview of the LLMs-as-judges system.\n\n\n\n\n2.2.2. Evaluation Criteria \ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C.\n\nThe evaluation criteria \ud835\udc9e\ud835\udc9e\\mathcal{C}caligraphic_C define the specific standards that determine which aspects of the output should be assessed. These criteria are designed to cover a broad range of quality attributes and can be tailored based on the nature of the task. Typically, the criteria encompass the following aspects:\n\n\n\n\n\u2022", "f7761dfd-0d85-43c9-a855-b33292a6ebf3": "\u2022\n\nLinguistic Quality\u00a0(Fabbri et\u00a0al., 2021; Zhang, 2018; Chhun et\u00a0al., 2022): This category evaluates the language-related features of the output, such as fluency, grammatical accuracy, coherence, and Conciseness. Linguistic quality is crucial in tasks like text generation, machine translation, and summarization, where clarity and readability are essential.\n\n\n\n\u2022", "71f5a3b4-2208-4d0a-ac90-57815f01f0ac": "\u2022\n\nContent Accuracy\u00a0(Chen et\u00a0al., 2021; Jimenez et\u00a0al., 2023; Tan et\u00a0al., 2024): This dimension focuses on the correctness and relevance of the content. It includes evaluating aspects such as factual accuracy, ensuring that the output does not contain misleading or incorrect information. Content accuracy is particularly crucial in tasks such as code generation and fact-checking.\n\n\n\n\u2022", "a1e6d51c-ed66-43c3-af20-9d3378013c0e": "\u2022\n\nTask-Specific Metrics\u00a0(Ji et\u00a0al., 2024; Lin et\u00a0al., 2024; Tan et\u00a0al., 2024): In addition to general quality metrics, many tasks require evaluation based on standards specific to their respective domains. These standards may include metrics such as informativeness (assessing whether the output provides comprehensive and valuable information) or completeness (ensuring all key aspects of the input are covered). Other criteria may include diversity, well-structured content, and logical clarity.", "21b16653-2d6e-410f-8672-acfe47e9679f": "In addition to providing clear evaluation criteria, offering several examples can also be beneficial for the assessment. By incorporating well-structured examples, LLMs can better align its output with user expectations, especially when handling complex tasks or ambiguous queries.\n\n\n\n\n2.2.3. Evaluation References \u211b\u211b\\mathcal{R}caligraphic_R.\n\nEvaluation References \u211b\u211b\\mathcal{R}caligraphic_R are optional. Depending on the availability of evaluation reference, the evaluation process can be broadly divided into reference-based and reference-free scenarios.\n\n\n\n\n\u2022", "9ac886f3-0190-46c5-9d05-6ff7c21e1897": "Reference-Based Evaluation\u00a0(Freitag et\u00a0al., 2021b; Karpinska and Iyyer, 2023):", "3bca102c-090c-425d-8906-60ac119d7757": "The reference-based evaluation leverages reference data to determine whether the performance meets the expected standards. It is commonly applied in tasks where the quality of the output can be objectively judged by its similarity to established reference. In Natural Language Generation (NLG) tasks, this method is widely used to evaluate the resemblance between generated content and reference content. For example, in machine translation or text summarization, an LLM can compare the generated translations or summaries against high-quality references. The key strength of this approach is its well-defined benchmarking process; however, its effectiveness may be constrained by the quality and", "38d5ea9c-96a3-499a-84e0-fec24b97e853": "may be constrained by the quality and variety of the reference data.", "5a017769-8dd9-4a6c-a5c1-3807232ac432": "\u2022", "2c1735f3-ebcf-45e7-9591-5216a79eef1a": "Reference-Free Evaluation\u00a0(Shen and Wan, 2023; Zheng et\u00a0al., 2023a; He et\u00a0al., 2023b):", "e653b082-0977-4b69-bbcb-df7da69ad259": "The reference-free evaluation does not rely on a specific reference \u211b\u211b\\mathcal{R}caligraphic_R, instead, it evaluates \ud835\udcb3\ud835\udcb3\\mathcal{X}caligraphic_X based on intrinsic quality standards or its alignment with the source context. For example, when assessing language fluency or content coherence, an LLM can autonomously generate evaluation results using internal grammatical and semantic rules. This method is widely used in fields like sentiment analysis and dialogue generation. The main advantage of this approach is its independence from specific references, providing greater flexibility for open-ended tasks. However, its drawback lies in the difficulty of obtaining satisfactory evaluations in", "e423baba-3dba-42f8-b8c3-f553453b3053": "of obtaining satisfactory evaluations in domains where the LLM lacks relevant knowledge.", "ca5d2cb0-3c7f-44c8-b319-1c9c8e5272fc": "2.3. Evaluation Output\n\nIn the LLMs-as-judges paradigm, the LLM typically generates three types of outputs: the evaluation result \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y, the explanation \u2130\u2130\\mathcal{E}caligraphic_E, and the feedback \u2131\u2131\\mathcal{F}caligraphic_F. Below are detailed descriptions.\n\n\n\n\n\u2022", "e5d46b4e-1c38-4a04-b0be-bf7e040a658c": "Evaluation Result \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y\u00a0(Zhao et\u00a0al., 2024b; Saad-Falcon et\u00a0al., 2023):  The evaluation result \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y is the primary output, which can take the form of a numerical score, a ranking, a categorical label, or a qualitative assessment. It reflects the quality, relevance, or performance of the candidate items according to the specified evaluation criteria. For example, in a machine translation task, \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y could be a score indicating translation quality, while in a dialogue generation task, it might be a rating of coherence and appropriateness on a scale from 1 to 5. The evaluation result \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y provides a clear", "0e271224-7d09-474c-bed8-052cc88133a4": "\ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y provides a clear measure of performance, enabling researchers to effectively compare different models or outputs.", "f774f4b3-7b20-4b4c-8531-f6499675e344": "\u2022", "15be8395-1afc-4fb1-973c-ce495170426b": "Explanation \u2130\u2130\\mathcal{E}caligraphic_E\u00a0(Ye et\u00a0al., 2024a; Xie et\u00a0al., 2024d):  The explanation \u2130\u2130\\mathcal{E}caligraphic_E provides detailed reasoning and justifications for the evaluation result. It offers insights into why certain result received higher or lower scores, highlighting specific features of the candidate item that influenced the evaluation. For example, in a summarization task, the LLM judges might explain that the score was lowered due to missing critical information or the presence of redundant content. The explanation component enhances transparency, allowing users to understand the decision-making process of the LLM and gain deeper insights into the strengths and", "a635296b-7baf-4168-add8-3b3d6ed4e9a6": "and gain deeper insights into the strengths and weaknesses of the evaluated content.", "0e7597b1-fed1-475f-b664-4005f3fc4ca2": "\u2022", "23f91a32-e362-4b43-b11c-12b3c5fbebf5": "Feedback \u2131\u2131\\mathcal{F}caligraphic_F\u00a0(Madaan et\u00a0al., 2024; Chen et\u00a0al., 2023a):  The feedback \u2131\u2131\\mathcal{F}caligraphic_F consists of actionable suggestions or recommendations aimed at improving the evaluated output. Unlike the evaluation result, which merely indicates performance, the feedback component is designed to guide the refinement of the content. For instance, in a creative writing task, feedback might include recommendations for enhancing the narrative flow or improving clarity. This component is especially valuable for the iterative development of the evaluated item, as it provides concrete pointers that help both the LLM and content creators enhance the quality of the generated", "a5ab3faa-5a60-40d1-8951-6fdc997e1d33": "creators enhance the quality of the generated outputs.", "77f095cd-def2-4c87-974f-c1c883959918": "Depending on the intended purpose and specific requirements of the evaluation, the LLM judges can generate various combinations of the three outputs ( \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y, \u2130\u2130\\mathcal{E}caligraphic_E, \u2131\u2131\\mathcal{F}caligraphic_F ) for a given task.", "9088952a-eb7e-4441-8ac4-6c940d7482ec": "In most cases, providing explanation \u2130\u2130\\mathcal{E}caligraphic_E not only helps users better understand and trust the evaluation results but also leads to more human-aligned and accurate evaluation result \ud835\udcb4\ud835\udcb4\\mathcal{Y}caligraphic_Y. Moreover, generating feedback \u2131\u2131\\mathcal{F}caligraphic_F generally demands a higher level of model capability, as it requires not only assessing the quality of the input but also providing concrete, actionable recommendations for improvement.", "3b75c13c-6ac2-4dbb-988f-14204b379cef": "3. Functionality\n\nAs an emerging evaluation paradigm, LLMs-as-judges play a significant role across various scenarios. Based on their functionality, we categorize the application of LLM evaluators into three main directions: Performance Evaluation (\u00a73.1), Model Enhancement (\u00a73.2), and Data Construction (\u00a73.3). In this section, we will delve into these functionalities, explore their potential, and discuss specific implementation methods.\n\n\nFigure 4. Overview of the Functionality of LLMs-as-judges.\n\n\n\n3.1. Performance Evaluation", "f8adcc04-b13f-4271-8285-f29b09a77f7f": "3.1. Performance Evaluation\n\nPerformance evaluation represents the most fundamental application objective of an LLM judges, serving as the cornerstone for understanding and optimizing their other function. It can be broadly divided into two components: Responses Evaluation (\u00a73.1.1) and Model Evaluation (\u00a73.1.2).\nResponse Evaluation focuses on aspects such as the quality, relevance, and coherence, and fluency of the responses for a given task. In contrast, model evaluation takes a holistic approach, assessing the overall capabilities of LLMs. Although these two aspects are interconnected, they focus on different levels of analysis, providing multidimensional insights into performance.", "207d9171-d14c-45ac-b9a3-5d2e58869be0": "3.1.1. Responses Evaluation\n\nThe purpose of evaluating responses is to identify better answers within the context of a specific question or task, which can enhance overall decision-making.\nThese responses can originate from either AI models or humans.\nEvaluation criteria typically consider general attributes such as accuracy, relevance, coherence, and fluency. However, in practical applications, the evaluation of responses often requires customized metrics tailored to specific tasks. For instance, in the education domain, the focus may be more on the inspirational and educational value of the answers.", "66e5d1d0-5a91-486f-9bcd-74c3651278d1": "LLM judges have also been widely applied in the assessment of text response\u00a0(Lin and Chen, 2023; Wang et\u00a0al., 2024c; Zhou et\u00a0al., 2024a). Lin et al.\u00a0(Lin and Chen, 2023) propose LLM-Eval, a unified framework employing a single-prompt strategy to evaluate the performance of open-domain dialogue systems across multiple dimensions, including content, grammar, relevance, and appropriateness.", "63a237f5-9822-46e3-b2eb-62f4138ea8b4": "Wang et al.\u00a0(Wang et\u00a0al., 2024c) proposed an article scoring and feedback system tailored to different genres, such as essays, narratives, and question-answering articles. Using BERT and ChatGPT models, they enabled automated scoring and detailed feedback, showcasing the potential of LLMs in article evaluation.\nMoreover, Zhou et al.\u00a0(Zhou et\u00a0al., 2024a) conduct a detailed evaluation of whether LLMs can serve as reliable tools for automated paper review. Their findings indicate that current LLMs are still not sufficiently reliable for such tasks, particularly in scenarios requiring logical reasoning or a deep knowledge base.", "c489be35-182f-4889-a63d-e68c5ac21e91": "Furthermore, the evaluation of a single response is not limited to assessing the quality of the final answer but can also extend to analyzing the response process\u00a0(Saad-Falcon et\u00a0al., 2023; Asai et\u00a0al., 2023; Lei et\u00a0al., 2024). For instance, this can include evaluating whether retrieval is necessary at a given step, the relevance of the retrieved documents, and the interpretability of the response.", "73b5a481-33f4-4578-b3ee-5ebff6434c79": "For example, ARES\u00a0(Saad-Falcon et\u00a0al., 2023) uses LLM judges to evaluate RAG systems across three dimensions: Contextual Relevance, Answer Faithfulness, and Answer Relevance. Similarly, Asai et al. \u00a0(Asai et\u00a0al., 2023) proposed SELF-RAG, which employs reflective token to determine whether retrieval is required and to self-assess the quality of generated outputs. Lei et al.\u00a0(Lei et\u00a0al., 2024) introduced LLMs to evaluate the quality of generated explanations, demonstrating the effectiveness of LLMs in understanding and generating explanations for recommendation tasks.", "05e906d4-c11f-46bc-8fe2-da635f7b8077": "3.1.2. Model Evaluation\n\nModel evaluation typically begins with assessing individual responses and then extends to analyzing overall capabilities. This wider perspective aims to analyze the model\u2019s performance across various tasks or domains, such as coding ability, instruction-following proficiency, reasoning, and other specialized skills relevant to its intended applications.", "69707767-f1cc-4151-9817-f55506e29994": "A common and straightforward approach is to represent model performance using average performance on static benchmarks\u00a0(Zheng et\u00a0al., 2023a; Lin et\u00a0al., 2024; Tan et\u00a0al., 2024). LLM judges assess the model\u2019s performance using a set of carefully designed metrics, which results in a performance ranking. This method is widely adopted due to its simplicity and comparability. For example, task sets can be designed to evaluate the model\u2019s knowledge coverage, reasoning depth, and language generation quality\u00a0(Son et\u00a0al., 2024b; Liu et\u00a0al., 2024b; Lambert et\u00a0al., 2024), or real-world scenarios can be simulated to assess the model\u2019s ability to handle complex situations\u00a0(Liu et\u00a0al., 2023e; Trivedi", "aa51b9c7-3687-4873-bc61-af53a3041538": "complex situations\u00a0(Liu et\u00a0al., 2023e; Trivedi et\u00a0al., 2024b).", "21bed430-7f84-45df-bcb5-9c98ca192803": "As the demand for evaluation increases, the evaluation process has gradually shifted from traditional static testing to more dynamic, interactive assessments\u00a0(Bai et\u00a0al., 2024; Zhao et\u00a0al., 2024b; Yu et\u00a0al., 2024).\nLLMs-as-judges has pioneered this approach, similar to Chatbot Arena\u00a0(Zheng et\u00a0al., 2023a), a crowdsourced platform that collects anonymous votes on LLM performance and ranks them using Elo scores.", "d2b0d815-2236-417a-8cea-995f4e239d93": "Auto-Arena\u00a0(Zhao et\u00a0al., 2024b; Luo et\u00a0al., 2024) and LMExam\u00a0(Bai et\u00a0al., 2024) assess model capabilities by using LLMs as both question setters and evaluators. These frameworks innovatively combine diverse question generation, multi-turn question-answering evaluation, and a decentralized model-to-model evaluation mechanism, providing more detailed and granular performance assessments.", "05bbcde8-d512-40d8-9fad-82e9912a637c": "Additionally, KIEval\u00a0(Yu et\u00a0al., 2024) introduces an LLM-driven \u201cinteractor\u201d role, which evaluates the knowledge mastery and generation abilities of LLMs through dynamic multi-turn conversations. These dynamic evaluation methods effectively address data leakage and evaluation bias issues common in traditional benchmark tests.", "8201fdbe-2e24-44a4-9005-c750ca576e14": "3.2. Model Enhancement\n\nIn addition to Performance Evaluation, LLMs-as-judges is also widely used for Model Enhancement. From training to inference, LLMs-as-judges plays a key role in improving model performance. Its application in model enhancement offers a novel optimization pathway for artificial intelligence, fostering the refinement and personalization of intelligent systems across a broader spectrum of real-world applications.\n\n\n\n3.2.1. Reward Modeling During Training", "50f16fd0-f1cd-4e38-9387-d375f24d0f7b": "3.2.1. Reward Modeling During Training\n\nA primary application of LLMs-as-judges is in reward modeling during training, particularly in reinforcement learning with feedback\u00a0(Yuan et\u00a0al., 2024; Guo et\u00a0al., 2024; Wang et\u00a0al., 2024d; Xu et\u00a0al., 2024a; Cao et\u00a0al., 2024b). LLM judges assign scores to model outputs by evaluating them against human-defined criteria, guiding optimization toward desired behaviors. This ensures alignment with human values, improving the quality and relevance of the generated outputs and improving the effectiveness of LLMs in real-world tasks.", "ec92cc49-9a8d-4790-8bfb-33a54f5507ac": "A series of works, such as SRLMs\u00a0(Yuan et\u00a0al., 2024), OAIF\u00a0(Guo et\u00a0al., 2024), and RLAIF\u00a0(Lee et\u00a0al., 2023), have enabled LLMs to become their own reward models. This overcomes the traditional RLHF dependency on fixed reward models, allowing the model to iteratively reward and self-optimize, fostering self-evolution through continuous self-assessment.\nRELC\u00a0(Cao et\u00a0al., 2024b) tackles the challenge of sparse rewards in traditional RL by introducing a Critic Language Model (Critic LM) to evaluate intermediate generation steps. This dense feedback at each step helps mitigate reward sparsity, offering more detailed guidance to the model during training.", "91db738b-daba-4bc9-99ce-f51bdd1fdc0a": "However, using the same LLM for both policy generation and reward modeling can pose challenges in ensuring the accuracy of the rewards. This dual role setup may lead to accumulated biases and preference data noise, which can undermine the training effectiveness. To address this issue, CREAM\u00a0(Wang et\u00a0al., 2024d) introduces cross-iteration consistency constraints to regulate the training process and prevent the model from learning unreliable preference data. This significantly enhances reward consistency and alignment performance. In addition, CGPO\u00a0(Xu et\u00a0al., 2024a) groups tasks by category (such as dialogue, mathematical reasoning, safety, etc.) and uses \u201cMixed Judges\u201d to assign a specific", "0c1a08bd-5019-49da-91b4-a64c755ad486": "and uses \u201cMixed Judges\u201d to assign a specific reward model to each task group. This ensures that the reward signals are closely aligned with the task objectives, thereby preventing conflicts between different goals.", "1c5ae81d-86e9-4d49-a70e-214827e0f368": "3.2.2. Acting as Verifier During Inference\n\nDuring inference, LLM judges serve as verifier, responsible for selecting the optimal response from multiple candidates\u00a0(Yao et\u00a0al., 2024; Besta et\u00a0al., 2024; Lightman et\u00a0al., 2023; Musolesi, 2024; Besta et\u00a0al., 2024). By comparing the outputs based on various metrics, such as factual accuracy and reasoning consistency, they are able to identify the best fit for the given task or context, thereby optimizing the inference process or improving the quality of the generated results.", "3658e8c0-330b-4ef9-b051-d9432ce7b423": "One of the simplest applications is Best-of-N sampling\u00a0(Jinnai et\u00a0al., 2024; Sun et\u00a0al., 2024), where the model is sampled N times, and the best result is selected to improve model performance. Similarly, Wang et al.\u00a0(Wang et\u00a0al., 2022) introduced a promising sampling method called self-consistency, where n samples are drawn from the judge model, and the average score is output. These sampling methods enhance inference stability by selecting the best result from multiple evaluations.", "f3453376-a1fa-4b89-a8c1-6ea2bc0f9c8a": "Further optimization strategies include the Tree of Thoughts (ToT)\u00a0(Yao et\u00a0al., 2024) method, which models the problem-solving process as a tree structure. This allows the model to explore multiple solution paths and optimize path selection through self-assessment mechanisms.\nThe Graph of Thoughts (GoT)\u00a0(Besta et\u00a0al., 2024) method extends this concept by introducing directed graphs, where the non-linear interactions between nodes improve the efficiency and precision of multi-step reasoning. In both methods, LLM judges play a crucial role in guiding the model to select the most promising paths, thereby enhancing the quality and accuracy of reasoning.", "ab8a75c6-5152-498c-bd18-a0f40ea1cc22": "Similarly, Lightman et al.\u00a0(Lightman et\u00a0al., 2023) discuss how step-by-step validation can enhance the performance of LLMs in multi-step reasoning tasks, particularly in the domain of mathematics. SE-GBS\u00a0(Xie et\u00a0al., 2024a) integrates self-assessment into the multi-step reasoning decoding process, generating scores that reflect logical correctness and further ensuring the accuracy and consistency of the reasoning chain. The REPS\u00a0(Kawabata and Sugawara, 2024) improves the accuracy and reliability of reasoning validation models by comparing reasoning paths pairwise, verifying their logical consistency and factual basis.", "dc4bbb6d-488d-4d50-8266-aec71481de02": "Also, Musolesi et al. \u00a0(Musolesi, 2024) proposed Creative Beam Search, with the LLM acting as a judge to simulate the human creative selection process, thereby enhancing the diversity and creativity of the generated results.", "b67dc10f-c2e0-4084-a7cf-868b411517e3": "3.2.3. Feedback for Refinement\n\nAfter receiving the initial response, LLM judges provide actionable feedback to iteratively improve output quality. By analyzing the response based on specific task criteria, such as accuracy, coherence, or creativity, the LLM can identify weaknesses in the output and offer suggestions for improvement. This iterative refinement process plays a crucial role in applications that require adaptability\u00a0(Madaan et\u00a0al., 2024; Paul et\u00a0al., 2023; Chen et\u00a0al., 2023a; Xu et\u00a0al., 2023c; Huang et\u00a0al., 2023).", "4ba4a76f-fca9-4c0b-9b7e-671fb6475e8f": "SELF-REFINE\u00a0(Madaan et\u00a0al., 2024) enables LLMs to iteratively improve output quality through feedback generated by the model itself, without requiring additional training or supervision data. On the other hand, SELF-DEBUGGING\u00a0(Chen et\u00a0al., 2023a) demonstrates a practical application of self-correction in code generation by identifying and rectifying errors through self-explanation and feedback. This approach has significantly enhanced the performance of LLMs across various code generation tasks.", "187e58d3-3c9b-476c-b487-bd7e6eb9e4d9": "In addition to refining response quality, LLMs judges are also widely used to enhance reasoning abilities. For example, REFINER\u00a0(Paul et\u00a0al., 2023) optimizes the reasoning performance of LLMs through interactions between a generator model and a critic model. In this framework, the generator model is responsible for producing intermediate reasoning steps, while the critic model analyzes these steps and provides detailed feedback, such as identifying calculation errors or logical inconsistencies. Xu et al.\u00a0(Xu et\u00a0al., 2023c) propose a multi-agent collaboration strategy to enhance the reasoning abilities of LLMs by simulating the academic peer review process. The framework is divided into", "ade012f7-64f3-40d7-be01-7e147e2842ae": "review process. The framework is divided into three stages: generation, review, and revision. Agents provide feedback and attach confidence scores to refine the initial answers, with the final result determined through majority voting.", "525562ca-e99f-432e-8ad8-f0d9051b5416": "While the feedback and correction mechanisms of LLMs judges are continually evolving, the limitations of self-feedback in improving quality should not be overlooked. Research on Self-Correct\u00a0(Huang et\u00a0al., 2023; Tyen et\u00a0al., 2023) shows that, the intrinsic self-correction capabilities of LLMs often fall short of effectively improving reasoning quality.", "53de0d1e-7d71-47e4-86a5-8dc794b17d8e": "Valmeekam et al.\u00a0(Valmeekam et\u00a0al., 2023) also raise concerns about the effectiveness of LLMs as self-validation tools in the absence of reliable external validators. Future research can focus on improving the accuracy of feedback provided by these LLM judges and incorporating external validation mechanisms to optimize their performance in complex reasoning tasks.", "2babc226-629e-4011-a7b9-9a7865e480b0": "3.3. Data Construction\n\nData collection is a crucial stage in the development of machine learning systems, especially those driven by the rapid advancements in deep learning. The quality of the data directly determines the performance of the trained models.\nThe LLMs-as-judges has significantly transformed the landscape of data collection, substantially reducing reliance on human effort.\nIn this section, we will explore the pivotal role of LLMs-as-judges in data collection from two key perspectives: Data Annotation (\u00a73.3.1) and Data Synthesize (\u00a73.3.2).\n\n\n\n3.3.1. Data Annotation", "257e193b-50b9-486a-8f5a-a80a50467abe": "3.3.1. Data Annotation\n\nData Annotation involves leveraging LLM judges to label large, unlabeled datasets efficiently\u00a0(He et\u00a0al., 2024a; Gilardi et\u00a0al., 2023; T\u00f6rnberg, 2023; Hao et\u00a0al., 2024). By utilizing the advanced natural language understanding and reasoning capabilities of LLMs, the annotation process can be automated to a significant extent, enabling the generation of high-quality labels with reduced human intervention.", "aa192f4b-5d18-4568-a4a2-623b98bae8a9": "LLMs have demonstrated remarkable potential in text annotation tasks, consistently outperforming traditional methods and human annotators in various settings.\nHe et al.\u00a0(He et\u00a0al., 2024a) evaluated the performance of GPT-4 in crowdsourced data annotation workflows, particularly in text annotation tasks. Their comparative study revealed that, even with best practices, the highest accuracy achievable by MTurk workers was 81.5%, whereas GPT-4 achieved an accuracy of 83.6%.\nSimilarly, Gilardi et al.\u00a0(Gilardi et\u00a0al., 2023) analyzed 6,183 tweets and news articles, demonstrating that ChatGPT outperformed crowdsourced workers in tasks such as stance detection, topic detection, and framing.", "8513b2d5-00ae-42f3-80af-8e881b3d9cec": "T\u00f6rnberg et al.\u00a0(T\u00f6rnberg, 2023) further investigated the classification of Twitter users\u2019 political leanings based on their tweet content. Their findings revealed that ChatGPT-4 not only surpassed human classifiers in accuracy and reliability but also exhibited bias levels that were comparable to or lower than those of human classifiers.", "2a6c3a5b-b635-432d-8127-06cf8d264f62": "As technology advances, more and more research is exploring their application in multimodal data annotation. For example, the FullAnno\u00a0(Hao et\u00a0al., 2024) uses the GPT-4V model to generate image annotations, significantly improving the quality of image descriptions through a multi-stage annotation process.", "1bd1aa91-43eb-4da2-95a6-0d94de7a914d": "Furthermore, Latif et al.\u00a0(Latif et\u00a0al., 2023) explored the application of LLMs in speech emotion annotation, demonstrating that, with data augmentation, LLM-annotated samples can significantly enhance the performance of speech emotion recognition models. By integrating text, audio features, and gender information, the effectiveness of LLM-based annotations was further improved, highlighting their potential in advancing multimodal annotation tasks.", "f564a653-ed3c-4279-ae06-955525df3abf": "As LLMs perform excellently in annotation tasks, researchers are actively exploring methods to further improve annotation quality and address potential challenges. For example, AnnoLLM\u00a0(He et\u00a0al., 2023a) introducedthe \u201cexplain-then-annotate\u201d method, which enhances both the accuracy and transparency of annotations by prompting the LLM to justify its label assignments.", "d0f42152-d4e0-434c-a3b0-73598d452909": "Additionally, the LLMAAA\u00a0(Zhang et\u00a0al., 2023a) framework incorporates an active learning strategy to efficiently select high-information samples for annotation, thereby mitigating the effects of noisy labels and reducing the reliance on costly human annotation. These approach not only enhance the performance of task-specific models but also offer new perspectives on the efficient application of LLMs in annotation workflows.", "51f5add2-53ea-4802-860a-3091c6409f51": "3.3.2. Data Synthesize\n\nThe goal of Data Synthesis is to create entirely new data, either from scratch or based on seed data, while ensuring it is similar in distribution to real data. Data Synthesis enables the generation of diverse data samples, enhancing a model\u2019s generalization ability to unseen examples while reducing reliance on sensitive real-world data\u00a0(Ye et\u00a0al., 2023a; Dong et\u00a0al., 2024a; Arif et\u00a0al., 2024; Wang et\u00a0al., 2022; Kim et\u00a0al., 2024a; Mendon\u00e7a et\u00a0al., 2024).", "7902ec38-a0ac-4784-80e2-f9a0bf237fc9": "In recent years, advancements in LLMs have led to significant improvements in both the quality and efficiency of data synthesis methods. In this domain, methods like SELFEE\u00a0(Ye et\u00a0al., 2023a) and SynPO\u00a0(Dong et\u00a0al., 2024a) have effectively enhanced the alignment capabilities of LLMs by leveraging small amounts of labeled data and iteratively generating preference-aligned data. Arif et al.\u00a0(Arif et\u00a0al., 2024) also introduce a multi-agent workflow for generating optimized preference datasets.", "646d15ac-c004-4c3c-8dff-255bef055a58": "SELF-INSTRUCT\u00a0(Wang et\u00a0al., 2022) and Evol-Instruct\u00a0(Xu et\u00a0al., 2023d; Zeng et\u00a0al., 2024) represent innovative approaches to improving model alignment and performance through self-generated instruction data. SELF-INSTRUCT\u00a0(Wang et\u00a0al., 2022) requires minimal human annotation, instead relying on self-generated instruction data to align pre-trained models. Evol-Instruct\u00a0(Xu et\u00a0al., 2023d; Zeng et\u00a0al., 2024) further enhances LLM performance by automatically generating instruction data, significantly boosting model capabilities.", "9080a671-c947-47e8-ac5a-233215315cec": "STaR\u00a0(Zelikman et\u00a0al., 2024) and ReSTEM\u00a0(Singh et\u00a0al., 2023) are research efforts aimed at enhancing reasoning capabilities through synthetic data. STaR\u00a0(Zelikman et\u00a0al., 2024) employs a self-guided iterative process to improve model performance on complex reasoning tasks, offering an effective solution for tackling increasingly sophisticated reasoning challenges in the future. ReSTEM\u00a0(Singh et\u00a0al., 2023), on the other hand, utilizes a self-training approach based on the expectation-maximization framework to enhance the problem-solving capabilities of large language models, particularly in areas such as solving mathematical problems and generating code.\n\n\n\n\n\n\n4. Methodology", "4cd7d65c-63c1-438d-b6b6-274808e7390f": "4. Methodology\n\nThe use of LLM judges requires careful methodological considerations to ensure the accuracy and consistency of judgments.\nResearchers have developed various approaches according to the complexity and specific requirements of different judgment tasks, each offering unique advantages.\nIn this section, we categorize these methodologies into three broad approaches: Single-LLM System (\u00a74.1): evaluation by a single-LLM, Multi-LLM System (\u00a74.2): evaluation by cooperation among multi-LLMs, and Human-AI Collaboration (\u00a74.3): evaluation by cooperation of LLMs and Human.\nFigure \u00a05 presents an overview of methodology.\n\n\nFigure 5. Overview of the Methodology of LLMs-as-judges.", "877c574c-2c30-477c-b8cb-3d39dbd2a687": "4.1. Single-LLM System\n\nSingle-LLM System relies on a single model to perform judgment tasks, with its effectiveness largely determined by the LLM\u2019s capabilities and the strategies used to process input data. This approach can generally be divided into three fundamental components: Prompt Engineering (\u00a74.1.1), Tuning (\u00a74.1.2), and Post-processing (\u00a74.1.3) of model outputs.\n\n\n\n4.1.1. Prompt-based", "c67b9fcc-cf42-4d0c-a2f9-5cf156c1bc98": "4.1.1. Prompt-based\n\nPrompt engineering\u00a0(Sahoo et\u00a0al., 2024) involves crafting clear and structured input prompts tailored to elicit accurate and contextually appropriate responses from LLM judges. This approach is crucial for ensuring that LLMs grasp the complexities of specific tasks and provide relevant, consistent, and goal-aligned judgments.\nIn many cases, well-designed prompts significantly reduce the need for extensive model training.", "ff637ae5-5f8c-421d-a87a-bf928ca26a30": "In-Context Learning. In-Context Learning (ICL) is a distinctive capability of LLMs that allows them to dynamically adapt to evaluation tasks using carefully curated examples or explanations within the prompt\u00a0(Dong et\u00a0al., 2022). Several recent methods have demonstrated the power of ICL in LLM-as-judges, showcasing how it enhances the flexibility and performance of LLMs in diverse settings. For example, GPTScore\u00a0(Fu et\u00a0al., 2023) leverages the few-shot learning capability of generative pre-trained models to evaluate generated text. By using relevant examples to customize prompts, it provides a flexible, training-free approach to assess multiple aspects of text quality.", "8d8d88dd-cc7f-4ace-bb9c-1c63a971e7bb": "Similarly, LLM-EVAL\u00a0(Lin and Chen, 2023) incorporates carefully crafted examples into prompts, proposing a unified, multi-dimensional automatic evaluation method for open-domain dialogue.\nAnother notable example is TALEC\u00a0(Zhang et\u00a0al., 2024c), a model-based evaluation method that leverages in-context learning to enable users to set custom evaluation criteria for LLMs in specific domains. Through careful prompt engineering, users can iteratively adjust the examples to refine the evaluation process as needed.", "b668fa40-6592-4300-911e-dcc762c7e130": "In addition, Jain et al.\u00a0(Jain et\u00a0al., 2023) proposed the In-Context Learning-based Evaluator (ICE) for multi-dimensional text evaluation. ICE leverages LLMs and a small number of in-context examples to evaluate generated text summaries, achieving competitive results.", "ddbfc224-3adb-4b59-9003-33a064d4f4b6": "While ICL can enable effective evaluation, it is not without challenges. One major issue is that the model\u2019s responses may be influenced by the selection of prompt examples, potentially leading to bias\u00a0(Zhao et\u00a0al., 2021; Zhou et\u00a0al., 2023a; Han et\u00a0al., 2022; Fei et\u00a0al., 2023).", "185afd7e-fae3-4b61-9d68-b4e2a795109b": "To address this issue, Hasanbeig et al. proposed ALLURE\u00a0(Hasanbeig et\u00a0al., 2023), a comprehensive protocol designed to mitigate bias in ICL for LLMs during text evaluation. ALLURE\u00a0(Hasanbeig et\u00a0al., 2023) improves evaluator accuracy by iteratively incorporating discrepancies between its assessments and annotated data into the learning context. Moreover, after uncovering the existence of symbol bias within LLM evaluators when using ICL, Song et al.\u00a0(Song et\u00a0al., 2024b) proposed two effective mitigation strategy prompt templates, Many-Shot with Reference (MSwR) and Many-Shot without Reference (MSoR), to bolster the reliability and precision of LLM-based assessments.", "2b8ece9f-e5ff-4cb4-bd14-6664728789f1": "Step-by-step.\nStep-by-step involves breaking down complex evaluation tasks into fine-grained components, leveraging the reasoning capabilities of LLMs to simplify the evaluation process. The most straightforward example of which is perhaps Chain-of-Thought (CoT)\u00a0(Wei et\u00a0al., 2022; Kotonya et\u00a0al., 2023). Building on that, frameworks like G-EVAL\u00a0(Liu et\u00a0al., 2023a) have been proposed to assess the quality of NLG outputs. G-EVAL\u00a0(Liu et\u00a0al., 2023a) combines CoT with a form-filling paradigm, allowing the LLM to assess outputs in a structured manner.", "18b95cba-56f8-47c6-9aca-263613a4627b": "Similarly, ICE-Score\u00a0(Zhuo, 2023) introduces a step-by-step framework for evaluating code, in which the LLM is instructed with task definitions, evaluation criteria, and detailed evaluation steps. By breaking the task down into clear steps, ICE-Score\u00a0(Zhuo, 2023) improves the quality and consistency of code evaluation. Also, ProtocoLLM\u00a0(Yi et\u00a0al., 2024) employs a similar step-by-step approach to evaluate the specialized capabilities of LLMs in generating scientific protocols.", "a8cfb0fe-572a-4b0a-b47c-ca067e0ef1d7": "Portia\u00a0(Li et\u00a0al., 2023d) achieves better evaluation results in a lightweight yet effective manner. It divides the answer into multiple parts, aligns similar content between candidate answers, and then merges them back into a single prompt for evaluation by the LLM.", "47e68ab7-06bc-4110-9508-a1fdf6d498ec": "Some studies break down evaluations into two steps: \u201cexplanation-rating.\u201d This approach suggests that providing an explanation enhances the reliability of the rating. Chiang et al.\u00a0(Chiang and Lee, 2023) offer empirical guidelines to improve the quality of LLM evaluations, demonstrating that combining rating with explanation (rate-explain) or explanation with rating (explain-rate) leads to higher correlations with human ratings.", "71c88e9b-dc75-4994-81d0-ad718e4393ce": "Another effective strategy is to decompose complex evaluation standards into specific, discrete criteria, allowing the LLM to assess each aspect independently. FineSurE\u00a0(Song et\u00a0al., 2024a) is an advanced example of this method, offering a framework for the fine-grained evaluation of text summarization quality. It breaks down the evaluation into multiple dimensions, such as faithfulness, completeness, and conciseness. Through detailed analysis, including fact-checking and key fact alignment, FineSurE\u00a0(Song et\u00a0al., 2024a) outperforms traditional methods in terms of evaluation accuracy.", "45d2e516-48df-4ac3-81a3-3eeb7a55b9dc": "Definition Augmentation.\nThe Enhanced Definition approach involves refining prompts to inject improved evaluation criteria, establish assessment principles, or incorporate external knowledge into the LLM judge\u2019s decision-making process.\nSome studies focus on enriching and clarifying the prompts to ensure that the evaluation criteria are both comprehensive and well-defined.", "5ab4ce9f-50ad-494c-9458-c1a91433ebfa": "For example, Liu et al. propose AUTOCALIBRATE\u00a0(Liu et\u00a0al., 2023d), a multi-stage, gradient-free approach. This method involves the drafting, revision, and application of calibrated criteria, and it automatically calibrates and aligns an LLM-based evaluator to match human preferences for NLG quality assessment.\nFurthermore, SALC\u00a0(Gupta et\u00a0al., 2024) enables LLMs to autonomously generate context-aware evaluation criteria for self-assessment, overcoming the limitations of static, human-defined metrics.", "e7e4c7fa-9064-4f67-bbdb-93e9adc1bc27": "On the other hand, the LLM-as-a-Personalized-Judge approach\u00a0(Dong et\u00a0al., 2024b) introduces a novel perspective by incorporating diverse evaluative roles and principles. This allows LLMs to adapt to complex, varied evaluation scenarios, resulting in more nuanced and context-sensitive assessments.", "b89b1da1-c2df-4c75-93e5-789233bcd07b": "Another key aspect of Definition Augmentation is the retrieval of external knowledge, which helps reduce hallucinations and provides more factual support. For instance, BiasAlert\u00a0(Fan et\u00a0al., 2024), a tool designed to detect social bias in LLM-generated open-text outputs. It integrates external human knowledge with the LLM judge\u2019s inherent reasoning capabilities to reliably identify and mitigate bias, outperforming GPT4-as-A-Judge across various scenarios.\nMoreover, Chen et al.\u00a0(Chen et\u00a0al., 2024c) found that within retrieval-augmented generation (RAG) frameworks, LLM judges do not exhibit a significant self-preference effect during evaluation.", "c98de2e3-0733-437d-a1b2-5bac282dfa9f": "Multi-turn Optimization.\nMulti-turn optimization involves iterative interactions between the evaluator and the evaluated entity, refining evaluation results through diverse forms of feedback, thus fostering deeper analysis and a progressive improvement in evaluation quality\u00a0(Zhou et\u00a0al., 2024e).\nUnlike traditional methods that rely on predefined criteria, Xu et al. proposed ACTIVE-CRITIC\u00a0(Xu et\u00a0al., 2024b), enabling LLMs to infer evaluation criteria from data and dynamically optimize prompts through multiple rounds of interaction. Moreover,", "5d3540c9-efae-45c0-9037-d56b52aebfd2": "Some studies\u00a0(Zhao et\u00a0al., 2024b; Luo et\u00a0al., 2024; Bai et\u00a0al., 2024; Yu et\u00a0al., 2024) leverage LLMs as question designers to engage in dynamic interactions with the evaluated entities, adjusting the questions and task design in real time. This allows for flexible modification of the evaluation content based on the performance of the evaluated entity, thereby enabling more comprehensive assessments.", "a2eab9db-4cce-4797-bce5-43d4b37fa579": "4.1.2. Tuning-based\n\nTuning involves training a pre-existing LLM on a specialized dataset to adapt it to specific judgment tasks. It\u2019s especially useful when the judgment domain involves highly specialized knowledge or nuanced decision-making\u00a0(Huang et\u00a0al., 2024b).\n\n\nScore-based Tuning. Score-based tuning involves using data with scores to train models and enhance their ability to predict judgment scores based on specific evaluation criteria\u00a0(Chen et\u00a0al., 2023c; Deshwal and Chawla, 2024; Wang et\u00a0al., 2023e).", "64e473dc-ae1a-4a74-aeff-e335219c69b1": "Many studies have explored the enhancement of LLM-as-judges by fine-tuning them on human-labeled datasets.\nFor instance, PHUDGE\u00a0(Deshwal and Chawla, 2024), fine-tuned from the Phi-3 model, achieves state-of-the-art performance in terms of latency and throughput when automatically evaluating the quality of outputs from LLMs. This fine-tuning process equips the model with the necessary judgment skills, enabling it to assess various types of content in a structured and accurate manner.", "08427c60-eb63-4ee0-8afa-b44934294c4f": "Additionally, ECT\u00a0(Wang et\u00a0al., 2023e) introduces a novel method for transferring scoring capabilities from LLMs to lighter models. This allows the lighter models to function as effective reward models for sequence generation tasks, enhancing sequence generation models through reinforcement learning and reranking approaches.\nAttrScore\u00a0(Yue et\u00a0al., 2023b) is another framework for evaluating attribution and identifying specific types of attribution errors, using a curated test set from a generative search engine and simulated examples from existing benchmarks.", "e620ae0b-039e-42a3-803c-76c54b93b638": "The above research highlights that LLMs can better align their decision-making process with humans through fine-tuning with human-constructed datasets.", "9f683436-662c-47f7-a4a9-799457abd39c": "In addition to human-labeled data, some studies have also attempted to fine-tune models using synthetic datasets like SorryBench\u00a0(Xie et\u00a0al., 2024b) generated for evaluation tasks. These datasets are often created through rule-based methods or by generating artificial evaluation examples, which also give rise to some metrics like TIGERScore\u00a0(Jiang et\u00a0al., 2023b).\nSELF-J\u00a0(Ye and Ng, 2024) is a self-training framework for developing judge models to evaluate LLMs\u2019 adherence to human instructions without human-annotated quality scores. SELF-J\u00a0(Ye and Ng, 2024) proposes selective instruction following, allowing systems to decline low-quality instructions.", "9d086d5b-0034-484a-823a-f0783b607d1b": "FENCE\u00a0(Xie et\u00a0al., 2024d) is another factuality evaluator designed to provide claim-level feedback to language model generators. It details a data augmentation approach that enriches public datasets with textual critiques and diverse source documents from various tools, thereby enhancing factuality without introducing lesser-known facts.\nUtilizing synthetic training data to fine-tune lightweight language model judges and employing prediction-powered inference (PPI) for statistical confidence to mitigate potential prediction errors, ARES\u00a0(Saad-Falcon et\u00a0al., 2023) can automatically assess RAG systems.", "5175e9b7-ab3e-4ef7-a555-f8ddd13ddae7": "Preference-based Learning.\nPreference-based learning focuses on training LLMs to make inferences and learn based on preferences, enabling the development of more adaptive and customizable evaluation capabilities.", "30cf7388-71a5-4655-b0e9-0aaa8e692352": "Initially, researchers leverage these data in conjunction with advanced techniques like Direct Preference Optimization (DPO)\u00a0(Rafailov et\u00a0al., 2024) to train LLMs for more nuanced evaluative capabilities. In this method, the model is trained to predict which of two outputs is preferred according to human-like values, rather than learning a scalar reward signal. Such self-improving approach is well reflected in Meta-Rewarding\u00a0(Wu et\u00a0al., 2024b).\nCon-J\u00a0(Ye et\u00a0al., 2024a) trains a generative judge by using the DPO loss on contrastive judgments and the SFT loss on positive judgments to align LLMs with human values.", "3cf99d95-b227-4316-bcb2-8436bb3f5dfb": "In terms of evaluating other LLMs effectively in open-ended scenarios, JudgeLM\u00a0(Zhu et\u00a0al., 2023) addresses key biases in the fine-tuning process with a high-quality preference dataset.\nAnother typical method is PandaLM\u00a0(Wang et\u00a0al., 2023d), which is trained on a reliable human-annotated preference dataset, focusing extends beyond just the objective correctness of responses, and addresses vital subjective factors.", "fb714e88-9244-4a05-801e-73bc09147111": "Moreover, Self-Taught\u00a0(Wang et\u00a0al., 2024e) is another approach to train LLMs as effective evaluators without relying on human-annotated preference judgments, using synthetic training data only. Through an iterative self-improvement scheme, LLM judges are able to produce reasoning traces and final judgments.\nNot quite the same, FedEval-LLM\u00a0(He et\u00a0al., 2024b) fine-tunes many personalized LLMs without relying on labeled datasets to provide domain-specific evaluation, mitigating biases associated with single referees. It is designed to assess the performance of LLMs on downstream tasks, at the same time, ensuring privacy preservation.", "b110c011-3c63-4f9a-9c9a-64ea4fdbe255": "As research has progressed, newer methods have emerged that combine both score-based and preference-based data to refine model evaluation capabilities, not to mention some novel metrics like INSTRUCTSCORE\u00a0(Xu et\u00a0al., 2023e).\nFLAMe\u00a0(Vu et\u00a0al., 2024) is an example of such an approach. It\u2019s a family of Foundational Large Autorater Models which significantly improves generalization to a wide variety of held-out tasks using both pointwise and pairwise methods during training.\nAs generative judge model, AUTO-J\u00a0(Li et\u00a0al., 2023c) addresses challenges in generality, flexibility, and interpretability by training on a diverse dataset containing scoring and preference.", "6b28b0da-e17f-4cba-9937-b48c50bb7a30": "To critique and refine the outputs of large language models, Shepherd\u00a0(Wang et\u00a0al., 2023c) leverages a high-quality feedback dataset to identify errors and suggest improvements across various domains.\nIn the domain of NLG, X-EVAL\u00a0(Liu et\u00a0al., 2023c) consists of a vanilla instruction tuning stage and an enhanced instruction tuning stage that exploits connections between fine-grained evaluation aspects.\nNotably, Themis\u00a0(Hu et\u00a0al., 2024a) also achieved outstanding results acting as a reference-free NLG evaluation language model designed for flexibility and interpretability.", "8dad49ac-7195-4cc6-b652-205254d7e136": "Similarly, CritiqueLLM\u00a0(Ke et\u00a0al., 2024) provides effective and explainable evaluations of LLM outputs, and uses a dialogue-based prompting method to generate high-quality referenced and reference-free evaluation data.\nSelf-Rationalization\u00a0(Trivedi et\u00a0al., 2024a) enhances LLM performance by iteratively fine-tuning the judge via DPO, which allows LLMs to learn from their own reasoning.\nBased on pointwise and pairwise dataset, CompassJudger-1\u00a0(Cao et\u00a0al., 2024a) acts as an open-source, versatile LLM for efficient and accurate evaluation of other LLMs.", "2135b88c-648d-4abc-95ab-df68cdf358ad": "Likewise, Zhou et al.\u00a0(Zhou et\u00a0al., 2024c) introduces a systematic framework for bias reduction, employing calibration for closed-source models and contrastive training for open-source models.\nApart from that, HALU-J\u00a0(Wang et\u00a0al., 2024b) is designed to enhance hallucination detection in LLMs by selecting pertinent evidence and providing detailed critiques.", "4960d73d-a9c2-4075-86d3-bb0542e216a6": "PROMETHEUS\u00a0(Kim et\u00a0al., 2023) and PROMETHEUS 2\u00a0(Kim et\u00a0al., 2024b) are open-source LLMs specialized for fine-grained evaluation that can generalize to diverse, real-world scoring rubrics beyond a single-dimensional preference, supporting both direct assessment and pairwise ranking, and can evaluate based on custom criteria. What\u2019s more, the following PROMETHEUS-VISION\u00a0(Lee et\u00a0al., 2024a) fills the gap in the visual field.\nAs for various multimodal tasks, LLaVA-Critic\u00a0(Xiong et\u00a0al., 2024) demonstrates its effectiveness in providing reliable evaluation scores and generating reward signals for preference learning, highlighting the potential of open-source LMMs in self-critique and evaluation.", "b3923a15-1d3a-4756-8bb1-76aa9afdfbfe": "Table 1. An Overview of Fine-Tuning Methods in Single-LLM Evaluation (Sorted in ascending alphabetical order).\n\n\n\n\nMethod\nData Construction\nTuning Method\nBase LLM\n\n\nAnnotator\nDomain\nScale\nEvaluation Type\nTechnique\n\n\n\n\nARES\u00a0(Saad-Falcon et\u00a0al., 2023)\n\nHuman & LLM\nRAG System\n-\nPairwise\nPPI\nDeBERTa-v3-Large\n\n\nAttrScore\u00a0(Yue et\u00a0al., 2023b)\nHuman\nVarious\n63.8K\nPointwise\nSFT\nMultiple LLMs\n\n\nAUTO-J\u00a0(Li et\u00a0al., 2023c)\n\nHuman & GPT-4\nVarious\n4396\nPointwise & Pairwise\nSFT\nLlama2-13B-Chat\n\n\n\n\n\n\nPointwise, Pairwise,\n\n\n\n\nCompassJudger-1\u00a0(Cao et\u00a0al., 2024a)\nHuman & LLM\nVarious\n900K\n& Generative\nSFT\nQwen2.5 Series\n\n\nCon-J\u00a0(Ye et\u00a0al., 2024a)", "17b20f75-704c-4b60-8739-9c069b615bfa": "Con-J\u00a0(Ye et\u00a0al., 2024a)\n\nHuman & ChatGPT\nCreation, Math, & Code\n220K\nPairwise\nSFT & DPO\nQwen2-7B-Instruct\n\n\nCritiqueLLM\u00a0(Ke et\u00a0al., 2024)\nHuman & GPT-4\nVarious\n7722\nPointwise & Pairwise\nSFT\nChatGLM3-6B\n\n\n\n\nMachine Translation,\n\n\n\n\n\n\n\n\nText Style Transfer,\n\n\n\n\n\n\nECT\u00a0(Wang et\u00a0al., 2023e)\nChatGPT\n& Summarization\n-\nPointwise\nSFT & RLHF\nRoBERTa\n\n\n\n\nInstruct-tuning\n5K, 10K,\n\n\n\n\n\nFedEval-LLM\u00a0(He et\u00a0al., 2024b)\nHuman\n& Summary\nper client\nPairwise\nLoRA\nLlama-7B\n\n\n\n\nSummarization,\n\n\n\n\n\n\nFENCE\u00a0(Xie et\u00a0al., 2024d)\nHuman & LLM\nQA, & Dialogue\n-\nPointwise\nSFT & DPO\nLlama3-8B-Chat\n\n\n\n\n\n\nPointwise, Pairwise,\n\n\n\n\n\n\n\n\nClassification,", "9f0e2322-ff1d-4cd4-aa4a-bd2db245c74e": "Pointwise, Pairwise,\n\n\n\n\n\n\n\n\nClassification,\n\n\n\n\nFLAMe\u00a0(Vu et\u00a0al., 2024)\nHuman\nVarious\n5.3M\n& Open-ended generation\nRLHF\nPaLM-2-24B\n\n\n\nGPT-4-Turbo\nMultiple-Evidence\n\n\n\n\n\n\nHALU-J\u00a0(Wang et\u00a0al., 2024b)\n& GPT-3.5-Turbo\nHallucination Detection\n2663\nPointwise & Pairwise\nSFT & DPO\nMistral-7B-Instruct\n\n\nHelpSteer2\u00a0(Wang et\u00a0al., 2024a)\nHuman\nVarious\n-\nPointwise & Pairwise\nPPI & RLHF\nLlama3.1-70B-Instruct\n\n\nINSTRUCTSCORE\u00a0(Xu et\u00a0al., 2023e)\n\nGPT-4\nVarious\n40K\nPointwise & Pairwise\nSFT\nLlama-7B\n\n\nJudgeLM\u00a0(Zhu et\u00a0al., 2023)\nGPT-4\nOpen-ended Tasks\n100K\nPairwise\nSFT\nVicuna Series\n\n\nLLaVA-Critic\u00a0(Xiong et\u00a0al., 2024)\n\nGPT-4o\nVarious\n113K\nPointwise & Pairwise\nDPO\nLLaVA-OneVision(OV) 7B & 72B", "1da43f09-62c7-4011-a0f6-88aa6f35f761": "Meta-Rewarding\u00a0(Wu et\u00a0al., 2024b)\nLlama3\nVarious\n20K\nPairwise\nDPO\nLlama3-8B-Instruct\n\n\nOffsetBias\u00a0(Park et\u00a0al., 2024)\n\nHuman & LLM\nBias Detection\n268K\nPairwise\nRLHF\nLlama3-8B-Instruct\n\n\nPandaLM\u00a0(Wang et\u00a0al., 2023d)\nHuman\nVarious\n300K\nPairwise\nSFT\nLlama-7B\n\n\nPHUDGE\u00a0(Deshwal and Chawla, 2024)\n\nHuman & GPT-4\nNLG\n-\nPointwise & Pairwise\nLoRA\nPhi-3\n\n\nPROMETHEUS\u00a0(Kim et\u00a0al., 2023)\nHuman\nVarious\n100K\nPointwise\nSFT\nLlama2-Chat-7B & 13B\n\n\nPROMETHEUS2\u00a0(Kim et\u00a0al., 2024b)\n\nHuman\nVarious\n300K\nPointwise & Paiwise\nSFT\nMistral-7B & Mistral-8x7B\n\n\nPROMETHEUS-VISION\u00a0(Lee et\u00a0al., 2024a)\nGPT-4V\nVarious\n15K\nPointwise\nSFT\nLlava-1.5\n\n\n\n\nCommon, Coding,", "b16b38ad-a950-4303-8f74-924ef0c16b61": "Common, Coding,\n\n\n\n\n\n\nSELF-J\u00a0(Ye and Ng, 2024)\nHuman & GPT-4\n& Academic\n5.7M\nPointwise\nLoRA\nLlama2-13B\n\n\nSelf-Rationalization\u00a0(Trivedi et\u00a0al., 2024a)\nLLM\nVarious\n-\nPointwise & Pairwise\nSFT & DPO\nLlama3.1-8B-Instruct\n\n\nSelf-Taught\u00a0(Wang et\u00a0al., 2024e)\n\nLLM\nVarious\n20K\nPairwise\n-\nLlama3-70B-Instruct\n\n\nShepherd\u00a0(Wang et\u00a0al., 2023c)\nHuman\nVarious\n-\nPointwise & Pairwise\nSFT\nLlama-7B\n\n\nSorryBench\u00a0(Xie et\u00a0al., 2024b)\n\nHuman & GPT-4\nUnsafe Topics\n2.7K\nPointwise\nSFT\nMultiple LLMs\n\n\nThemis\u00a0(Hu et\u00a0al., 2024a)\nHuman & GPT-4\nNLG\n67K\nPointwise & Pairwise\nSFT & DPO\nLlama3-8B\n\n\nTIGERScore\u00a0(Jiang et\u00a0al., 2023b)\n\nHuman & GPT-4\nText Generation\n42K\nPointwise\nSFT\nLlama2-7B & 13B", "b32a6b5b-a08f-4ca6-931b-e12bd130e7dc": "X-EVAL\u00a0(Liu et\u00a0al., 2023c)\nHuman\nNLG\n55,602\nPointwise & Pairwise\nSFT\nFlan-T5\n\n\n\n\n\n\n\n\n4.1.3. Post-processing\n\nPost-processing involves further refining evaluation results to extract more precise and reliable outcomes. This step typically includes analyzing the initial outputs to identify patterns, inconsistencies, or areas requiring improvement, followed by targeted adjustments and in-depth analysis. By addressing these issues, post-processing ensures that the evaluation results are not only accurate but also aligned with the specific objectives and standards of the task.", "c6152016-2d06-4ec0-815b-670fa852ed9d": "Probability Calibration.\nDuring the post-hoc process of the model output, some studies use rigorous mathematical derivations to quantify the differences, thereby optimizing them.\nFor instance, Daynauth et al.\u00a0(Daynauth and Mars, 2024) investigates the discrepancy between human preferences and automated evaluations in language model assessments, particularly employs Bayesian statistics and a t-test to quantify bias towards higher token counts, and develops a recalibration procedure to adjust the GPTScorers.", "55d1d8fc-9b9e-4ecf-b3cb-8cdc0699cf77": "Apart from that, ProbDiff\u00a0(Xia et\u00a0al., 2024b) is another novel self-evaluation method for LLMs that assesses model efficacy by computing the probability discrepancy between initial responses and their revised versions.\nMoreover, Liusie et al.\u00a0(Liusie et\u00a0al., 2024) introduces a Product of Experts (PoE) framework for efficient comparative assessment using LLMs, which yield an expression that can be maximized with respect to the underlying set of candidates. This paper proposes two experts, a soft Bradley-Terry expert and a Gaussian expert that has closed-form solutions.", "3505b143-1859-4ba0-b9ba-6aadd091ad4b": "Unlike from frameworks above, CRISPR\u00a0(Yang et\u00a0al., 2024) is a novel bias mitigation method for LLMs executing instruction-based tasks, which identifies and prunes bias neurons with probability calibration, reducing bad performance without compromising pre-existing knowledge.", "505523e4-2c89-4bbf-a1a9-9d01b0a5f99b": "Text Reprocessing.\nIn LLMs-as-judges, text reprocessing methods are essential for enhancing the accuracy and reliability of evaluation outcomes. Specifically, text processing can improve the evaluation process by integrating multiple evaluation results or outcomes from several rounds of assessment.\nFor example, Sottana et al.\u00a0(Sottana et\u00a0al., 2023) employs a multi-round evaluation process. Each round involves scoring model outputs based on specific criteria, with the human and GPT-4 evaluations ranking model performances from best to worst and averaging these rankings to mitigate subjectivity.", "3e2a007e-9292-45e7-997a-97a44a80cb78": "For the single-response evaluation, AUTO-J\u00a0(Li et\u00a0al., 2023c) employs a \"divide-and-conquer\" strategy. Critiques that either adhere to or deviate from the scenario-specific criteria are consolidated to form a comprehensive evaluation judgment and then generate the final assessment.\nConsistent with former aforementioned studies, Yan et al.\u00a0(Yan et\u00a0al., 2024a) introduces a post-processing method to consolidate the relevance labels generated by LLMs. It demonstrates that this approach effectively combines both the ranking and labeling abilities of LLMs through post-processing.", "bec87117-95c5-40d7-b81a-759caec9365b": "Furthermore, REVISEVAL\u00a0(Zhang et\u00a0al., 2024b) is a novel evaluation paradigm that enhances the reliability of LLM Judges by generating response-adapted references through text revision capabilities of LLMs.\nApart from that, Tessler et al.\u00a0(Tessler et\u00a0al., 2024) explores the use of AI as a mediator in democratic deliberation, aiming to help diverse groups find common ground on complex social and political issues. With the goal of maximizing group approval, the researchers developed the \"Habermas Machine\", which iteratively generate group statements based on individual opinions.", "b511ce58-a85c-408d-816f-d29b88a8484f": "Another category of text reprocessing methods involves task transformation, primarily focusing on the conversion between open-ended and multiple-choice question (MCQ) formats. Ren et al.\u00a0(Ren et\u00a0al., 2023) explores the use of self-evaluation to enhance the selective generation capabilities of LLMs. Specifically, the authors reformulate open-ended generation tasks into token-level prediction tasks, reduce sequence-level scores to token-level scores to improve quality calibration.", "b7fb5aeb-5a6f-4bfb-859e-5416a908e9e3": "Conversely, Myrzakhan et al.\u00a0(Myrzakhan et\u00a0al., 2024) introduces the Open-LLM-Leaderboard, a new benchmark for evaluating LLMs using open-style questions, which eliminates selection bias and random guessing issues associated with multiple-choice questions. It presents a method to identify suitable open-style questions and validate the correctness of LLM open-style responses against human-annotated ground-truths.", "c41bcbd3-30b7-424c-8005-8e501ef1512a": "4.2. Multi-LLM System\n\nMulti-LLM Evaluation harnesses the collective intelligence of multiple LLMs to bolster the robustness and reliability of evaluations. By either facilitating inter-model communication or independently aggregating their outputs, these systems can effectively mitigate biases, leverage complementary strengths across different models, refine decision-making precision, and foster a more nuanced understanding of complex judgments.\n\n\n\n4.2.1. Communication", "1b4c1bb8-93ed-452d-be10-fc455377084d": "4.2.1. Communication\n\nCommunication means the dynamic flow of information between LLMs, which is pivotal for sparking insights and sharing rationales during the judgment process.\nRecent research has shown that communication among LLMs can enable emergent abilities through their interactions\u00a0(Xu et\u00a0al., 2023c), leading to a cohesive decision-making process and better judgment performance.\nThe Multi-LLM system can benefit from LLM interactions in two ways: cooperation and competition.", "d94e8c90-4405-471d-a597-3f17afb77473": "Cooperation.\nMulti-LLMs can work together to achieve a common goal with information and rationales sharing through interactions to enhance the overall evaluation process.\nFor example, Zhang et\u00a0al. (2023b) proposed an architecture named WideDeep to aggregate information at the LLM\u2019s neuro-level.\nIn addition, Xu et\u00a0al. (2023c) introduced a multi-agent collaboration strategy that mimics the academic peer review process to enhance complex reasoning in LLMs.\nThe approach involves agents creating solutions, reviewing each other\u2019s work, and revising their initial submissions based on feedback.", "dd602890-5ff4-4c8d-aabb-93f710a2df2c": "Similarly, ABSEval\u00a0(Liang et\u00a0al., 2024b) utilizes four agents for answer synthesize, critique, execution, and commonsense, to build the overall workflow.\nAlthough the cooperation can complement each other\u2019s strengths between LLMs to a certain degree, this method still includes the risk of groupthink, where similar models reinforce each other\u2019s biases rather than providing diverse insights.", "f7967eb7-cea8-4208-adec-360cad8b5ad7": "Competition. Multi-LLMs systems can also benefit from competitive or adversarial communication, i.e., LLMs argue or debate to evaluate each other\u2019s outputs\u00a0(Zhao et\u00a0al., 2024b; Moniri et\u00a0al., 2024; Chan et\u00a0al., 2023; Li et\u00a0al., 2023b).\nSuch multi-LLMs systems could be categorized into centralized and decentralized structures\u00a0(Owens et\u00a0al., 2024).", "9ebbadc2-3d68-4260-a1b8-8d48c39b4743": "In the centralized structure, a single central LLM acts as the orchestrator of the conversation, highlighting the efficiency of a unified decision-making process.\nAuto-Arena\u00a0(Zhao et\u00a0al., 2024b) is such a novel framework that automates the evaluation of LLMs through agent peer battles and committee discussions, aiming to provide timely and reliable assessments. In detail, the framework conducts multi-round debates between LLM candidates, and uses an LLM judge committee to decide the winner.\nInspired by courtroom dynamics, Bandi and Harrasse (2024) propose two architectures, MORE and SAMRE, which utilize multiple advocates and iterative debates to dynamically assess LLM outputs.", "25eeec48-9aa9-4d1a-9bdb-4e436d957391": "In contrast, the decentralized structure emphasizes a collective intelligence where all models engage in direct communication, promoting a resilient and distributed decision-making structure.\nIn the domain of LLM debates, Moniri et al.\u00a0(Moniri et\u00a0al., 2024) introduced a unique automated benchmarking framework, employing another LLM as the judge to assess not only the models\u2019 domain knowledge but also their abilities in problem definition and inconsistency recognition.", "ae48837f-7b9e-43cf-8804-a89edc83f81b": "ChatEval\u00a0(Chan et\u00a0al., 2023) is another multi-agent debate framework that utilizes multiple LLMs with diverse role prompts and communication strategies on open-ended questions and traditional NLG tasks, significantly improves evaluation performance compared to single-agent methods.\nMoreover, PRD\u00a0(Li et\u00a0al., 2023b) applied peer rank and discussion to address issues like self-enhancement and positional bias in current LLM evaluation methods, leading to better alignment with human judgments and a path for fair model capability ranking.", "3c7c9fa5-d428-4f21-820e-ca28739bc057": "4.2.2. Aggregation\n\nAlternatively, in multi-LLM systems without communication, judgments are independently generated by multiple models, which are subsequently synthesized into a final decision through various aggregation strategies. Techniques such as majority vote, weighted averages, and prioritizing the highest confidence predictions, each play a crucial role. These methods allow each model to assess without interference, and eventually extract and combine the most effective elements from each model\u2019s response.", "669b219c-3bec-4835-a3fe-deb49970036a": "Simple voting methods, such as majority voting, by selecting the most frequent answers, offers a straightforward approach to synthesize evaluations.\nFor example, Badshah et al.\u00a0(Badshah and Sajjad, 2024) introduced a reference-guided verdict method for evaluating free-form text using multiple LLMs as judges. Combining these LLMs through majority vote significantly improves the reliability and accuracy of evaluations, particularly for complex tasks.", "469127de-89db-4797-a1e1-106b49554fb3": "Furthermore, PoLL\u00a0(Verga et\u00a0al., 2024) demonstrates that using a diverse panel of smaller models as judges through max voting and average pooling is not only an effective method for evaluating LLM performance, but also reduces intra-model bias of a single large model.\nLanguage-Model-as-an-Examiner\u00a0(Bai et\u00a0al., 2024) is another benchmarking framework to evaluate the performance of foundation models on open-ended question answering through voting. In the peer-examination mechanism, the LM serves as a knowledgeable examiner that formulates questions and evaluates responses in a reference-free manner.", "f5ef1efa-22ac-4a31-ab45-569e59e9497a": "What\u2019s more, multi-LLM evaluation could also be used in improving dataset quality. Choi et al.\u00a0(Choi et\u00a0al., 2024) provided an enhanced dataset, MULTI-NEWS+, which is the result of a cleansing strategy leveraging CoT and majority voting to identify and exclude irrelevant documents through LLM-based data annotation.", "00c8526d-829b-4c24-b7f3-dc4fd7959b77": "Weighted scoring aggregation involves assigning different importance to different model outputs, either by aggregating multiple overall scores for the same response or by combining assessments of different aspects of the response to form a comprehensive evaluation.\nOn the one hand, through a peer-review mechanism, PiCO\u00a0(Ning et\u00a0al., 2024) allows LLMs to answer unlabeled questions and evaluate each other without human annotations. It formalizes the evaluation as a constrained optimization problem, maximizing the consistency between LLMs\u2019 capabilities and corresponding weights.", "be95a2a2-aebb-4ae2-977f-60a24f7161c6": "Likewise, PRE\u00a0(Chu et\u00a0al., 2024; Chen et\u00a0al., 2024e) can automatically evaluate LLMs through a peer-review process. It selects qualified LLMs as reviewers through a qualification exam and aggregates their ratings using weights which is proportional to their agreement of humans, demonstrating effectiveness and robustness in evaluating text summarization tasks.\nIn the field of recommendation explanations, Zhang et al.\u00a0(Zhang et\u00a0al., 2024a) suggests that ensembles like averaging ratings of multiple LLMs can enhance evaluation accuracy and stability.", "46fbdd80-f0be-44b5-8ba1-06db2e6e5470": "On the other hand, for example, AIME\u00a0(Patel et\u00a0al., 2024) is an evaluation protocol that utilizes multiple LLMs that each with a specific role independently generate an evaluation on separate criteria and then combine them via concatenation.\nSimilarly, a paper introduces HD-EVAL\u00a0(Liu et\u00a0al., 2024a), which iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. By decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria, HD-EVAL demonstrates its superiority.", "e46c647e-a332-461a-8656-e1d21c0fa2cb": "Apart from weighting methods, there are some other advance mathematical aggregation techniques, such as Bayesian methods and graph-based approaches, offering more robust ways to handle uncertainties and inconsistencies across multiple evaluators.\nNotably, a paper introduces two calibration methods, Bayesian Win Rate Sampling (BWRS) and Bayesian Dawid-Skene\u00a0(Gao et\u00a0al., 2024), to address the win rate estimation bias when using many LLMs as evaluators for text generation quality.", "99c0742a-a224-4e13-8b7e-d788221170ca": "In addition to that, GED\u00a0(Hu et\u00a0al., 2024c) addresses inconsistencies in LLM preference evaluations by leveraging multiple weak evaluators to construct preference graphs, and then utilize DAG structure to ensemble and denoise these graphs for better, non-contradictory evaluation results.", "f7388864-ec3c-46e3-8236-31f6fa0842fc": "LLM-based aggregation is a grand-new perspective like Fusion-Eval\u00a0(Shu et\u00a0al., 2024). It\u2019s a novel framework that integrates various assistant evaluators using LLMs, each of which specializes in assessing distinct aspects of responses, to enhance the correlation of evaluation scores with human judgments for natural language systems.", "a7eee4ec-72b1-4eb9-a1ac-d9c8eef5efd9": "In addition to the above direct use of multiple model evaluation, the cascade framework employs a tiered approach, where weaker models are used initially for evaluations, and stronger models are engaged only when higher confidence is required, optimizing resource use and enhancing evaluation precision.\nJung et al.\u00a0(Jung et\u00a0al., 2024) proposes \"Cascaded Selective Evaluation\" to ensure high agreement with human judgments while using cheaper models.\nSimilar to the work above, Huang et al.\u00a0(Huang et\u00a0al., 2024b) proposes CascadedEval, a novel method integrating proprietary models, in order to compensate for the limitations of fine-tuned judge models.\n\n\n\n\n\n4.3. Human-AI Collaboration System", "a954e08e-4d91-44a7-97ab-40316e4ebb38": "4.3. Human-AI Collaboration System\n\nHuman-AI Collaboration Systems bridge the gap between automated LLM judgments and the essential need for human oversight, particularly in high-stakes domains such as law, healthcare, and education. Human evaluators act either as the ultimate deciders, or as intermediaries who verify and refine model outputs. By incorporating human insights, Hybrid systems can ensure the final judgment is more reliable and aligned with ethical considerations, and empower continuous model improvement through feedback loops.", "37074aa6-839c-4ad9-b3e8-c691d34ac431": "In many Human-AI Collaboration systems, human evaluators play a vital role during the evaluation process itself, actively collaborating with the LLMs to review and refine the generated outputs.\nFor example, COEVAL(Li et\u00a0al., 2023a) introduces a collaborative evaluation pipeline where LLMs generate initial criteria and evaluations for open-ended natural language tasks. These machine-generated outputs are then reviewed and refined by human evaluators to guarantee reliability.", "db71458f-6c71-42e8-bfda-89708f83b7f8": "To address a significant positional bias in LLMs when used as evaluators, Wang et al.\u00a0(Wang et\u00a0al., 2023b) proposes a calibration framework with three strategies: Multiple Evidence Calibration, Balanced Position Calibration, and Human-in-the-Loop Calibration.\nSimilarly, EvalGen(Shankar et\u00a0al., 2024) integrates human feedback iteratively to refine evaluation criteria, addressing challenges such as \u201ccriteria drift\u201d, where the standards of evaluation evolve as humans interact with the model.\nThese systems allow human evaluators to provide real-time adjustments, enhancing the accuracy and trustworthiness of the evaluation process.", "d2d5ad00-6a11-4d43-8a4d-914825752034": "While in other systems, human involvement takes place after the LLM has completed its evaluations, providing a final layer of verification and adjustment. This method ensures that the LLM\u2019s judgments are thoroughly scrutinized and aligned with human values.\nEvaluLLM(Pan et\u00a0al., 2024a) allows humans to intervene and refine the evaluation results, thereby enhancing trust in the model\u2019s performance while also controlling for potential biases.", "52f0f01e-8628-474a-a394-db44271c3f8e": "Additionally, Chiang et al.(Chiang et\u00a0al., 2024) tried LLM TAs as an assignment evaluator in a large university course. After students submit assignments and receive LLM-generated feedback, the teaching team reviews and finalizes the evaluation results. This process illustrates how human oversight after the initial automated evaluation can guarantee fairness and consistentcy with academic standards.", "7b79639f-8db4-4030-92cc-aa430aebc39c": "5. Application\n\nDue to the convenience and effectiveness of LLM Judges, they have been widely applied as judges across various domains.\nThese applications not only cover general domains but also specific domains such as multimodal, medical, legal, financial, education, information retrieval and others. In this section, we will provide a detailed introduction to these applications, demonstrating how LLMs achieve precise and efficient evaluations in different domains.\n\n\nFigure 6. LLMs-as-judges are widely applied across various domains.\n\n\n\n5.1. General", "62872579-71c9-4ad9-83f0-e37a6d47a4a1": "In general domains, LLM Judges are applied to tasks requiring both understanding and generation, such as dialogue generation, open-ended question answering, summarization, and language translation. Each task follows its own set of evaluation criteria to meet its specific requirements.\nFor instance, in dialogue generation (Li et\u00a0al., 2017), the criteria emphasize the natural flow, emotional resonance, and contextual relevance of the conversation. In summarization tasks (Narayan et\u00a0al., 2018), the evaluation focuses on the coherence, consistency, fluency, and relevance of the text. In translation tasks (Feng et\u00a0al., 2024), the assessment prioritizes the quality, accuracy, fluency, and style.", "90d4c359-560c-4374-beca-9b01d31ee0bb": "As these diverse sub-tasks require specialized evaluation criteria, LLM judges provides refined evaluation methods that go beyond traditional metrics, paving the way for more comprehensive and in-depth assessments. For instance, Shu et al. (Shu et\u00a0al., 2024) introduced Fusion-Eval, an innovative approach that leverages LLMs to integrate insights from various assistant evaluators. Fusion-Eval evaluated summary quality across four dimensions\u2014coherence, consistency, fluency, and relevance, achieving a system-level Kendall-Tau correlation of 0.962 with human judgments. For dialogue quality, it assessed six aspects: coherence, engagingness, naturalness, groundedness, understandability, and", "9bc6682d-ba91-466c-bc5a-e16cb8ce9043": "naturalness, groundedness, understandability, and overall quality, attaining a turn-level Spearman correlation of 0.744. Furthermore, Xu et al. (Xu et\u00a0al., 2024b) proposed the ACTIVE-CRITIC framework, which enables LLMs to actively infer the target task and relevant evaluation criteria while dynamically optimizing prompts. In the story generation task, this framework achieved superior evaluation performance.", "6ca78702-c65c-4e4b-9d95-2b86ff960332": "5.2. Multimodal\n\nIn the multimodal domain, the evaluation objects of LLMs are not limited to textual data but extend to various forms of information such as images, audio, and video. One of the primary challenges in evaluating multimodal tasks lies in the significant heterogeneity among these modalities, including substantial differences in data structures, representation methods, and feature distributions.", "2b86c335-4291-44df-bed1-a0a8ac8a2c72": "To address this challenge, advanced techniques are often required to help LLMs integrate different forms of information, ensuring that they can provide accurate and meaningful evaluations. For example, Xiong et al. (Xiong et\u00a0al., 2024) trained an open-source multimodal LLM, LLaVA-Critic, specifically to evaluate model performance in multimodal scenarios. Similarly, Chen et al. (Chen et\u00a0al., 2024b) developed a Multimodal LLM-as-a-judge for 14 Vision-Language tasks, providing a unified evaluation framework. In addition, LLMs-as-judges can also be used in audio. For instance, Latif et al. (Latif et\u00a0al., 2023) used LLMs for identifying and evaluating emotional cues in speech, achieving", "137487af-2d01-4b77-8498-c30263a1eac1": "evaluating emotional cues in speech, achieving remarkable accuracy in the process. Beyond these efforts, some recent studies (Zhou et\u00a0al., 2024b; Deng et\u00a0al., 2024) have also explored the potential of multimodal LLMs to self-evaluate and self-reward, enhancing their performance without the need for external evaluators or human annotations.", "ab564625-0f29-4802-bee8-c4be60f97ee3": "As the application of LLMs-as-judges continues to expand in multimodal domains, there is a growing interest in exploring their use in more specific real-world scenarios, such as autonomous driving. Chen et al. (Chen et\u00a0al., 2024d) proposed CODA-LM, a novel vision-language benchmark for self-driving, which provides automatic and systematic evaluation of Large Vision-Language Models (LVLMs) on road corner cases. Interestingly, they found that using the text-only LLM judges resulted in a closer alignment with human preferences than LVLMs.\n\n\n\n\n5.3. Medical", "30af432f-73fe-416d-8ef1-0bec5d523ccc": "5.3. Medical\n\nIn the medical field, LLMs-as-judges have demonstrated significant potential, particularly in areas such as diagnostic support, medical text analysis, clinical decision-making, and patient education.\nIn this domain, high-quality evaluation requires LLM judges to possess precise interpretation capabilities for domain-specific terminology, the ability to comprehensively analyze diverse data types (such as clinical records and medical imaging), and strict compliance with high accuracy standards and ethical guidelines.", "ea755269-8678-442e-a9c8-0c46031c6aaf": "In the realm of medical text generation, Xie et al. (Xie et\u00a0al., 2024c) used LLMs to evaluate the compduikeyi1leteness, conciseness, and attribution of medical texts at a fine-grained level. Similarly, Brake et al. (Brake and Schaaf, 2024) leveraged LLMs, such as Llama2, to assess clinical note consistency, with results indicating agreement levels comparable to human annotators. When it comes to medical question answering, Krolik et al. (Krolik et\u00a0al., 2024) explored the use of LLMs to automatically evaluate answer quality. Their focus was on evaluating adherence to medical knowledge and professional standards, completeness of information, accuracy of terminology, clarity of expression, and", "b9c95be6-886f-44d1-9802-92dde18e7c28": "of terminology, clarity of expression, and relevance to the question.", "3d7e26fd-d22b-4711-9590-5f124d5055a2": "In the area of mental health counseling, Li et al. (Li et\u00a0al., 2024c) utilized LLMs to automate the evaluation of counseling effectiveness and quality. Key assessments included whether the counseling identified the client\u2019s emotional needs, provided appropriate responses, demonstrated empathy, managed negative emotions, and met the overall goals of mental health support. Beyond these above applications, LLMs\u2019 judging capabilities have also been applied to assist in improving performance in specialized medical reasoning tasks. For instance, many studies (Jeong et\u00a0al., 2024) employed LLMs to evaluate and filter medical information, thereby supporting enhanced medical reasoning.", "2d7a7505-0450-41dd-b411-bc047429e16a": "5.4. Legal", "6ea9e6c3-f08a-414a-9689-ebf7d1555953": "Due to the powerful evaluation capabilities of LLMs, LLMs-as-judges have been widely applied in the legal domain, covering multiple key scenarios, including evaluating the performance of law LLMs and relevance judgment in legal case retrieval. In the legal domain, the application of LLMs requires a deep understanding of the legal framework of specific jurisdictions, complex legal language, and rigorous logical reasoning abilities\u00a0(Li et\u00a0al., 2024b). At the same time, interpretability and transparency of the evaluation results are essential core requirements, as legal practice highly depends on clear logic and verifiable conclusions. Furthermore, the bias and fairness of the model are of", "52fab6a3-3caf-4146-a9d6-bafc2ec445bb": "the bias and fairness of the model are of significant concern, as any bias in legal evaluations could have a profound impact on judicial fairness. These unique demands set higher standards for LLM judges.", "62941548-45c0-43ae-80a8-ae5ab945f6e4": "In response to these challenges, recent research has explored various ways in which LLMs can be effectively employed in legal evaluations. Some research used LLMs as judges to assist in evaluating the performance of Law LLMs. For example, Yue et al. (Yue et\u00a0al., 2023a) introduced DISC-LawLLM to provide a wide range of legal services and utilized GPT-3.5 as a referee to evaluate the model\u2019s performance. They assessed three key criteria\u2014accuracy, completeness, and clarity\u2014by assigning a rating score from 1 to 5. Similarly, Ryu et al. (Ryu et\u00a0al., 2023) applied retrieval-based evaluation to assess the performance of LLMs in Korean legal question-answering tasks, which applied RAG not for", "6f7c4b10-ab23-497e-bed9-6f8dcf822745": "tasks, which applied RAG not for generation but for evaluation. What\u2019s more, LLMs have also been utilized to construct evaluation sets. Raju et al. (Raju et\u00a0al., 2024) explored methods for constructing these domain-specific evaluation sets, which are essential for enabling LLMs-as-judges to perform effective evaluation in legal domain. Beyond performance evaluation, LLMs have also been utilized for relevance judgment in legal case retrieval. For instance, Ma et al. (Ma et\u00a0al., 2024) used LLMs to automate the evaluation of large numbers of retrieved legal documents, improving both the scalability and accuracy of legal case retrieval systems. In conclusion, the application of LLMs-as-judges", "c0a70619-d442-4ac7-b34b-4128e88e5e60": "In conclusion, the application of LLMs-as-judges in law holds significant promise in future.", "5f44bc21-5bda-4980-850b-b7e786380a32": "5.5. Financial\n\nIn the financial domain, LLM judges have been extensively explored in scenarios such as investment risk assessment and credit scoring, which presenting unique challenges. For example, the complexity of risk assessment requires LLMs to accurately capture the influence of multifaceted factors, including market volatility, regulatory changes, and geopolitical events. Real-time processing demands further elevate the challenge, requiring LLMs not only to be computationally efficient but also to deliver rapid response times. Additionally, the dynamic nature of high-frequency trading demanding that LLMs swiftly adapt to fluctuating market conditions.", "6c3336d0-dd06-47ee-91ad-1d0229322acf": "In investment risk assessment, LLMs have proven effective due to their ability to process large amounts of data and make informed judgments. For instance, Xie et al. (Xie et\u00a0al., 2023) developed a financial LLM, FinMA, fine-tuned on LLaMA to evaluate investment risks more effectively. Their model is designed to follow instructions for risk assessment and decision analysis, improving the accuracy and efficiency of financial evaluations.", "b5ade1fa-8fbe-46f0-8e9a-23054e0420bc": "Another key application in the financial domain is credit scoring, which predicts the future repayment ability and default risk of individuals or businesses. By analyzing a vast array of data, including credit history, financial status, and other relevant factors, LLMs can help financial institutions make more accurate credit scoring assessments. For example, Babaei et al. (Babaei and Giudici, 2024) demonstrated how LLMs can process unstructured text data, such as customer histories, contract terms, and news reports, to enhance the precision of credit assessments.", "ff3c4702-155b-453d-8f6e-a779cbcf65fb": "Furthermore, as the use of LLMs in finance continues to grow, there is a rising need to evaluate the performance of these financial LLMs. To address this, Son et al. (Son et\u00a0al., 2024a) developed an automated financial evaluation benchmark that leverages LLMs to extract valuable insights from both unstructured and structured data. This framework helps optimize the construction, updating, and compliance checks of financial benchmarks, supporting more efficient and scalable evaluation processes. Based on this, they facilitated the continuous optimization of financial LLMs, driving further advancements in the financial domain.\n\n\n\n\n5.6. Education", "eb793fbc-46dd-4799-a2a6-5c589802e15f": "LLMs-as-judges have found extensive applications in the education domain, covering a wide range of tasks, such as grading student assignments, evaluating essays, assessing mathematical reasoning, and judging debate performance. These applications present several key challenges, including the diversity of student responses and individual differences, as well as the need for multidimensional evaluation. Effective evaluation in education requires LLMs to consider not only correctness but also creativity, clarity, and logical coherence. Additionally, the interpretability and fairness of the evaluation results are crucial, as educational assessments significantly impact students\u2019 development and", "3453510b-a406-4873-87b6-4c55ac7da1f3": "significantly impact students\u2019 development and future opportunities.", "8465be0c-697d-4e0c-881f-f501e99ed9a3": "In assignment grading, Chiang et al. (Chiang et\u00a0al., 2024) introduced\nthe concept of an LLM Teaching Assistant (LLM TA) in university classrooms, utilizing GPT-4 to\nautomate the grading of student assignments. By employing prompt engineering to define scoring\ncriteria and task descriptions, LLM TA generates quantitative scores and detailed feedback. Their\nstudy emphasized the system\u2019s ability to maintain consistency, adhere to grading standards, and\nresist adversarial prompts, highlighting its robustness and practicality for classroom use.", "db0eb3a2-60d7-4af5-b808-14a48b00f9d3": "In addition to assignment grading, LLMs-as-judges are also being explored for automated essay scoring. Wang et al. (Wang et\u00a0al., 2024c) proposed an advanced intelligent essay scoring system, integrating LLMs such as BERT and ChatGPT to enable automated scoring and feedback generation for essays across various genres. Similarly, Song et al. (Song et\u00a0al., 2024c) investigated a framework and methodology for automated essay scoring and revision based on open-source LLMs. Furthermore, Zhou et al. (Zhou et\u00a0al., 2024a) explored the potential of LLMs in academic paper reviewing tasks, assessing their reliability, effectiveness, and possible biases as reviewer. They found that while LLMs show", "ee02bebf-0523-483a-993a-6da567ecd915": "as reviewer. They found that while LLMs show certain promise in the domain of automated reviewing, they are not yet sufficient to fully replace human reviewers, particularly in areas with high technical complexity or strong innovation.", "52405fd2-6b9a-4137-8d4d-7e9e6ecd4db7": "Another area where LLMs-as-judges are making an impact is in the evaluation of math reasoning. Unlike traditional mathematical task evaluation, which focuses solely on the correctness of the final results, Xia et al. (Xia et\u00a0al., 2024a) argued that additional aspects of the reasoning process should also be assessed, such as logical errors or unnecessary steps. In their work, the authors proposed ReasonEval, a new methodology for evaluating the quality of reasoning steps based on LLMs-as-judges.", "8a8e20dc-9936-4e61-8766-5ab5d7542466": "LLMs have also been employed in judging debate performance. Liang et al. (Liang et\u00a0al., 2024a) proposed Debatrix, a new method which leverages LLMs to evaluate and analyze debates. The main aspects assessed include the logical consistency of arguments, the effectiveness of rebuttals, the appropriateness of emotional expression, and the coherence of the debate.\n\n\n\n\n5.7. Information Retrieval", "94d0dec6-0e49-4564-bc70-cb89b3e2935e": "5.7. Information Retrieval\n\nInformation retrieval refers to the process of effectively retrieving, filtering, and ranking relevant information from a large collection of data, matching information resources to users\u2019 needs (queries). However, evaluating these systems presents several challenges, particularly due to the the complexity of real-world data, the diversity of user needs, and personalization. To solve these challenges, LLMs-as-judges have been used across various applications, including relevance judgment, text ranking, recommendation explanations evaluation, and assessing retrieval-augmented generation (RAG) systems.", "4506adfd-9e4f-4a13-b511-f0170f92946b": "One key area in information retrieval is the evaluation of the relevance of retrieved results to user queries, a task that traditionally relies on manual annotations\u00a0(Soboroff, 2024; Rahmani et\u00a0al., 2024). Rahmani et al. (Rahmani et\u00a0al., 2024) proposed a framework called LLMJudge which leveraged LLMs to assess the relevance of information retrieval system results to user queries, providing a more scalable and efficient evaluation approach.", "f6d2577e-a394-4b09-baea-88a939136116": "Another important aspect of information retrieval is the ranking of search results or recommendation lists. Traditional ranking models often rely on shallow features or direct matching scores, which may not yield optimal results. To address this, Qin et al. (Qin et\u00a0al., 2023) examined the performance of LLMs in text ranking tasks and proposed a novel method based on pairwise ranking prompting, utilizing LLMs for text ranking. Additionally, Niu et al. (Niu et\u00a0al., 2024) introduced a framework called JudgeRank, which leveraged LLMs to rerank results in reasoning-intensive tasks. By evaluating the logic, relevance, and quality of candidate results, this approach tried to enhance ranking", "d57b8f3d-af10-4e6c-8c9f-3fc27afe1640": "results, this approach tried to enhance ranking performance.", "bebd53dc-d449-4415-a3a0-2ea16ea6dcc3": "In recommendation systems, explanation evaluation plays a crucial role in helping users understand why a specific product, movie, or piece of content is recommended. Zhang et al. (Zhang et\u00a0al., 2024a) investigated the potential of LLMs as automated evaluators of recommendation explanations, assessing them across multiple dimensions such as quality, clarity, and relevance. This approach provides a more efficient way to evaluate the effectiveness of explanations, which is essential for improving user trust and satisfaction.", "9e53416e-7f20-40e9-b9f6-782128748ef8": "Furthermore, with the growing use of retrieval-augmented generation (RAG) systems in tasks like question answering, fact-checking, and customer support, there is an increasing need to evaluate the quality of these systems. Traditional evaluation methods rely on large manually annotated datasets, which are time-consuming and costly. To address this, Saad et al. (Saad-Falcon et\u00a0al., 2023) proposed a new automated evaluation framework called ARES, which leveraged LLMs as the core evaluation tool to directly assess retrieval and generated content across multiple dimensions, including relevance, accuracy, coverage, fluency, and coherence.\n\n\n\n\n5.8. Others\n\n\n5.8.1. Soft Engineering", "03df9c96-4248-48c5-94b2-4dc42c5fe25f": "5.8. Others\n\n\n5.8.1. Soft Engineering\n\nThe challenges that LLMs-as-judges need to overcome in the software engineering domain include complex code structures and the diversity of evaluation criteria. A numerous of articles (Patel et\u00a0al., 2024; Weyssow et\u00a0al., 2024) used LLMs-as-judges to assess the quality of code generation. Moreover, Kumar et al. (Kumar et\u00a0al., 2024) employed LLMs to evaluate the quality of Bug Report Summarization.\n\n\n\n\n5.8.2. Biology", "3ab46d61-53ae-4626-8c77-10e9a4deecea": "5.8.2. Biology\n\nThe main evaluation challenges in biological field include the complexity and diversity of the data and the need for specialized biological knowledge\u00a0(O\u2019Donoghue et\u00a0al., 2023; Hijazi et\u00a0al., 2024). For example, Hijazi et al. (Hijazi et\u00a0al., 2024) used LLMs to evaluate Query-focused summarization (QFS), which refers to generating concise and accurate summaries from a large set of biomedical literature based on a specified query. In this context, the LLMs are used to assess whether these summaries accurately answer the specified query and whether they cover the correct biological knowledge.\n\n\n\n\n5.8.3. Social Science", "1311e539-512a-46b4-ac21-2e716b6de8de": "LLMs-as-Judges have also found applications in social sciences. On one hand, they are used in real-world human social contexts. For example, Tessler et al. (Tessler et\u00a0al., 2024) used LLMs to participate in democratic discussions, assessing the quality of arguments, identifying fallacies, or providing a balanced view of an issue, thus helping people reach consensus on complex social and political matters. On the other hand, LLMs-as-judges are also used in social scenarios constructed by language agents. Zhou et al. (Zhou et\u00a0al., 2023b) proposed an interactive evaluation framework called Sotopia, which used LLMs to assess the social intelligence of language agents from multiple dimensions,", "9a5369d8-3bf3-4bc8-a722-a6b41968edd6": "of language agents from multiple dimensions, such as emotional understanding, response adaptability, and other social skills.", "ad287034-854a-4a89-a1df-59cfd666bd42": "In this section, we have outlined the specific applications of LLMs-as-judges across various domains. In these applications, LLMs leverage their powerful text understanding and generation capabilities to perform effective evaluations and judgments, providing accurate feedback and improvement suggestions. Although LLMs-as-judges have shown tremendous potential in these areas, especially in handling large-scale data and automating assessments, they still face challenges such as the depth of domain-specific knowledge, limitations in reasoning abilities, and the diversity of evaluation criteria. In the future, with continuous improvements in model performance and domain adaptation", "257cf73e-add9-4031-8c3c-1650bc9a3411": "in model performance and domain adaptation capabilities,, we believe the application of LLMs-as-judges will become more widespread and precise across various domains.", "6ddfe620-4bb5-451d-aeae-1e468625ce2c": "6. Meta-evaluation\n\nMeta-evaluation, the process of assessing the quality of the evaluator itself, is a crucial step in determining the reliability, consistency, and validity of LLM judges. Given the diverse applications of LLMs as evaluators, meta-evaluation methods have also been evolving. Researchers have proposed various datasets and metrics tailored to different tasks and evaluation objectives to assess the reliability and validity of LLM-based evaluations. This chapter will explore state-of-the-art Benchmarks (\u00a76.1) and evaluation Metrics (\u00a76.2), categorize existing approaches, and discuss their advantages and limitations.", "82097155-5317-48fc-b07b-5be41385a77b": "Table 2. Statistical information of different benchmarks (Part 1).\n\n\n\nBenchmark\nTask\nType\nNum\nEvaluation Criteria\nLanguage\n\n\n\n\nHumanEval\u00a0(Chen et\u00a0al., 2021)\nCode\nPointwise\n164\nFunctional correctness\nEnglish\n\n\n\nSWE-bench\u00a0(Jimenez et\u00a0al., 2023)\n\nCode\nPointwise\n2,294\nTask solve\nEnglish\n\n\nDevAI\u00a0(Zhuge et\u00a0al., 2024)\nCode\nPointwise\n365\n\n\n\nDisagreement, Task solve,\n\n\nRequirements met\n\n\n\nEnglish\n\n\n\nCrossCodeEval\u00a0(Ding et\u00a0al., 2024)\n\nCode\nPointwise\n1,002\nCode match, Identifier match\nEnglish\n\n\nCodeUltraFeedback\u00a0(Weyssow et\u00a0al., 2024)\nCode\n\n\n\nPointwise\n\n\nPairwise\n\n\n\n10k\n\n\n\nCode explanation,\n\n\nCode complexity and efficiency,\n\n\nCode readability, Coding style\n\n\n\nEnglish\n\n\n\n\n\nLiterary Translation", "0b8da397-4dd0-489b-8ee9-82e6062cb704": "English\n\n\n\n\n\nLiterary Translation\n\n\n\nComparisons\u00a0(Karpinska and Iyyer, 2023)\n\n\n\n\nTranslation\nPairwise\n720\nTranslation quality and errors\nMultilingual\n\n\nMQM\u00a0(Freitag et\u00a0al., 2021a)\nTranslation\nPointwise\n100k\n\n\n\nAccuracy, Fluency,\n\n\nTerminology, Style, Locale\n\n\n\nMultilingual\n\n\n\n\n\nWMT Metrics\n\n\n\nShared Task\u00a0(Freitag et\u00a0al., 2021b)\n\n\n\n\nTranslation\nPointwise\n-\nAdequacy, Fluency\nMultilingual\n\n\nSummEval\u00a0(Fabbri et\u00a0al., 2021)\nSummary\nPointwise\n1,600\n\n\n\nCoherence, Consistency,\n\n\nFluency, Relevance\n\n\n\nEnglish\n\n\n\nOpinsummeval\u00a0(Shen and Wan, 2023)\n\nSummary\nPointwise\n1,400\n\n\n\nAspect relevance,\n\n\nSelf-coherence, Readability\n\n\nSentiment consistency,\n\n\n\nEnglish", "49cf468e-c35a-453c-bb12-87705828ad61": "Sentiment consistency,\n\n\n\nEnglish\n\n\nFrank \u00a0(Pagnoni et\u00a0al., 2021)\nSummary\nPointwise\n2,250\nFactual errors\nEnglish\n\n\n\nTopical-Chat\u00a0(Gopalakrishnan et\u00a0al., 2023)\n\nDialogue\nPointwise\n60\n\n\n\nUnderstandable, Natural,\n\n\nMaintains context, Interesting,\n\n\nUses knowledge, Overall quality\n\n\n\nEnglish\n\n\nPersonal-Chat\u00a0(Zhang, 2018)\nDialogue\nPointwise\n60\n\n\n\nUnderstandable, Natural,\n\n\nMaintains context, Interesting,\n\n\nUses knowledge, Overall quality\n\n\n\nEnglish\n\n\n\nDSTC10 Hidden Set\u00a0(Zhang et\u00a0al., 2021)\n\nDialogue\nPointwise\n9,500\n\n\n\nCoherence, Appropriateness,\n\n\nNaturalness, Toxicity control\n\n\n\nEnglish\n\n\nHANNA\u00a0(Chhun et\u00a0al., 2022)\nStory\nPointwise\n1,056\n\n\n\nRelevance, Coherence,\n\n\nEmpathy, Surprise,", "9be78fe9-296a-4b60-a908-2d7628929763": "Relevance, Coherence,\n\n\nEmpathy, Surprise,\n\n\nEngagement, Complexity\n\n\n\nEnglish\n\n\n\nMANS\u00a0(Guan et\u00a0al., 2021)\n\nStory\nPointwise\n2,000\nCoherence\nEnglish\n\n\nStoryER\u00a0(Chen et\u00a0al., 2023b)\nStory\nPairwise\n100k\nUpvoted\nEnglish\n\n\n\nPer-DOC\u00a0(Wang et\u00a0al., 2023e)\n\nStory\nPointwise\n7,000\n\n\n\nInterestingness, Adaptability,\n\n\nCharacter developmentSurprise, Ending\n\n\n\nEnglish\n\n\nPKU-SafeRLHF\u00a0(Ji et\u00a0al., 2024)\nValue\nPairwise\n83.4K\nHelpfulness, Harmlessness\nEnglish\n\n\n\nHHH\u00a0(Askell et\u00a0al., 2021)\n\nValue\nPairwise\n221\n\n\n\nHelpfulness, Honesty,\n\n\nHarmlessness\n\n\n\nEnglish\n\n\nCvalue\u00a0(Xu et\u00a0al., 2023b)\nValue\nPairwise\n145k\nSafety, Responsibility\nChinese\n\n\n\nYelp \u00a0(Asghar, 2016)\n\nRecom\nPointwise\n8,630k\nUser perference\nEnglish", "ae159843-307d-4b90-b21f-8b593523023f": "Recom\nPointwise\n8,630k\nUser perference\nEnglish\n\n\nMovielens_Explanation\u00a0(Zhang et\u00a0al., 2024a)\nRecom\nPointwise\n2,496\n\n\n\nPersuasiveness, Transparency,\n\n\nAccuracy, Satisfaction\n\n\n\nEnglish\n\n\n\nTrec DL21&22 \u00a0(Craswell et\u00a0al., 2021, 2022)\n\nSearch\nPointwise\n\n\n\n1,549/\n\n\n2,673\n\n\n\nRelevacne\nEnglish\n\n\nLeCarDv2\u00a0(Li et\u00a0al., 2024d)\nSearch\nPointwise\n55,192\n\n\n\nCharacterization relevance,\n\n\nPenalty relevance,\n\n\nProcedure relevance\n\n\n\nEnglish\n\n\n\n\n\nTable 3. Statistical information of different benchmarks (Part 2).\n\n\n\nBenchmark\nDomain\nType\nNum\nEvaluation Criteria\nLanguage\n\n\n\n\nUltraFeedback\u00a0(Cui et\u00a0al., 2024)\nCompre.\nPairwise\n64k\n\n\n\nHelpfulness, Honesty,\n\n\nInstruction following,\n\n\nTruthfulness\n\n\n\nEnglish", "4a9f2084-d858-47ab-a56b-9eae523aa41d": "Truthfulness\n\n\n\nEnglish\n\n\n\nAlpacaEval\u00a0(Dubois et\u00a0al., 2024)\n\nCompre.\nPairwise\n20k\nInstruction-following\nEnglish\n\n\nChatbot Arena\u00a0(Zheng et\u00a0al., 2023a)\nCompre.\nPairwise\n33k\nUser perference\nEnglish\n\n\n\nMTBench\u00a0(Zheng et\u00a0al., 2023a)\n\nCompre.\nPairwise\n3,000\n\n\n\nMulti-turn conversational,\n\n\nInstruction-following\n\n\n\nEnglish\n\n\nRewardBench\u00a0(Lambert et\u00a0al., 2024)\nCompre.\nPairwise\n2,998\nUser perference\nEnglish\n\n\n\nJudgerBench\u00a0(Cao et\u00a0al., 2024a)\n\nCompre.\nPairwise\n1,900\nInstruction following\n\n\n\nEnglish\n\n\nChinese\n\n\n\n\n\nRM-Benchh\u00a0(Liu et\u00a0al., 2024b)\nCompre.\nPairwise\n1,327\nInstruction following\nEnglish\n\n\n\nJUDGEBENCH\u00a0(Tan et\u00a0al., 2024)\n\nCompre.\nPairwise\n350\nFactual, Logical correctness\nEnglish", "c01c1740-b4c7-4fb2-9a49-ec36bf5e3a77": "Infinity-Preference111https://huggingface.co/datasets/BAAI/Infinity-Preference\nCompre.\nPairwise\n59.3k\nUser perference\n\n\n\nEnglish\n\n\nChinese\n\n\n\n\n\n\nLLMeval\u00a0(Zhang et\u00a0al., 2023b)\n\nCompre.\n\n\n\nPointwise\n\n\nPairwise\n\n\n\n453\n\n\n\nCorrectness, Fluency, Logic,\n\n\nInformativeness, Harmlessness\n\n\n\nChinese\n\n\nWildBench\u00a0(Lin et\u00a0al., 2024)\nCompre.\nPointwise\n1,024\nChecklists\nEnglish\n\n\n\nFlask\u00a0(Ye et\u00a0al., 2023b)\n\nCompre.\nPointwise\n1,740\n\n\n\nLogical thinking,\n\n\nBackground knowledge,\n\n\nProblem handling, User alignment\n\n\n\nEnglish\n\n\nAlignBench\u00a0(Liu et\u00a0al., 2023b)\nCompre.\nPointwise\n683\n\n\n\nTask-oriented, Clarity & Fluency,\n\n\nComplexity & Difficulty,\n\n\nDesensitization\n\n\n\nChinese\n\n\n\nHELPSTEER\u00a0(Wang et\u00a0al., 2023a)", "62680352-30f3-4902-9379-ad6e15d2f543": "Chinese\n\n\n\nHELPSTEER\u00a0(Wang et\u00a0al., 2023a)\n\nCompre.\n\n\n\nPointwise\n\n\nPairwise\n\n\n\n37,120\n\n\n\nHelpfulness, Correctness,\n\n\nCoherence, Complexity Verbosity\n\n\n\nEnglish\n\n\nHELPSTEER2\u00a0(Wang et\u00a0al., 2024a)\nCompre.\n\n\n\nPointwise\n\n\nPairwise\n\n\n\n21,362\n\n\n\nHelpfulness, Correctness,\n\n\nCoherence, Complexity, Verbosity\n\n\n\nEnglish\n\n\n\nMLLM-as-a-Judge\u00a0(Chen et\u00a0al., 2024b)\n\nCompre.\n\n\n\nPointwise\n\n\nPairwise\n\n\nListwise\n\n\n\n17,000\n\n\n\nRelevance, Accuracy,\n\nCreativity, Response granularity\n\nEnglish\n\n\nMM-EvalMM-Eval\u00a0(Son et\u00a0al., 2024b)\nCompre.\nPairwise\n4,981\nTask-oriented\nMultilingual\n\n\n\n\n\n\n6.1. Benchmarks", "5899ed4b-8827-4ec8-a384-4e0c56837f95": "To evaluate LLM-based judges, a common approach is to measure their alignment with human preferences, as human judgments are often considered the gold standard for quality and reliability. Given the diverse range of applications for LLM-based judges, different benchmarks have been created, each tailored to specific evaluation criteria and use cases.\nIn this section, we present a comprehensive collection of 40 widely-used benchmarks, each designed to capture different aspects of evaluation, such as language understanding, factual accuracy, coherence, creativity, and fairness. To enhance clarity and facilitate comparison, we categorize these benchmarks by application domain.", "3e724225-69df-4e90-a0bf-ce99189a8473": "6.1.1. Code Generation\n\nCode generation aims to produce executable program code from natural language input. This task typically involves translating user requirements or descriptions into precise code. The applications of code generation are vast, including automated script creation, bug fixing, and the generation of complex programming tasks.Evaluating code generation is highly challenging, and LLMs are increasingly being used as evaluators for assessing code quality.", "dd396ff3-76e2-4501-9df7-e808b845705b": "HumanEval\u00a0(Chen et\u00a0al., 2021) is a widely used benchmark dataset designed to evaluate programming capabilities. It consists of 164 coding tasks, each accompanied by a brief natural language description. The tasks primarily involve algorithmic problems and data structure exercises, with difficulty levels ranging from basic to intermediate.\nOne notable feature of HumanEval\u00a0(Chen et\u00a0al., 2021) is the inclusion of input-output examples, which facilitate the assessment of functional correctness. However, the dataset\u2019s limited size and scope may not sufficiently capture the diversity of real-world programming challenges.", "154b3d97-26ec-49fa-9c6c-334d04d95eb4": "SWEBench\u00a0(Jimenez et\u00a0al., 2023) targets more complex programming tasks that are closer to real-world software development scenarios. It includes 2,294 tasks requiring advanced operations such as reasoning, multi-step problem solving, and API usage. Unlike simpler benchmarks, SWEBench\u00a0(Jimenez et\u00a0al., 2023) assesses the model\u2019s ability to handle comprehensive problem-solving and logical reasoning. However, the increased complexity also introduces challenges in establishing consistent evaluation criteria, particularly when it comes to subjective aspects like code style and efficiency. Moreover, DevAI\u00a0(Zhuge et\u00a0al., 2024) was introduced to address the limitations of existing benchmarks, which", "49220757-4b8b-4b36-bf24-1379e8409fb8": "the limitations of existing benchmarks, which often fail to capture the iterative nature of software development and lack adequate signals for measuring long-term progress. The dataset includes 365 task requirements, focusing on more complex and challenging programming scenarios.", "49e2b4f7-c28e-455f-a012-2f6460eeaaf3": "CrossCodeEval\u00a0(Ding et\u00a0al., 2024) focuses on assessing cross-language programming models, containing over 1,000 tasks that involve translating code between different programming language pairs, such as Python to Java or JavaScript to C++. This dataset tests the model\u2019s ability to adapt and transform code across languages, highlighting the challenges of understanding varied syntax and semantics. CodeUltraFeedback\u00a0(Weyssow et\u00a0al., 2024) is designed to evaluate and enhance the alignment between LLMs and user-defined programming preferences. It includes 10,000 programming instructions, each paired with four responses from 14 different LLMs. These responses are scored by GPT-3.5 based on five", "f92c5bb9-2931-4619-9221-44098fb9f465": "responses are scored by GPT-3.5 based on five distinct programming preferences, such as readability, efficiency, and adherence to user specifications. The dataset emphasizes fine-grained feedback and user-centered evaluation, making it a useful tool for analyzing preference alignment.", "9c7f67c8-f578-4bf0-b295-4317554c1d08": "6.1.2. Machine Translation\n\nMachine Translation (MT) refers to the process of automatically translating text from a source language to a target language. Over time, MT technology has progressed significantly, evolving from rule-based methods to Statistical Machine Translation (SMT), and more recently to Neural Machine Translation (NMT), which is now the dominant approach. With the widespread adoption of NMT and the emergence of LLMs, evaluating translation quality has become a complex task, requiring robust evaluation frameworks that can assess accuracy, fluency, and contextual relevance across diverse language pairs.", "50f722b0-3885-47a6-9e1e-021e05f3f083": "The Workshop on Machine Translation (WMT)\u00a0(Freitag et\u00a0al., 2021b) is a prominent annual evaluation event in the field of MT. It provides large-scale, human-annotated datasets for a variety of language pairs, including English-French, English-German, and English-Russian. Each year, WMT releases benchmark datasets that include source texts, model-generated translations, reference translations, and human evaluation scores. These datasets are widely used for assessing the performance of automated evaluation metrics by comparing their outputs against human judgments. WMT covers a broad range of tasks, from sentence-level translation to document-level and domain-specific challenges, making it a", "dc1017b1-ca60-48c7-9060-93161be02bbc": "and domain-specific challenges, making it a comprehensive resource for evaluating the correlation between automated evaluators and human assessments. However, WMT primarily focuses on high-resource languages, which may limit its applicability to low-resource or underrepresented languages.", "d4e69064-d25b-4f82-adb6-61f52e418c71": "Literary Translation Comparisons\u00a0(Karpinska and Iyyer, 2023) is designed to assess document-level translation quality, particularly in the context of literary works. It includes carefully selected paragraphs from various literary pieces, covering 18 language pairs such as Japanese-English, Polish-English, and French-English. Unlike sentence-level benchmarks, this dataset emphasizes the importance of evaluating translations in a broader context, as literary texts often require understanding of stylistic elements and cultural subtleties. This makes it particularly useful for evaluating the performance of LLMs, which may excel in capturing broader contextual information.", "47f68128-29c1-4c04-aab7-4cbfb48ff933": "The MQM\u00a0(Freitag et\u00a0al., 2021a) study is the largest evaluation effort to date focusing on machine translation quality. It involves professional translators annotating the outputs of top-performing systems from the WMT 2020 shared task, specifically targeting English-German and Chinese-English translations. MQM introduces a multidimensional quality assessment framework that goes beyond traditional metrics like BLEU or ROUGE. It evaluates translations across multiple dimensions, including accuracy, fluency, terminology, style, and locale, providing a more nuanced understanding of translation quality.", "227f6a66-ca12-4f0f-84a2-9c02fb80e523": "6.1.3. Text Summarization\n\nText Summarization (TS) is the task of generating a concise and coherent summary from a given piece of text while preserving its essential meaning. The main goal is to provide a quick, accurate overview of the source content, capturing key information and eliminating unnecessary details. As LLMs have shown impressive capabilities in generating summaries, the need for robust meta-evaluation benchmarks is critical to effectively assess their performance across various dimensions like coherence, relevance, consistency, and fluency.", "21e7fc97-f3f6-4d0a-a8b9-d77367dbf16a": "SummEval\u00a0(Fabbri et\u00a0al., 2021) is one of the most widely used benchmarks for evaluating summarization models. It includes summaries generated by 16 different models based on 100 news articles randomly sampled from the CNN/DailyMail test set. Each summary was annotated by five independent crowd-sourced workers and three expert evaluators, using a Likert scale from 1 to 5 across four key dimensions: coherence, consistency, fluency, and relevance. The dataset is valuable for analyzing the correlation between human judgments and automated evaluation metrics.", "7d6eb21b-99b4-47b6-9302-3aa15102742e": "The FRANK\u00a0(Pagnoni et\u00a0al., 2021) dataset is dedicated to assessing the factual accuracy of summaries generated by automatic summarization systems. It provides detailed human annotations of factual errors, including semantic frame errors, discourse errors, and content verifiability issues. The dataset includes summaries from both the CNN/DailyMail and XSum datasets, making it a comprehensive resource for evaluating factual correctness. FRANK\u2019s detailed categorization of errors offers valuable insights into the types of factual inaccuracies common in generated summaries, highlighting areas where LLMs often struggle. However, focusing solely on factual errors may overlook other aspects of", "93611f94-6558-4b2a-8a26-3e97d7ce920e": "on factual errors may overlook other aspects of summary quality, such as coherence and fluency.", "80ced9db-6594-4cbd-b7f8-b956afda5751": "OpinsummEval\u00a0(Shen and Wan, 2023) is a meta-evaluation benchmark specifically designed for opinion summarization tasks, where the goal is to extract and summarize opinions from a large volume of user reviews. This dataset includes outputs from 14 different opinion summarization models and provides human annotations across four dimensions: aspect relevance, self-consistency, sentiment consistency, and readability.", "5575f668-b7a7-4e3f-bc70-dd8987755600": "6.1.4. Dialogue Generation\n\nDialogue Generation is the task of automatically generating natural language conversations that are relevant to a given context. The primary goal is to develop dialogue systems that can understand context, generate fluent responses, and maintain logical consistency and contextual accuracy. Dialogue generation encompasses a wide range of applications, from chatbots and virtual assistants to social conversational agents. With the increasing capabilities of large language models (LLMs), evaluating dialogue generation has become more complex, requiring multi-faceted evaluation frameworks to assess various aspects of conversational quality.", "94833f0a-4973-4e83-a39b-2f777f8076c4": "In the field of dialogue generation, the most commonly used datasets include Topical-Chat\u00a0(Gopalakrishnan et\u00a0al., 2023) and PERSONA-CHAT\u00a0(Zhang, 2018). The Topical-Chat\u00a0(Gopalakrishnan et\u00a0al., 2023) dataset aims to advance research in open-domain conversational AI, covering eight major topics such as entertainment, health, and technology. The PERSONA-CHAT\u00a0(Zhang, 2018) dataset, on the other hand, focuses on enhancing dialogue systems by incorporating predefined personas to generate more personalized responses. Each dialogue participant is assigned a persona profile, consisting of several descriptive sentences about their personality or preferences.", "64441995-17b5-4f60-9100-752e311fd4b5": "Mehri and Eskenazi\u00a0(Mehri and Eskenazi, 2020) conducted a meta-evaluation study on these two widely-used open-domain dialogue corpora. They manually annotated 60 dialogue contexts from each dataset, with six responses per context for Topical-Chat and five for PERSONA-CHAT\u00a0(Zhang, 2018), including both model-generated and human responses. Each response was evaluated across six key dimensions: naturalness, coherence, engagement, groundedness, understandability, and overall quality. This study highlights the importance of multi-dimensional evaluation in dialogue generation, providing valuable insights into the strengths and weaknesses of different dialogue models.", "65002c2c-9170-4e2a-8454-913d78ba5a7f": "Additionally, the dataset from DSTC10 Track 5\u00a0(Yoshino et\u00a0al., 2023; Zhang et\u00a0al., 2021) focuses on evaluating open-domain dialogue systems and is designed for automatic evaluation and moderation of dialogue systems. The challenge aims to develop automatic evaluation mechanisms that accurately reflect human judgments while effectively handling harmful user inputs, maintaining conversational flow and engagement. The dataset includes annotations across four aspects: coherence, appropriateness, naturalness, and toxicity control.", "4a2d7b3b-b70c-4d5f-be98-452e014fb626": "6.1.5. Automatic Story Generation\n\nAutomatic Story Generation (ASG) is a challenging task that aims to enable models to create coherent, engaging narratives based on a given prompt or context. It emulates human storytelling abilities by generating stories that exhibit a logical structure, compelling characters, and interesting plot developments. Evaluating story generation systems is inherently complex, as it involves assessing not only linguistic quality but also narrative elements like coherence, engagement, and surprise.", "ae73000d-452e-4a17-90af-2487a3ce6648": "The HANNA\u00a0(Chhun et\u00a0al., 2022) dataset is tailored for evaluating automatic story generation (ASG), featuring 1,056 stories generated by 10 different systems from 96 prompts. Each story is annotated by three human reviewers across six criteria: relevance, coherence, resonance, surprise, engagement, and complexity. This comprehensive annotation framework provides a detailed assessment of narrative quality, making HANNA a valuable benchmark for comparing ASG models.", "0eee3dfc-313b-4762-999d-fdaf987cec1f": "Another notable dataset is the MANS\u00a0(Guan et\u00a0al., 2021), which forms part of the OpenMEVA\u00a0(Guan et\u00a0al., 2021) framework. It compiles stories from various natural language generation models using well-known corpora like ROCStories\u00a0(Mostafazadeh et\u00a0al., 2016) and WritingPrompts\u00a0(Fan et\u00a0al., 2018). MANS\u00a0(Guan et\u00a0al., 2021) focuses on manual annotations of narrative elements, serving as a robust testbed for exploring diverse evaluation metrics.", "374ed34f-bfc2-43f7-9f22-2cd074166196": "The StoryER\u00a0(Chen et\u00a0al., 2023b) dataset offers a distinct approach to evaluating story generation by focusing on preference prediction and aspect-based rating. StoryER is divided into two primary components: the first is a 100k Story Ranking Data, which pairs stories from the WritingPrompts dataset. Each pair includes one story with high user engagement (upvotes \u2265\\geq\u2265 50) and another with low engagement (upvotes \u2264\\leq\u2264 0). This component leverages real-world user feedback to capture implicit preferences, providing a practical basis for training models to predict story quality. The second component, Aspect Rating and Reasoning Data, contains 46,000 entries where annotators provide detailed", "27eded69-bc8c-4a5b-945f-0582abad68a4": "46,000 entries where annotators provide detailed ratings (on a scale of 1-5) for various story aspects such as introduction, character development, and plot description, along with explanatory comments. This combination of quantitative rankings and qualitative reasoning enables a nuanced evaluation of stories, making StoryER particularly useful for both automated scoring and interpretability research.", "c171263a-d733-4b57-aa9a-7edfb093dc8e": "The PERSER\u00a0(Wang et\u00a0al., 2023e) dataset takes a different approach by addressing the subjectivity inherent in open-domain text generation evaluations. PERSER restructures existing datasets and introduces personalized tags, resulting in two sub-datasets: Per-MPST and Per-DOC. Per-MPST is an adapted version of the Movie Plot Synopsis Dataset, while Per-DOC includes 7,000 instances of paired stories generated from the same premise. These stories are evaluated based on dimensions such as interestingness, adaptability, surprise, character development, and the quality of the ending.", "0b8ff81d-8116-4596-b343-e7289feff69f": "6.1.6. Values Alignment\n\nValues alignment is a critical task in the development of AI systems, focused on ensuring that their behavior and decisions consistently reflect core human values and ethical standards. In the context of LLM-as-Judge, the alignment process is vital to verify that the model\u2019s outputs adhere to societal norms and ethical principles, minimizing risks related to harmful, biased, or unethical behavior. To support research and model development in values alignment, several datasets have been created, each with unique characteristics designed to evaluate or enhance the ethical behavior of LLMs.", "41c4badc-1d08-494e-b72a-06b95b87a1b2": "One notable dataset is PKU-SafeRLHF\u00a0(Ji et\u00a0al., 2024), which was specifically curated for studying safe alignment in large language models. The dataset comprises 83.4K preference entries, focusing on two primary dimensions: harmlessness and usefulness. In each sample, the dataset presents a pair of model responses to a given prompt, annotated with safety meta-labels and preferences based on the levels of safety and utility.", "1a026a4f-53c8-44af-948f-8a505d4f1f56": "Another influential dataset is the HHH\u00a0(Askell et\u00a0al., 2021) (Honesty, Helpfulness, and Harmlessness) dataset, designed to evaluate LLM performance across various human-model interaction scenarios. The dataset emphasizes three core human-centered values: honesty, helpfulness, and harmlessness. It includes a diverse collection of conversational examples where models are tested on their adherence to these values. By exposing models to a wide range of contexts, the HHH dataset serves as a comprehensive benchmark for assessing whether LLMs align with essential ethical standards and effectively mitigate risks of misinformation, harmful advice, or biased outputs.", "b2c23485-d0cf-4045-9bd3-91485f2b7fdd": "Moreover, the CVALUES\u00a0(Xu et\u00a0al., 2023b) benchmark is a more recent contribution aimed at evaluating human values alignment specifically for Chinese LLMs. It represents the first comprehensive framework tailored to assess values alignment in the Chinese language context, focusing on two critical criteria: Safety and Responsibility.", "5a3e9f9d-44cf-433d-9f7f-ba7e26dbff0d": "6.1.7. Recommendation\n\nRecommendation systems aim to provide personalized suggestions based on users\u2019 preferences and historical behavior. As the use of large language models (LLMs) expands, their role in evaluating the performance of recommendation systems has garnered increasing attention. LLMs can serve as versatile evaluators, offering insights into multiple aspects of recommendation systems beyond traditional metrics like accuracy. They can assess factors such as user engagement, satisfaction, and the quality of generated explanations.", "74a3854c-1c4f-496b-80ab-857c3a502384": "The MovieLens\u00a0(Harper and Konstan, 2015) dataset is a widely-used public dataset for movie recommendations, available in multiple versions with varying scales, ranging from thousands of users and ratings to millions. Zhang et al.\u00a0(Zhang et\u00a0al., 2024a) further annotated the MovieLens\u00a0(Harper and Konstan, 2015) data to create a sub-dataset featuring user self-explanation texts. In this sub-dataset, users write explanatory texts after being presented with a recommended movie. These explanations are then rated on a five-point Likert scale across four dimensions: Persuasiveness, Transparency, Accuracy, and Satisfaction. This annotated data provides valuable reference texts for LLMs in the", "0db8bc66-7b08-4b51-bcad-8a1a7f217320": "provides valuable reference texts for LLMs in the context of explainability evaluation.", "3925b4d6-045a-4767-889c-adc0ddeff611": "Another commonly used dataset is the Yelp dataset\u00a0(Asghar, 2016), which contains detailed review data from 11 metropolitan areas, covering approximately 150,000 businesses, nearly 7 million user reviews, and over 200,000 images. User reviews include ratings for businesses, such as hotel ratings (1-5 stars), as well as additional feedback like \u201ccool\u201d and \u201cfunny\u201d votes. Furthermore, the Yelp dataset provides extensive business attribute information (e.g., operating hours, parking availability, and delivery options), offering rich contextual information that can be leveraged for developing and evaluating recommendation systems.\n\n\n\n\n6.1.8. Search", "6c5937d4-c964-4701-9449-c674c6759618": "The search task is a fundamental component of information retrieval (IR), focusing on identifying the most relevant documents from extensive text collections based on user queries. Traditionally, relevance assessments in search tasks have been conducted by human annotators following established guidelines. However, recent advances in large language models (LLMs) have opened up new opportunities for utilizing these models as evaluators, offering an automated and scalable approach to relevance assessment.", "e5a7dfde-34f1-417c-b69b-ddae9da6b545": "With the advent of retrieval-augmented generation (RAG) models, the role of LLMs as evaluators has expanded. There is now a growing need to assess various dimensions of retrieved contexts, including context relevance, answer faithfulness, and answer relevance. This shift highlights the potential of LLMs to provide nuanced judgments that go beyond simple topical relevance.", "0b050884-5ef9-454a-b3fb-007cbb57ac27": "A key resource for evaluating the performance of LLMs as relevance assessors is the series of datasets from the Text Retrieval Conference (TREC). TREC workshops aim to advance research in IR by offering large-scale test collections, standardized evaluation procedures, and a platform for benchmarking retrieval models.", "a9c34cb2-2184-43f7-a93f-ce88d718d09a": "The datasets from the TREC Deep Learning Track\u00a0(Lawrie et\u00a0al., 2024), specifically from 2021 (DL21)\u00a0(Craswell et\u00a0al., 2021) and 2022 (DL22)\u00a0(Craswell et\u00a0al., 2022), are commonly used for this purpose. These datasets are derived from the expanded MS MARCO v2 collection\u00a0(Bajaj et\u00a0al., 2016), which contains approximately 138 million passages. Relevance judgments are provided by assessors from the National Institute of Standards and Technology (NIST) using a 4-point scale (0 to 3). This structured and fine-grained annotation scheme allows for a detailed comparison between LLM-generated relevance scores and human judgments.", "80a26e83-2b8b-4dda-875b-5a7b35fe4f71": "While general-purpose datasets offer valuable benchmarks, specialized retrieval tasks often require domain-specific datasets that reflect unique relevance criteria. One notable example is LeCaRDv2\u00a0(Li et\u00a0al., 2024d), a large-scale dataset tailored for legal case retrieval. LeCaRDv2 enriches the concept of relevance by incorporating three distinct aspects: characterization, penalty, and procedure. These additional criteria provide a more comprehensive perspective on relevance.", "0bf93cab-a60a-4455-af9b-b31de2fecf8c": "6.1.9. Comprehensive Data\n\nTo thoroughly assess the role of LLMs-as-Judges and better align them with human preferences, a diverse set of comprehensive datasets has been developed. These datasets provide large-scale, well-annotated data, allowing for the effective training and evaluation of LLMs in complex, real-world contexts. As a result, they contribute to improving the models\u2019 reliability and effectiveness in their role as evaluators.", "279c3e50-c2dc-42e6-8699-95ed10c1ac64": "Datasets such as HelpSteer\u00a0(Wang et\u00a0al., 2023a) and HelpSteer2\u00a0(Wang et\u00a0al., 2024a) are designed to improve the alignment and usefulness of LLMs. They provide multi-attribute data, enabling the training of models that can generate responses that are factually correct, coherent, and tailored to diverse user preferences. These open-source datasets support adjustments in response complexity and verbosity, catering to varying user needs. Additionally, UltraFeedback\u00a0(Cui et\u00a0al., 2024) offers a large-scale dataset with around 64,000 prompts from sources like UltraChat\u00a0(Ding et\u00a0al., 2023), ShareGPT\u00a0(Chiang et\u00a0al., 2023), and TruthfulQA\u00a0(Lin et\u00a0al., 2021). It includes multiple responses per prompt", "c96120ad-3e16-4fd0-8954-2356212fe0a4": "2021). It includes multiple responses per prompt generated by different LLMs, with high-quality preference labels and textual feedback covering aspects like instruction-following, truthfulness, and helpfulness. UltraFeedback\u2019s fine-grained annotations and diverse prompts provide a robust resource for training reward and critic models, enhancing the evaluative capabilities of LLMs.", "c363a336-d891-485b-982f-89a1b0bc7e18": "In exploring instruction following and dialogue capabilities, specialized tools like AlpacaEval\u00a0(Dubois et\u00a0al., 2024), alongside interactive platforms such as Chatbot Arena\u00a0(Zheng et\u00a0al., 2023a) and benchmarks like MT-Bench\u00a0(Zheng et\u00a0al., 2023a), provide critical insights. AlpacaEval is an automated evaluation tool using GPT-4 or Claude as evaluators. It assesses chat-based LLMs against the AlpacaFarm dataset, providing win-rate calculations across a variety of tasks, enabling rapid and cost-effective comparisons with baseline models like GPT-3.5 (Davinci-003). Chatbot Arena, on the other hand, offers a user-driven evaluation framework where participants interact with anonymous models and", "37dfdbc6-43d4-4761-a1dc-dbe6e2971232": "participants interact with anonymous models and vote based on their preferences. The platform has collected over 1,000,000 user votes, using the Bradley-Terry model to rank LLMs and chatbots, providing valuable insights into user preferences and model performance in open-domain dialogue.", "e0751ff2-bfa2-42aa-bd70-743377505543": "Benchmarks like WildBench\u00a0(Lin et\u00a0al., 2024) and FLASK\u00a0(Ye et\u00a0al., 2023b) aim to evaluate LLMs on tasks more reflective of real-world applications. WildBench\u00a0(Lin et\u00a0al., 2024) collects challenging examples from real users via the AI2 WildChat project, providing fine-grained annotations, task types, and checklists for response quality evaluation, and employs length-penalized Elo ratings to ensure unbiased assessments. FLASK\u00a0(Ye et\u00a0al., 2023b) introduces a fine-grained evaluation protocol that decomposes overall scoring into skill set-level scoring for each instruction, enhancing interpretability and reliability in both human-based and model-based evaluations. Additionally, comprehensive", "4b126ecd-26bb-420a-82ee-d78286845dcd": "evaluations. Additionally, comprehensive evaluations covering multiple domains\u2014including factual question answering, reading comprehension, summarization, mathematical problem-solving, reasoning, poetry generation, and programming\u2014have been conducted. These evaluations involve assessing models across multiple criteria such as correctness, fluency, informativeness, logicality, and harmlessness.", "819e56fe-ae7e-4f75-b256-920b30b3e604": "Reward models and LLM-based judges face the crucial task of ensuring alignment with human expectations, a challenge addressed by datasets like RewardBench\u00a0(Lambert et\u00a0al., 2024), RM-Bench\u00a0(Liu et\u00a0al., 2024b), and JudgerBench\u00a0(Cao et\u00a0al., 2024a).", "08b38cd0-5994-4fa1-9ba9-cc722583133b": "RewardBench\u00a0(Lambert et\u00a0al., 2024) focuses on assessing models through complex prompt-choice trios, covering diverse areas like chat, reasoning, and safety, with a particular emphasis on out-of-distribution scenarios. RM-Bench\u00a0(Liu et\u00a0al., 2024b) introduces a new benchmark for evaluating reward models based on their sensitivity to subtle content differences and resistance to stylistic biases, emphasizing the need for refined assessments that correlate highly with aligned language models\u2019 performance.", "6add79ac-d85c-4e2f-9ce7-7333ab4b7058": "JudgerBench\u00a0(Cao et\u00a0al., 2024a), with its dual components (JDB-A and JDB-B), offers a structured framework for evaluating alignment and critique abilities. By including data from human voting results and combining insights from varied sources, JudgerBench\u00a0(Cao et\u00a0al., 2024a) provides a nuanced understanding of model performance across different languages and dialogue formats.", "48f89ff8-02bd-494c-ab98-54b8c08ff226": "With the growing complexity of tasks handled by LLMs, there is an increasing demand for more objective and reliable evaluation frameworks. JUDGEBENCH\u00a0(Tan et\u00a0al., 2024) proposes a novel approach to assessing LLM-based judges on challenging response pairs across domains like knowledge, reasoning, mathematics, and coding. It addresses the limitations of existing benchmarks by introducing preference labels that reflect objective correctness, providing a robust platform for evaluating the capabilities of advanced LLM-based judges.", "c2b0bfb7-ebf4-48af-8526-bc99fa5e6fc8": "As LLMs evolve beyond text-only tasks, evaluation frameworks have expanded to encompass multimodal and multilingual contexts. MLLM-as-a-Judge\u00a0(Chen et\u00a0al., 2024b) serves as a benchmark for assessing Multimodal LLMs, covering tasks like image description, mathematical reasoning, and infographic interpretation. By integrating human annotations, it provides a comprehensive evaluation across visual and textual domains, reflecting the growing demand for models capable of processing diverse inputs. In a parallel effort, MM-Eval\u00a0(Son et\u00a0al., 2024b) addresses the multilingual aspect, offering extensive analysis across 18 languages. With core subsets like Chat, Reasoning, and Linguistics, alongside", "e438837b-fe3a-48f5-ac2e-3145adbd81bf": "like Chat, Reasoning, and Linguistics, alongside a broader Language Resource subset spanning 122 languages, MM-Eval\u00a0(Son et\u00a0al., 2024b) highlights performance discrepancies, especially in low-resource languages where models tend to default to neutral scores.", "e1dc4570-af27-4e44-a594-ba5c251e7149": "6.2. Metric\n\nThe evaluation of LLMs-as-Judges models centers around assessing the extent to which the model\u2019s judgments align with human evaluations, which are typically considered the benchmark for quality. Given the complexity and subjectivity of many evaluation tasks, achieving high agreement with human ratings is a key indicator of the LLM\u2019s performance. To quantify this agreement, a range of statistical metrics is employed. Below, we outline these metrics and their applications in evaluating LLMs-as-Judges models.\n\n\n\n6.2.1. Accuracy", "953d2807-8a5c-45d8-bd7a-e29e6d1b184b": "6.2.1. Accuracy\n\nAccuracy is a fundamental metric used to assess the proportion of correct judgments made by the LLM compared to human evaluations. In classification tasks, it is defined as:\n\n\n\n\n(2)\n\nAccuracy=Number of Correct PredictionsTotal Number of Predictions,AccuracyNumber of Correct PredictionsTotal Number of Predictions\\text{Accuracy}=\\frac{\\text{Number of Correct Predictions}}{\\text{Total Number%\n of Predictions}},Accuracy = divide start_ARG Number of Correct Predictions end_ARG start_ARG Total Number of Predictions end_ARG ,", "6dbdf6cf-fd83-4e9a-b05e-95555bf4b6c9": "where the number of correct predictions corresponds to instances where the LLM\u2019s judgment matches the human evaluator\u2019s judgment. While accuracy is simple to compute and intuitive, it may not fully capture the quality of the model, especially when dealing with tasks that involve nuanced or continuous evaluations.\n\n\n\n\n6.2.2. Pearson Correlation Coefficient\n\nThe Pearson Correlation Coefficient\u00a0(Cohen et\u00a0al., 2009) measures the linear relationship between two continuous variables, in this case, the evaluation scores assigned by the LLM and those assigned by human evaluators. It is defined as:\n\n\n\n\n(3)", "a3584533-ced3-4dee-bcf9-e847718cb3f5": "r=\u2211(xi\u2212x\u00af)\u2062(yi\u2212y\u00af)\u2211(xi\u2212x\u00af)2\u2062\u2211(yi\u2212y\u00af)2,\ud835\udc5fsubscript\ud835\udc65\ud835\udc56\u00af\ud835\udc65subscript\ud835\udc66\ud835\udc56\u00af\ud835\udc66superscriptsubscript\ud835\udc65\ud835\udc56\u00af\ud835\udc652superscriptsubscript\ud835\udc66\ud835\udc56\u00af\ud835\udc662r=\\frac{\\sum(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sqrt{\\sum(x_{i}-\\bar{x})^{2}\\sum(%", "35321efb-e891-4cfd-913f-17edf49f51ac": "y_{i}-\\bar{y})^{2}}},italic_r = divide start_ARG \u2211 ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over\u00af start_ARG italic_x end_ARG ) ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over\u00af start_ARG italic_y end_ARG ) end_ARG start_ARG square-root start_ARG \u2211 ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over\u00af start_ARG italic_x end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT \u2211 ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over\u00af start_ARG italic_y end_ARG ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG end_ARG ,", "63c2bf7a-93ca-4799-88b7-de33bea53452": "where xisubscript\ud835\udc65\ud835\udc56x_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT and yisubscript\ud835\udc66\ud835\udc56y_{i}italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT are the scores from the LLM and the human, respectively, and x\u00af\u00af\ud835\udc65\\bar{x}over\u00af start_ARG italic_x end_ARG and y\u00af\u00af\ud835\udc66\\bar{y}over\u00af start_ARG italic_y end_ARG are their means. Pearson correlation values range from \u221211-1- 1 to 1111.\n\n\n\n\n6.2.3. Spearman\u2019s Rank Correlation Coefficient\n\nSpearman\u2019s Rank Correlation Coefficient (\u03c1\ud835\udf0c\\rhoitalic_\u03c1) \u00a0(Sedgwick, 2014) assesses the monotonic relationship between two variables by comparing their ranked values rather than the raw scores. It is defined as:\n\n\n\n\n(4)", "dbf3da71-2e01-4117-a330-aec3c3305f6a": "(4)\n\n\u03c1=1\u22126\u2062\u2211di2n\u2062(n2\u22121),\ud835\udf0c16superscriptsubscript\ud835\udc51\ud835\udc562\ud835\udc5bsuperscript\ud835\udc5b21\\rho=1-\\frac{6\\sum d_{i}^{2}}{n(n^{2}-1)},italic_\u03c1 = 1 - divide start_ARG 6 \u2211 italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_n ( italic_n start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - 1 ) end_ARG ,", "c8f8a6b6-cf66-4180-b66b-67a10d2fb706": "where disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is the difference between the ranks of corresponding scores from the LLM and the human evaluator, and n\ud835\udc5bnitalic_n is the number of paired scores.\nSpearman\u2019s \u03c1\ud835\udf0c\\rhoitalic_\u03c1 is less sensitive to outliers and non-linear relationships compared to Pearson\u2019s correlation, making it a robust choice for evaluating tasks where the relative order of scores is more important than the exact values. It is commonly used in ranking-based evaluations such as preference judgments or ranking tasks.\n\n\n\n\n6.2.4. Kendall\u2019s Tau", "ee281701-6ecf-4774-80a1-69c8a6a06ef6": "6.2.4. Kendall\u2019s Tau\n\nKendall\u2019s Tau (\u03c4\ud835\udf0f\\tauitalic_\u03c4) \u00a0(Sen, 1968) is another rank-based correlation metric that measures the ordinal association between two ranked lists. It is defined as:\n\n\n\n\n(5)\n\n\u03c4=C\u2212D12\u2062n\u2062(n\u22121),\ud835\udf0f\ud835\udc36\ud835\udc3712\ud835\udc5b\ud835\udc5b1\\tau=\\frac{C-D}{\\frac{1}{2}n(n-1)},italic_\u03c4 = divide start_ARG italic_C - italic_D end_ARG start_ARG divide start_ARG 1 end_ARG start_ARG 2 end_ARG italic_n ( italic_n - 1 ) end_ARG ,", "aca80990-85c8-46ee-a3a7-235d6fbea85c": "where C\ud835\udc36Citalic_C is the number of concordant pairs (where the rank order agrees between the LLM and human), and D\ud835\udc37Ditalic_D is the number of discordant pairs. Kendall\u2019s \u03c4\ud835\udf0f\\tauitalic_\u03c4 is particularly useful when evaluating the consistency of rankings produced by LLMs and human evaluators. It is often preferred when the dataset contains many ties, as it provides a more nuanced measure of agreement than Spearman\u2019s \u03c1\ud835\udf0c\\rhoitalic_\u03c1.\n\n\n\n\n6.2.5. Cohen\u2019s Kappa\n\nCohen\u2019s Kappa (\u03ba\ud835\udf05\\kappaitalic_\u03ba) \u00a0(Warrens, 2015) measures the level of agreement between two raters (in this case, the LLM and the human) beyond what would be expected by chance. It is defined as:\n\n\n\n\n(6)", "7318b191-ebc2-43fc-966d-fc5a8d40ba5f": "(6)\n\n\u03ba=po\u2212pe1\u2212pe,\ud835\udf05subscript\ud835\udc5d\ud835\udc5csubscript\ud835\udc5d\ud835\udc521subscript\ud835\udc5d\ud835\udc52\\kappa=\\frac{p_{o}-p_{e}}{1-p_{e}},italic_\u03ba = divide start_ARG italic_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT - italic_p start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_ARG start_ARG 1 - italic_p start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT end_ARG ,", "94f9d19e-190e-468f-a9e6-4d32239c612c": "where posubscript\ud835\udc5d\ud835\udc5cp_{o}italic_p start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT is the observed agreement and pesubscript\ud835\udc5d\ud835\udc52p_{e}italic_p start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT is the expected agreement by chance.\nCohen\u2019s Kappa is particularly effective in classification tasks where both the LLM and the human evaluators assign categorical labels. It accounts for the possibility of random agreement, making it a more robust metric than simple accuracy.\n\n\n\n\n6.2.6. Intraclass Correlation Coefficient (ICC)", "1d062945-8a5f-481f-9d1c-01e59b3e9704": "6.2.6. Intraclass Correlation Coefficient (ICC)\n\nThe Intraclass Correlation Coefficient (ICC)\u00a0(Bartko, 1966) assesses the reliability of ratings when there are multiple evaluators. It evaluates the consistency or conformity of measurements made by different raters, including LLMs and human annotators. ICC is defined based on the variance components derived from a one-way or two-way ANOVA model.\nThe ICC is particularly useful when comparing multiple LLMs or when evaluating the consistency of an LLM across different subsets of data, providing a broader view of its reliability as an evaluator.\n\n\nTable 4. Summary of Common Metrics for Evaluating LLMs-as-Judges Models", "2946d9da-7431-4787-a0a3-37ad168aea57": "Metric\nType\nUse Case\nRobustness to Outliers\n\n\n\n\nAccuracy\nAgreement measure\nProportion of correct judgments\nSensitive\n\n\nPearson\nLinear correlation\nContinuous score comparison\nSensitive\n\n\nSpearman\nRank correlation\nRank-based evaluation tasks\nRobust\n\n\nKendall\u2019s Tau\nRank correlation\nConsistency in ordinal rankings\nRobust, handles ties\n\n\nCohen\u2019s Kappa\nAgreement measure\nTwo raters, consistency analysis\nAdjusts for chance\n\n\nICC\nAgreement measure\nMultiple raters, consistency analysis\nRobust for group ratings\n\n\n\n\n\n\n\n\n\n7. Limitation", "f72b18cb-7e37-4247-b8b7-dabc9cfd1c2c": "7. Limitation\n\nAlthough the application of LLMs-as-judges holds great promise, there are still several significant limitations that can affect their effectiveness, reliability, and fairness\u00a0(Thakur et\u00a0al., 2024; Stureborg et\u00a0al., 2024).\nThese limitations arise from the inherent characteristics of LLMs, including their reliance on large-scale data for training and token-based decoding mechanisms.\nIn this section, we will primarily explore the limitations in the following three key aspets: Biases (\u00a77.1), Adversarial Attacks (\u00a77.2), and Inherent Weaknesses (\u00a77.3).\n\n\nTable 5. Overview of different biases.\n\n\n\nBias\nDescription\n\n\n\nPresentation-Related Biases (\u00a77.1.1)\n\n\n\n\nPosition Bias", "be2daffd-fbc1-4e39-a97f-dde4e6e3bb00": "Position Bias\n\n\n\n\n\nA tendency to make judgments based on the position of the input, where responses earlier or later in the sequence are favored over those in other positions.\n\n\n\n\n\n\n\nVerbosity Bias\n\n\n\n\n\nA tendency to favor longer responses, potentially equating length with quality, regardless of the content\u2019s actual value.\n\n\n\n\n\n\n\n\nSocial-Related Biases (\u00a77.1.2)\n\n\nAuthority Bias\n\n\n\n\n\nA tendency to be swayed by references to authoritative sources, such as books, websites, or famous individuals.\n\n\n\n\n\n\n\nBandwagon-Effect Bias\n\n\n\n\n\nA tendency to align with majority opinions, where LLMs-as-judges favor prevailing views over objectively assessing the content.\n\n\n\n\n\n\n\nCompassion-Fade Bias", "bc634732-9300-422d-a06f-6ecdb0e97eb5": "Compassion-Fade Bias\n\n\n\n\n\nA tendency to be influenced by anonymization strategies, such as the removal of model names, affecting the judgments of LLMs.\n\n\n\n\n\n\n\nDiversity Bias\n\n\n\n\n\nA tendency to shift judgments based on identity-related markers, such as gender, ethnicity, or other social categorizations.\n\n\n\n\n\n\n\n\nContent-Related Biases (\u00a77.1.3)\n\n\nSentiment Bias\n\n\n\n\n\nA tendency to favor responses with specific emotional tones, such as cheerful or neutral, over negative or fearful ones.\n\n\n\n\n\n\n\nToken Bias\n\n\n\n\n\nA tendency of LLMs to favor more frequent or prominent tokens during pre-training, leading to skewed judgments.\n\n\n\n\n\n\n\nContext Bias", "1279e685-e0a6-49e9-b1a5-9de870ba5332": "Context Bias\n\n\n\n\n\nA tendency of LLMs to produce biased judgments influenced by contextual examples or cultural contexts, potentially leading to biased or culturally insensitive outcomes.\n\n\n\n\n\n\n\n\nCognitive-Related Biases (\u00a77.1.4)\n\n\nOverconfidence Bias\n\n\n\n\n\nA tendency to exhibit inflated confidence in evaluation judgments, leading to overly assertive but potentially incorrect conclusions\n\n\n\n\n\n\n\nSelf-Enhancement Bias\n\n\n\n\n\nA tendency to favor outputs generated by the same model acting as a judge, undermining objectivity.\n\n\n\n\n\n\n\nRefinement-Aware Bias", "c779b403-5ac1-4cf0-9f3d-476d7b3d16af": "Refinement-Aware Bias\n\n\n\n\n\nA tendency for scoring variations influenced by whether an answer is original, refined, or accompanied by conversation history during evaluation.\n\n\n\n\n\n\n\nDistraction Bias\n\n\n\n\n\nA tendency to be influenced by irrelevant content, which can detract from the quality of judgments by diverting attention from critical elements.\n\n\n\n\n\n\n\nFallacy-Oversight Bias\n\n\n\n\n\nA tendency to overlook logical fallacies, which can undermine the accuracy of judgments.\n\n\n\n\n\n\n\n\n\n\n\n7.1. Biases", "992ec3d5-95d9-420c-b650-b95aaae00009": "7.1. Biases\n\nEssentially, LLMs are trained on vast amounts of data gathered from diverse sources. While this allows them to generate human-like responses, it also makes them inherit to the biases inherent in the training data.\nThese biases are presented in various forms, which can significantly affect evaluation results, compromising the fairness and accuracy of decisions.", "d40842db-4bbf-4fbe-89af-d83b15cf6e86": "To gain a deeper understanding of the impact of bias, we have provided a detailed classification of bias. As shown in Table\u00a05, the biases exhibited by LLMs-as-judges can be systematically categorized into four groups based on their underlying causes and manifestations: Presentation-Related Biases (\u00a77.1.1), Social-Related Biases (\u00a77.1.2), Content-Related Biases (\u00a77.1.3), and Cognitive-Related Biases (\u00a77.1.4). In this section, we provide a detailed overview of the definition, impact, and solutions to these biases.\n\n\n\n7.1.1. Presentation-Related Biases", "e589ef41-039f-4855-94b0-125ad3fe9ba0": "7.1.1. Presentation-Related Biases\n\nPresentation-Related Biases refer to tendencies in LLMs where judgments are influenced more by the structure or presentation of information than by its substantive content.\nFor example, models may prioritize certain formats, styles, or patterns of expression, which can affect the quality of the input. Next, we introduce two biases related to Presentation-Related Biases: position bias and verbosity bias.", "f0e1cf0a-a021-4f06-9ae1-4b948f3348f5": "Position bias is a prevalent issue not only in the context of LLMs-as-judges but also in human decision-making and across various machine learning domains.\nResearch has shown that humans are often influenced by the order of options presented to them, leading to biased decision that can impact fairness and objectivity\u00a0(Shi et\u00a0al., 2024a; Blunch, 1984; Raghubir and Valenzuela, 2006; Zhao et\u00a0al., 2024a).\nSimilarly, in other ML applications, models trained on ordered data exhibit a positional preference, skewing outcomes based on the sequence of input\u00a0(Ko et\u00a0al., 2020; Wang et\u00a0al., 2018).", "9d1f9c02-cbf2-4a09-992a-e195d6fe6978": "Position bias in LLMs-as-judges refers to the tendency of LLMs to favor certain answers based on their position in the response set.\nFor example, when presented with multiple answer choices or compared pairwise, LLMs disproportionately select options that appear earlier in the list, leading to skewed judgment.", "1da5023d-bdb6-497c-b93b-6ccc2c5be160": "Recent studies have further examined position bias in the LLMs-as-judges context.\nFor instance, a framework\u00a0(LLMS, 2025) is proposed to investigate position bias in pairwise comparisons, introducing metrics such as repetition stability, position consistency, and preference fairness to better understand how positions affect LLM judgments.\nAnother study\u00a0(Zheng et\u00a0al., 2023a) explores the limitations of LLMs-as-judges, including position biases, and verifies agreement between LLM judgments and human preferences across multiple benchmarks.\nThese findings underscore the need for robust debiasing strategies to enhance the fairness and reliableness of LLMs-as-judges.", "5a32fb91-eb24-41cf-b69e-0893850cdff2": "Several methods are proposed to mitigate position bias.\nThe naive approach involves excluding inconsistent judgments by swapping the positions of the candidate answers and verifying whether the LLM\u2019s judgment remains consistent. Inconsistent judgments are then filtered out\u00a0(Zheng et\u00a0al., 2023a; Chen et\u00a0al., 2024a; Wang et\u00a0al., 2023b; Li et\u00a0al., 2023c; Zheng et\u00a0al., 2023b; Li et\u00a0al., 2024a).", "95450b08-b676-4e68-bd58-8c37e9b02b8a": "The swap-based debiasing method can be further divided into two categories: score-based and comparison-based. Both approaches start by swapping the positions of the candidate answers. The difference lies in how the final judgment is determined. In the score-based method, each candidate answer is scored, and the average score across multiple swaps is taken as the final score for that answer\u00a0(Zheng et\u00a0al., 2023a; Wang et\u00a0al., 2023b; Li et\u00a0al., 2023c; Raina et\u00a0al., 2024; Hou et\u00a0al., 2024).In contrast, the comparison-based method considers the outcome a tie if the LLM\u2019s judgments are inconsistent after swapping.", "419f7bee-e376-4f9c-bc2f-b177a49dca37": "The conclusion of a tie is based on an analysis of the quality gap between answers. The larger the quality gap between candidate answers, the smaller the impact of position bias, resulting in higher consistency in predictions after swapping their positions, which is detailed in a recently study\u00a0(LLMS, 2025).", "a62ed3f5-f1b3-4dc4-97c7-222879f1e8bf": "In addition to the aforementioned methods, PORTIA\u00a0(Li et\u00a0al., 2023d) employs an alignment-based approach that simulates human comparison strategies. It divides each answer into multiple segments, aligns similar content across candidate answers, and then merges these aligned segments into a single prompt for the LLM to evaluate. By presenting content in a balanced and aligned format, PORTIA enables the model to make more consistent and unbiased judgments, focusing on answer quality rather than order. This approach is effective across various LLMs, significantly improving evaluation consistency and reducing costs.", "ce6b582c-0bc8-41db-848a-f252399f4028": "Further efforts to enhance LLM-based evaluations have explored new techniques to address position bias and other judgment inconsistencies. Discussion-based methods\u00a0(Li et\u00a0al., 2023b; Khan et\u00a0al., 2024) incorporate peer ranking and discussion to improve evaluation accuracy. Instead of relying solely on a single LLM\u2019s judgment, these methods prompt multiple LLMs to compare answers and discuss preferences to reach a consensus, thereby reducing individual positional bias and enhancing alignment with human judgments. This collaborative evaluation approach represents a promising direction for mitigating biases inherent in LLM assessments.", "ec6098e4-5393-498d-9c19-e2078cd5eaf5": "Verbosity bias\u00a0(Khan et\u00a0al., 2024; Chen et\u00a0al., 2024a; Zheng et\u00a0al., 2023a; Nasrabadi, 2024) refers to the tendency of a judge, whether human or model-based, to favor lengthier responses over shorter ones, irrespective of the actual content quality or relevance. This bias may cause LLMs prefer longer responses, even if the extended content does not contribute substantively to the correctness of the judgments.", "6bc34fb7-cbe7-4496-8d05-ee3e55e45b3b": "To mitigate verbosity bias in LLMs-as-judges, several approaches\u00a0(Khan et\u00a0al., 2024; Ye et\u00a0al., 2024b, a) have been proposed. One approach\u00a0(Khan et\u00a0al., 2024) employs persuasive debating techniques, structuring responses to prioritize substance. By training LLMs-as-judges to engage in a debate-like format, this method encourages clarity and relevance, reducing the tendency to favor verbose arguments that lack substantive content. Additionally, the CALM\u00a0(Ye et\u00a0al., 2024b) framework introduces controlled modifications to systematically assess and quantify verbosity\u2019s impact on judgments, using automated perturbations to evaluate robustness against verbosity bias and refine LLMs-as-judges", "0f5260a0-12c9-4bdd-b6b8-b3b475d05be7": "against verbosity bias and refine LLMs-as-judges toward objective and concise assessments. Complementing these methods, the contrastive judgments (Con-J)\u00a0(Ye et\u00a0al., 2024a) approach trains models with structured rationale pairs instead of scalar scores, encouraging LLMs-as-judges to focus on well-reasoned content rather than associating verbosity with quality.", "86590aaa-d451-4f23-801b-e251cea00441": "7.1.2. Social-Related Biases\n\nSocial-Related Biases refer to biases in language models that resemble social phenomena\u00a0(Zhao et\u00a0al., 2023a). These biases may manifest when models are swayed by references to authoritative sources (Authority Bias), align with prevailing majority opinions without independent evaluation (Bandwagon-Effect Bias), or adjust their judgments based on anonymization strategies or identity markers such as gender and ethnicity (Compassion-Fade Bias and Diversity Bias). Next, we present the details of these biases.", "64fe2866-8d47-4076-8dda-97a7bbdc856e": "Authority bias\u00a0(Chen et\u00a0al., 2024a; Ye et\u00a0al., 2024b) in the context of LLMs-as-judges refers to the tendency of the model to attribute greater credibility to statements associated with authoritative references, regardless of the actual evidence supporting them. For instance, LLMs-as-judges may favor responses that include references to well-known sources or experts, even when the content is inaccurate or irrelevant. This bias highlights a critical vulnerability where the appearance of authority can unduly influence judgment outcomes.", "3ffcabaa-c3de-4069-9b4c-b328cd514225": "While specific solutions to mitigate authority bias in LLMs-as-judges are still under active exploration, potential approaches include using retrieval-augmented generation (RAG) techniques to verify the validity of authoritative claims against external knowledge bases. This approach allows the model to cross-check referenced information and ensure its alignment with factual evidence. Another possible strategy is to design prompts that explicitly emphasize semantic accuracy and relevance over perceived authority. Further research is needed to validate these approaches and develop robust methods for addressing authority bias effectively in evaluative contexts.", "b765be94-adb1-45a2-9254-531cccb5e604": "Bandwagon-effect bias\u00a0(Koo et\u00a0al., 2023; Ye et\u00a0al., 2024b) in the context of LLMs-as-judges refers to the tendency of the model to align its judgments with the majority opinion or prevailing trends, regardless of the actual quality or correctness of the evaluated content. For instance, when multiple responses are presented with indications of popular support or consensus, LLMs-as-judges may disproportionately favor these responses over alternatives, even when the consensus is flawed or biased. This bias reflects a susceptibility to groupthink dynamics, undermining the objectivity and fairness of the judgment process.", "c760d27c-4114-4b8a-bad2-a74928737c89": "Solutions to address bandwagon-effect bias include designing evaluation prompts that anonymize information about majority opinions, ensuring that judgments are based solely on the intrinsic quality of the responses rather than external indicators of popularity. Further exploration of debiasing strategies tailored to specific evaluative contexts is necessary to mitigate the impact of bandwagon-effect bias effectively.", "44b19e29-7292-48d5-9e7e-2f3ce18e0aff": "Compassion-fade bias\u00a0(Koo et\u00a0al., 2023; Ye et\u00a0al., 2024b) occurs when the anonymity of model names or the absence of identifiable contextual cues affects the judgments made by LLMs-as-judges. For example, anonymizing model names or using neutral identifiers may lead to shifts in evaluation outcomes. This bias highlights how the lack of personalized or contextual information can diminish the model\u2019s sensitivity to equitable considerations.", "6becc746-3470-4d3c-887f-c790e7879246": "To mitigate compassion-fade bias, it is important to design evaluation prompts that standardize judgment criteria, ensuring that assessments remain consistent regardless of whether identifying details are present. Additionally, fairness-driven frameworks that explicitly address anonymization effects can further enhance the reliability of LLMs-as-judges.", "80330536-13b5-42c8-8df9-c7089f7e8dfa": "Diversity bias\u00a0(Chen et\u00a0al., 2024a; Ye et\u00a0al., 2024b) in the context of LLMs-as-judges refers to the model\u2019s tendency to exhibit judgment shifts based on identity-related markers, such as gender, ethnicity, religion, or other social categorizations. For example, LLMs-as-judges might favor responses associated with certain demographic groups over others, leading to unfair or skewed judgments. This bias reflects the model\u2019s susceptibility to implicit stereotypes or unequal treatment of diverse identities present in the training data.\nContinued efforts to address this bias are crucial for ensuring fairness and inclusivity in the judgments conducted by LLMs-as-judges.", "db637895-faed-458b-a57a-dd8aac7f4361": "7.1.3. Content-Related Biases\n\nContent-Related Biases involve preferences or skewed judgments based on the content\u2019s characteristics. An LLM might favor responses with certain emotional tones (Sentiment Bias), prefer frequently occurring words from its training data (Token Bias), or be influenced by specific cultural or domain contexts leading to insensitive outcomes (Context Bias). We present the details of these biases in the following.", "c9e5f39d-95c5-4c37-a847-edf222e633ef": "Sentiment bias\u00a0(Ye et\u00a0al., 2024b) in the context of LLMs-as-judges refers to the tendency of the model to favor responses that exhibit certain emotional tones, such as positive or neutral sentiments, over others, regardless of their actual content quality or relevance. For instance, LLMs-as-judges may disproportionately reward responses that are cheerful or optimistic while penalizing those that are negative or emotionally intense, even if the latter are more contextually appropriate or accurate.\n\n\nTo address sentiment bias, potential solution is the use of sentiment-neutralizing mechanisms, such as filtering or adjusting responses to remove sentiment-driven influences during evaluation.", "4fe973ce-d7ff-4d20-a38e-6b0ad1b74486": "Token Bias\u00a0(Jiang et\u00a0al., 2024; Li et\u00a0al., 2024a; Pezeshkpour and Hruschka, 2023; Raina et\u00a0al., 2024) refers to that LLMs favor certain tokens during the evaluation process. This bias often arises from the model\u2019s pre-training data, where more frequently occurring tokens are prioritized over less common ones, regardless of the contextual appropriateness or correctness in judgment.", "9d342ad1-4af1-47b2-b9f1-df606efe6f76": "Contextual Bias refers to the tendency of LLMs to produce skewed or biased judgments based on the specific context in which they are applied. For instance, models used in healthcare may propagate biases found in medical datasets, potentially influencing diagnoses or treatment recommendations\u00a0(Poulain et\u00a0al., 2024), while in finance, they might reflect biases in credit scoring or loan approval processes\u00a0(Zhou et\u00a0al., 2024d).\nIn addition, the selection of contextual examples may also introduce bias\u00a0(Zhou et\u00a0al., 2023a; Fei et\u00a0al., 2023; Zhao et\u00a0al., 2021; Han et\u00a0al., 2022).\n\n\n\n\n7.1.4. Cognitive-Related Biases", "c28c5c0e-1473-40f0-9340-9070cfe8184f": "7.1.4. Cognitive-Related Biases\n\nCognitive-Related Biases pertain to the inherent cognitive tendencies of LLMs in processing information. This includes exhibiting unwarranted confidence in judgments (Overconfidence Bias), favoring outputs generated by themselves (Self-Enhancement Bias), varying scores based on whether an answer is original or refined (Refinement-Aware Bias), being distracted by irrelevant information (Distraction Bias), or overlooking logical fallacies (Fallacy-Oversight Bias). The details of these biases are presented in the following.", "968412f3-c487-4221-a736-b343e7781f42": "Overconfidence bias\u00a0(Khan et\u00a0al., 2024; Jung et\u00a0al., 2024) in the context of LLMs-as-judges refers to the tendency of models to exhibit an inflated level of confidence in their judgments, often resulting in overly assertive evaluations that may not accurately reflect the true reliability of the answer. This bias is particularly concerning in evaluative contexts, as it can lead LLMs-as-judges to overstate the correctness of certain outputs, compromising the objectivity and dependability of assessments.", "94daa8be-a247-4615-b178-a161d3f624c5": "To address Overconfidence bias, researchers have proposed several methods.\nCascaded Selective Evaluation\u00a0(Jung et\u00a0al., 2024) addresses overconfidence by using Simulated Annotators to estimate confidence. This involves simulating diverse annotator preferences through in-context learning, which provides a more realistic measure of the likelihood that a human would agree with the LLM\u2019s judgment. By analyzing multiple simulated responses, this method offers a confidence metric that reflects human-like disagreement, which helps to avoid overconfidence bias.", "d27ff5c8-6730-4e0a-8773-0ed3d23aa8ce": "Another method uses an adversarial debate mechanism, where two LLMs argue for different outcomes. Through structured debate rounds, each model is required to substantiate its position, which can reveal overconfidence by prompting self-reflection and critical analysis. This approach has been shown to improve truthfulness and reduces overconfidence by fostering a balanced evaluation, aligning LLMs\u2019 judgments more closely with accurate and reasoned conclusions.", "7bf72139-df7e-4724-84a9-44c69afef35f": "Self-enhancement bias is the tendency to favor their own outputs\u00a0(Liu et\u00a0al., 2023a; Zheng et\u00a0al., 2023a; Li et\u00a0al., 2023b; Liu et\u00a0al., 2023a; Panickssery et\u00a0al., 2024).\nThis concept of self-enhancement is drawn from social psychology, as discussed in Brown\u2019s work in social cognition literature\u00a0(Brown, 1986).\nIn the context of LLMs-as-judges, this bias manifests when a LLM evaluates its own generated outputs more favorably than those of other LLMs. Such bias is particularly concerning in applications involving self-assessment or feedback generation, as it compromises the objectivity of the LLMs-as-judges.", "91c4d0d7-12a7-4877-90cb-514197a22613": "To address self-enhancement bias, PRD\u00a0(Li et\u00a0al., 2023b) introduces Peer Rank (PR) and Peer Discussion (PD) mechanisms. PR mitigates bias by using multiple LLMs as reviewers, each assessing pairwise comparisons between responses from different LLMs. By aggregating evaluations from several peer LLMs and weighting their preferences based on consistency with human judgments, PR reduces the impact of any single LLM\u2019s self-enhancement bias, as more reliable reviewers have a greater influence. PD further alleviates self-enhancement bias by enabling two LLMs to engage in a dialogue to reach a mutual agreement on their preference between two answers. This multi-turn discussion encourages models to", "900b89d1-f85d-4774-8695-296543731d89": "This multi-turn discussion encourages models to re-evaluate their initial judgments and consider alternative perspectives, focusing on content quality rather than self-generated responses. By promoting collaborative assessment and accountability, PR and PD effectively mitigate self-enhancement bias, aligning evaluations more closely with human standards.", "294d76fb-50c2-4629-8adb-fd19c5c80db7": "Recently, an automated bias quantification framework named CALM\u00a0(Ye et\u00a0al., 2024b) has been proposed to systematically evaluate biases in LLMs-as-judges. CALM\u2019s findings suggest that one effective way to reduce self-enhancement bias is to avoid using the same model to both generate and judge answers, thereby ensuring that evaluation remains more impartial.", "c2c245b0-4594-4acd-a455-7d5d3a4d2a2c": "Moreover, the Reference-Guided Verdict method\u00a0(Badshah and Sajjad, 2024) further addresses self-enhancement bias by providing a definitive gold-standard answer as a reference for LLM judges. This reference anchor helps align judgments to objective criteria, even when an LLM evaluates its own output, thus reducing the tendency to favor self-generated answers. Through structured prompts, this method has been shown to enhance reliability and mitigate variability in judgments, especially when multiple LLMs are used collectively. The integration of multiple LLMs, trained on varied datasets or fine-tuned with different parameters, has proven instrumental in producing less biased, more balanced", "c2dba223-e997-428b-a70e-6adf0ba830a3": "in producing less biased, more balanced evaluations, highlighting the effectiveness of model diversity and reference-guided criteria in combating self-enhancement bias.", "fbdef364-2c62-4bc9-a78c-6e9aed932885": "Refinement-aware bias\u00a0(Ye et\u00a0al., 2024b) in the context of LLMs-as-judges refers to the tendency of the model to evaluate responses differently based on whether they are original, refined, or include revision history. For instance, an answer that has been iteratively refined may be judged more favorably than an original response, even if the refinement process does not significantly improve the content quality. Similarly, responses that explicitly present their improvement process or revision rationale might receive undue preference, skewing the evaluation outcomes.", "83607e9a-2775-4ca8-b88e-56dcda4fc158": "While research on refinement-aware bias in the specific context of LLMs-as-judges remains limited, solution\u00a0(Xu et\u00a0al., 2024c) developed for general LLMs offer valuable insights. One potential solution involves incorporating external feedback mechanisms during judgment, as it introduces an objective and independent judgment mechanism that is not influenced by the LLM\u2019s internal iterations or self-perception.", "c8434f58-edf8-429d-8a1c-2cccfe4f3c6a": "Distraction bias in the context of LLMs-as-judges refers to the model\u2019s tendency to be influenced by irrelevant or unimportant details when making judgments. For instance, introducing unrelated information, such as a meaningless statement like \u201cSystem Star likes to eat oranges and apples,\u201d\u00a0(Ye et\u00a0al., 2024b; Koo et\u00a0al., 2023; Shi et\u00a0al., 2023) can significantly alter the model\u2019s evaluation outcomes. This bias highlights the vulnerability of LLMs to attentional diversion caused by inconsequential content.", "25ccbcc8-bdb4-42b2-a128-463e487eb729": "While existing studies\u00a0(Ye et\u00a0al., 2024b; Koo et\u00a0al., 2023) have analyzed and discussed distraction bias, effective strategies to mitigate this issue in LLMs-as-judges remain underexplored. Potential solutions could involve input sanitization to preprocess and remove irrelevant information before presenting it to the model, ensuring that evaluations focus solely on relevant content. Additionally, explicit prompting with clear and strict guidelines could be designed to direct the LLM to evaluate only task-related aspects, reducing its susceptibility to distractions. Further research is needed to develop and validate robust methods that can systematically address distraction bias in the", "f8760410-5191-4923-a44e-d34540e2ef7d": "systematically address distraction bias in the context of LLMs-as-judges.", "a4fd9402-84e9-4914-8f89-fe5800a136eb": "Fallacy-oversight bias\u00a0(Chen et\u00a0al., 2024a; Ye et\u00a0al., 2024b) refers to the tendency of LLMs-as-judges to overlook logical fallacies or inconsistencies within the evaluated responses. For instance, when presented with arguments or answers containing reasoning errors\u2014such as circular reasoning, false dilemmas, or strawman arguments\u2014LLMs-as-judges may fail to identify these issues and treat the responses as valid, potentially compromising the integrity of their evaluations.", "419ee15d-f7fc-4ab1-a601-eb3cd026277c": "In summary, while LLMs-as-judges have garnered significant attention for their effectiveness in diverse scenarios, the exploration of various biases that impact their performance remains relatively underdeveloped.\nThese biases pose significant challenges to ensuring fair, objective, and reliable judgments across tasks, particularly in various applications where the implications of biased judgments can be severe.\nFuture research must focus on systematically identifying, quantifying, and addressing these biases within the LLMs-as-judges framework.", "7565dfb8-0769-4342-abd6-e4b17568e9e3": "Drawing from methodologies developed for general LLMs, such as external feedback mechanisms, balanced datasets, and fairness-aware prompting, could offer initial insights. However, domain-specific challenges require tailored solutions that align with the unique demands of LLMs-as-judges.", "8bdc6ba4-04b3-4c4f-847d-b3299103e032": "7.2. Adversarial Attacks", "546b30ee-04e5-4568-9733-b2622e822fdb": "Adversarial attacks involve carefully crafted inputs designed to deceive the model into producing incorrect or unintended outputs. For LLM judges, attackers may subtly modify the input content, alter the wording of questions, or introduce misleading context to influence the model\u2019s evaluation results.", "e5aad670-0386-4a33-8514-ac4a8c1bbe28": "Researchers have found that for LLMs, even small, seemingly insignificant changes to the input data, such as adding or removing words, changing word order, or introducing ambiguous phrasing, can significantly affect the model\u2019s response\u00a0(Shen et\u00a0al., 2023; Jiang et\u00a0al., 2023a; Zou et\u00a0al., 2023). Such attacks can lead to inaccurate ratings or assessments, particularly when evaluating complex or high-risk tasks.", "1d8e06e2-1854-4fb7-8e01-3f9653d2c302": "In this section, we first review research on adversarial attacks on LLMs within general domains. Then, we specifically focus on adversarial attacks in the context of LLMs-as-judges.\n\n\n\n7.2.1. Adversarial Attacks on LLMs\n\nAdversarial attacks on LLMs focus on exploiting vulnerabilities within the general framework of language model functionality. These attacks can be classified into three main categories based on the manipulation level: text-level manipulations, structural and semantic distortions, and optimization-based attacks.", "2545b10c-8376-4dfb-8ebf-437eddaadcae": "Text-Level Manipulations involve subtle changes to the input text to deceive the model. Character-level perturbations, such as introducing typos, swapping letters, or inserting unnecessary characters, can cause significant changes in predictions despite minimal visible alterations\u00a0(Ebrahimi et\u00a0al., 2017; Jiang et\u00a0al., 2023a). Sentence-level modifications, such as rearranging phrases, adding irrelevant information, or paraphrasing inputs, further exploit the model\u2019s sensitivity to surface-level changes\u00a0(Branch et\u00a0al., 2022; Perez and Ribeiro, 2022).", "5f1b9574-0685-4d48-aad2-01825e4ec034": "Structural and Semantic Distortions focus on the syntactic and semantic properties of the input. Syntactic attacks rewrite sentence structures while preserving semantic meaning, targeting the model\u2019s reliance on specific linguistic patterns\u00a0(Xu et\u00a0al., 2023a). Semantic preservation with perturbations modifies critical tokens identified through saliency analysis, ensuring the attack minimally affects meaning but significantly alters predictions.", "60575f88-2308-4913-96e3-854fedd83d7e": "Optimization-Based Attacks leverage algorithmic techniques to craft adversarial inputs. Gradient-based methods utilize the model\u2019s gradients to identify and manipulate influential input features, causing substantial shifts in predictions\u00a0(Sun, 2020; Sun et\u00a0al., 2020). Population-based optimization techniques iteratively generate adversarial examples in black-box settings, exploiting the model\u2019s outputs to refine attacks\u00a0(Lee et\u00a0al., 2022).", "06183f93-8a1b-4a74-afe3-c7ee6d5cf1d6": "These attacks highlight the vulnerabilities in LLMs, demonstrating their susceptibility to subtle manipulations. Studying these adversarial attacks is essential, as it provides insights that can guide the development of robust defense mechanisms, ensuring that LLMs maintain reliability against such manipulations.\n\n\n\n\n7.2.2. Adversarial Attacks on LLMs-as-judges", "66389d31-f140-4da9-a525-a8f5137a80c1": "7.2.2. Adversarial Attacks on LLMs-as-judges\n\nRecent studies have unveiled significant vulnerabilities in LLMs-as-judges to adversarial attacks\u00a0(Zheng et\u00a0al., 2024; Doddapaneni et\u00a0al., 2024; Raina et\u00a0al., 2024; Shi et\u00a0al., 2024b).\nZheng et al.\u00a0(Zheng et\u00a0al., 2024) and Doddapaneni et al.\u00a0(Doddapaneni et\u00a0al., 2024) demonstrated that automatic benchmarking systems like MT-Bench\u00a0(Zheng et\u00a0al., 2023a) can be easily deceived to yield artificially high scores. These findings highlight that malicious inputs can manipulate evaluation metrics, undermining the reliability of such benchmarks.", "ddfb2071-9566-4e4e-8aad-bd495576a9bd": "Building on this, Raina et al.\u00a0(Raina et\u00a0al., 2024) investigated the robustness of LLMs-as-judges against universal adversarial attacks. Their work showed that appending short, carefully crafted phrases to evaluated texts can effortlessly manipulate LLM scores, inflating them to their maximum regardless of the actual quality. Remarkably, these universal attack phrases are transferable across models; phrases optimized on smaller surrogate models (e.g., FlanT5-xl\u00a0(Chung et\u00a0al., 2024)) can successfully deceive larger models like GPT-3.5 and Llama2\u00a0(Touvron et\u00a0al., 2023).", "b4fedfcf-8674-4c65-b410-bda8da9f8a76": "Furthermore, Shi et al.\u00a0(Shi et\u00a0al., 2024b) introduced JudgeDeceiver, an optimization-based prompt injection attack tailored for the LLMs-as-judges framework. Unlike handcrafted methods, JudgeDeceiver formulates a precise optimization objective to efficiently generate adversarial sequences. These sequences can mislead LLMs-as-judges into selecting biased or incorrect responses among candidate answers, thereby compromising the evaluation process.", "d920cb93-a0bf-476a-b442-2c9f6d8c00df": "Although preliminary studies\u00a0(Shi et\u00a0al., 2024b; Raina et\u00a0al., 2024) have highlighted the vulnerability of LLMs-as-judges to adversarial manipulations, this field remains largely underexplored.\nIt is imperative to advance our understanding of these weaknesses and devise effective defense strategies.\nAs the use of LLMs-as-judges grows across diverse applications, future research should focus on uncovering new attack methods and strengthening the models against such adversarial threats.\n\n\n\n\n\n7.3. Inherent Weaknesses", "6021c705-dec5-4850-bb7c-d432a6f5221e": "7.3. Inherent Weaknesses\n\nDespite the remarkable capabilities of LLMs, they possess several inherent weaknesses that can compromise their reliability and robustness in LLMs-as-judges. This subsection discusses key limitations, including issues related to knowledge recency, hallucination, and other domain-specific knowledge gaps\u00a0(Zhao et\u00a0al., 2023b).\n\n\n\n7.3.1. Knowledge Recency", "149ef129-80c1-4bcb-a8ee-47a751fc5160": "One significant limitation of LLMs is their inability to access or incorporate up-to-date information reliably. LLMs are generally trained on static datasets that may become outdated over time, limiting their ability to evaluate scenarios that require knowledge of recent events, legislation, or rapidly evolving fields. The most straightforward solution is to retrain the model on new data; however, this approach is resource-intensive and risks catastrophic forgetting\u00a0(Luo et\u00a0al., 2023), where previously learned knowledge is overwritten during training.", "e706091b-aa89-4706-9afe-ad0c02844f2e": "This temporal disconnect can lead to judgments based on invalid data, or obsolete practices, compromising their reliability in real-world, time-sensitive applications. Consider a case where LLMs-as-judges are used to evaluate which of two responses from LLMs better answers a prompt about the COVID-19 pandemic. Suppose one response references the WHO guidelines updated timely, while the other relies on outdated 2020 guidelines. If the LLM-as-Judge has not been updated with the latest guidelines, it might erroneously prefer the outdated response, incorrectly deeming it more accurate. This failure to account for recent developments highlights the importance of addressing knowledge recency in", "ac89ac07-9978-48f4-81d1-e2afef2a551a": "the importance of addressing knowledge recency in LLMs-as-judges.", "191597f2-0deb-4f81-aac8-6aaddf5b04e2": "Addressing the issue of knowledge recency can involve integrating retrieval-augmented generation (RAG) methods\u00a0(Gao et\u00a0al., 2023; Lewis et\u00a0al., 2020), which enable LLMs to query external, dynamically updated databases or knowledge sources during evaluation.\nAdditionally, periodic fine-tuning with updated datasets or leveraging continual learning frameworks\u00a0(Wu et\u00a0al., 2024a) can ensure that LLMs-as-judges remain aligned with the latest information. Combining these approaches with robust fact-checking mechanisms\u00a0(Dierickx et\u00a0al., 2024) can further enhance temporal reliability in judgment contexts.\n\n\n\n\n7.3.2. Hallucination", "53756370-147d-4967-a184-ec91e6333273": "7.3.2. Hallucination\n\nAnother critical issue in LLMs is the hallucination problem, where models generate incorrect or fabricated information with high confidence. In the context of LLMs-as-judges, hallucination can manifest as the invention of non-existent precedents, misinterpretation of facts, or fabrication of sources, which can severely undermine the reliability of their judgments. This issue is particularly concerning in various applications, where such errors can lead to unfair or harmful outcomes.", "56d7e3bb-f5df-4b74-84d9-32af3697b74f": "Employing fact-checking mechanisms\u00a0(Dierickx et\u00a0al., 2024; Ji et\u00a0al., 2023; Tonmoy et\u00a0al., 2024) during evaluation is crucial to mitigate hallucination. By cross-verifying the outputs of LLMs-as-judges with trusted databases and external knowledge sources, hallucinated information can be identified and corrected.\n\n\n\n\n7.3.3. Domain-Specific Knowledge Gaps", "040c79f3-405e-438e-8268-0eb0fbaed727": "7.3.3. Domain-Specific Knowledge Gaps\n\nWhile LLMs demonstrate broad generalization capabilities, they often lack the depth of understanding required for specialized domains\u00a0(Feng et\u00a0al., 2023; Pan et\u00a0al., 2024b; Szymanski et\u00a0al., 2024; Dorner et\u00a0al., 2024). For instance, legal judgments demand intricate knowledge of statutes, precedents, and contextual nuances, which may not be adequately captured in the training data of general-purpose LLMs. This limitation can lead to shallow or incorrect judgments in domain-specific contexts.", "4caf2969-19f6-4b7b-a02a-40636663a69e": "Domain adaptation techniques, such as integrating LLMs with domain-specific knowledge graphs\u00a0(Feng et\u00a0al., 2023; Pan et\u00a0al., 2024b) or leveraging RAG systems\u00a0(Gao et\u00a0al., 2023), can substantially improve their performance in specialized domains. Knowledge graphs provide structured, expert-curated information that enhances context-awareness, while RAG enables LLMs to dynamically retrieve relevant knowledge from specific domain.", "c1bf72bd-5686-4ea8-a11b-3be77a70391a": "The inherent weaknesses of LLMs highlight the need for continued research and innovation. Addressing these limitations through RAG, enhanced training methods, and knowledge graph techniques is crucial for ensuring that LLMs-as-judges deliver reliable, accurate, and trustworthy evaluations in diverse applications.\n\n\n\n\n\n\n8. Future Work\n\nIn this section, we will explore the key directions for future work, focusing on how to build more efficient, more effective, and more reliable LLM judges. These directions aim to address the bottlenecks and challenges in current technologies and practices, while also promoting broader applications and deeper integration of LLM judges in diverse scenarios.", "6facb05a-4b89-4f0d-b860-74fb6aca4021": "8.1. More Efficient LLMs-as-Judges\n\n\n8.1.1. Automated Construction of Evaluation Criteria and Tasks", "7f3fa0f3-81cf-42d5-a70f-531293dddd89": "Current LLM judges often rely on manually predefined evaluation criteria, lacking the ability to adapt dynamically during the assessment process. Designing prompts for these systems is not only tedious and time-consuming but also struggles to address the diverse requirements of various task scenarios\u00a0(Bai et\u00a0al., 2024; Zhao et\u00a0al., 2024b; Yu et\u00a0al., 2024; Zhang et\u00a0al., 2024c; Wang et\u00a0al., 2024f). To overcome these limitations, future LLM judges could incorporate enhanced adaptability by tailoring evaluation criteria based on task types, target audiences, and domain-specific knowledge. Such advancements would significantly streamline the configuration process of LLM judges, while also", "fb5e18e4-26db-4da5-842a-4d8640e9333b": "configuration process of LLM judges, while also greatly improving their practicality and efficiency in real-world applications.", "176cf018-5336-4c38-94de-c0ce0c2e3113": "Moreover, existing static evaluation datasets are prone to issues such as training data contamination, which can compromise their effectiveness in accurately assessing the evolving capabilities of LLMs. To address this, future LLM judges could focus on dynamically constructing more suitable evaluation tasks and continuously optimizing the evaluation process, thereby enhancing applicability and precision\u00a0(Bai et\u00a0al., 2024; Zhao et\u00a0al., 2024b).\n\n\n\n\n8.1.2. Scalable Evaluation Systems", "8fe5bbf8-6b00-4d71-a9a6-8a2de19e1f1e": "8.1.2. Scalable Evaluation Systems\n\nExisting LLM judges often exhibit limited adaptability in practical applications. While these judges may perform effectively on specific downstream tasks, they frequently struggle in cross-domain or multi-task settings, thereby falling short of meeting the diverse and broader demands of real-world applications.", "83f77e42-56a7-4d3d-aa22-773a58e0d132": "To address these limitations, future research could focus on modular design principles to create scalable evaluation frameworks\u00a0(Xu et\u00a0al., 2024a). Such frameworks would allow users to flexibly add or customize evaluation modules to suit their specific needs. This modular approach not only enhances the usability and flexibility of the system but also significantly reduces the cost and complexity of transferring the framework across different domains.\n\n\n\n\n8.1.3. Accelerating Evaluation Processes", "a8518c41-e10f-4b25-a625-2a85ea70c97f": "8.1.3. Accelerating Evaluation Processes\n\nExisting LLM systems often face significant computational costs when performing evaluation tasks. For example, pairwise comparison methods require multiple rounds of comparisons for each candidate, which becomes extremely time-consuming as the number of candidates grows. In resource-constrained environments, such high-cost evaluation methods are challenging to deploy effectively. To address this issue, future research could focus on developing more efficient candidate selection algorithms, thereby unlocking new opportunities for the use of LLMs in low-resource settings\u00a0(Lee et\u00a0al., 2024b; Liu et\u00a0al., 2024c).", "ae85f326-3bd5-4071-b294-dc507a676dac": "Similarly, the multi-LLM evaluation paradigm, which relies on multiple rounds of interaction, further exacerbates computational demands. To mitigate these challenges, future efforts could explore streamlined communication frameworks that support high-quality evaluation tasks while minimizing resource requirements\u00a0(Chen et\u00a0al., 2024f). Advances in these areas could lead to the development of more efficient and scalable evaluation systems, making LLM-based evaluations more practical across diverse and resource-limited scenarios.\n\n\n\n\n\n8.2. More Effective LLMs-as-Judges\n\n\n8.2.1. Integration of Reasoning and Judge Capabilities", "81656deb-f12a-403c-8c41-bb6607576200": "Current LLMs-as-judges systems often treat reasoning and evaluation capabilities as distinct and independent modules, which can hinder effectiveness when addressing complex tasks. As the demand for evaluating increasingly complex systems grows, future LLM-as-Judge systems should prioritize the deep integration of reasoning and evaluation capabilities to achieve a seamless synergy\u00a0(Zhuo, 2023; Yi et\u00a0al., 2024; Stephan et\u00a0al., 2024). For instance, in legal scenarios, the model could first infer the relevant legal provisions and then assess the case\u2019s relevance, making the evaluation process more effective.\n\n\n\n\n8.2.2. Establishing a Collective Judgment Mechanism", "1edf66d4-0cea-496c-975c-dfe1ed7502c6": "Current LLMs-as-Judge systems typically rely on a single model for evaluation. While this approach is straightforward, it is prone to biases inherent in individual models, leading to reduced accuracy and stabilitys. Moreover, a single model often struggles to comprehensively address the diverse requirements of such tasks. Future research could investigate collaborative multi-agent mechanisms to enable \u201ccollective judgment\u201d where multiple LLMs work together, leveraging their respective strengths in reasoning and knowledge\u00a0(Chan et\u00a0al., 2023; Chu et\u00a0al., 2024),. Additionally, ensemble techniques could be employed to dynamically balance the contributions of different models, leading to more", "6ea30b8a-e0ba-4644-82e9-0a55d5720faa": "of different models, leading to more stable and reliable judgment outcomes.", "c6495d56-6b5b-4988-b968-4ffb32a59958": "8.2.3. Enhancing Domain Knowledge\n\nCurrent LLMs-as-Judge systems often fall short when handling tasks in specialized fields due to insufficient domain knowledge. Furthermore, as domain knowledge continues to evolve, these models struggle to keep up with the latest developments, further limiting their effectiveness and applicability in real-world scenarios.", "1b746a6a-d32e-4e3d-a2a8-609317218f64": "To address these challenges, future LLMs-as-judges systems should focus on integrating comprehensive domain knowledge to enhance their performance in specialized tasks\u00a0(Raju et\u00a0al., 2024). This can be achieved by utilizing knowledge graphs, embedding domain-specific expertise, and fine-tuning models based on feedback from subject-matter experts.\nIn addition, these systems should incorporate dynamic knowledge updating capabilities. For instance, in the legal domain, models could regularly acquire and integrate updates on new statutes, case law, and policy changes, ensuring that their judgments remain current and aligned with the latest legal standards.", "5cc73ae3-1809-4e25-adcf-ea985f0a0708": "8.2.4. Cross-Domain and Cross-Language Transferability\n\nCurrent LLMs-as-Judge systems are often confined to specific domains or languages, making it challenging for them to transfer across different fields. For instance, an LLM proficient in processing legal texts may struggle to effectively handle evaluation tasks in the medical or financial domains. This limitation greatly restricts the applicability of such systems.", "3d6c7af4-981a-4958-b975-408ec7e446c6": "Future research can focus on exploring cross-domain and cross-language transfer learning techniques to enhance the adaptability of LLMs in diverse fields. By leveraging shared general knowledge across fields, models can quickly adapt to new tasks with minimal additional training costs\u00a0(Son et\u00a0al., 2024b; Hada et\u00a0al., 2023; Watts et\u00a0al., 2024). For example, evaluation capabilities developed in English could be transferred to contexts in German, thereby improving the evaluation performance in these new areas.\n\n\n\n\n8.2.5. Multimodal Integration Evaluation", "2d9cfa39-0d0a-45a8-8154-9c7bac085dae": "Current LLM-as-Judge systems primarily focus on processing textual data, with limited attention to integrating other modalities like images, audio, and video. This single-modal approach falls short in complex scenarios requiring multimodal analysis, such as combining visual and textual information in medical assessments. Future systems should develop cross-modal integration capabilities to process and evaluate multimodal data simultaneously\u00a0(Chen et\u00a0al., 2024b). Leveraging cross-modal validation can enhance evaluation accuracy. Key research areas include efficient multimodal feature extraction, integration, and the design of unified frameworks for more comprehensive and precise evaluations.", "58302961-14c6-4685-a91d-d7f14fef522b": "8.3. More Reliable LLMs-as-Judges\n\n\n8.3.1. Enhancing Interpretability and Transparency", "83e07b9b-79ce-4350-889a-58bb65b9e53b": "Current LLM-as-Judge systems often operate as black boxes, with their rulings lacking transparency and a clear reasoning process. This opacity is particularly concerning in high-stakes domains such as legal judgments, where users cannot fully understand the basis of the model\u2019s decisions or trust its outputs.\nFuture research should focus on improving the interpretability of LLMs\u00a0(Liu et\u00a0al., 2024a). For example, the LLM judges should not only provide evaluation results but also present a clear explanation. Research could explore designing validation models based on logical frameworks to make the decision-making process more transparent.\n\n\n\n\n8.3.2. Mitigating Bias and Ensuring Fairness", "a32b445f-7be5-404f-b6ec-6a2662f49705": "8.3.2. Mitigating Bias and Ensuring Fairness\n\nLLMs may be influenced by biases present in their training data, leading to unfair judgments in different social, cultural, or legal contexts. These biases could be amplified by the model and compromise the fairness of its decisions. Future research could focus on ensuring fairness in model outputs through debiasing algorithms and fairness constraints\u00a0(Li et\u00a0al., 2024a). Targeted approaches, such as adversarial debiasing training or bias detection tools, can dynamically identify and mitigate potential biases during the model\u2019s reasoning process.\n\n\n\n\n8.3.3. Enhancing Robustness", "cb2fd25d-bc26-4baa-a8b9-2aae4c42e572": "8.3.3. Enhancing Robustness\n\nLLMs are sensitive to noise, incompleteness, or ambiguity in input instructions, which may lead to errors or instability in evaluation results when handling complex or highly uncertain texts. This lack of robustness significantly limits their reliability in practical applications.\nFuture research can adopt several methods to enable LLMs robust and reliable performance in real-world environments\u00a0(Shi et\u00a0al., 2024b; Elangovan et\u00a0al., 2024). For instance, introducing more advanced data augmentation techniques to generate diverse and uncertain simulated cases can help train models to adapt to various complex input conditions.\n\n\n\n\n\n\n9. Conclusion", "3edf4dc2-c639-4b40-9d68-3291c00a0917": "9. Conclusion\n\nThis survey systematically examined the LLMs-as-Judges framework across five dimensions: functionality, methodology, applications, meta-evaluation, and limitations, providing a comprehensive understanding of its advantages, limitations, practical implementations, applications, and methods for evaluating its effectiveness.\nTo advance research in this field, we also outlined several promising directions for future exploration, including the development of more efficient, effective, and reliable LLM judges. We hope to promote the ongoing development of this field by providing foundational resources and will continue to update relevant content.\n\n\n\n\nReferences\n\n\n(1)", "18c84214-cd2a-4cd2-8c6f-0a9ad7400141": "References\n\n\n(1)\n\n\n\n\nAchiam et\u00a0al. (2023)\n\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia\u00a0Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et\u00a0al. 2023.\n\n\nGpt-4 technical report.\n\n\narXiv preprint arXiv:2303.08774 (2023).\n\n\n\n\n\n\nArif et\u00a0al. (2024)\n\nSamee Arif, Sualeha Farid, Abdul\u00a0Hameed Azeemi, Awais Athar, and Agha\u00a0Ali Raza. 2024.\n\n\nThe fellowship of the llms: Multi-agent workflows for synthetic preference optimization dataset generation.\n\n\narXiv preprint arXiv:2408.08688 (2024).\n\n\n\n\n\n\nAsai et\u00a0al. (2023)\n\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.", "120ba925-fefe-428c-8ca4-3b799e86f6bb": "Self-rag: Learning to retrieve, generate, and critique through self-reflection.\n\n\narXiv preprint arXiv:2310.11511 (2023).\n\n\n\n\n\n\nAsghar (2016)\n\nNabiha Asghar. 2016.\n\n\nYelp dataset challenge: Review rating prediction.\n\n\narXiv preprint arXiv:1605.05362 (2016).\n\n\n\n\n\n\nAshktorab et\u00a0al. (2024)\n\nZahra Ashktorab, Michael Desmond, Qian Pan, James\u00a0M Johnson, Martin\u00a0Santillan Cooper, Elizabeth\u00a0M Daly, Rahul Nair, Tejaswini Pedapati, Swapnaja Achintalwar, and Werner Geyer. 2024.\n\n\nAligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences.\n\n\narXiv preprint arXiv:2410.00873 (2024).\n\n\n\n\n\n\nAskell et\u00a0al. (2021)", "28f00ab2-161f-4702-800f-c385de1c6a47": "Askell et\u00a0al. (2021)\n\nA Askell, Y Bai, A Chen, D Drain, D Ganguli, T Henighan, A Jones, N Joseph, B Mann, N DasSarma, et\u00a0al. 2021.\n\n\nA general language assistant as a laboratory for alignment. arXiv.\n\n\nPreprint posted online December 1 (2021).\n\n\n\n\n\n\nBabaei and Giudici (2024)\n\nGolnoosh Babaei and Paolo Giudici. 2024.\n\n\nGPT classifications, with application to credit lending.\n\n\nMachine Learning with Applications 16 (2024), 100534.\n\n\n\n\n\n\nBadshah and Sajjad (2024)\n\nSher Badshah and Hassan Sajjad. 2024.\n\n\nReference-Guided Verdict: LLMs-as-Judges in Automatic Evaluation of Free-Form Text.\n\n\narXiv preprint arXiv:2408.09235 (2024).\n\n\n\n\n\n\nBai et\u00a0al. (2023)", "cdcfef42-d57a-4e8d-a757-ee5dd8c60ed5": "Bai et\u00a0al. (2023)\n\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et\u00a0al. 2023.\n\n\nQwen technical report.\n\n\narXiv preprint arXiv:2309.16609 (2023).\n\n\n\n\n\n\nBai et\u00a0al. (2024)\n\nYushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et\u00a0al. 2024.\n\n\nBenchmarking foundation models with language-model-as-an-examiner.\n\n\nAdvances in Neural Information Processing Systems 36 (2024).\n\n\n\n\n\n\nBajaj et\u00a0al. (2016)\n\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, et\u00a0al. 2016.", "378c5ac2-54af-4178-a6aa-a03286b229cc": "Ms marco: A human generated machine reading comprehension dataset.\n\n\narXiv preprint arXiv:1611.09268 (2016).\n\n\n\n\n\n\nBandi and Harrasse (2024)\n\nChaithanya Bandi and Abir Harrasse. 2024.\n\n\nAdversarial Multi-Agent Evaluation of Large Language Models through Iterative Debates.\n\n\narXiv preprint arXiv:2410.04663 (2024).\n\n\n\n\n\n\nBartko (1966)\n\nJohn\u00a0J Bartko. 1966.\n\n\nThe intraclass correlation coefficient as a measure of reliability.\n\n\nPsychological reports 19, 1 (1966), 3\u201311.\n\n\n\n\n\n\nBavaresco et\u00a0al. (2024)\n\nAnna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fern\u00e1ndez, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, et\u00a0al. 2024.", "59d27793-c918-4d10-a7a7-c9f421ddbf25": "Llms instead of human judges? a large scale empirical study across 20 nlp evaluation tasks.\n\n\narXiv preprint arXiv:2406.18403 (2024).\n\n\n\n\n\n\nBesta et\u00a0al. (2024)\n\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et\u00a0al. 2024.\n\n\nGraph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol.\u00a038. 17682\u201317690.\n\n\n\n\n\n\nBlunch (1984)\n\nNiels\u00a0J Blunch. 1984.\n\n\nPosition bias in multiple-choice questions.\n\n\nJournal of Marketing Research 21, 2 (1984), 216\u2013220.\n\n\n\n\n\n\nBrake and Schaaf (2024)", "509e72aa-96cd-40c8-86a6-8a013b7cedc4": "Brake and Schaaf (2024)\n\nNathan Brake and Thomas Schaaf. 2024.\n\n\nComparing Two Model Designs for Clinical Note Generation; Is an LLM a Useful Evaluator of Consistency?\n\n\narXiv preprint arXiv:2404.06503 (2024).\n\n\n\n\n\n\nBranch et\u00a0al. (2022)\n\nHezekiah\u00a0J Branch, Jonathan\u00a0Rodriguez Cefalu, Jeremy McHugh, Leyla Hujer, Aditya Bahl, Daniel del\u00a0Castillo Iglesias, Ron Heichman, and Ramesh Darwishi. 2022.\n\n\nEvaluating the susceptibility of pre-trained language models via handcrafted adversarial examples.\n\n\narXiv preprint arXiv:2209.02128 (2022).\n\n\n\n\n\n\nBrown (1986)\n\nJonathon\u00a0D Brown. 1986.\n\n\nEvaluations of self and others: Self-enhancement biases in social judgments.", "209758fa-9ac6-4634-9796-492ff1f6e7ff": "Social cognition 4, 4 (1986), 353\u2013376.\n\n\n\n\n\n\nCao et\u00a0al. (2024a)\n\nMaosong Cao, Alexander Lam, Haodong Duan, Hongwei Liu, Songyang Zhang, and Kai Chen. 2024a.\n\n\nCompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution.\n\n\narXiv preprint arXiv:2410.16256 (2024).\n\n\n\n\n\n\nCao et\u00a0al. (2024b)\n\nMeng Cao, Lei Shu, Lei Yu, Yun Zhu, Nevan Wichers, Yinxiao Liu, and Lei Meng. 2024b.\n\n\nEnhancing Reinforcement Learning with Dense Rewards from Language Model Critic. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. 9119\u20139138.\n\n\n\n\n\n\nChan et\u00a0al. (2023)", "a7e6833b-d179-4deb-abc0-1472a691a7a3": "Chan et\u00a0al. (2023)\n\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023.\n\n\nChateval: Towards better llm-based evaluators through multi-agent debate.\n\n\narXiv preprint arXiv:2308.07201 (2023).\n\n\n\n\n\n\nChang et\u00a0al. (2024)\n\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et\u00a0al. 2024.\n\n\nA survey on evaluation of large language models.\n\n\nACM Transactions on Intelligent Systems and Technology 15, 3 (2024), 1\u201345.\n\n\n\n\n\n\nChen et\u00a0al. (2024b)", "bcf72fe9-9032-41aa-924f-6489518f535e": "Chen et\u00a0al. (2024b)\n\nDongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, and Lichao Sun. 2024b.\n\n\nMllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark.\n\n\narXiv preprint arXiv:2402.04788 (2024).\n\n\n\n\n\n\nChen et\u00a0al. (2024a)\n\nGuiming\u00a0Hardy Chen, Shunian Chen, Ziche Liu, Feng Jiang, and Benyou Wang. 2024a.\n\n\nHumans or llms as the judge? a study on judgement biases.\n\n\narXiv preprint arXiv:2402.10669 (2024).\n\n\n\n\n\n\nChen et\u00a0al. (2023b)\n\nHong Chen, Duc\u00a0Minh Vo, Hiroya Takamura, Yusuke Miyao, and Hideki Nakayama. 2023b.\n\n\nStoryER: Automatic story evaluation via ranking, rating and reasoning.", "50ed275f-ae2c-4764-a08e-4b4036e8d256": "Journal of Natural Language Processing 30, 1 (2023), 243\u2013249.\n\n\n\n\n\n\nChen et\u00a0al. (2024e)\n\nJunjie Chen, Weihang Su, Zhumin Chu, Haitao Li, Qinyao Ai, Yiqun Liu, Min Zhang, and Shaoping Ma. 2024e.\n\n\nAn Automatic and Cost-Efficient Peer-Review Framework for Language Generation Evaluation.\n\n\n\n\narXiv:2410.12265\u00a0[cs.CL]\n\nhttps://arxiv.org/abs/2410.12265\n\n\n\nChen et\u00a0al. (2023c)\n\nJiefeng Chen, Jinsung Yoon, Sayna Ebrahimi, Sercan\u00a0O Arik, Tomas Pfister, and Somesh Jha. 2023c.\n\n\nAdaptation with self-evaluation to improve selective prediction in llms.\n\n\narXiv preprint arXiv:2310.11689 (2023).\n\n\n\n\n\n\nChen et\u00a0al. (2024d)", "842f517a-76a2-445f-9800-5406a9fe4dff": "Chen et\u00a0al. (2024d)\n\nKai Chen, Yanze Li, Wenhua Zhang, Yanxin Liu, Pengxiang Li, Ruiyuan Gao, Lanqing Hong, Meng Tian, Xinhai Zhao, Zhenguo Li, et\u00a0al. 2024d.\n\n\nAutomated evaluation of large vision-language models on self-driving corner cases.\n\n\narXiv preprint arXiv:2404.10595 (2024).\n\n\n\n\n\n\nChen et\u00a0al. (2021)\n\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De\u00a0Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et\u00a0al. 2021.\n\n\nEvaluating large language models trained on code.\n\n\narXiv preprint arXiv:2107.03374 (2021).\n\n\n\n\n\n\nChen et\u00a0al. (2024f)", "e1c103e1-300c-486d-9768-e7fcc2b9f77b": "Chen et\u00a0al. (2024f)\n\nWeize Chen, Ziming You, Ran Li, Yitong Guan, Chen Qian, Chenyang Zhao, Cheng Yang, Ruobing Xie, Zhiyuan Liu, and Maosong Sun. 2024f.\n\n\nInternet of agents: Weaving a web of heterogeneous agents for collaborative intelligence.\n\n\narXiv preprint arXiv:2407.07061 (2024).\n\n\n\n\n\n\nChen et\u00a0al. (2023a)\n\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. 2023a.\n\n\nTeaching large language models to self-debug.\n\n\narXiv preprint arXiv:2304.05128 (2023).\n\n\n\n\n\n\nChen et\u00a0al. (2024c)\n\nYen-Shan Chen, Jing Jin, Peng-Ting Kuo, Chao-Wei Huang, and Yun-Nung Chen. 2024c.\n\n\nLLMs are Biased Evaluators But Not Biased for Retrieval Augmented Generation.", "e270a88e-8798-4bca-944c-c8510c0344fb": "arXiv preprint arXiv:2410.20833 (2024).\n\n\n\n\n\n\nChhun et\u00a0al. (2022)\n\nCyril Chhun, Pierre Colombo, Chlo\u00e9 Clavel, and Fabian\u00a0M Suchanek. 2022.\n\n\nOf human criteria and automatic metrics: A benchmark of the evaluation of story generation.\n\n\narXiv preprint arXiv:2208.11646 (2022).\n\n\n\n\n\n\nChiang et\u00a0al. (2024)\n\nCheng-Han Chiang, Wei-Chih Chen, Chun-Yi Kuan, Chienchou Yang, and Hung-yi Lee. 2024.\n\n\nLarge Language Model as an Assignment Evaluator: Insights, Feedback, and Challenges in a 1000+ Student Course.\n\n\narXiv preprint arXiv:2407.05216 (2024).\n\n\n\n\n\n\nChiang and Lee (2023)\n\nCheng-Han Chiang and Hung-yi Lee. 2023.\n\n\nA closer look into automatic evaluation using large language models.", "193e179b-fd79-49ad-9634-d6072283147d": "arXiv preprint arXiv:2310.05657 (2023).\n\n\n\n\n\n\nChiang et\u00a0al. (2023)\n\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph\u00a0E Gonzalez, et\u00a0al. 2023.\n\n\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.\n\n\nSee https://vicuna. lmsys. org (accessed 14 April 2023) 2, 3 (2023), 6.\n\n\n\n\n\n\nChoi et\u00a0al. (2024)\n\nJuhwan Choi, Jungmin Yun, Kyohoon Jin, and YoungBin Kim. 2024.\n\n\nMulti-News+: Cost-efficient Dataset Cleansing via LLM-based Data Annotation.\n\n\narXiv preprint arXiv:2404.09682 (2024).\n\n\n\n\n\n\nChu et\u00a0al. (2024)\n\nZhumin Chu, Qingyao Ai, Yiteng Tu, Haitao Li, and Yiqun Liu. 2024.", "36a605cf-00d1-4fe9-bc33-13be0bba5f75": "Pre: A peer review based large language model evaluator.\n\n\narXiv preprint arXiv:2401.15641 (2024).\n\n\n\n\n\n\nChung et\u00a0al. (2024)\n\nHyung\u00a0Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et\u00a0al. 2024.\n\n\nScaling instruction-finetuned language models.\n\n\nJournal of Machine Learning Research 25, 70 (2024), 1\u201353.\n\n\n\n\n\n\nCohen et\u00a0al. (2009)\n\nIsrael Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009.\n\n\nPearson correlation coefficient.\n\n\nNoise reduction in speech processing (2009), 1\u20134.\n\n\n\n\n\n\nCraswell et\u00a0al. (2021)", "18e9c3c9-b1a9-43a1-ba89-ead9eba0ee78": "Craswell et\u00a0al. (2021)\n\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Jimmy Lin. 2021.\n\n\nOverview of the TREC 2021 Deep Learning Track. In TREC.\n\n\n\nhttps://trec.nist.gov/pubs/trec30/papers/Overview-DL.pdf\n\n\n\nCraswell et\u00a0al. (2022)\n\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen\u00a0M. Voorhees, and Ian Soboroff. 2022.\n\n\nOverview of the TREC 2022 Deep Learning Track. In TREC.\n\n\n\nhttps://trec.nist.gov/pubs/trec31/papers/Overview_deep.pdf\n\n\n\nCui et\u00a0al. (2024)\n\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et\u00a0al. 2024.", "a90410f7-e0f4-4aac-a4ee-ef239859712a": "ULTRAFEEDBACK: Boosting Language Models with Scaled AI Feedback. In Forty-first International Conference on Machine Learning.\n\n\n\n\n\n\nDaynauth and Mars (2024)\n\nRoland Daynauth and Jason Mars. 2024.\n\n\nAligning Model Evaluations with Human Preferences: Mitigating Token Count Bias in Language Model Assessments.\n\n\narXiv preprint arXiv:2407.12847 (2024).\n\n\n\n\n\n\nDeng et\u00a0al. (2024)\n\nShijian Deng, Wentian Zhao, Yu-Jhe Li, Kun Wan, Daniel Miranda, Ajinkya Kale, and Yapeng Tian. 2024.\n\n\nEfficient Self-Improvement in Multimodal Large Language Models: A Model-Level Judge-Free Approach.\n\n\narXiv preprint arXiv:2411.17760 (2024).\n\n\n\n\n\n\nDeshwal and Chawla (2024)\n\nMahesh Deshwal and Apoorva Chawla. 2024.", "e075c35f-0971-482f-913b-b3716ac37eb3": "Mahesh Deshwal and Apoorva Chawla. 2024.\n\n\nPHUDGE: Phi-3 as Scalable Judge.\n\n\narXiv preprint arXiv:2405.08029 (2024).\n\n\n\n\n\n\nDierickx et\u00a0al. (2024)\n\nLaurence Dierickx, Arjen Van\u00a0Dalen, Andreas\u00a0L Opdahl, and Carl-Gustav Lind\u00e9n. 2024.\n\n\nStriking the balance in using LLMs for fact-checking: A narrative literature review. In Multidisciplinary International Symposium on Disinformation in Open Online Media. Springer, 1\u201315.\n\n\n\n\n\n\nDing et\u00a0al. (2023)\n\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023.\n\n\nEnhancing chat language models by scaling high-quality instructional conversations.\n\n\narXiv preprint arXiv:2305.14233 (2023)."}}