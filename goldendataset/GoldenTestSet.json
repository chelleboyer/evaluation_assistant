[
  {
    "eval_sample": {
      "user_input": "What is the role of an LLM in the context of retriever-augmented generation systems?",
      "retrieved_contexts": null,
      "reference_contexts": [
        "TRACe Evaluation Framework We propose a suite of four comprehensive metrics to evaluate the quality of the retriever and the response generator components of RAG. An optimal RAG system must balance accuracy and efficiency. The retriever should precisely return all the necessary information to address the user query, avoiding any superfluous data. The generator must effectively utilize the retrieved information, ensuring the response is strictly based on the provided context without introducing any hallucinations in the output. Towards comprehensive evaluation of the abovementioned criteria, we introduce the TRACe evaluation framework to measure uTilization, Relevance, Adherence, and Completeness of a RAG system. Utilization, Adherence, and Completeness measure the quality of the generator. Adherence here is synonymous with previously proposed answer faithfullness, groundednes, and attribution, all terms used in literature to measure how well an LLM output adheres to a source of factual information. Relevance measures the quality of the retriever output with respect to the query. Below we formalize the definition of each metric. Definitions Let D𝐷Ditalic_D be a set of context documents {d1⁢…⁢dn}subscript𝑑1…subscript𝑑𝑛\\{d_{1}...d_{n}\\}{ italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT … italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } retrieved for a RAG input query. We define a set of relevant tokens in disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as Ri={t1,…⁢tr}subscript𝑅𝑖subscript𝑡1…subscript𝑡𝑟R_{i}=\\{t_{1},...t_{r}\\}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT }. Risubscript𝑅𝑖R_{i}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT encodes information in context document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT that is useful for answering the query. Similarly, we define Ui={t1,…⁢tu}subscript𝑈𝑖subscript𝑡1…subscript𝑡𝑢U_{i}=\\{t_{1},...t_{u}\\}italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT } as the set of utilized tokens in document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which reflect information that the generation model is using to produce a response. Refer to Figure 2 for a visual representation of relevant and utilized spans. L⁢e⁢n⁢(x)𝐿𝑒𝑛𝑥Len(x)italic_L italic_e italic_n ( italic_x ) measures the length of strings in x𝑥xitalic_x, which can be interpreted as character length, token length, or sentence length. For calculating ground-truth metrics, we employ sentence-length, since it aligns best with our annotation schema (Section 3.4). However, token or character length may also be suitable for other use cases. Figure 2: Example of RAG Question, Context, and Response. Relevant context spans are highlighted, and utilized spans are underlined. Context Relevance Context Relevance is defined in [9, 32] as the fraction of the retrieved context that is relevant to the input query. Low relevance points to an inefficient retriever that supplies excess information to the generation model. Long context inputs into the generator may accrue unnecessary costs, as well as compromise the quality of the generated output. We measure relevance of context document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as: document relevance=L⁢e⁢n⁢(Ri)L⁢e⁢n⁢(di)document relevance𝐿𝑒𝑛subscript𝑅𝑖𝐿𝑒𝑛subscript𝑑𝑖\\text{document relevance}=\\frac{{Len(R_{i})}}{Len(d_{i})}document relevance = divide start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (1) Example-level relevance can be aggregated over all context documents in the example as: example relevance=∑i=1|D|L⁢e⁢n⁢(Ri)∑i=1|D|L⁢e⁢n⁢(di)example relevancesuperscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑅𝑖superscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑑𝑖\\text{example relevance}=\\frac{\\sum_{i=1}^{|D|}{Len(R_{i})}}{\\sum_{i=1}^{|D|}% Len(d_{i})}example relevance = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (2) Context Utilization Context Utilization is a new metric introduced in TRACe. We aim to measure the the fraction of the retrieved context that is used by the generator to produce the response. Low Utilization in combination with low Relevance points to a greedy retriever, while low Utilization alone points to a weak generator that fails to leverage the provided context efficiently. Document-level and example-level Utilization are defined as: document utilization=L⁢e⁢n⁢(Ui)L⁢e⁢n⁢(di)example utilization=∑i=1|D|L⁢e⁢n⁢(Ui)∑i=1|D|L⁢e⁢n⁢(di)formulae-sequencedocument utilization𝐿𝑒𝑛subscript𝑈𝑖𝐿𝑒𝑛subscript𝑑𝑖example utilizationsuperscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑈𝑖superscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑑𝑖\\text{document utilization}=\\frac{{Len(U_{i})}}{Len(d_{i})}\\quad\\text{example % utilization}=\\frac{\\sum_{i=1}^{|D|}{Len(U_{i})}}{\\sum_{i=1}^{|D|}Len(d_{i})}document utilization = divide start_ARG italic_L italic_e italic_n ( italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG example utilization = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (3) Completeness Completeness is another new metrics we introduce to measure how well the response incorporates all the relevant information in the context. Note that this is different from Utilization; it is possible to have high Relevance and high Utilization, but low Completeness when the generator utilizes irrelevant information in the context to produce a low quality response. Completeness for document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is calculated as the fraction of utilized substrings among all relevant substrings: completeness=L⁢e⁢n⁢(Ri∩Ui)L⁢e⁢n⁢(Ri)completeness𝐿𝑒𝑛subscript𝑅𝑖subscript𝑈𝑖𝐿𝑒𝑛subscript𝑅𝑖\\text{completeness}=\\frac{Len(R_{i}\\cap U_{i})}{Len(R_{i})}completeness = divide start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∩ italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (4) And can be extended to example-level by considering all relevant and utilized substrings across all context documents. Adherence Adherence is designed to detect hallucinations in RAG responses. Our definition of Adherence is synonymous with answer faithfullness [9, 32], groundednes [36], and attribution [31]. For alignment with existing hallucination detection approaches, we define example-level adherence as a boolean indicating whether or not all parts of the response are grounded in the context. However, in our annotation schema (Section 3.4) we also define Ai={t1,…⁢ta}subscript𝐴𝑖subscript𝑡1…subscript𝑡𝑎A_{i}=\\{t_{1},...t_{a}\\}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT } as the set of response tokens that are supported by the context to enable granular Adherence evaluation. Figure 3: Distributions of relevance, utilization, and completeness labels in RAGBench. Y-axis is normalized to visualize densities. 3.3 RAGBench Statistics RAGBench component datasets contain between 1% - 20% hallucinations. ExpertQA, CovidQA, and MS Marco contain the highest fraction of hallucinated responses (12%, 16%, and 13%, respectively), while Cuad, FinQA, and TAT-QA contain the least (about 1% for each). We visualize distributions of relevance, utilization, and completeness scores in Figure 3. 3.4 LLM annotator We prompt GPT-4 (gpt-4-0125-preview) to produce ground truth Adherence, Relevance, and Utilization labels for input (documents, query, response)"
      ],
      "response": null,
      "multi_responses": null,
      "reference": "In the context of retriever-augmented generation systems, an LLM is used to generate responses based on retrieved information, ensuring adherence to the provided context without hallucinations, and effectively utilizing relevant data to produce accurate and grounded outputs.",
      "rubrics": null
    },
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "approval_status": "approved"
  },
  {
    "eval_sample": {
      "user_input": "What is the role of an LLM in the TRACe evaluation framework?",
      "retrieved_contexts": null,
      "reference_contexts": [
        "TRACe Evaluation Framework We propose a suite of four comprehensive metrics to evaluate the quality of the retriever and the response generator components of RAG. An optimal RAG system must balance accuracy and efficiency. The retriever should precisely return all the necessary information to address the user query, avoiding any superfluous data. The generator must effectively utilize the retrieved information, ensuring the response is strictly based on the provided context without introducing any hallucinations in the output. Towards comprehensive evaluation of the abovementioned criteria, we introduce the TRACe evaluation framework to measure uTilization, Relevance, Adherence, and Completeness of a RAG system. Utilization, Adherence, and Completeness measure the quality of the generator. Adherence here is synonymous with previously proposed answer faithfullness, groundednes, and attribution, all terms used in literature to measure how well an LLM output adheres to a source of factual information. Relevance measures the quality of the retriever output with respect to the query. Below we formalize the definition of each metric. Definitions Let D𝐷Ditalic_D be a set of context documents {d1⁢…⁢dn}subscript𝑑1…subscript𝑑𝑛\\{d_{1}...d_{n}\\}{ italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT … italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } retrieved for a RAG input query. We define a set of relevant tokens in disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as Ri={t1,…⁢tr}subscript𝑅𝑖subscript𝑡1…subscript𝑡𝑟R_{i}=\\{t_{1},...t_{r}\\}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT }. Risubscript𝑅𝑖R_{i}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT encodes information in context document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT that is useful for answering the query. Similarly, we define Ui={t1,…⁢tu}subscript𝑈𝑖subscript𝑡1…subscript𝑡𝑢U_{i}=\\{t_{1},...t_{u}\\}italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT } as the set of utilized tokens in document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which reflect information that the generation model is using to produce a response. Refer to Figure 2 for a visual representation of relevant and utilized spans. L⁢e⁢n⁢(x)𝐿𝑒𝑛𝑥Len(x)italic_L italic_e italic_n ( italic_x ) measures the length of strings in x𝑥xitalic_x, which can be interpreted as character length, token length, or sentence length. For calculating ground-truth metrics, we employ sentence-length, since it aligns best with our annotation schema (Section 3.4). However, token or character length may also be suitable for other use cases. Figure 2: Example of RAG Question, Context, and Response. Relevant context spans are highlighted, and utilized spans are underlined. Context Relevance Context Relevance is defined in [9, 32] as the fraction of the retrieved context that is relevant to the input query. Low relevance points to an inefficient retriever that supplies excess information to the generation model. Long context inputs into the generator may accrue unnecessary costs, as well as compromise the quality of the generated output. We measure relevance of context document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as: document relevance=L⁢e⁢n⁢(Ri)L⁢e⁢n⁢(di)document relevance𝐿𝑒𝑛subscript𝑅𝑖𝐿𝑒𝑛subscript𝑑𝑖\\text{document relevance}=\\frac{{Len(R_{i})}}{Len(d_{i})}document relevance = divide start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (1) Example-level relevance can be aggregated over all context documents in the example as: example relevance=∑i=1|D|L⁢e⁢n⁢(Ri)∑i=1|D|L⁢e⁢n⁢(di)example relevancesuperscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑅𝑖superscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑑𝑖\\text{example relevance}=\\frac{\\sum_{i=1}^{|D|}{Len(R_{i})}}{\\sum_{i=1}^{|D|}% Len(d_{i})}example relevance = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (2) Context Utilization Context Utilization is a new metric introduced in TRACe. We aim to measure the the fraction of the retrieved context that is used by the generator to produce the response. Low Utilization in combination with low Relevance points to a greedy retriever, while low Utilization alone points to a weak generator that fails to leverage the provided context efficiently. Document-level and example-level Utilization are defined as: document utilization=L⁢e⁢n⁢(Ui)L⁢e⁢n⁢(di)example utilization=∑i=1|D|L⁢e⁢n⁢(Ui)∑i=1|D|L⁢e⁢n⁢(di)formulae-sequencedocument utilization𝐿𝑒𝑛subscript𝑈𝑖𝐿𝑒𝑛subscript𝑑𝑖example utilizationsuperscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑈𝑖superscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑑𝑖\\text{document utilization}=\\frac{{Len(U_{i})}}{Len(d_{i})}\\quad\\text{example % utilization}=\\frac{\\sum_{i=1}^{|D|}{Len(U_{i})}}{\\sum_{i=1}^{|D|}Len(d_{i})}document utilization = divide start_ARG italic_L italic_e italic_n ( italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG example utilization = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (3) Completeness Completeness is another new metrics we introduce to measure how well the response incorporates all the relevant information in the context. Note that this is different from Utilization; it is possible to have high Relevance and high Utilization, but low Completeness when the generator utilizes irrelevant information in the context to produce a low quality response. Completeness for document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is calculated as the fraction of utilized substrings among all relevant substrings: completeness=L⁢e⁢n⁢(Ri∩Ui)L⁢e⁢n⁢(Ri)completeness𝐿𝑒𝑛subscript𝑅𝑖subscript𝑈𝑖𝐿𝑒𝑛subscript𝑅𝑖\\text{completeness}=\\frac{Len(R_{i}\\cap U_{i})}{Len(R_{i})}completeness = divide start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∩ italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (4) And can be extended to example-level by considering all relevant and utilized substrings across all context documents. Adherence Adherence is designed to detect hallucinations in RAG responses. Our definition of Adherence is synonymous with answer faithfullness [9, 32], groundednes [36], and attribution [31]. For alignment with existing hallucination detection approaches, we define example-level adherence as a boolean indicating whether or not all parts of the response are grounded in the context. However, in our annotation schema (Section 3.4) we also define Ai={t1,…⁢ta}subscript𝐴𝑖subscript𝑡1…subscript𝑡𝑎A_{i}=\\{t_{1},...t_{a}\\}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT } as the set of response tokens that are supported by the context to enable granular Adherence evaluation. Figure 3: Distributions of relevance, utilization, and completeness labels in RAGBench. Y-axis is normalized to visualize densities. 3.3 RAGBench Statistics RAGBench component datasets contain between 1% - 20% hallucinations. ExpertQA, CovidQA, and MS Marco contain the highest fraction of hallucinated responses (12%, 16%, and 13%, respectively), while Cuad, FinQA, and TAT-QA contain the least (about 1% for each). We visualize distributions of relevance, utilization, and completeness scores in Figure 3. 3.4 LLM annotator We prompt GPT-4 (gpt-4-0125-preview) to produce ground truth Adherence, Relevance, and Utilization labels for input (documents, query, response)"
      ],
      "response": null,
      "multi_responses": null,
      "reference": "In the TRACe evaluation framework, an LLM is used to produce ground truth Adherence, Relevance, and Utilization labels for input documents, queries, and responses, aiding in the comprehensive assessment of RAG system components.",
      "rubrics": null
    },
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "approval_status": "approved"
  },
  {
    "eval_sample": {
      "user_input": "What is the RAGTruth term in the context of RAGBench and how is it used?",
      "retrieved_contexts": null,
      "reference_contexts": [
        "tuples in RAGBench. Completeness is easily derived from span-level Relevance and Utilization annotations, thus we don’t request explicit annotations for it. For high quality labels, we use proven techniques like chain of thought [39] that have been shown to maximize the correlation between GPT-4 and human judgements [43, 46]. For relevance and utilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the input documents. For adherence, we instruct the LLM to identify which response sentences, if any, are supported by the provided context. We can then derive an example-level boolean adherence label by checking if all response sentences are supported. The exact prompt used for annotation is provided in Appendix 7.4. We apply post-processing steps to ensure high quality, reliable annotations from our GPT-labeler, which we outline in Appendix 7.5. Table 2: GPT-4 annotator achieves high alignment with human judgements. We report F1 and Accuracy alignment metrics on human annotated subsets of DelucionQA. Adherence annotations are evaluated against DelucionQA human-annotated labels. Relevance and Utilization are evaluated against DelucionQA(40): a subset of 40 examples randomly sampled from the DelucionQA test set and annotated by the authors. Test Set Metric F1 Accuracy DelucionQA - example level Adherence 0.96 0.93 DelucionQA - span level Adherence 0.97 0.95 DelucionQA(40) - span level Utilization 0.92 0.94 DelucionQA(40) - span level Relevance 0.76 0.78 Alignment with Human Judgements We validate our metric formulations and labeling approach on a human annotated benchmark. DelucionQA [33] is a curated collection of user queries on the operation of Jeep’s 2023 Gladiator model. Natural language queries are first generated by an LLM, then reviewed and filtered by human annotators. Context documents are sourced from Jeep’s Gladiator User Manual, and responses are generated by various LLMs. Combined with example-level and span-level annotations for hallucination, DelucionQA represents a realistic distribution of real-world user queries and RAG responses. We find that our GPT annotator achieves 93% and 95% example- and span-level agreement with human judgements on the DelucionQA test split (Table 2). To validate relevance and utilization annotations, we also annotate a small subset of DelucionQA with granular relevance and utilization labels. We refer to this subset as DelucionQA(40) in Table 2. Similar to adherence, we observe high correlation between relevance and utilization judgements from GPT-4 and humans. Details of the annotation process and additional validations are found in Appendix 7.6. (a) Retriever vs. Relevance (b) LLM vs. Hallucination (c) Prompt vs. Utilization (d) Prompt vs. Completeness Figure 4: Relationship between RAG system configuration and TRACe metrics. (a) The choice and configuration of RAG retriever component affects the average relevance of the retrieved context. In this example, a dense retriever with a low number of documents per query (k=2) yields the highest average context relevance. (b) The choice of LLM and generation prompt affect how well the RAG system utilizes the provided context. Prompting the LLM with a detailed chain-of-thought prompt leads to reduced hallucinations, as well as higher response utilization (c) and completeness (d) rates. 3.5 RAG Case Study We design a case study to further motivate and validate the proposed TRACe framework. We sample 100 realistic world knowledge queries from the RAGTruth [40] QA training set, along with 2,512 unique context document chunks from the same dataset to use as inputs into our mock RAG systems. We simulate RAG setups of varying quality by controlling four configurable parameters: the retriever (sparse TF-IDF vs. dense), number of retrieved documents (2 vs. 4), generation model (GPT-3.5-turbo vs. GPT-4o), and 4 versions of the generation prompt template. For illustrative purposes, we evaluate one prompt template comprising of only the question (no context) vs. three RAG-style templates that encourage the LLM to utilize the provided context and/or utilize chain-of-thought (CoT). The full generation templates are provided in Appendix 7.7. We generate responses for the 100 RAGTruth queries with each of the 32 resulting RAG systems and use our LLM annotation prompt to evaluate the TRACe metrics. Figure 4 demonstrates how the different RAG configurations influence TRACe metrics. For example, we confirm that the choice of the generative LLM model affects the amount of hallucinations (or non-adherent responses) in the generated text. Not surprisingly, GPT-4o combined with a chain-of-thought prompt yields the lowest amount of hallucination compared to the weaker GPT-3.5 model and less detailed prompts. Curiously, we find that GPT-3.5 hallucinates more when prompted with CoT, which may point to limitations in the model’s ability to reason about complex concepts 4(b). Overall, we find that prompting the LLM to think step-by-step and explain its reasoning leads to higher utilization of the provided context and more complete responses (4(c), 4(d)). Finally, we show that the choice of the retriever affects average relevance of the retrieved context documents per query 4(a). 4 Experiments 4.1 LLM Judge We benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query GPT-3.5 with our annotation prompt, (2) RAGAS [9], and (3) TruLens [36]. RAGAS employs a series of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context Relevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness (Adherence) and Context Relevance. 4.2 Fine-tuned Judge We fine-tune a DeBERTa-v3-Large [11] NLI checkpoint333https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli from Laurer et al. [18] with one key architecture modification: we add a shallow prediction head for each of the output RAG metrics, which allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective and enables transfer learning from head to head through back-propagation down to the shared base layers. Each prediction head is a single layer feed-forward net that acts on the token-level output of the last DeBERTa layer. We attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and another head on the response tokens to estimate Adherence. For training, we broadcast sentence-level annotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and Adherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized spans and Adherent spans and calculate TRACe metrics"
      ],
      "response": null,
      "multi_responses": null,
      "reference": "RAGTruth is a term referenced in the context of RAGBench, which involves evaluating tuples with annotations such as span-level Relevance and Utilization, as well as adherence. It is used as part of a case study to sample realistic world knowledge queries from the RAGTruth QA training set, and to simulate RAG systems with varying configurations for evaluation purposes.",
      "rubrics": null
    },
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "approval_status": "approved"
  },
  {
    "eval_sample": {
      "user_input": "Who are the authors referenced by Laurer et al. in the context of RAGBench annotations?",
      "retrieved_contexts": null,
      "reference_contexts": [
        "tuples in RAGBench. Completeness is easily derived from span-level Relevance and Utilization annotations, thus we don’t request explicit annotations for it. For high quality labels, we use proven techniques like chain of thought [39] that have been shown to maximize the correlation between GPT-4 and human judgements [43, 46]. For relevance and utilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the input documents. For adherence, we instruct the LLM to identify which response sentences, if any, are supported by the provided context. We can then derive an example-level boolean adherence label by checking if all response sentences are supported. The exact prompt used for annotation is provided in Appendix 7.4. We apply post-processing steps to ensure high quality, reliable annotations from our GPT-labeler, which we outline in Appendix 7.5. Table 2: GPT-4 annotator achieves high alignment with human judgements. We report F1 and Accuracy alignment metrics on human annotated subsets of DelucionQA. Adherence annotations are evaluated against DelucionQA human-annotated labels. Relevance and Utilization are evaluated against DelucionQA(40): a subset of 40 examples randomly sampled from the DelucionQA test set and annotated by the authors. Test Set Metric F1 Accuracy DelucionQA - example level Adherence 0.96 0.93 DelucionQA - span level Adherence 0.97 0.95 DelucionQA(40) - span level Utilization 0.92 0.94 DelucionQA(40) - span level Relevance 0.76 0.78 Alignment with Human Judgements We validate our metric formulations and labeling approach on a human annotated benchmark. DelucionQA [33] is a curated collection of user queries on the operation of Jeep’s 2023 Gladiator model. Natural language queries are first generated by an LLM, then reviewed and filtered by human annotators. Context documents are sourced from Jeep’s Gladiator User Manual, and responses are generated by various LLMs. Combined with example-level and span-level annotations for hallucination, DelucionQA represents a realistic distribution of real-world user queries and RAG responses. We find that our GPT annotator achieves 93% and 95% example- and span-level agreement with human judgements on the DelucionQA test split (Table 2). To validate relevance and utilization annotations, we also annotate a small subset of DelucionQA with granular relevance and utilization labels. We refer to this subset as DelucionQA(40) in Table 2. Similar to adherence, we observe high correlation between relevance and utilization judgements from GPT-4 and humans. Details of the annotation process and additional validations are found in Appendix 7.6. (a) Retriever vs. Relevance (b) LLM vs. Hallucination (c) Prompt vs. Utilization (d) Prompt vs. Completeness Figure 4: Relationship between RAG system configuration and TRACe metrics. (a) The choice and configuration of RAG retriever component affects the average relevance of the retrieved context. In this example, a dense retriever with a low number of documents per query (k=2) yields the highest average context relevance. (b) The choice of LLM and generation prompt affect how well the RAG system utilizes the provided context. Prompting the LLM with a detailed chain-of-thought prompt leads to reduced hallucinations, as well as higher response utilization (c) and completeness (d) rates. 3.5 RAG Case Study We design a case study to further motivate and validate the proposed TRACe framework. We sample 100 realistic world knowledge queries from the RAGTruth [40] QA training set, along with 2,512 unique context document chunks from the same dataset to use as inputs into our mock RAG systems. We simulate RAG setups of varying quality by controlling four configurable parameters: the retriever (sparse TF-IDF vs. dense), number of retrieved documents (2 vs. 4), generation model (GPT-3.5-turbo vs. GPT-4o), and 4 versions of the generation prompt template. For illustrative purposes, we evaluate one prompt template comprising of only the question (no context) vs. three RAG-style templates that encourage the LLM to utilize the provided context and/or utilize chain-of-thought (CoT). The full generation templates are provided in Appendix 7.7. We generate responses for the 100 RAGTruth queries with each of the 32 resulting RAG systems and use our LLM annotation prompt to evaluate the TRACe metrics. Figure 4 demonstrates how the different RAG configurations influence TRACe metrics. For example, we confirm that the choice of the generative LLM model affects the amount of hallucinations (or non-adherent responses) in the generated text. Not surprisingly, GPT-4o combined with a chain-of-thought prompt yields the lowest amount of hallucination compared to the weaker GPT-3.5 model and less detailed prompts. Curiously, we find that GPT-3.5 hallucinates more when prompted with CoT, which may point to limitations in the model’s ability to reason about complex concepts 4(b). Overall, we find that prompting the LLM to think step-by-step and explain its reasoning leads to higher utilization of the provided context and more complete responses (4(c), 4(d)). Finally, we show that the choice of the retriever affects average relevance of the retrieved context documents per query 4(a). 4 Experiments 4.1 LLM Judge We benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query GPT-3.5 with our annotation prompt, (2) RAGAS [9], and (3) TruLens [36]. RAGAS employs a series of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context Relevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness (Adherence) and Context Relevance. 4.2 Fine-tuned Judge We fine-tune a DeBERTa-v3-Large [11] NLI checkpoint333https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli from Laurer et al. [18] with one key architecture modification: we add a shallow prediction head for each of the output RAG metrics, which allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective and enables transfer learning from head to head through back-propagation down to the shared base layers. Each prediction head is a single layer feed-forward net that acts on the token-level output of the last DeBERTa layer. We attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and another head on the response tokens to estimate Adherence. For training, we broadcast sentence-level annotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and Adherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized spans and Adherent spans and calculate TRACe metrics"
      ],
      "response": null,
      "multi_responses": null,
      "reference": "The context mentions Laurer et al. in relation to the evaluation techniques used for high quality labels, such as chain of thought, which maximize the correlation between GPT-4 and human judgements. It also references Laurer et al. in the context of a fine-tuned DeBERTa-v3-Large model with a prediction head inspired by their work. Therefore, Laurer et al. are authors associated with techniques and models used for evaluating and improving retrieval-augmented generation systems.",
      "rubrics": null
    },
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "approval_status": "approved"
  },
  {
    "eval_sample": {
      "user_input": "Can you tell me what TRACe stands for in the context of RAGBench and evaluation frameworks?",
      "retrieved_contexts": null,
      "reference_contexts": [
        "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems 1 Introduction 2 Related Work RAG evaluation Finetuned RAG evaluation models 3 RAGBench Construction 3.1 Component Datasets Source Domains Context Token Length Task Types Question Sources Response Generation Data Splits 3.2 TRACe Evaluation Framework Definitions Context Relevance Context Utilization Completeness Adherence 3.3 RAGBench Statistics 3.4 LLM annotator Alignment with Human Judgements 3.5 RAG Case Study 4 Experiments 4.1 LLM Judge 4.2 Fine-tuned Judge 4.3 Evaluation 5 Results Estimating Context Relevance is Difficult"
      ],
      "response": null,
      "multi_responses": null,
      "reference": "TRACe refers to the Evaluation Framework Definitions related to context relevance, context utilization, completeness, and adherence within RAGBench, which is an explainable benchmark for retrieval-augmented generation systems.",
      "rubrics": null
    },
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "approval_status": "approved"
  },
  {
    "eval_sample": {
      "user_input": "What is RAGBench used for in retrieval-augmented generation systems?",
      "retrieved_contexts": null,
      "reference_contexts": [
        "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems 1 Introduction 2 Related Work RAG evaluation Finetuned RAG evaluation models 3 RAGBench Construction 3.1 Component Datasets Source Domains Context Token Length Task Types Question Sources Response Generation Data Splits 3.2 TRACe Evaluation Framework Definitions Context Relevance Context Utilization Completeness Adherence 3.3 RAGBench Statistics 3.4 LLM annotator Alignment with Human Judgements 3.5 RAG Case Study 4 Experiments 4.1 LLM Judge 4.2 Fine-tuned Judge 4.3 Evaluation 5 Results Estimating Context Relevance is Difficult"
      ],
      "response": null,
      "multi_responses": null,
      "reference": "RAGBench is an explainable benchmark for retrieval-augmented generation systems, involving components such as datasets, evaluation frameworks, and case studies to assess aspects like context relevance, utilization, and adherence.",
      "rubrics": null
    },
    "synthesizer_name": "single_hop_specifc_query_synthesizer",
    "approval_status": "approved"
  },
  {
    "eval_sample": {
      "user_input": "How do GPT-4 models compare in terms of relevance and adherence when evaluating RAG responses, considering the multi-hop context from the TRACe evaluation framework?",
      "retrieved_contexts": null,
      "reference_contexts": [
        "<1-hop>\n\nTRACe Evaluation Framework We propose a suite of four comprehensive metrics to evaluate the quality of the retriever and the response generator components of RAG. An optimal RAG system must balance accuracy and efficiency. The retriever should precisely return all the necessary information to address the user query, avoiding any superfluous data. The generator must effectively utilize the retrieved information, ensuring the response is strictly based on the provided context without introducing any hallucinations in the output. Towards comprehensive evaluation of the abovementioned criteria, we introduce the TRACe evaluation framework to measure uTilization, Relevance, Adherence, and Completeness of a RAG system. Utilization, Adherence, and Completeness measure the quality of the generator. Adherence here is synonymous with previously proposed answer faithfullness, groundednes, and attribution, all terms used in literature to measure how well an LLM output adheres to a source of factual information. Relevance measures the quality of the retriever output with respect to the query. Below we formalize the definition of each metric. Definitions Let D𝐷Ditalic_D be a set of context documents {d1⁢…⁢dn}subscript𝑑1…subscript𝑑𝑛\\{d_{1}...d_{n}\\}{ italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT … italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } retrieved for a RAG input query. We define a set of relevant tokens in disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as Ri={t1,…⁢tr}subscript𝑅𝑖subscript𝑡1…subscript𝑡𝑟R_{i}=\\{t_{1},...t_{r}\\}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT }. Risubscript𝑅𝑖R_{i}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT encodes information in context document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT that is useful for answering the query. Similarly, we define Ui={t1,…⁢tu}subscript𝑈𝑖subscript𝑡1…subscript𝑡𝑢U_{i}=\\{t_{1},...t_{u}\\}italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT } as the set of utilized tokens in document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which reflect information that the generation model is using to produce a response. Refer to Figure 2 for a visual representation of relevant and utilized spans. L⁢e⁢n⁢(x)𝐿𝑒𝑛𝑥Len(x)italic_L italic_e italic_n ( italic_x ) measures the length of strings in x𝑥xitalic_x, which can be interpreted as character length, token length, or sentence length. For calculating ground-truth metrics, we employ sentence-length, since it aligns best with our annotation schema (Section 3.4). However, token or character length may also be suitable for other use cases. Figure 2: Example of RAG Question, Context, and Response. Relevant context spans are highlighted, and utilized spans are underlined. Context Relevance Context Relevance is defined in [9, 32] as the fraction of the retrieved context that is relevant to the input query. Low relevance points to an inefficient retriever that supplies excess information to the generation model. Long context inputs into the generator may accrue unnecessary costs, as well as compromise the quality of the generated output. We measure relevance of context document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as: document relevance=L⁢e⁢n⁢(Ri)L⁢e⁢n⁢(di)document relevance𝐿𝑒𝑛subscript𝑅𝑖𝐿𝑒𝑛subscript𝑑𝑖\\text{document relevance}=\\frac{{Len(R_{i})}}{Len(d_{i})}document relevance = divide start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (1) Example-level relevance can be aggregated over all context documents in the example as: example relevance=∑i=1|D|L⁢e⁢n⁢(Ri)∑i=1|D|L⁢e⁢n⁢(di)example relevancesuperscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑅𝑖superscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑑𝑖\\text{example relevance}=\\frac{\\sum_{i=1}^{|D|}{Len(R_{i})}}{\\sum_{i=1}^{|D|}% Len(d_{i})}example relevance = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (2) Context Utilization Context Utilization is a new metric introduced in TRACe. We aim to measure the the fraction of the retrieved context that is used by the generator to produce the response. Low Utilization in combination with low Relevance points to a greedy retriever, while low Utilization alone points to a weak generator that fails to leverage the provided context efficiently. Document-level and example-level Utilization are defined as: document utilization=L⁢e⁢n⁢(Ui)L⁢e⁢n⁢(di)example utilization=∑i=1|D|L⁢e⁢n⁢(Ui)∑i=1|D|L⁢e⁢n⁢(di)formulae-sequencedocument utilization𝐿𝑒𝑛subscript𝑈𝑖𝐿𝑒𝑛subscript𝑑𝑖example utilizationsuperscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑈𝑖superscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑑𝑖\\text{document utilization}=\\frac{{Len(U_{i})}}{Len(d_{i})}\\quad\\text{example % utilization}=\\frac{\\sum_{i=1}^{|D|}{Len(U_{i})}}{\\sum_{i=1}^{|D|}Len(d_{i})}document utilization = divide start_ARG italic_L italic_e italic_n ( italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG example utilization = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (3) Completeness Completeness is another new metrics we introduce to measure how well the response incorporates all the relevant information in the context. Note that this is different from Utilization; it is possible to have high Relevance and high Utilization, but low Completeness when the generator utilizes irrelevant information in the context to produce a low quality response. Completeness for document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is calculated as the fraction of utilized substrings among all relevant substrings: completeness=L⁢e⁢n⁢(Ri∩Ui)L⁢e⁢n⁢(Ri)completeness𝐿𝑒𝑛subscript𝑅𝑖subscript𝑈𝑖𝐿𝑒𝑛subscript𝑅𝑖\\text{completeness}=\\frac{Len(R_{i}\\cap U_{i})}{Len(R_{i})}completeness = divide start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∩ italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (4) And can be extended to example-level by considering all relevant and utilized substrings across all context documents. Adherence Adherence is designed to detect hallucinations in RAG responses. Our definition of Adherence is synonymous with answer faithfullness [9, 32], groundednes [36], and attribution [31]. For alignment with existing hallucination detection approaches, we define example-level adherence as a boolean indicating whether or not all parts of the response are grounded in the context. However, in our annotation schema (Section 3.4) we also define Ai={t1,…⁢ta}subscript𝐴𝑖subscript𝑡1…subscript𝑡𝑎A_{i}=\\{t_{1},...t_{a}\\}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT } as the set of response tokens that are supported by the context to enable granular Adherence evaluation. Figure 3: Distributions of relevance, utilization, and completeness labels in RAGBench. Y-axis is normalized to visualize densities. 3.3 RAGBench Statistics RAGBench component datasets contain between 1% - 20% hallucinations. ExpertQA, CovidQA, and MS Marco contain the highest fraction of hallucinated responses (12%, 16%, and 13%, respectively), while Cuad, FinQA, and TAT-QA contain the least (about 1% for each). We visualize distributions of relevance, utilization, and completeness scores in Figure 3. 3.4 LLM annotator We prompt GPT-4 (gpt-4-0125-preview) to produce ground truth Adherence, Relevance, and Utilization labels for input (documents, query, response)",
        "<2-hop>\n\ntuples in RAGBench. Completeness is easily derived from span-level Relevance and Utilization annotations, thus we don’t request explicit annotations for it. For high quality labels, we use proven techniques like chain of thought [39] that have been shown to maximize the correlation between GPT-4 and human judgements [43, 46]. For relevance and utilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the input documents. For adherence, we instruct the LLM to identify which response sentences, if any, are supported by the provided context. We can then derive an example-level boolean adherence label by checking if all response sentences are supported. The exact prompt used for annotation is provided in Appendix 7.4. We apply post-processing steps to ensure high quality, reliable annotations from our GPT-labeler, which we outline in Appendix 7.5. Table 2: GPT-4 annotator achieves high alignment with human judgements. We report F1 and Accuracy alignment metrics on human annotated subsets of DelucionQA. Adherence annotations are evaluated against DelucionQA human-annotated labels. Relevance and Utilization are evaluated against DelucionQA(40): a subset of 40 examples randomly sampled from the DelucionQA test set and annotated by the authors. Test Set Metric F1 Accuracy DelucionQA - example level Adherence 0.96 0.93 DelucionQA - span level Adherence 0.97 0.95 DelucionQA(40) - span level Utilization 0.92 0.94 DelucionQA(40) - span level Relevance 0.76 0.78 Alignment with Human Judgements We validate our metric formulations and labeling approach on a human annotated benchmark. DelucionQA [33] is a curated collection of user queries on the operation of Jeep’s 2023 Gladiator model. Natural language queries are first generated by an LLM, then reviewed and filtered by human annotators. Context documents are sourced from Jeep’s Gladiator User Manual, and responses are generated by various LLMs. Combined with example-level and span-level annotations for hallucination, DelucionQA represents a realistic distribution of real-world user queries and RAG responses. We find that our GPT annotator achieves 93% and 95% example- and span-level agreement with human judgements on the DelucionQA test split (Table 2). To validate relevance and utilization annotations, we also annotate a small subset of DelucionQA with granular relevance and utilization labels. We refer to this subset as DelucionQA(40) in Table 2. Similar to adherence, we observe high correlation between relevance and utilization judgements from GPT-4 and humans. Details of the annotation process and additional validations are found in Appendix 7.6. (a) Retriever vs. Relevance (b) LLM vs. Hallucination (c) Prompt vs. Utilization (d) Prompt vs. Completeness Figure 4: Relationship between RAG system configuration and TRACe metrics. (a) The choice and configuration of RAG retriever component affects the average relevance of the retrieved context. In this example, a dense retriever with a low number of documents per query (k=2) yields the highest average context relevance. (b) The choice of LLM and generation prompt affect how well the RAG system utilizes the provided context. Prompting the LLM with a detailed chain-of-thought prompt leads to reduced hallucinations, as well as higher response utilization (c) and completeness (d) rates. 3.5 RAG Case Study We design a case study to further motivate and validate the proposed TRACe framework. We sample 100 realistic world knowledge queries from the RAGTruth [40] QA training set, along with 2,512 unique context document chunks from the same dataset to use as inputs into our mock RAG systems. We simulate RAG setups of varying quality by controlling four configurable parameters: the retriever (sparse TF-IDF vs. dense), number of retrieved documents (2 vs. 4), generation model (GPT-3.5-turbo vs. GPT-4o), and 4 versions of the generation prompt template. For illustrative purposes, we evaluate one prompt template comprising of only the question (no context) vs. three RAG-style templates that encourage the LLM to utilize the provided context and/or utilize chain-of-thought (CoT). The full generation templates are provided in Appendix 7.7. We generate responses for the 100 RAGTruth queries with each of the 32 resulting RAG systems and use our LLM annotation prompt to evaluate the TRACe metrics. Figure 4 demonstrates how the different RAG configurations influence TRACe metrics. For example, we confirm that the choice of the generative LLM model affects the amount of hallucinations (or non-adherent responses) in the generated text. Not surprisingly, GPT-4o combined with a chain-of-thought prompt yields the lowest amount of hallucination compared to the weaker GPT-3.5 model and less detailed prompts. Curiously, we find that GPT-3.5 hallucinates more when prompted with CoT, which may point to limitations in the model’s ability to reason about complex concepts 4(b). Overall, we find that prompting the LLM to think step-by-step and explain its reasoning leads to higher utilization of the provided context and more complete responses (4(c), 4(d)). Finally, we show that the choice of the retriever affects average relevance of the retrieved context documents per query 4(a). 4 Experiments 4.1 LLM Judge We benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query GPT-3.5 with our annotation prompt, (2) RAGAS [9], and (3) TruLens [36]. RAGAS employs a series of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context Relevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness (Adherence) and Context Relevance. 4.2 Fine-tuned Judge We fine-tune a DeBERTa-v3-Large [11] NLI checkpoint333https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli from Laurer et al. [18] with one key architecture modification: we add a shallow prediction head for each of the output RAG metrics, which allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective and enables transfer learning from head to head through back-propagation down to the shared base layers. Each prediction head is a single layer feed-forward net that acts on the token-level output of the last DeBERTa layer. We attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and another head on the response tokens to estimate Adherence. For training, we broadcast sentence-level annotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and Adherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized spans and Adherent spans and calculate TRACe metrics"
      ],
      "response": null,
      "multi_responses": null,
      "reference": "Based on the provided context, GPT-4 is used as an annotator to evaluate RAG responses, specifically measuring relevance, utilization, and adherence. The first segment (<1-hop>) introduces the TRACe evaluation framework, which includes metrics like relevance and adherence to assess the quality of retriever and generator components. The second segment (<2-hop>) explains how GPT-4 annotates these metrics by identifying relevant and utilized substrings and determining whether responses are supported by the context. Therefore, GPT-4 models are employed to provide high-quality, span-level annotations that reflect their ability to evaluate relevance and adherence accurately within the multi-hop evaluation process, ensuring responses are grounded and relevant according to the comprehensive TRACe framework.",
      "rubrics": null
    },
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "approval_status": "approved"
  },
  {
    "eval_sample": {
      "user_input": "How does GPT-4 contribute to evaluating the relevance, utilization, and adherence in RAG systems according to the TRACe evaluation framework?",
      "retrieved_contexts": null,
      "reference_contexts": [
        "<1-hop>\n\nTRACe Evaluation Framework We propose a suite of four comprehensive metrics to evaluate the quality of the retriever and the response generator components of RAG. An optimal RAG system must balance accuracy and efficiency. The retriever should precisely return all the necessary information to address the user query, avoiding any superfluous data. The generator must effectively utilize the retrieved information, ensuring the response is strictly based on the provided context without introducing any hallucinations in the output. Towards comprehensive evaluation of the abovementioned criteria, we introduce the TRACe evaluation framework to measure uTilization, Relevance, Adherence, and Completeness of a RAG system. Utilization, Adherence, and Completeness measure the quality of the generator. Adherence here is synonymous with previously proposed answer faithfullness, groundednes, and attribution, all terms used in literature to measure how well an LLM output adheres to a source of factual information. Relevance measures the quality of the retriever output with respect to the query. Below we formalize the definition of each metric. Definitions Let D𝐷Ditalic_D be a set of context documents {d1⁢…⁢dn}subscript𝑑1…subscript𝑑𝑛\\{d_{1}...d_{n}\\}{ italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT … italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } retrieved for a RAG input query. We define a set of relevant tokens in disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as Ri={t1,…⁢tr}subscript𝑅𝑖subscript𝑡1…subscript𝑡𝑟R_{i}=\\{t_{1},...t_{r}\\}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT }. Risubscript𝑅𝑖R_{i}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT encodes information in context document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT that is useful for answering the query. Similarly, we define Ui={t1,…⁢tu}subscript𝑈𝑖subscript𝑡1…subscript𝑡𝑢U_{i}=\\{t_{1},...t_{u}\\}italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT } as the set of utilized tokens in document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which reflect information that the generation model is using to produce a response. Refer to Figure 2 for a visual representation of relevant and utilized spans. L⁢e⁢n⁢(x)𝐿𝑒𝑛𝑥Len(x)italic_L italic_e italic_n ( italic_x ) measures the length of strings in x𝑥xitalic_x, which can be interpreted as character length, token length, or sentence length. For calculating ground-truth metrics, we employ sentence-length, since it aligns best with our annotation schema (Section 3.4). However, token or character length may also be suitable for other use cases. Figure 2: Example of RAG Question, Context, and Response. Relevant context spans are highlighted, and utilized spans are underlined. Context Relevance Context Relevance is defined in [9, 32] as the fraction of the retrieved context that is relevant to the input query. Low relevance points to an inefficient retriever that supplies excess information to the generation model. Long context inputs into the generator may accrue unnecessary costs, as well as compromise the quality of the generated output. We measure relevance of context document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as: document relevance=L⁢e⁢n⁢(Ri)L⁢e⁢n⁢(di)document relevance𝐿𝑒𝑛subscript𝑅𝑖𝐿𝑒𝑛subscript𝑑𝑖\\text{document relevance}=\\frac{{Len(R_{i})}}{Len(d_{i})}document relevance = divide start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (1) Example-level relevance can be aggregated over all context documents in the example as: example relevance=∑i=1|D|L⁢e⁢n⁢(Ri)∑i=1|D|L⁢e⁢n⁢(di)example relevancesuperscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑅𝑖superscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑑𝑖\\text{example relevance}=\\frac{\\sum_{i=1}^{|D|}{Len(R_{i})}}{\\sum_{i=1}^{|D|}% Len(d_{i})}example relevance = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (2) Context Utilization Context Utilization is a new metric introduced in TRACe. We aim to measure the the fraction of the retrieved context that is used by the generator to produce the response. Low Utilization in combination with low Relevance points to a greedy retriever, while low Utilization alone points to a weak generator that fails to leverage the provided context efficiently. Document-level and example-level Utilization are defined as: document utilization=L⁢e⁢n⁢(Ui)L⁢e⁢n⁢(di)example utilization=∑i=1|D|L⁢e⁢n⁢(Ui)∑i=1|D|L⁢e⁢n⁢(di)formulae-sequencedocument utilization𝐿𝑒𝑛subscript𝑈𝑖𝐿𝑒𝑛subscript𝑑𝑖example utilizationsuperscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑈𝑖superscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑑𝑖\\text{document utilization}=\\frac{{Len(U_{i})}}{Len(d_{i})}\\quad\\text{example % utilization}=\\frac{\\sum_{i=1}^{|D|}{Len(U_{i})}}{\\sum_{i=1}^{|D|}Len(d_{i})}document utilization = divide start_ARG italic_L italic_e italic_n ( italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG example utilization = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (3) Completeness Completeness is another new metrics we introduce to measure how well the response incorporates all the relevant information in the context. Note that this is different from Utilization; it is possible to have high Relevance and high Utilization, but low Completeness when the generator utilizes irrelevant information in the context to produce a low quality response. Completeness for document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is calculated as the fraction of utilized substrings among all relevant substrings: completeness=L⁢e⁢n⁢(Ri∩Ui)L⁢e⁢n⁢(Ri)completeness𝐿𝑒𝑛subscript𝑅𝑖subscript𝑈𝑖𝐿𝑒𝑛subscript𝑅𝑖\\text{completeness}=\\frac{Len(R_{i}\\cap U_{i})}{Len(R_{i})}completeness = divide start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∩ italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (4) And can be extended to example-level by considering all relevant and utilized substrings across all context documents. Adherence Adherence is designed to detect hallucinations in RAG responses. Our definition of Adherence is synonymous with answer faithfullness [9, 32], groundednes [36], and attribution [31]. For alignment with existing hallucination detection approaches, we define example-level adherence as a boolean indicating whether or not all parts of the response are grounded in the context. However, in our annotation schema (Section 3.4) we also define Ai={t1,…⁢ta}subscript𝐴𝑖subscript𝑡1…subscript𝑡𝑎A_{i}=\\{t_{1},...t_{a}\\}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT } as the set of response tokens that are supported by the context to enable granular Adherence evaluation. Figure 3: Distributions of relevance, utilization, and completeness labels in RAGBench. Y-axis is normalized to visualize densities. 3.3 RAGBench Statistics RAGBench component datasets contain between 1% - 20% hallucinations. ExpertQA, CovidQA, and MS Marco contain the highest fraction of hallucinated responses (12%, 16%, and 13%, respectively), while Cuad, FinQA, and TAT-QA contain the least (about 1% for each). We visualize distributions of relevance, utilization, and completeness scores in Figure 3. 3.4 LLM annotator We prompt GPT-4 (gpt-4-0125-preview) to produce ground truth Adherence, Relevance, and Utilization labels for input (documents, query, response)",
        "<2-hop>\n\ntuples in RAGBench. Completeness is easily derived from span-level Relevance and Utilization annotations, thus we don’t request explicit annotations for it. For high quality labels, we use proven techniques like chain of thought [39] that have been shown to maximize the correlation between GPT-4 and human judgements [43, 46]. For relevance and utilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the input documents. For adherence, we instruct the LLM to identify which response sentences, if any, are supported by the provided context. We can then derive an example-level boolean adherence label by checking if all response sentences are supported. The exact prompt used for annotation is provided in Appendix 7.4. We apply post-processing steps to ensure high quality, reliable annotations from our GPT-labeler, which we outline in Appendix 7.5. Table 2: GPT-4 annotator achieves high alignment with human judgements. We report F1 and Accuracy alignment metrics on human annotated subsets of DelucionQA. Adherence annotations are evaluated against DelucionQA human-annotated labels. Relevance and Utilization are evaluated against DelucionQA(40): a subset of 40 examples randomly sampled from the DelucionQA test set and annotated by the authors. Test Set Metric F1 Accuracy DelucionQA - example level Adherence 0.96 0.93 DelucionQA - span level Adherence 0.97 0.95 DelucionQA(40) - span level Utilization 0.92 0.94 DelucionQA(40) - span level Relevance 0.76 0.78 Alignment with Human Judgements We validate our metric formulations and labeling approach on a human annotated benchmark. DelucionQA [33] is a curated collection of user queries on the operation of Jeep’s 2023 Gladiator model. Natural language queries are first generated by an LLM, then reviewed and filtered by human annotators. Context documents are sourced from Jeep’s Gladiator User Manual, and responses are generated by various LLMs. Combined with example-level and span-level annotations for hallucination, DelucionQA represents a realistic distribution of real-world user queries and RAG responses. We find that our GPT annotator achieves 93% and 95% example- and span-level agreement with human judgements on the DelucionQA test split (Table 2). To validate relevance and utilization annotations, we also annotate a small subset of DelucionQA with granular relevance and utilization labels. We refer to this subset as DelucionQA(40) in Table 2. Similar to adherence, we observe high correlation between relevance and utilization judgements from GPT-4 and humans. Details of the annotation process and additional validations are found in Appendix 7.6. (a) Retriever vs. Relevance (b) LLM vs. Hallucination (c) Prompt vs. Utilization (d) Prompt vs. Completeness Figure 4: Relationship between RAG system configuration and TRACe metrics. (a) The choice and configuration of RAG retriever component affects the average relevance of the retrieved context. In this example, a dense retriever with a low number of documents per query (k=2) yields the highest average context relevance. (b) The choice of LLM and generation prompt affect how well the RAG system utilizes the provided context. Prompting the LLM with a detailed chain-of-thought prompt leads to reduced hallucinations, as well as higher response utilization (c) and completeness (d) rates. 3.5 RAG Case Study We design a case study to further motivate and validate the proposed TRACe framework. We sample 100 realistic world knowledge queries from the RAGTruth [40] QA training set, along with 2,512 unique context document chunks from the same dataset to use as inputs into our mock RAG systems. We simulate RAG setups of varying quality by controlling four configurable parameters: the retriever (sparse TF-IDF vs. dense), number of retrieved documents (2 vs. 4), generation model (GPT-3.5-turbo vs. GPT-4o), and 4 versions of the generation prompt template. For illustrative purposes, we evaluate one prompt template comprising of only the question (no context) vs. three RAG-style templates that encourage the LLM to utilize the provided context and/or utilize chain-of-thought (CoT). The full generation templates are provided in Appendix 7.7. We generate responses for the 100 RAGTruth queries with each of the 32 resulting RAG systems and use our LLM annotation prompt to evaluate the TRACe metrics. Figure 4 demonstrates how the different RAG configurations influence TRACe metrics. For example, we confirm that the choice of the generative LLM model affects the amount of hallucinations (or non-adherent responses) in the generated text. Not surprisingly, GPT-4o combined with a chain-of-thought prompt yields the lowest amount of hallucination compared to the weaker GPT-3.5 model and less detailed prompts. Curiously, we find that GPT-3.5 hallucinates more when prompted with CoT, which may point to limitations in the model’s ability to reason about complex concepts 4(b). Overall, we find that prompting the LLM to think step-by-step and explain its reasoning leads to higher utilization of the provided context and more complete responses (4(c), 4(d)). Finally, we show that the choice of the retriever affects average relevance of the retrieved context documents per query 4(a). 4 Experiments 4.1 LLM Judge We benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query GPT-3.5 with our annotation prompt, (2) RAGAS [9], and (3) TruLens [36]. RAGAS employs a series of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context Relevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness (Adherence) and Context Relevance. 4.2 Fine-tuned Judge We fine-tune a DeBERTa-v3-Large [11] NLI checkpoint333https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli from Laurer et al. [18] with one key architecture modification: we add a shallow prediction head for each of the output RAG metrics, which allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective and enables transfer learning from head to head through back-propagation down to the shared base layers. Each prediction head is a single layer feed-forward net that acts on the token-level output of the last DeBERTa layer. We attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and another head on the response tokens to estimate Adherence. For training, we broadcast sentence-level annotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and Adherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized spans and Adherent spans and calculate TRACe metrics"
      ],
      "response": null,
      "multi_responses": null,
      "reference": "According to the provided context, GPT-4 is used as an annotator to produce ground truth labels for evaluating RAG systems within the TRACe framework. Specifically, GPT-4 helps identify relevant and utilized substrings in input documents to assess relevance and utilization metrics. Additionally, GPT-4 is instructed to determine which response sentences are supported by the context, enabling the calculation of adherence. The high alignment metrics reported (F1 and accuracy above 0.9) indicate that GPT-4 effectively contributes to evaluating these aspects, ensuring responses are grounded, relevant, and efficiently utilize the provided context in RAG systems.",
      "rubrics": null
    },
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "approval_status": "approved"
  },
  {
    "eval_sample": {
      "user_input": "how GPT-4 and GPT-4o compare in relevance and adherence in RAG system evaluation?",
      "retrieved_contexts": null,
      "reference_contexts": [
        "<1-hop>\n\nTRACe Evaluation Framework We propose a suite of four comprehensive metrics to evaluate the quality of the retriever and the response generator components of RAG. An optimal RAG system must balance accuracy and efficiency. The retriever should precisely return all the necessary information to address the user query, avoiding any superfluous data. The generator must effectively utilize the retrieved information, ensuring the response is strictly based on the provided context without introducing any hallucinations in the output. Towards comprehensive evaluation of the abovementioned criteria, we introduce the TRACe evaluation framework to measure uTilization, Relevance, Adherence, and Completeness of a RAG system. Utilization, Adherence, and Completeness measure the quality of the generator. Adherence here is synonymous with previously proposed answer faithfullness, groundednes, and attribution, all terms used in literature to measure how well an LLM output adheres to a source of factual information. Relevance measures the quality of the retriever output with respect to the query. Below we formalize the definition of each metric. Definitions Let D𝐷Ditalic_D be a set of context documents {d1⁢…⁢dn}subscript𝑑1…subscript𝑑𝑛\\{d_{1}...d_{n}\\}{ italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT … italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } retrieved for a RAG input query. We define a set of relevant tokens in disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as Ri={t1,…⁢tr}subscript𝑅𝑖subscript𝑡1…subscript𝑡𝑟R_{i}=\\{t_{1},...t_{r}\\}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT }. Risubscript𝑅𝑖R_{i}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT encodes information in context document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT that is useful for answering the query. Similarly, we define Ui={t1,…⁢tu}subscript𝑈𝑖subscript𝑡1…subscript𝑡𝑢U_{i}=\\{t_{1},...t_{u}\\}italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT } as the set of utilized tokens in document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which reflect information that the generation model is using to produce a response. Refer to Figure 2 for a visual representation of relevant and utilized spans. L⁢e⁢n⁢(x)𝐿𝑒𝑛𝑥Len(x)italic_L italic_e italic_n ( italic_x ) measures the length of strings in x𝑥xitalic_x, which can be interpreted as character length, token length, or sentence length. For calculating ground-truth metrics, we employ sentence-length, since it aligns best with our annotation schema (Section 3.4). However, token or character length may also be suitable for other use cases. Figure 2: Example of RAG Question, Context, and Response. Relevant context spans are highlighted, and utilized spans are underlined. Context Relevance Context Relevance is defined in [9, 32] as the fraction of the retrieved context that is relevant to the input query. Low relevance points to an inefficient retriever that supplies excess information to the generation model. Long context inputs into the generator may accrue unnecessary costs, as well as compromise the quality of the generated output. We measure relevance of context document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as: document relevance=L⁢e⁢n⁢(Ri)L⁢e⁢n⁢(di)document relevance𝐿𝑒𝑛subscript𝑅𝑖𝐿𝑒𝑛subscript𝑑𝑖\\text{document relevance}=\\frac{{Len(R_{i})}}{Len(d_{i})}document relevance = divide start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (1) Example-level relevance can be aggregated over all context documents in the example as: example relevance=∑i=1|D|L⁢e⁢n⁢(Ri)∑i=1|D|L⁢e⁢n⁢(di)example relevancesuperscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑅𝑖superscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑑𝑖\\text{example relevance}=\\frac{\\sum_{i=1}^{|D|}{Len(R_{i})}}{\\sum_{i=1}^{|D|}% Len(d_{i})}example relevance = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (2) Context Utilization Context Utilization is a new metric introduced in TRACe. We aim to measure the the fraction of the retrieved context that is used by the generator to produce the response. Low Utilization in combination with low Relevance points to a greedy retriever, while low Utilization alone points to a weak generator that fails to leverage the provided context efficiently. Document-level and example-level Utilization are defined as: document utilization=L⁢e⁢n⁢(Ui)L⁢e⁢n⁢(di)example utilization=∑i=1|D|L⁢e⁢n⁢(Ui)∑i=1|D|L⁢e⁢n⁢(di)formulae-sequencedocument utilization𝐿𝑒𝑛subscript𝑈𝑖𝐿𝑒𝑛subscript𝑑𝑖example utilizationsuperscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑈𝑖superscriptsubscript𝑖1𝐷𝐿𝑒𝑛subscript𝑑𝑖\\text{document utilization}=\\frac{{Len(U_{i})}}{Len(d_{i})}\\quad\\text{example % utilization}=\\frac{\\sum_{i=1}^{|D|}{Len(U_{i})}}{\\sum_{i=1}^{|D|}Len(d_{i})}document utilization = divide start_ARG italic_L italic_e italic_n ( italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG example utilization = divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (3) Completeness Completeness is another new metrics we introduce to measure how well the response incorporates all the relevant information in the context. Note that this is different from Utilization; it is possible to have high Relevance and high Utilization, but low Completeness when the generator utilizes irrelevant information in the context to produce a low quality response. Completeness for document disubscript𝑑𝑖d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is calculated as the fraction of utilized substrings among all relevant substrings: completeness=L⁢e⁢n⁢(Ri∩Ui)L⁢e⁢n⁢(Ri)completeness𝐿𝑒𝑛subscript𝑅𝑖subscript𝑈𝑖𝐿𝑒𝑛subscript𝑅𝑖\\text{completeness}=\\frac{Len(R_{i}\\cap U_{i})}{Len(R_{i})}completeness = divide start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∩ italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG (4) And can be extended to example-level by considering all relevant and utilized substrings across all context documents. Adherence Adherence is designed to detect hallucinations in RAG responses. Our definition of Adherence is synonymous with answer faithfullness [9, 32], groundednes [36], and attribution [31]. For alignment with existing hallucination detection approaches, we define example-level adherence as a boolean indicating whether or not all parts of the response are grounded in the context. However, in our annotation schema (Section 3.4) we also define Ai={t1,…⁢ta}subscript𝐴𝑖subscript𝑡1…subscript𝑡𝑎A_{i}=\\{t_{1},...t_{a}\\}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … italic_t start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT } as the set of response tokens that are supported by the context to enable granular Adherence evaluation. Figure 3: Distributions of relevance, utilization, and completeness labels in RAGBench. Y-axis is normalized to visualize densities. 3.3 RAGBench Statistics RAGBench component datasets contain between 1% - 20% hallucinations. ExpertQA, CovidQA, and MS Marco contain the highest fraction of hallucinated responses (12%, 16%, and 13%, respectively), while Cuad, FinQA, and TAT-QA contain the least (about 1% for each). We visualize distributions of relevance, utilization, and completeness scores in Figure 3. 3.4 LLM annotator We prompt GPT-4 (gpt-4-0125-preview) to produce ground truth Adherence, Relevance, and Utilization labels for input (documents, query, response)",
        "<2-hop>\n\ntuples in RAGBench. Completeness is easily derived from span-level Relevance and Utilization annotations, thus we don’t request explicit annotations for it. For high quality labels, we use proven techniques like chain of thought [39] that have been shown to maximize the correlation between GPT-4 and human judgements [43, 46]. For relevance and utilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the input documents. For adherence, we instruct the LLM to identify which response sentences, if any, are supported by the provided context. We can then derive an example-level boolean adherence label by checking if all response sentences are supported. The exact prompt used for annotation is provided in Appendix 7.4. We apply post-processing steps to ensure high quality, reliable annotations from our GPT-labeler, which we outline in Appendix 7.5. Table 2: GPT-4 annotator achieves high alignment with human judgements. We report F1 and Accuracy alignment metrics on human annotated subsets of DelucionQA. Adherence annotations are evaluated against DelucionQA human-annotated labels. Relevance and Utilization are evaluated against DelucionQA(40): a subset of 40 examples randomly sampled from the DelucionQA test set and annotated by the authors. Test Set Metric F1 Accuracy DelucionQA - example level Adherence 0.96 0.93 DelucionQA - span level Adherence 0.97 0.95 DelucionQA(40) - span level Utilization 0.92 0.94 DelucionQA(40) - span level Relevance 0.76 0.78 Alignment with Human Judgements We validate our metric formulations and labeling approach on a human annotated benchmark. DelucionQA [33] is a curated collection of user queries on the operation of Jeep’s 2023 Gladiator model. Natural language queries are first generated by an LLM, then reviewed and filtered by human annotators. Context documents are sourced from Jeep’s Gladiator User Manual, and responses are generated by various LLMs. Combined with example-level and span-level annotations for hallucination, DelucionQA represents a realistic distribution of real-world user queries and RAG responses. We find that our GPT annotator achieves 93% and 95% example- and span-level agreement with human judgements on the DelucionQA test split (Table 2). To validate relevance and utilization annotations, we also annotate a small subset of DelucionQA with granular relevance and utilization labels. We refer to this subset as DelucionQA(40) in Table 2. Similar to adherence, we observe high correlation between relevance and utilization judgements from GPT-4 and humans. Details of the annotation process and additional validations are found in Appendix 7.6. (a) Retriever vs. Relevance (b) LLM vs. Hallucination (c) Prompt vs. Utilization (d) Prompt vs. Completeness Figure 4: Relationship between RAG system configuration and TRACe metrics. (a) The choice and configuration of RAG retriever component affects the average relevance of the retrieved context. In this example, a dense retriever with a low number of documents per query (k=2) yields the highest average context relevance. (b) The choice of LLM and generation prompt affect how well the RAG system utilizes the provided context. Prompting the LLM with a detailed chain-of-thought prompt leads to reduced hallucinations, as well as higher response utilization (c) and completeness (d) rates. 3.5 RAG Case Study We design a case study to further motivate and validate the proposed TRACe framework. We sample 100 realistic world knowledge queries from the RAGTruth [40] QA training set, along with 2,512 unique context document chunks from the same dataset to use as inputs into our mock RAG systems. We simulate RAG setups of varying quality by controlling four configurable parameters: the retriever (sparse TF-IDF vs. dense), number of retrieved documents (2 vs. 4), generation model (GPT-3.5-turbo vs. GPT-4o), and 4 versions of the generation prompt template. For illustrative purposes, we evaluate one prompt template comprising of only the question (no context) vs. three RAG-style templates that encourage the LLM to utilize the provided context and/or utilize chain-of-thought (CoT). The full generation templates are provided in Appendix 7.7. We generate responses for the 100 RAGTruth queries with each of the 32 resulting RAG systems and use our LLM annotation prompt to evaluate the TRACe metrics. Figure 4 demonstrates how the different RAG configurations influence TRACe metrics. For example, we confirm that the choice of the generative LLM model affects the amount of hallucinations (or non-adherent responses) in the generated text. Not surprisingly, GPT-4o combined with a chain-of-thought prompt yields the lowest amount of hallucination compared to the weaker GPT-3.5 model and less detailed prompts. Curiously, we find that GPT-3.5 hallucinates more when prompted with CoT, which may point to limitations in the model’s ability to reason about complex concepts 4(b). Overall, we find that prompting the LLM to think step-by-step and explain its reasoning leads to higher utilization of the provided context and more complete responses (4(c), 4(d)). Finally, we show that the choice of the retriever affects average relevance of the retrieved context documents per query 4(a). 4 Experiments 4.1 LLM Judge We benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query GPT-3.5 with our annotation prompt, (2) RAGAS [9], and (3) TruLens [36]. RAGAS employs a series of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context Relevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness (Adherence) and Context Relevance. 4.2 Fine-tuned Judge We fine-tune a DeBERTa-v3-Large [11] NLI checkpoint333https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli from Laurer et al. [18] with one key architecture modification: we add a shallow prediction head for each of the output RAG metrics, which allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective and enables transfer learning from head to head through back-propagation down to the shared base layers. Each prediction head is a single layer feed-forward net that acts on the token-level output of the last DeBERTa layer. We attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and another head on the response tokens to estimate Adherence. For training, we broadcast sentence-level annotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and Adherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized spans and Adherent spans and calculate TRACe metrics"
      ],
      "response": null,
      "multi_responses": null,
      "reference": "the context shows that GPT-4 and GPT-4o are used as generation models in RAG systems, with GPT-4o being a variation. the evaluation framework measures how well these models produce responses grounded in context (adherence) and how relevant the retrieved context is. the context indicates that GPT-4 and GPT-4o are assessed for their ability to utilize context effectively, with GPT-4o showing lower hallucinations and higher utilization and completeness when prompted with chain-of-thought. thus, GPT-4 and GPT-4o are compared based on their relevance and adherence metrics, with GPT-4o generally performing better in reducing hallucinations and improving response quality.",
      "rubrics": null
    },
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "approval_status": "approved"
  },
  {
    "eval_sample": {
      "user_input": "How does the RAGBench framework evaluate the relevance and utilization of context in retrieval-augmented generation systems, and what role does the TRACe evaluation framework play in this process?",
      "retrieved_contexts": null,
      "reference_contexts": [
        "<1-hop>\n\ntuples in RAGBench. Completeness is easily derived from span-level Relevance and Utilization annotations, thus we don’t request explicit annotations for it. For high quality labels, we use proven techniques like chain of thought [39] that have been shown to maximize the correlation between GPT-4 and human judgements [43, 46]. For relevance and utilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the input documents. For adherence, we instruct the LLM to identify which response sentences, if any, are supported by the provided context. We can then derive an example-level boolean adherence label by checking if all response sentences are supported. The exact prompt used for annotation is provided in Appendix 7.4. We apply post-processing steps to ensure high quality, reliable annotations from our GPT-labeler, which we outline in Appendix 7.5. Table 2: GPT-4 annotator achieves high alignment with human judgements. We report F1 and Accuracy alignment metrics on human annotated subsets of DelucionQA. Adherence annotations are evaluated against DelucionQA human-annotated labels. Relevance and Utilization are evaluated against DelucionQA(40): a subset of 40 examples randomly sampled from the DelucionQA test set and annotated by the authors. Test Set Metric F1 Accuracy DelucionQA - example level Adherence 0.96 0.93 DelucionQA - span level Adherence 0.97 0.95 DelucionQA(40) - span level Utilization 0.92 0.94 DelucionQA(40) - span level Relevance 0.76 0.78 Alignment with Human Judgements We validate our metric formulations and labeling approach on a human annotated benchmark. DelucionQA [33] is a curated collection of user queries on the operation of Jeep’s 2023 Gladiator model. Natural language queries are first generated by an LLM, then reviewed and filtered by human annotators. Context documents are sourced from Jeep’s Gladiator User Manual, and responses are generated by various LLMs. Combined with example-level and span-level annotations for hallucination, DelucionQA represents a realistic distribution of real-world user queries and RAG responses. We find that our GPT annotator achieves 93% and 95% example- and span-level agreement with human judgements on the DelucionQA test split (Table 2). To validate relevance and utilization annotations, we also annotate a small subset of DelucionQA with granular relevance and utilization labels. We refer to this subset as DelucionQA(40) in Table 2. Similar to adherence, we observe high correlation between relevance and utilization judgements from GPT-4 and humans. Details of the annotation process and additional validations are found in Appendix 7.6. (a) Retriever vs. Relevance (b) LLM vs. Hallucination (c) Prompt vs. Utilization (d) Prompt vs. Completeness Figure 4: Relationship between RAG system configuration and TRACe metrics. (a) The choice and configuration of RAG retriever component affects the average relevance of the retrieved context. In this example, a dense retriever with a low number of documents per query (k=2) yields the highest average context relevance. (b) The choice of LLM and generation prompt affect how well the RAG system utilizes the provided context. Prompting the LLM with a detailed chain-of-thought prompt leads to reduced hallucinations, as well as higher response utilization (c) and completeness (d) rates. 3.5 RAG Case Study We design a case study to further motivate and validate the proposed TRACe framework. We sample 100 realistic world knowledge queries from the RAGTruth [40] QA training set, along with 2,512 unique context document chunks from the same dataset to use as inputs into our mock RAG systems. We simulate RAG setups of varying quality by controlling four configurable parameters: the retriever (sparse TF-IDF vs. dense), number of retrieved documents (2 vs. 4), generation model (GPT-3.5-turbo vs. GPT-4o), and 4 versions of the generation prompt template. For illustrative purposes, we evaluate one prompt template comprising of only the question (no context) vs. three RAG-style templates that encourage the LLM to utilize the provided context and/or utilize chain-of-thought (CoT). The full generation templates are provided in Appendix 7.7. We generate responses for the 100 RAGTruth queries with each of the 32 resulting RAG systems and use our LLM annotation prompt to evaluate the TRACe metrics. Figure 4 demonstrates how the different RAG configurations influence TRACe metrics. For example, we confirm that the choice of the generative LLM model affects the amount of hallucinations (or non-adherent responses) in the generated text. Not surprisingly, GPT-4o combined with a chain-of-thought prompt yields the lowest amount of hallucination compared to the weaker GPT-3.5 model and less detailed prompts. Curiously, we find that GPT-3.5 hallucinates more when prompted with CoT, which may point to limitations in the model’s ability to reason about complex concepts 4(b). Overall, we find that prompting the LLM to think step-by-step and explain its reasoning leads to higher utilization of the provided context and more complete responses (4(c), 4(d)). Finally, we show that the choice of the retriever affects average relevance of the retrieved context documents per query 4(a). 4 Experiments 4.1 LLM Judge We benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query GPT-3.5 with our annotation prompt, (2) RAGAS [9], and (3) TruLens [36]. RAGAS employs a series of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context Relevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness (Adherence) and Context Relevance. 4.2 Fine-tuned Judge We fine-tune a DeBERTa-v3-Large [11] NLI checkpoint333https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli from Laurer et al. [18] with one key architecture modification: we add a shallow prediction head for each of the output RAG metrics, which allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective and enables transfer learning from head to head through back-propagation down to the shared base layers. Each prediction head is a single layer feed-forward net that acts on the token-level output of the last DeBERTa layer. We attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and another head on the response tokens to estimate Adherence. For training, we broadcast sentence-level annotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and Adherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized spans and Adherent spans and calculate TRACe metrics",
        "<2-hop>\n\nRAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems 1 Introduction 2 Related Work RAG evaluation Finetuned RAG evaluation models 3 RAGBench Construction 3.1 Component Datasets Source Domains Context Token Length Task Types Question Sources Response Generation Data Splits 3.2 TRACe Evaluation Framework Definitions Context Relevance Context Utilization Completeness Adherence 3.3 RAGBench Statistics 3.4 LLM annotator Alignment with Human Judgements 3.5 RAG Case Study 4 Experiments 4.1 LLM Judge 4.2 Fine-tuned Judge 4.3 Evaluation 5 Results Estimating Context Relevance is Difficult"
      ],
      "response": null,
      "multi_responses": null,
      "reference": "The RAGBench framework evaluates the relevance and utilization of context by employing annotations derived from techniques like chain of thought and GPT-4-based labeling, which identify relevant and utilized sub-strings within input documents. Completeness and adherence are assessed through the identification of response sentences supported by the context. The TRACe evaluation framework complements this by providing specific metrics—such as Context Relevance, Context Utilization, and Adherence—that quantify how well the system retrieves and utilizes context, as well as how faithfully responses adhere to the provided information. Together, RAGBench's annotations and the TRACe metrics enable a comprehensive assessment of the system's ability to effectively incorporate context into its responses.",
      "rubrics": null
    },
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "approval_status": "approved"
  },
  {
    "eval_sample": {
      "user_input": "How does the RAGBench framework utilize chain-of-thought techniques to improve the accuracy of relevance and adherence annotations, and how do these multi-hop evaluation strategies influence the overall assessment of retrieval-augmented generation systems as demonstrated in the case study involving different RAG configurations?",
      "retrieved_contexts": null,
      "reference_contexts": [
        "<1-hop>\n\ntuples in RAGBench. Completeness is easily derived from span-level Relevance and Utilization annotations, thus we don’t request explicit annotations for it. For high quality labels, we use proven techniques like chain of thought [39] that have been shown to maximize the correlation between GPT-4 and human judgements [43, 46]. For relevance and utilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the input documents. For adherence, we instruct the LLM to identify which response sentences, if any, are supported by the provided context. We can then derive an example-level boolean adherence label by checking if all response sentences are supported. The exact prompt used for annotation is provided in Appendix 7.4. We apply post-processing steps to ensure high quality, reliable annotations from our GPT-labeler, which we outline in Appendix 7.5. Table 2: GPT-4 annotator achieves high alignment with human judgements. We report F1 and Accuracy alignment metrics on human annotated subsets of DelucionQA. Adherence annotations are evaluated against DelucionQA human-annotated labels. Relevance and Utilization are evaluated against DelucionQA(40): a subset of 40 examples randomly sampled from the DelucionQA test set and annotated by the authors. Test Set Metric F1 Accuracy DelucionQA - example level Adherence 0.96 0.93 DelucionQA - span level Adherence 0.97 0.95 DelucionQA(40) - span level Utilization 0.92 0.94 DelucionQA(40) - span level Relevance 0.76 0.78 Alignment with Human Judgements We validate our metric formulations and labeling approach on a human annotated benchmark. DelucionQA [33] is a curated collection of user queries on the operation of Jeep’s 2023 Gladiator model. Natural language queries are first generated by an LLM, then reviewed and filtered by human annotators. Context documents are sourced from Jeep’s Gladiator User Manual, and responses are generated by various LLMs. Combined with example-level and span-level annotations for hallucination, DelucionQA represents a realistic distribution of real-world user queries and RAG responses. We find that our GPT annotator achieves 93% and 95% example- and span-level agreement with human judgements on the DelucionQA test split (Table 2). To validate relevance and utilization annotations, we also annotate a small subset of DelucionQA with granular relevance and utilization labels. We refer to this subset as DelucionQA(40) in Table 2. Similar to adherence, we observe high correlation between relevance and utilization judgements from GPT-4 and humans. Details of the annotation process and additional validations are found in Appendix 7.6. (a) Retriever vs. Relevance (b) LLM vs. Hallucination (c) Prompt vs. Utilization (d) Prompt vs. Completeness Figure 4: Relationship between RAG system configuration and TRACe metrics. (a) The choice and configuration of RAG retriever component affects the average relevance of the retrieved context. In this example, a dense retriever with a low number of documents per query (k=2) yields the highest average context relevance. (b) The choice of LLM and generation prompt affect how well the RAG system utilizes the provided context. Prompting the LLM with a detailed chain-of-thought prompt leads to reduced hallucinations, as well as higher response utilization (c) and completeness (d) rates. 3.5 RAG Case Study We design a case study to further motivate and validate the proposed TRACe framework. We sample 100 realistic world knowledge queries from the RAGTruth [40] QA training set, along with 2,512 unique context document chunks from the same dataset to use as inputs into our mock RAG systems. We simulate RAG setups of varying quality by controlling four configurable parameters: the retriever (sparse TF-IDF vs. dense), number of retrieved documents (2 vs. 4), generation model (GPT-3.5-turbo vs. GPT-4o), and 4 versions of the generation prompt template. For illustrative purposes, we evaluate one prompt template comprising of only the question (no context) vs. three RAG-style templates that encourage the LLM to utilize the provided context and/or utilize chain-of-thought (CoT). The full generation templates are provided in Appendix 7.7. We generate responses for the 100 RAGTruth queries with each of the 32 resulting RAG systems and use our LLM annotation prompt to evaluate the TRACe metrics. Figure 4 demonstrates how the different RAG configurations influence TRACe metrics. For example, we confirm that the choice of the generative LLM model affects the amount of hallucinations (or non-adherent responses) in the generated text. Not surprisingly, GPT-4o combined with a chain-of-thought prompt yields the lowest amount of hallucination compared to the weaker GPT-3.5 model and less detailed prompts. Curiously, we find that GPT-3.5 hallucinates more when prompted with CoT, which may point to limitations in the model’s ability to reason about complex concepts 4(b). Overall, we find that prompting the LLM to think step-by-step and explain its reasoning leads to higher utilization of the provided context and more complete responses (4(c), 4(d)). Finally, we show that the choice of the retriever affects average relevance of the retrieved context documents per query 4(a). 4 Experiments 4.1 LLM Judge We benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query GPT-3.5 with our annotation prompt, (2) RAGAS [9], and (3) TruLens [36]. RAGAS employs a series of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context Relevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness (Adherence) and Context Relevance. 4.2 Fine-tuned Judge We fine-tune a DeBERTa-v3-Large [11] NLI checkpoint333https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli from Laurer et al. [18] with one key architecture modification: we add a shallow prediction head for each of the output RAG metrics, which allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective and enables transfer learning from head to head through back-propagation down to the shared base layers. Each prediction head is a single layer feed-forward net that acts on the token-level output of the last DeBERTa layer. We attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and another head on the response tokens to estimate Adherence. For training, we broadcast sentence-level annotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and Adherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized spans and Adherent spans and calculate TRACe metrics",
        "<2-hop>\n\nRAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems 1 Introduction 2 Related Work RAG evaluation Finetuned RAG evaluation models 3 RAGBench Construction 3.1 Component Datasets Source Domains Context Token Length Task Types Question Sources Response Generation Data Splits 3.2 TRACe Evaluation Framework Definitions Context Relevance Context Utilization Completeness Adherence 3.3 RAGBench Statistics 3.4 LLM annotator Alignment with Human Judgements 3.5 RAG Case Study 4 Experiments 4.1 LLM Judge 4.2 Fine-tuned Judge 4.3 Evaluation 5 Results Estimating Context Relevance is Difficult"
      ],
      "response": null,
      "multi_responses": null,
      "reference": "The RAGBench framework employs proven techniques like chain of thought [39] to maximize the correlation between GPT-4 and human judgments for high-quality labels, particularly for span-level relevance, utilization, and adherence annotations. By instructing the LLM to directly identify relevant and utilized substrings in input documents and to determine which response sentences are supported by the context, the framework effectively derives example-level boolean adherence labels. This multi-hop approach—combining span-level annotations with chain-of-thought prompting—enhances the reliability and granularity of the evaluation metrics. The case study demonstrates that different RAG configurations, such as the choice of retriever and prompt design, significantly impact metrics like relevance, utilization, and adherence. For instance, prompting the LLM with detailed chain-of-thought prompts leads to reduced hallucinations and higher response utilization and completeness rates, thereby providing a more nuanced and accurate assessment of the system's performance across various configurations.",
      "rubrics": null
    },
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "approval_status": "approved"
  },
  {
    "eval_sample": {
      "user_input": "how RAGBench use relevance and utilization for quality check and how they relate to TRACe?",
      "retrieved_contexts": null,
      "reference_contexts": [
        "<1-hop>\n\ntuples in RAGBench. Completeness is easily derived from span-level Relevance and Utilization annotations, thus we don’t request explicit annotations for it. For high quality labels, we use proven techniques like chain of thought [39] that have been shown to maximize the correlation between GPT-4 and human judgements [43, 46]. For relevance and utilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the input documents. For adherence, we instruct the LLM to identify which response sentences, if any, are supported by the provided context. We can then derive an example-level boolean adherence label by checking if all response sentences are supported. The exact prompt used for annotation is provided in Appendix 7.4. We apply post-processing steps to ensure high quality, reliable annotations from our GPT-labeler, which we outline in Appendix 7.5. Table 2: GPT-4 annotator achieves high alignment with human judgements. We report F1 and Accuracy alignment metrics on human annotated subsets of DelucionQA. Adherence annotations are evaluated against DelucionQA human-annotated labels. Relevance and Utilization are evaluated against DelucionQA(40): a subset of 40 examples randomly sampled from the DelucionQA test set and annotated by the authors. Test Set Metric F1 Accuracy DelucionQA - example level Adherence 0.96 0.93 DelucionQA - span level Adherence 0.97 0.95 DelucionQA(40) - span level Utilization 0.92 0.94 DelucionQA(40) - span level Relevance 0.76 0.78 Alignment with Human Judgements We validate our metric formulations and labeling approach on a human annotated benchmark. DelucionQA [33] is a curated collection of user queries on the operation of Jeep’s 2023 Gladiator model. Natural language queries are first generated by an LLM, then reviewed and filtered by human annotators. Context documents are sourced from Jeep’s Gladiator User Manual, and responses are generated by various LLMs. Combined with example-level and span-level annotations for hallucination, DelucionQA represents a realistic distribution of real-world user queries and RAG responses. We find that our GPT annotator achieves 93% and 95% example- and span-level agreement with human judgements on the DelucionQA test split (Table 2). To validate relevance and utilization annotations, we also annotate a small subset of DelucionQA with granular relevance and utilization labels. We refer to this subset as DelucionQA(40) in Table 2. Similar to adherence, we observe high correlation between relevance and utilization judgements from GPT-4 and humans. Details of the annotation process and additional validations are found in Appendix 7.6. (a) Retriever vs. Relevance (b) LLM vs. Hallucination (c) Prompt vs. Utilization (d) Prompt vs. Completeness Figure 4: Relationship between RAG system configuration and TRACe metrics. (a) The choice and configuration of RAG retriever component affects the average relevance of the retrieved context. In this example, a dense retriever with a low number of documents per query (k=2) yields the highest average context relevance. (b) The choice of LLM and generation prompt affect how well the RAG system utilizes the provided context. Prompting the LLM with a detailed chain-of-thought prompt leads to reduced hallucinations, as well as higher response utilization (c) and completeness (d) rates. 3.5 RAG Case Study We design a case study to further motivate and validate the proposed TRACe framework. We sample 100 realistic world knowledge queries from the RAGTruth [40] QA training set, along with 2,512 unique context document chunks from the same dataset to use as inputs into our mock RAG systems. We simulate RAG setups of varying quality by controlling four configurable parameters: the retriever (sparse TF-IDF vs. dense), number of retrieved documents (2 vs. 4), generation model (GPT-3.5-turbo vs. GPT-4o), and 4 versions of the generation prompt template. For illustrative purposes, we evaluate one prompt template comprising of only the question (no context) vs. three RAG-style templates that encourage the LLM to utilize the provided context and/or utilize chain-of-thought (CoT). The full generation templates are provided in Appendix 7.7. We generate responses for the 100 RAGTruth queries with each of the 32 resulting RAG systems and use our LLM annotation prompt to evaluate the TRACe metrics. Figure 4 demonstrates how the different RAG configurations influence TRACe metrics. For example, we confirm that the choice of the generative LLM model affects the amount of hallucinations (or non-adherent responses) in the generated text. Not surprisingly, GPT-4o combined with a chain-of-thought prompt yields the lowest amount of hallucination compared to the weaker GPT-3.5 model and less detailed prompts. Curiously, we find that GPT-3.5 hallucinates more when prompted with CoT, which may point to limitations in the model’s ability to reason about complex concepts 4(b). Overall, we find that prompting the LLM to think step-by-step and explain its reasoning leads to higher utilization of the provided context and more complete responses (4(c), 4(d)). Finally, we show that the choice of the retriever affects average relevance of the retrieved context documents per query 4(a). 4 Experiments 4.1 LLM Judge We benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query GPT-3.5 with our annotation prompt, (2) RAGAS [9], and (3) TruLens [36]. RAGAS employs a series of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context Relevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness (Adherence) and Context Relevance. 4.2 Fine-tuned Judge We fine-tune a DeBERTa-v3-Large [11] NLI checkpoint333https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli from Laurer et al. [18] with one key architecture modification: we add a shallow prediction head for each of the output RAG metrics, which allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective and enables transfer learning from head to head through back-propagation down to the shared base layers. Each prediction head is a single layer feed-forward net that acts on the token-level output of the last DeBERTa layer. We attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and another head on the response tokens to estimate Adherence. For training, we broadcast sentence-level annotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and Adherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized spans and Adherent spans and calculate TRACe metrics",
        "<2-hop>\n\nRAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems 1 Introduction 2 Related Work RAG evaluation Finetuned RAG evaluation models 3 RAGBench Construction 3.1 Component Datasets Source Domains Context Token Length Task Types Question Sources Response Generation Data Splits 3.2 TRACe Evaluation Framework Definitions Context Relevance Context Utilization Completeness Adherence 3.3 RAGBench Statistics 3.4 LLM annotator Alignment with Human Judgements 3.5 RAG Case Study 4 Experiments 4.1 LLM Judge 4.2 Fine-tuned Judge 4.3 Evaluation 5 Results Estimating Context Relevance is Difficult"
      ],
      "response": null,
      "multi_responses": null,
      "reference": "RAGBench uses relevance and utilization to evaluate the quality of retrieval-augmented generation systems by directly identifying relevant and utilized sub-strings in input documents. These annotations help measure how well the context supports the response. The TRACe framework incorporates these metrics, and the high alignment with human judgments indicates their effectiveness in assessing system performance.",
      "rubrics": null
    },
    "synthesizer_name": "multi_hop_specific_query_synthesizer",
    "approval_status": "approved"
  }
]