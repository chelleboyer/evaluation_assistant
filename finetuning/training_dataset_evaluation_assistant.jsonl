{"questions": {"6ac2ecc9-5251-40f9-b8aa-1c0b2bf2b67f": "What are the key components and criteria used in the TRACe Evaluation Framework within RAGBench?", "d874aaca-b1af-460c-adda-59b66df00bc5": "How does RAGBench utilize component datasets to construct a benchmark for Retrieval-Augmented Generation systems?", "b7185d80-ae31-4864-b8b4-99ded47b302d": "What are the key components and findings discussed in the RAGBench Statistics and Case Study sections?", "9df7ada3-ed92-4333-8429-313875852a6b": "How do the LLM annotator and different judge models contribute to the evaluation process in the experiments?", "668fb562-182f-4aff-bd39-27d334d0b7a6": "What are the key components discussed in the annotation alignment with human judgements section of the RAGBench benchmark?", "ce57db82-d980-40ab-bcdd-c0f7380e66ee": "How does the RAG case study contribute to the understanding of retrieval-augmented generation systems in the context of RAGBench?", "252e2c0b-6e95-4c12-81c3-ffed2708a22c": "What company do Robert Friel, Masha Belyi, and Atindriyo Sanyal work for?", "ceba922d-50b6-4906-aa71-0dd9aaf72a1a": "What are the email addresses of the individuals listed from Galileo Technologies Inc.?", "2ab191a6-8a10-4853-9b53-6b380d3c6bd5": "What is the main focus or objective presented in the abstract?", "5b9f0b7f-3ff5-4dbf-a331-ea0855cd6062": "What key findings or conclusions are summarized in the abstract?", "32554a71-2a14-4ead-ad12-08a29a317795": "What are the two main components that characterize Retrieval-Augmented Generation (RAG) systems?", "c3613716-29e4-4166-9235-b1b2fe4cb34d": "Why is the comprehensive evaluation of RAG systems considered a challenge?", "347e5c6e-e7aa-4e05-b04b-504b5f7abd64": "What is RAGBench and what makes it significant in the evaluation of RAG systems?", "0c6b5d69-bbff-414c-8624-6610dd194641": "How does the TRACe evaluation framework contribute to the assessment of RAG systems?", "e400ad5f-648a-4f06-aa17-403b76e124c7": "What is the purpose of the TRACe evaluation framework in the context of RAG systems?", "4ed16539-2e53-447e-886f-52de3f7f6889": "Where can the labeled dataset for RAGBench be accessed?", "c22b9310-c9f4-4276-bfdc-24ce6d201332": "What challenges do existing methods face when compared to a finetuned RoBERTa model on the RAG evaluation task?", "e0fa379f-fed9-4b55-a9a7-a642089dd211": "How does the adoption of RAGBench with TRACe aim to improve RAG evaluation systems?", "4aa0aff4-9fb4-4d8f-818e-6f93d421c745": "What does the footnote \"Equal Contributions\" indicate about the authors of the document?", "6fbbbf73-b9d9-44cd-9577-0b4987095367": "In which section of the document does the introduction begin?", "06735255-4df0-4a99-a86f-94db2cc1a935": "What challenges do out-of-the-box pre-trained Large Language Models (LLMs) face when handling out-of-domain, knowledge-intensive queries?", "8759b02e-9082-4a6d-8b33-f80b1c762141": "How do Retriever-Augmented Generation (RAG) systems address the limitations of LLMs in user-facing dialogue applications?", "45d4295c-b223-4265-944e-6ff10737e8ec": "How does incorporating retrieved documents into a downstream LLM generator model affect its performance on knowledge-intensive tasks?", "3bb52b6d-a064-4eef-bba3-cc819ab75d5b": "What role does additional context play in grounding a large language model (LLM) in factual information?", "7bdbfc9e-ea3f-4d5f-ae98-fd71522cd5cd": "What challenges do RAG systems face when used in production settings?", "e50eeceb-7dbc-40e0-832e-4d97ab9f4f03": "Why is it important to fine-tune application-specific RAG systems for domain-specific tasks?", "5ad83387-b1bb-432f-a0b0-bcdef8e51144": "What challenges are involved in tuning an optimal RAG system for a particular application?", "df04320b-02e4-48ee-9dac-b541bf643f94": "Why is there a need for automated RAG evaluation solutions?", "6edc2a4c-db90-4776-a003-e7e05879830f": "What challenges arise from the lack of unified RAG benchmarks in evaluating retrieval-augmented generation systems?", "f0eb7e48-7c9d-4f7a-9521-c7bb82800a44": "How do automated RAG evaluation systems like RAGAS and TruLens predict evaluation metrics?", "5dc0fbfb-3f17-47ca-920c-0c84ae485a1d": "What are some of the limitations of datasets like RGB, AttributionBench, and RAGTruth mentioned in the context?", "8ce5a40a-639f-4f5d-b81b-a1360436fedf": "Which evaluation metrics do ARES and RAGAS use to assess the quality of retrieved documents and generative models?", "a0f33ca0-78a7-41c6-8d83-90da3399938c": "What are some of the metrics mentioned for evaluating studies in the given context?", "27e06c27-a366-4829-86bb-c34b79efbe3d": "Why is cross-domain generalization considered an open question according to the context?", "4ba0f1e0-9ed9-4724-8e64-11aa41a37dd4": "What are the new metrics introduced in RAGBench for evaluating RAG system performance?", "d60233bb-857d-4cc3-a8da-701175da8017": "How does RAGBench improve the evaluation of RAG models compared to existing metrics?", "2da5a328-33ee-44e4-a64a-f0278d70fa26": "How does the overall RAG system performance impact the effectiveness of the RAG practitioner?", "8d49d4e4-9399-4c3f-822c-d4c2505c7f8b": "What types of granular, actionable insights can the RAG system provide to improve its performance?", "d9165a30-0634-44df-9e58-55a8421f9dd7": "What model was found to outperform few-shot LLM judges across numerous domains and task types in the RAGBench evaluation?", "ee1e097f-a55e-40ad-84ba-a578713325e0": "What is the purpose of highlighting the performance of the fine-tuned DeBERTa-large model in the context of RAG evaluation?", "2811ed4e-44ed-445f-a509-5ed1ec936ae7": "How does RAGBench differ from existing ground-truth RAG datasets such as ChatRAGBench, CRAG, DomainRAG, and ALCE?", "7f6cf1c8-acf8-4de8-b7aa-84a72015bd21": "What is the primary purpose of designing RAGBench in terms of evaluating RAG systems?", "8cabec3b-4b08-4e7a-8e89-0c2043c46319": "How does the system evaluate retriever relevance across multiple dimensions?", "15a652c5-57db-41a6-bfe4-05fad3055c47": "In what ways does the system ensure adherence and completeness of the response?", "ea14b712-8bf9-4c1a-a99c-0816e39de95f": "What do various RAG benchmarks specifically focus on according to the context?", "aecc111e-0b43-423b-93b7-9c1906d3a9aa": "Which aspect of RAG benchmarks is highlighted in the provided context?", "dc64991a-e040-426e-ac26-4ea4c3086cc9": "What are the key differences between FELM and RAG-specific datasets like RAGTruth, DelucionQA, and HaluEval in terms of their focus and labeling?", "487163ae-ec6f-4643-892a-3fa87b91d9a6": "Why does RAGBench provide a more granular understanding of the RAG system compared to existing hallucination detection benchmarks?", "8e6fba8b-b0b7-46cb-ac0e-6adbecfc7770": "What does RAG evaluation stand for?", "c9c0f378-8993-4b5c-9e45-1ff4e4b7caa1": "How is RAG evaluation typically used in assessing performance?", "d3cd89e5-b235-4c5b-9cd2-84c5dc530249": "What metrics does the RAGAS approach use to evaluate a RAG response?", "4f079def-e676-4b23-807f-bd8728a32b38": "How does the ARES framework proposed by Saad-Falcon et al. differ in its approach to evaluating RAG responses?", "441d4cdf-5571-43e7-8866-9199a3f6b216": "What challenge does the lack of established RAG benchmarks present in comparing different approaches?", "b152e915-7b3f-4d70-87da-a93268e8704f": "How does RAGBench aim to address the limitations in evaluating RAG correctness and faithfulness?", "ca11cadf-ebaf-4363-a308-aeb2f7a32c45": "What are the key metrics used to evaluate finetuned RAG models?", "82d25ff6-0290-472f-9894-89be161f33c2": "How does finetuning impact the performance of RAG evaluation models?", "b9ba1f5a-2797-4529-8d18-aa0cccf3116b": "What role do fine-tuned Natural Language Inference (NLI) models play in RAG hallucination detection?", "004a3330-e0f1-457c-9715-a6bdbe9e47b3": "How is the NLI model utilized in the context of RAG evaluation in this work?", "eb723fa9-2339-49ff-a04c-473de1a2c2ef": "What advantages does the fine-tuned NLI model demonstrate over LLM judges in hallucination and attribution detection?", "41af23c3-b775-4ebe-9c00-0f2bd707e134": "How does the fine-tuned NLI model perform on the new RAG evaluation metrics proposed in RAGBench?", "4ad9571e-25b9-46c1-8696-aae25abc895c": "What are the four variable parameters highlighted in the RAG system workflow shown in Figure 1?", "8c90acd0-0477-4bb6-b2b1-6d1866e4ce83": "What is the focus of section 3.1 in the RAGBench construction?", "e75f216d-57f4-4449-9f0b-4229343079f6": "What types of datasets are included in RAGBench?", "1cfbc678-a87b-4f1e-b37d-516ed3bb1668": "How many component datasets are transformed into the standardized RAG format in RAGBench?", "68f7f119-4ece-45a2-a9f9-ca7a284e7563": "What parameters are varied to construct the benchmark in real-world RAG scenarios?", "09be2ba5-08ae-42e1-aa3b-0d27fbb7f343": "Where does Figure 1 illustrate the variable parameters in the RAG pipeline?", "caff12e9-c83e-4cce-824d-4cfcd74df2a5": "What are the five distinct domains included in RAGBench?", "46000b59-5502-4996-ae28-2b96c10e517f": "On what basis were the specific domains in RAGBench selected?", "8bbc254c-29c6-41db-87d8-25778185a411": "What is the range of context token lengths reported in RAGBench?", "0c17ee53-0dcc-42f5-9ce0-ffafead3f6ae": "How do the context lengths of CUAD documents compare to those in PubMedQA?", "410199f9-5727-4636-beec-7416f1c452c0": "What are the different domains covered by the datasets mentioned in the context?", "716163cd-820c-42ff-97de-bc00f00ab056": "Which datasets utilize expert sources, and which rely on crowd-sourced or user-generated content?", "617b248e-5ff4-451e-91fd-31ea9cc6d6ff": "What are the different domains and expertise areas mentioned in the context for the various QA datasets?", "3c694402-416c-4df4-a184-b7954f08c365": "How many annotators or experts are associated with each QA dataset listed in the context?", "6fd5d27c-e7d4-47a8-8b06-f44e5e502ecc": "What are the main categories or topics discussed in the tech forums and finance-related forums mentioned in the context?", "5caf4469-9d6a-47e8-8c24-6dcb0eea7389": "How do the expert contributions compare between the FinQA and TAT-QA finance forums based on the provided data?", "bdfecb59-fbe8-4965-a52f-5eea22771b8a": "What types of tasks are included in the RAGBench dataset to test retrieval-augmented generation models?", "52217561-1cd9-4517-94e6-356f91fa2f2d": "Why is the CUAD dataset considered a challenging addition to RAGBench?", "ba267703-5f09-4b40-a01e-91b291eff982": "What challenges do off-the-shelf pre-trained LLM models face in the real-world domain mentioned?", "46e223cb-e1d0-44ab-abdb-1b1a1a5c78ee": "Why is the RAG context particularly challenging when dealing with legal contract documents?", "c8e4d6b5-6b22-49f5-8bad-33acf1e51195": "What are some common sources of questions in research or surveys?", "67dabf51-c85c-43cf-9942-18356d32e362": "How can the source of a question impact the quality of the information gathered?", "5b18f24c-04ba-4a47-841e-bce84e553cc6": "Which datasets include domain-specific questions composed by domain experts?", "c4f5733d-7f5e-4e01-91f8-a417d548aa53": "How are the questions in the MS Marco and TechQA datasets sourced?", "42e9f803-d106-46ef-b120-3230a7814183": "What are the benefits of using automatically-generated questions from research article titles?", "744a1705-ccdb-437d-8800-08a433b4b15a": "How can automatically-generated questions improve the research article review process?", "fb0500cb-43f3-4def-8826-3e831d90a15e": "What are the key components involved in response generation?", "c580ee6b-54e6-42fd-869c-9b40634ebe68": "How does response generation impact user interaction in conversational systems?", "7ec1c1a0-0b45-4427-b8b1-0f7a87be1e72": "What are the two modes used to generate responses for each component dataset?", "ea462615-b749-4a73-9fc4-590e204f29f7": "Which datasets already contain LLM-generated responses in the original data?", "bb2faf40-a4c3-4e11-8ccc-95703f346739": "Why are responses for CUAD only generated with Claude 3 Haiku instead of GPT-3.5 16k?", "0a914d29-a6f4-4ac9-a2cb-f092f9c57e0e": "What is the purpose of using a basic prompt that does not require the model to stick to the provided context in RAGBench?", "92acfa29-cfc5-4c36-9569-d05580cffadb": "How are the component datasets split in RAGBench to ensure no overlap in queries?", "282c7e45-2e2c-49ad-a309-694e051b6435": "What is the total number of samples in RAGBench, and how are they distributed across the different data splits?", "1c99f918-8d5c-4711-a92d-357ac8fe0db7": "What are the two main components of RAG that the proposed metrics aim to evaluate?", "0e05b57e-daaa-4eeb-9bd2-8fa307aadb35": "Why is it important for the retriever to avoid returning superfluous data in an optimal RAG system?", "eedb9202-0e56-49ff-a0d2-55aa06ebebe4": "What is the main instruction given in the context?", "20784a72-80e9-47a1-bbd4-0aa492f4229a": "How many questions are you required to generate based on the context?", "76d1d071-8cbb-475f-bfc6-d4f09e43ac69": "What are the four criteria measured by the TRACe evaluation framework in assessing a RAG system?", "b106d2b9-3d31-42c2-a376-b3f3614dd631": "How is the term \"Adherence\" defined within the TRACe framework and what other terms in literature are synonymous with it?", "64328b24-489b-47d1-aec3-f91068294b68": "What does relevance measure in the context of retriever output?", "965014ca-d700-4000-ab9d-026e841139dd": "What is the purpose of the formalization mentioned in the context?", "a7720017-2fef-4c94-b005-b7f2a084d65c": "What is the importance of having clear definitions in a given context?", "66321d2d-acc0-4dd6-b398-926ecff55100": "How do definitions help in understanding complex topics?", "7ac6fd48-dab4-4680-ada2-940571715e22": "What does the set D_D represent in the context of RAG input queries?", "cb427c27-f779-4f22-9e82-470412b39030": "How is the set of relevant tokens R_i defined for a document d_i in the given context?", "0b48a429-389d-4098-8411-587e86c7703c": "What does the notation \\( R_i \\) represent in the given context?", "3aa36fba-b3f1-4186-a801-388010c00b8d": "How is the set \\( U_i = \\{t_1, \\ldots, t_u\\} \\) defined and what might it signify?", "b879e406-8829-491a-a270-2bf297851e32": "What does the set \\( i = \\{ t_1, \\ldots, t_u \\} \\) represent in the context of document \\( d_i \\)?", "058d0250-9cae-4689-82d9-411d3f1179b9": "How is the set of utilized tokens related to the information used by the generation model to produce a response?", "6a7a452b-b955-4539-9b4b-d4d64dd6efd0": "What does italic_e italic_n ( italic_x ) measure in the context provided?", "38e4a8f0-282d-419e-be14-153539480672": "Why is sentence-length preferred for calculating ground-truth metrics according to the context?", "b2878f5d-0798-4c7c-942b-1c76b69701ea": "What does Figure 2 illustrate about the relationship between question, context, and response in RAG?", "f191bce0-4d4b-4ffb-bf39-bee95933e4ff": "How are relevant context spans indicated and utilized in the example shown in Figure 2?", "e84911dc-1bed-48dc-aaa2-17f2ce6547bb": "How is Context Relevance defined according to references [9, 32]?", "67b4b0d9-0253-455c-a79e-8c1994e05a8f": "What are the potential drawbacks of low context relevance in information retrieval systems?", "c36e56e9-f6c3-4f79-930a-b7693441c671": "How is document relevance calculated according to the given formula?", "ee54e0a7-523a-4794-9377-1181534653a7": "What does the example-level relevance represent in the context of aggregating document relevance?", "39d7fcac-1787-4321-b68b-b2e77501f077": "What does the formula for example relevance represent in terms of the lengths of \\( R_i \\) and \\( d_i \\)?", "e95447f6-a398-4210-bd07-a033cf82dfdf": "How is the summation notation used to calculate example relevance in the given context?", "33468818-829a-4161-9500-b06444180b78": "What does the formula for example relevance represent in terms of the variables \\( d_i \\) and \\( R_i \\)?", "1282d0bf-37cb-4304-bcb6-ef417b1a6f79": "How are the summations and norms used in the calculation of example relevance according to the given context?", "b4cb0f98-6d0f-4c01-b41d-c568e11198b0": "What does low Context Utilization combined with low Relevance indicate about the retriever in TRACe?", "e4555e5e-352f-4c50-aecc-3d2c0cdb3da9": "How is Context Utilization defined at the document-level and example-level in TRACe?", "7e8117bc-dae7-40ef-aa93-fa93c5d7e6b4": "What is the formula for calculating document utilization as given in the context?", "98521bf8-107c-4f9c-a935-a0742ac95634": "How is example utilization expressed in terms of the lengths of \\( U_i \\) and \\( d_i \\) across the document set \\( D \\)?", "895d3351-1d81-4e03-8cd5-afb58bf2c0a6": "What is the formula for calculating document utilization as given in the context?", "f933b361-594f-4647-9a43-f21b12794cee": "In the utilization formula, what do the terms \\( Len(U_i) \\) and \\( Len(d_i) \\) represent?", "6120c9b9-15dd-4bfa-a1d5-fab9a25035c2": "What does the summation notation \u2211 from i=1 to |D| represent in the given context?", "b6103ef2-93cc-4c66-b5cd-089e4bc39e87": "How are the functions \\( \\ell_n(U_i) \\) and \\( \\ell_n(d_i) \\) related within the summation expression?", "ffdf34b6-8a50-408b-9c21-f0ef0d5f300b": "What does the number (3) refer to in the given context?", "8d6d51b2-5fec-496e-b16d-257f3a44d3cc": "How is completeness related to the information provided in the context?", "604fb1a2-d1ef-4c9e-8f93-0edccb2b69aa": "What does the metric Completeness measure in the context of response evaluation?", "47e30937-2956-4e88-8467-c2170f3d767e": "How can a response have high Relevance and Utilization but still have low Completeness?", "433412f6-1585-4a25-baaa-f2cbdbae6077": "What does end_POSTSUBSCRIPT represent in the given context?", "913205b9-68f0-4ad5-9fb8-b57505907c9f": "How is end_POSTSUBSCRIPT calculated according to the provided information?", "ef528a66-3b50-4fd0-ab63-86367dee0020": "What does the formula for completeness represent in terms of the sets \\( R_i \\) and \\( U_i \\)?", "7a04d537-ab40-4b64-a320-3952bc7ba51c": "How is the completeness value calculated using the lengths of the intersections and the set \\( R_i \\)?", "3c393dbf-312b-4250-81ce-31d400736f75": "What does the number (4) refer to in the given context?", "1d36b34d-628b-415c-9b9f-acc7974a067e": "How can the concept of adherence be extended to example-level by considering relevant substrings across context documents?", "8213a1d2-5378-4b85-b9c4-726f1bf9f8db": "What is the definition of Adherence as described in the context?", "0b84c831-9e85-46f5-8cd7-b430b71dfe68": "How is example-level adherence defined for alignment with existing hallucination detection approaches?", "7fcb5cde-1196-42ce-b5a4-7d1b59aec2f2": "What does the set \\( A_i = \\{ t_1, \\ldots, t_a \\} \\) represent in the annotation schema described in Section 3.4?", "225a337a-6193-4850-83fe-59d921070d48": "How is the set of response tokens \\( A_i \\) used to enable granular Adherence evaluation?", "76c483d6-8459-48e4-83d8-4c9e48e1b7d0": "What types of label distributions are shown in Figure 3 of RAGBench?", "3e3a94d4-19a9-40ed-b4b5-39f4b311c97d": "How is the Y-axis scaled in the distributions presented in Figure 3?", "daa5bfb7-7b86-434b-bb44-f3ce3ca33a75": "What percentage of hallucinations do the ExpertQA, CovidQA, and MS Marco datasets contain according to the RAGBench statistics?", "04d22d9d-73cb-4d14-a11c-ae43c9b6bb91": "Which datasets in RAGBench have the lowest fraction of hallucinated responses, and what is their approximate percentage?", "91436727-9e85-4594-bb9f-9e6a89c47390": "What labels does GPT-4 produce for input tuples in RAGBench according to the 3.4 LLM annotator context?", "c5bea7af-6803-4331-acf5-a58752dd0d56": "Why are explicit annotations for Completeness not requested in the 3.4 LLM annotator process?", "4133f339-2942-4093-8366-9b6460337619": "What technique is used to ensure high quality labels and maximize the correlation between GPT-4 and human judgments?", "8b8dc0e8-95ed-4092-ba8a-8900dbb9ed22": "How does the LLM-annotator identify relevant and utilized information in the input documents?", "2e262bc2-7ef2-4118-969a-e2463f5b3a48": "What method is used to determine if response sentences are supported by the provided context?", "b9fc7e99-996f-4994-b6cb-280121c2cc16": "Where can the details of the exact prompt and post-processing steps be found?", "60805b32-271f-4a21-903c-51fe9fd4ef17": "What metrics are used to evaluate the alignment of the GPT-4 annotator with human judgments in the DelucionQA dataset?", "bacb3ba9-3f7f-4f14-b845-899340495c3f": "How does the GPT-4 annotator perform in terms of F1 and Accuracy on adherence annotations in the DelucionQA example level test set?", "cde3af8d-0841-47de-bd36-571170f181dc": "What are the adherence scores for DelucionQA at the example level and span level?", "282583e2-bc45-4c21-93c2-524e5b707237": "How do the utilization and relevance scores compare for DelucionQA(40) at the span level?", "ea380e63-22fd-4eb8-a047-70349b624413": "What is the source of the context documents used in the DelucionQA benchmark?", "fe323902-dbd4-4df0-88d4-fa791a78a9a0": "How are the natural language queries in DelucionQA generated and processed before use?", "2305283a-c77b-45f1-9aa2-a6c54ad31141": "What does DelucionQA represent in the context of hallucination annotations?", "abdc4ad1-3d89-4a04-ad1c-b8cfacffa756": "How does the GPT annotator's agreement with human judgments on the DelucionQA test split compare at the example and span levels?", "19843cd0-642d-4b88-a785-5fd33670cf5b": "What is the purpose of annotating the DelucionQA(40) subset with granular relevance and utilization labels?", "f876200c-ef0f-4082-a58c-db1c8a293d39": "How do the relevance and utilization judgments from GPT-4 compare to those from humans according to the context?", "5d22515e-7f30-4be8-adf5-55677c9a40ec": "What are the key differences between LLM and Hallucination as discussed in the context?", "849368fc-30c6-4e27-b035-ea87f6c51663": "How do Prompt and Utilization compare in terms of their roles and effectiveness?", "beca93d4-fe00-4a68-a746-1f86d1dc6491": "How does the configuration of the RAG retriever component impact the average relevance of the retrieved context?", "1ffd055d-cab5-4f65-a594-c3876c70e0d8": "In what way does the choice of LLM and generation prompt influence the RAG system's utilization of the provided context?", "9c96eecd-540b-488c-bd8c-4b98d046ded7": "How does a detailed chain-of-thought prompt affect hallucination rates in responses?", "842fe4db-8bfc-4493-a52f-b19781325557": "What impact does a detailed chain-of-thought prompt have on response utilization and completeness rates?", "c95b0bba-e2fc-4f00-a654-1e003dd0e754": "What are the key components discussed in the 3.5 RAG Case Study?", "22fc3488-9111-41a4-a14f-bef2744d3b69": "How does the 3.5 RAG Case Study illustrate the application of RAG methodology?", "3c1a8d5e-5357-45b1-ad2c-110d875d852c": "What are the four configurable parameters used to simulate RAG setups of varying quality in the case study?", "4943e313-241a-4f00-b5f3-33f60eb8f9a3": "How many unique context document chunks were sampled from the RAGTruth dataset for the mock RAG systems?", "ca3c3cfa-4611-43d1-949b-82909547f091": "What are the different variables compared in the evaluation of the 32 RAG systems?", "8102d78a-8ce0-474e-bc45-b01741be5e48": "How do the prompt templates differ in their approach to encouraging the language model's response generation?", "7c15ad97-116d-41bf-8889-f923a249cc74": "What are the 32 resulting RAG systems mentioned in the context?", "28cc727d-9b98-4f18-8288-537974a2a56c": "How is the LLM annotation prompt used to evaluate the TRACe metrics?", "ae6896e4-d8fd-464f-b995-82c239cd42e8": "How does the choice of the generative LLM model affect the amount of hallucinations in the generated text according to Figure 4?", "3fdae308-438b-449c-b5a9-6d7a3d7fa42b": "What impact does using a chain-of-thought prompt have on hallucination rates for GPT-4o and GPT-3.5 models?", "d3ac34f0-1e4d-4d4d-8c5f-4a9cdbbcf158": "How does prompting the LLM to think step-by-step and explain its reasoning affect its utilization of the provided context?", "8b4812be-e89d-490e-98f5-96069ac50018": "What impact does the choice of the retriever have on the relevance of the retrieved context documents per query?", "253aaf64-eeaa-4013-811b-0cdd9bf0a952": "What are the three LLM evaluators benchmarked on RAGBench mentioned in the context?", "8219c682-7a12-457a-a10b-ed0d34df44f4": "How do RAGAS and TruLens differ in their approach to measuring answer quality?", "ec9e5cc6-a2cd-4771-a161-44068ee432a6": "What key architecture modification is made to the DeBERTa-v3-Large NLI checkpoint in the fine-tuning process?", "923bbdd9-ffd1-4b44-860a-57a61dd5660a": "How does adding a shallow prediction head for each output RAG metric benefit the computation of TRACe metrics?", "5602197b-687a-4280-a239-d3a046ecbb86": "What type of neural network is used for each prediction head in the model?", "5225184a-8cd1-4a75-b75a-47987d0d6703": "On which output do the prediction heads operate in the described architecture?", "33ae719b-9c67-46ed-9640-87a0a24b413d": "What are the three types of probabilities estimated by attaching heads to context and response tokens?", "3fdc4738-cb41-4927-8697-f3a8e925cae5": "How is the probability threshold used during inference to predict Relevant, Utilized, and Adherent spans?", "56ded4c5-cb46-4a06-a88c-f6fb9f6da484": "How are Adherence probabilities aggregated to produce an example-level response adherence label?", "5bd375e3-de45-4317-920a-bc49d22c93e2": "Where can details about training and hyperparameters be found?", "f7122e63-62b3-4314-99f8-6896ed8a69e1": "What evaluation metrics are reported for the response-level hallucination detection task?", "22b5b310-1beb-47ff-a55c-3142062e84ee": "How does the granular annotation schema affect the evaluation setups?", "4e4f9d92-9d3d-4162-a68d-68e73e7d2acf": "What metrics are used in Table 3 to evaluate the performance of models on test splits?", "dbfb2f51-a154-4f2a-876a-9cf31165b750": "Which models do not evaluate Utilization according to the provided context?", "554de090-a6b1-4b5d-98ce-2d1a6ab7b7b6": "What trends can be observed in the Hal, Rel, and Util metrics across the PubMedQA, CovidQA-RAG, HotpotQA, and MS Marco datasets?", "c9a4cf6b-d84e-4416-bae5-d8d0aa26987e": "How does the performance indicated by the Hal metric compare to the Rel and Util metrics within each dataset?", "efefd776-b579-497b-a2fb-492ed9ffb349": "Which model shows the highest value marked with an asterisk (\u2217) in the given data?", "96f736ef-7955-483f-95f2-b3e68a19ea8f": "What are the values associated with the model named HAGRID in the provided context?", "3fd523b3-c944-4b66-8e03-7751e565d468": "What are the performance values indicated for the TechQA dataset in the given context?", "0e71e6ba-fe56-496a-9f01-2ca54c38c29a": "How does the CUAD dataset's performance compare to that of the FinQA dataset based on the provided numbers?", "748a91ee-77ff-41d5-968b-e4748a5aa383": "What does Table 3 report in the context of RAGBench component datasets?", "bae43270-202a-4d16-b1c0-aab2480993e1": "How does the finetuned DeBERTA encoder compare to baseline LLM methods according to the results?", "1d699c0f-ff02-4a93-8b1c-8887ac66be13": "What range of AUROC scores does the finetuned DeBERTa model achieve on the hallucination detection task in RAGBench?", "f42cc190-ac6a-4650-a806-12437a1230ae": "How does the RMSE for relevance and utilization vary across different domains and tasks according to the DeBERTa model's performance?", "f3bad1d5-6498-473f-888a-86398e7f2fe0": "Why are Relevance RMSE scores generally higher than those for Utilization according to the context?", "638ff4ba-38ed-4030-b51b-4fa716b9c446": "How does the nature of RAG affect the challenge of predicting relevance compared to utilization?", "c6d84534-77c1-4e04-baa6-8eaf5adfac8f": "What must the model determine about the provided context to accurately answer a question?", "71a989bb-45f6-4031-8956-395f2437f3e9": "Why does the task involve both deriving the correct answer and assessing the information in the context?", "059fc40d-cde2-4258-9ac0-4bf6fc9cbc07": "What are the key findings summarized in the conclusion section?", "7952a16a-0a2d-4d1d-ac36-6120336021e1": "How does the conclusion address the implications of the study or discussion?", "cee6fb53-cb82-4a5a-9543-cdd190499717": "What is the purpose of the RAGBench dataset introduced in the paper?", "659aad35-f836-4edb-bd08-3cb1d10b95c1": "What are the four metrics included in the TRACe evaluation framework for RAG systems?", "45cf80c3-80d4-4cf1-ae58-778f3ff5e069": "What approach is proposed to generate TRACe labels for RAGBench?", "6d4f2a16-6ec6-4c9c-8e8e-09af9773d1b9": "How does the automated approach's performance compare to human judgments?", "c5ba53f5-44c8-48f5-8e4f-584e5f891611": "How does a 400M parameter DeBERTa model finetuned on RAGBench data perform compared to billion-parameter LLM Judges and commercial RAG evaluation systems?", "30f42dda-6c2b-4e1a-b850-de1bdd2d6634": "What future work is motivated by the current performance gap between the best-performing RAG evaluator and ground truth?", "e3da1f9a-2e9c-497a-8ef3-7499d7378bdc": "What methods can be used for narrowing the performance gap between these models and the ground truth?", "579c9f1e-9356-4fac-a9a6-1868ee93a34d": "Why is it important to narrow the performance gap between these models and the ground truth?", "1812cad4-d6d3-4159-a8f3-319df704776c": "What is the primary purpose of the contributions mentioned in the context regarding RAG systems?", "7faa24b6-9eae-4e25-a0f6-657d2fad3feb": "How do standardized benchmarks and methodologies impact the development of retrieval-augmented generation models?", "2a19b927-72fc-4f19-94f7-2b22be9af876": "What methods are used to evaluate the correctness and faithfulness of instruction-following models for question answering in the study by Bohnet et al. (2023)?", "ea552173-2f84-49fe-9dd2-65befd41e4cd": "Who are the key contributors to the research on evaluating instruction-following models for question answering as presented in the arXiv preprint arXiv:2307.16877v1?", "61b13019-448a-445a-9989-6da49473458a": "What is the focus of the 2023 work on attributed question answering for large language models?", "1d02021e-b531-4abf-8d4c-9e7ec38e0eee": "Who are some of the contributors to the TechQA dataset mentioned by Castelli et al. (2020)?", "f8b1a899-5a3e-4d8f-9746-4babf3641836": "What is the focus of the paper presented at the 58th Annual Meeting of the Association for Computational Linguistics in July 2020 related to the TechQA dataset?", "6cd9097f-40b4-4084-9cb8-d5ff20ef92f3": "Who are the authors of the 2023 study on benchmarking large language models in retrieval-augmented generation?", "a92480bc-e549-4043-90d9-42e5cf8e0377": "What is the main focus of the paper \"Felm: Benchmarking factuality evaluation of large language models\" by Chen et al. (2023)?", "fe941c1b-a6bc-4fd3-89b8-d5a86a36af21": "In which conference proceedings was the paper by Chen et al. on factuality evaluation of large language models published?", "34c74176-68d6-480b-b5ce-42aa91b84ca7": "What is the primary focus of the FinQA dataset introduced by Chen et al. [2021]?", "47d01dd3-52c1-4942-a156-86fb9c5236bd": "Who are some of the key contributors to the FinQA dataset as mentioned in the context?", "01a852c6-2e89-4582-923c-d0129d86edd4": "What is the title of the conference where the paper edited by M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih was presented in 2021?", "6af626d1-9272-4798-b03f-83fe5f06b891": "Where can the paper with the DOI 10.18653/v1/2021.emnlp-main.300 be accessed online?", "a21def2b-99f4-4c4a-bb7d-fb3904e94b2a": "What are the main concerns discussed by Chiesurin et al. (2023) regarding the trustworthiness of stochastic parrots in open-domain conversational question answering?", "c69d1908-7a93-43f4-a04d-748d2569ffac": "In which publication and event was the work of Chiesurin et al. (2023) on faithfulness and trust in conversational question answering presented?", "c1af4256-6475-4bec-992b-aafc6fa8d86e": "What is the main contribution of the paper titled \"Wizard of Wikipedia: Knowledge-powered conversational agents\" by Dinan et al. (2019)?", "5da3655b-1e26-4afb-a16b-9f492f61b824": "What does the 2024 paper by Es et al. propose regarding the evaluation of retrieval augmented generation?", "c3806fef-bf31-46ad-916e-7749ff11807e": "What is the main focus of the work by Gao et al. presented at the 18th Conference of the European Chapter of the Association for Computational Linguistics in 2024?", "301bbbaa-928a-44a6-aa1a-d3e192126399": "Who are the authors of the paper titled \"Enabling large language models to generate text with citations\"?", "02247380-8d2a-4dce-9dd4-1d65347e9f5a": "What is the title of the paper authored by P. He, J. Gao, and W. Chen presented at the 2023 Conference on Empirical Methods in Natural Language Processing?", "52b4abff-dcf0-4797-b0de-a3996de12e9f": "What pre-training method is used in DeBERTav3 to improve deBERTa according to He et al. [2023]?", "af135dd4-81fd-484b-89dc-f2d176c554c9": "What is the main focus of the paper presented by Hendrycks et al. in NeurIPS 2021?", "a3615f51-d8e5-4b37-95f8-880e2deba530": "Which conference featured the work titled \"A survey on retrieval-augmented text generation for large language models\" by Huang and Huang in 2024?", "7c9f0047-25e1-41e1-b16d-e6ad680151ad": "What is the title of the dataset introduced by Jin, B. Dhingra, Z. Liu, W. Cohen, and X. Lu in their 2019 publication?", "7cf6c9f3-ec51-4f4f-89a9-02778b2e4aa2": "In which conference proceedings was the paper on PubMedQA published?", "b8c119b1-b56b-485a-bc90-a8195b3aff3b": "What is the main contribution of the Hagrid dataset introduced by Kamalloo et al. in 2023?", "379ba222-574b-4dcc-85ad-b66b53ac2930": "How does Prometheus 2, presented by Kim et al. in 2024, specialize in evaluating other language models?", "ebe38151-64c8-4f88-a7d0-a9d46a14a1e1": "What is the main contribution of Kwiatkowski et al. [2019] in the field of question answering research?", "c0c2ce4d-7db1-42c6-9334-735035443193": "Who are the authors of the work cited as Laurer et al. [2022]?", "6611a2c4-1fc5-4c62-9e13-6d3ba7675c27": "What approach does the paper \"Less annotating, more classifying\" propose to address the data scarcity issue in supervised machine learning?", "34588603-e82a-4df3-a089-36fba0e44966": "How does latent retrieval contribute to weakly supervised open domain question answering according to Lee et al. (2019)?", "44e14305-7a7b-420c-b5ec-8732e32fffcd": "What is the title of the conference where the paper edited by A. Korhonen, D. Traum, and L. M\u00e0rquez was presented?", "a3f22266-8579-4141-8932-310e40ea0520": "Who are some of the authors listed in the reference to Lewis et al. [2020]?", "14930bf3-732c-44ba-a349-de234e42c012": "What is the main focus of the paper titled \"Retrieval-augmented generation for knowledge-intensive NLP tasks\" presented at NIPS 2020?", "329590e0-f4fe-44c9-a8c8-b8bd2e22a4dc": "What is the purpose of the HaluEval benchmark introduced by Li et al. in 2023?", "eb5bcb00-f38d-48cc-a7d1-66ebe2e347b0": "What is the title of the paper authored by Y. Li, X. Yue, Z. Liao, and H. Sun in 2024?", "dfc9303c-55da-4fa9-a1e0-cd088a474bee": "In which conference proceedings was the paper edited by H. Bouamor, J. Pino, and K. Bali published?", "ba146f4b-8603-49af-beaf-c928f3733a1c": "What is the main focus of the work by Liu et al. (2024) as described in the context?", "e3d23a74-f4cd-4da3-aaac-03f4d4a6cae3": "Which aspect of AI legal research tools is assessed by Magesh et al. (2024) according to the provided information?", "2d46db04-bd82-4ec1-910b-cf88ba4155ff": "What is the focus of the COVID-QA dataset introduced by M\u00f6ller et al. in 2020?", "f50fa441-9fec-4a92-9da6-2ef28f4f5f6f": "In which event was the COVID-QA dataset by M\u00f6ller et al. presented?", "f258440d-b78d-4daa-bc14-71de23f2e599": "What is the main focus of the benchmark dataset introduced in the paper \"Question answering over electronic devices\"?", "d6b11936-ac0a-480f-a44d-a20493e7813d": "How does the multi-task learning based QA framework proposed in the paper contribute to question answering over electronic devices?", "ecbf30a2-307c-45d9-a640-2a2ff0a61862": "What is the MS MARCO dataset introduced by Nguyen et al. in 2016?", "ea699e13-24b2-49f8-8e22-deee382f6b25": "Where can the MS MARCO dataset by Nguyen et al. be accessed online?", "e23e0c9f-5573-4aba-985b-649a6224944c": "What is the main focus of the KILT benchmark introduced by Petroni et al. [2021]?", "d4c2f60d-3adf-4be3-8129-2d0fe5cf1f76": "Who are some of the key contributors to the KILT benchmark as listed in the context?", "864284dd-f8ce-4b5e-9e49-c8e86387e9dc": "What is the title of the conference where the paper edited by K. Toutanova and others was presented in June 2021?", "ba3e8695-ee5b-4349-b97f-d564beceabbf": "Where can the paper with DOI 10.18653/v1/2021.naacl-main.200 be accessed online?", "768d9dba-da04-41a9-9b2e-c41fe0290b38": "What is the main focus of the research conducted by Rashkin et al. in their 2021 paper?", "1e41d031-09e0-4b99-917f-3dfa2be6c5c5": "In which conference proceedings was the paper \"Increasing faithfulness in knowledge-grounded dialogue with controllable features\" published?", "bbb84668-6086-420a-bfc8-217486bd96bc": "What is the main focus of the paper authored by Rashkin et al. (2023) titled \"Measuring attribution in natural language generation models\"?", "72441e66-e46d-47cb-bf59-f0336423bca4": "Where can the paper with the DOI 10.18653/v1/2021.acl-long.58 be accessed online?", "d451e6e9-acbb-4f61-bb04-ed1b191ca96a": "What is the primary focus of the Ares framework mentioned in the context?", "64a9cd9b-b89b-438b-b875-13836fbdaaac": "Which study is associated with detecting hallucinations in domain-specific question answering?", "d72ded49-04db-4cec-85be-ac91e65cb196": "What is the main focus of the research conducted by Siriwardhana et al. (2023) as described in the provided context?", "9725769a-0a86-412e-a6d5-f1958657c76b": "In which publication and year was the work of Siriwardhana et al. on improving domain adaptation of RAG models published?", "ee57677b-75e0-40b2-b904-c8b4a0265cc8": "What is the main focus of the FEVER dataset mentioned in the context?", "9a631343-4ad6-4694-bf2b-5386377067a6": "In which conference proceedings was the FEVER dataset paper published?", "7973498e-251a-4ffb-8e1d-1fb92961255e": "What is the primary focus or purpose of the Trulens project mentioned in the 2023 reference?", "60cdfeb3-85dd-41c3-9699-70fc9186ed05": "What type of data does the CORD-19 dataset, introduced by Wang et al. in 2020, provide?", "bbf78094-5694-4cb8-bcaf-06efdb59cea5": "What is the CORD-19 dataset mentioned in the context?", "74c0f1ac-2fec-41f8-9207-19a8633eaa28": "Who are some of the editors involved in the Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020?", "218050c1-f8a9-43a3-b6f0-06030c0e24bb": "What is the purpose of the Domainrag benchmark introduced in the 2024 paper?", "3e74bfe3-7b07-4eff-90c0-d5d384f29b51": "How does chain-of-thought prompting contribute to reasoning in large language models according to Wei et al. [2022]?", "e7e2f106-4fe0-4add-998e-39bc8966a221": "What is the title of the publication edited by S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh in 2022?", "d4f18c68-638b-4cb9-80b7-bbdf7a449b38": "Who are the authors listed for the work by Wu et al. in 2023?", "aa0d96f6-f87a-4dd4-9bd2-ae2e20260e50": "What is the focus of the Ragtruth corpus mentioned in the context?", "d8c12c42-a1c8-4bb3-8ff5-3ab23df4d04e": "Who are some of the authors involved in the development of the Crag benchmark according to the provided information?", "778847b0-4ba4-4408-a39c-ba0875f02bac": "What is the main contribution of the HotpotQA dataset introduced by Yang et al. (2018)?", "c55576fa-06c5-4ba6-b2e6-bd64036d3fa8": "How does the FLASK framework proposed by Ye et al. (2024) evaluate language models?", "3a8a1f80-9a6e-4813-b404-c5031999c8ac": "What is the main topic addressed by Yue et al. in their 2023 paper presented at EMNLP 2023?", "5c0523ad-051d-4591-aa8b-ec16402cced0": "In which conference proceedings was the paper \"Automatic evaluation of attribution by large language models\" published?", "75896c2c-7971-4aeb-a738-67ffeb6ef24a": "What is the main focus of the work by Zhang et al. (2022) titled \"Making a miracl: Multilingual information retrieval across a continuum of languages\"?", "fe2b6901-c84f-47f2-9b82-8846fe9d6a97": "Who are some of the authors involved in the 2023 study by Zheng et al. referenced in the context?", "278b8cb4-cb33-48ae-a795-554cf5fc2c25": "What is the main focus of the paper titled \"Judging LLM-as-a-judge with MT-bench and chatbot arena\" presented at the Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track in 2023?", "93b75144-ea45-4339-b69e-7663e32dbf74": "What type of content does the TAT-QA benchmark, introduced by Zhu et al. [2021], combine for question answering in the finance domain?", "0e98bb1a-7b7a-477d-af47-3636a32c4f0d": "What is the title of the conference where the paper by C. Zong, F. Xia, W. Li, and R. Navigli was presented?", "fbd39819-e49d-4926-ba4e-c67503c855e6": "What section of the document contains information about RAGBench code and data?", "92c9e483-14c2-42e9-be02-5f9b5c7e0381": "Where can the RAGBench data be accessed according to the appendix?", "1b33fe33-904d-4b2c-bae4-3215613f1e43": "What resources related to RAGBench are published on GitHub as mentioned in the appendix?", "32882d04-07ee-4aeb-8c01-b67910325003": "What types of datasets is RAGBench sourced from?", "f14a7751-dc62-45e8-9fbc-f4fa85e6c498": "Does RAGBench contain any personally identifiable information or offensive content?", "e36b433c-7ca4-45ca-aa27-f7333f7e6347": "What are the three subsets included in the PubMedQA dataset and how many abstracts does each contain?", "0a118505-e76f-41a2-8222-664fd3c45aec": "How are the questions and long answers generated for the PubMedQA subsets?", "47fea870-ad3f-42d9-a195-ef6243607e3c": "What types of annotations do expert annotators provide for QA-L answers?", "5f2cf192-1700-4465-9c79-5941f31c7f19": "What does PQA-A exclusively comprise?", "b42d579d-839d-428d-8723-631b142afabf": "How is the PQA-U subset utilized and transformed for the RAGBench task?", "e4b8b690-ea8d-4152-a64d-2e2e7e5bf331": "What method and tools are used to retrieve context chunks for each PQA-U question in the RAGBench setup?", "fde76901-156d-4c29-9da6-cfaa943e8991": "What sources were used to create the CovidQA-RAG dataset?", "9ac9ef16-f47d-4e2e-a732-c6e05f75a92a": "How many expert-annotated questions are included in CovidQA-RAG?", "3b613bc7-0535-49fd-aa40-1161c44f84a2": "What method is used to embed questions and database passages in the described COVID-QA system?", "49e1ea1e-7c6c-46c2-ab04-5fe0601f7f5f": "How does the system retrieve relevant passages for each COVID-QA question from the vector database?", "fa9deb39-f804-4367-9244-d1fd4a6f6e8a": "What is the source of the question-answer pairs in the HotpotQA dataset?", "a55434e8-29bd-4aed-af0f-7a7de46f4c01": "Why is HotpotQA considered valuable for benchmarking multi-hop reasoning tasks?", "493471d8-3355-44ed-bf1d-5c1d92f824fc": "How many context documents are used per example after downsampling?", "d3ad2491-7fc8-457f-bd14-5e896a857019": "What type of documents are treated as RAG context documents in HotpotQA?", "47e4fe8b-ad05-4ed0-ae06-c16069e5ed3b": "What is the significance of the name \"MS Marco\" in the given context?", "3935227c-1b7c-43df-8c7d-f3fca9ce5a40": "What information does the number [28] represent in relation to MS Marco?", "e2de17d4-f32a-43bd-b813-b6686695e5ab": "What is the source of the MS Marco open-domain question answering dataset?", "a82a04e9-e3f2-4921-987f-4062cb6a9baa": "How many context passages are associated with each question in the MS Marco dataset?", "dccb37b3-d7ab-4188-8e84-7d2ccf3c3eae": "What approach is used to generate responses in the RAG setting according to the context?", "423232e5-e8f8-48df-86f3-a1e2fda59b32": "How are human annotated answers treated in the given datasets?", "83ee71cd-f2a3-4a88-9305-1363695a46eb": "What does the abbreviation CUAD stand for in the given context?", "e79c09c5-fcc8-4e74-a4df-eb3127f0a4da": "What is the significance of the number [12] associated with CUAD?", "47a8d295-a2e5-4ddb-94ff-9b261cea8240": "What is the source of the contracts included in the CUAD dataset?", "dc5a40ef-5d61-45f7-b197-ae256ca95d85": "How many questions and documents are included in the CUAD collection?", "eefdc5b7-d693-44e2-aa45-3889c537c7b4": "Why do we not perform additional retrieval over the contract corpus when forming RAG examples?", "a0316bdd-14c2-43d6-bb76-39f96fa3b73c": "How many questions are sampled per document due to the high annotation costs associated with long-context RAG?", "7b41c29c-9150-4d67-bc4d-47cd0dd3d7b5": "What is the source of knowledge used in the DelucionQA dataset?", "cb5d7311-bef1-4300-8614-2512e9167f9b": "How are the context passages retrieved in the DelucionQA dataset?", "cd177abd-bb49-4840-848d-2362761b4d9f": "How many relevant passages are associated with each question in the DelucionQA dataset?", "a3e434ce-d150-4434-8cf8-fb51dad272f8": "What method is used to build the vector database for retrieving context documents in DelucionQA?", "70348660-830f-401b-9d11-1e6748d86482": "What is the purpose of using an LLM in the given context?", "c4c4ce3c-3e64-4321-b6de-03ef781b378a": "How are responses generated according to the provided context?", "69aeee48-d3db-452c-bde6-122830deeee6": "What information is provided in EManual [27]?", "d6dbe4e7-5cbc-4f0e-9ebc-a8bcdc5cfdea": "How can EManual [27] be utilized effectively?", "67177f18-6f82-4a5f-a76e-33ebf7e24d14": "What type of devices does the EManual dataset focus on?", "fc87fa1d-2e92-47dd-b1e1-a61d60139e5f": "How are the manual segments utilized in forming the RAG dataset?", "f7bb0fd4-be52-44b5-8fcf-756fbce61b53": "What is the purpose of generating questions based on the provided context?", "58e17543-5934-4024-a2e1-a62870ba0cf6": "How are responses generated for each (context, question) sample?", "402eab35-8c56-42e0-a8ef-3bf5bbcc38b4": "What is the primary purpose of TechQA?", "e99fe596-3f78-4189-8e03-cc811a73cafe": "How does TechQA improve the process of answering technical questions?", "5aea11ab-d350-4e98-bdb9-f2cdf026b236": "What types of sources are used to compile the TechQA dataset?", "c635566f-3fe9-491d-b7d1-1341024c274a": "How is the TechQA dataset split for training, validation, and testing purposes?", "a2423980-a5e4-4ec2-8795-0e3306d21007": "What is the purpose of sub-sampling the data down to 10 documents per question in the described process?", "9d74f1cb-c20c-40a4-b89f-65fcd6627936": "How are the provided splits utilized when generating responses with an LLM?", "a642746a-2327-4fc5-a0f5-1f2facaa0a86": "What type of reasoning is required to answer questions in the FinQA dataset?", "4b761c3b-28ac-46fd-a4c5-ec618993f3d2": "How many financial QA pairs are included in the FinQA dataset?", "ae2e5c3a-0077-4542-b29b-88adaf5df0c9": "What type of reasoning does the TAT-QA dataset require for answering questions?", "c26b5f57-1ccf-4db4-9973-f0168b74d676": "How many training examples are included in the TAT-QA dataset?", "927b898d-220c-4498-a065-cc867396995e": "What is the source of the original context documents used in the HAGRID dataset?", "9301c9f1-2f55-479c-8a8c-56ec5a4d3b4d": "How are the responses in the HAGRID dataset evaluated by annotators?", "303433bd-fa08-4bf3-b77d-f53a28569267": "What approach is taken towards LLM-generated responses in RAGBench?", "83e30e59-4cd3-4e63-ab9a-14d936ec0d1e": "Why are additional responses not generated for RAGBench?", "865c9e4d-3012-4fbb-8d27-657a88712c89": "What types of content are included in the ExpertQA dataset?", "e268b1e9-4e24-4a63-95c5-1962a102e56e": "How are LLM-generated responses utilized in the ExpertQA and RAG datasets?", "cab18ebb-2551-4a18-8d6c-d6bd136d98fd": "What is the purpose of the prompt template described in section 7.3 Response Generation Prompt?", "bad35d02-8e3d-4866-a276-d6bf57fd862a": "How is GPT-4 utilized according to section 7.4 GPT Labeling Prompt?", "24ab7b2f-55ef-4ba4-9b63-76b118a96d8a": "What is your task when reviewing someone\u2019s response to a question based on documents?", "221c2c7a-601a-45de-9b0c-9866a7374254": "What information will you provide about the documents after assessing the response?", "348cd04d-e970-4595-918d-d4b839659c6d": "What is the purpose of the keys associated with each sentence in the documents and the response?", "7e1fe631-dbe3-4d77-b1f7-82f7d19cb0dd": "How are the keys in the response different from the keys in the documents?", "6c08d0a7-a1a4-4a7b-94e6-a66bdaf2f274": "```json", "a6751c3b-fcbb-4c10-81a1-94af9ee550d3": "{", "b005b997-3833-4b75-b880-ea68a1af0a1b": "\"1\": \"What is the significance of the provided answer in the given context?\",", "4764e9b3-9181-4199-8a67-fa589de11f74": "\"2\": \"How does the answer relate to the overall topic discussed in the context?\"", "4af5f2be-6d57-4b85-82b9-14c938c9e192": "}", "6a8206e8-0e4b-4427-81e6-c25c2d20269a": "```", "1537cabf-1df1-46fe-b1b0-90632c05c507": "What information does the \"relevance_explanation\" field provide in the given context?", "7171c790-b172-4e22-a9ce-efb777243108": "How is the \"sentence_support_information\" structured and what details does it contain?", "e8f59a91-0e14-42cc-922f-59e385e95ae2": "What is the purpose of providing a step-by-step breakdown of information in the documents?", "29df27cf-4c8c-435b-b731-b3305b9b4aa9": "How does the context suggest using the information contained in the documents to answer questions?", "57d98179-8975-4fdc-a7f4-7208fec3dfe3": "What is the purpose of the all_relevant_sentence_keys field in the context of document sentences?", "6bd50e6c-cdd7-4987-bb13-d4896799cd20": "How should one determine which sentences to include in the all_relevant_sentence_keys list?", "4e3f07d2-62f4-4ffa-9988-de84267ade58": "How can external factors impact someone\u2019s ability to answer a question?", "0a82c02f-9444-4fbd-a8af-71675a09e907": "What are some common reasons that might affect a person\u2019s ability to respond accurately to a question?", "ef2f6780-7d08-44df-8a5f-bb4ba346ef3b": "What is the purpose of the overall_supported_explanation field in the context provided?", "07942ab8-e5ae-4b29-9e57-659e306db35a": "How should claims be assessed when filling out the overall_supported_explanation field?", "bc6a296f-d801-4a5f-8be9-abe0fb734447": "What does the overall_supported field indicate about the response?", "e4be92dd-4787-4dbc-85a6-80b162385c54": "What type of information should be provided in the sentence_support_information field?", "c17f510d-8194-4402-9902-99f0c83879ef": "What fields must each object in the sentence_support_information list contain?", "afaef7ec-1640-4406-a738-99dfbe228fac": "How is the response_sentence_key used in the sentence_support_information objects?", "475ee9e2-659e-4ac2-98be-486a9611d5ff": "What should the list contain if the response sentence is supported?", "65041dbb-b55a-41fc-9fe6-333ef14a382f": "How should you indicate that a sentence is supported but not by any specific sentence?", "832c0023-08f7-49dd-b2fd-fae4a894d9bc": "What label should be used for sentences that make general statements or summarize previously stated sentences?", "b93f1291-1900-4158-a5b4-591a4c476da8": "How should sentences that correctly state well-known facts, such as mathematical formulas, be categorized?", "8454982b-d06d-487c-8204-4fb09b813c94": "What does the boolean \"fully_supported\" indicate in relation to the sentence and the documents?", "72a00814-181c-4d14-b7f2-8a64bc25360f": "Under what condition should the value of \"fully_supported\" be set to false?", "2c64e353-f778-473c-9f35-3c6651e18679": "What does it mean for a sentence to be fully supported by the document text indicated in supporting_sentence_keys?", "a7379903-83ed-46be-9f8c-12b9ae402714": "How is the term \"fully_supported\" used to describe the relationship between a sentence and the document text?", "691dc567-c483-4016-9d26-0da534bd096a": "What should the all_utilized_sentence_keys field include when constructing an answer?", "630b875d-5263-4a8d-806f-54e4a253eee1": "Should sentences that were not used in constructing the answer be included in the all_utilized_sentence_keys field?", "2f3ee13c-5628-42fa-8ca1-5f7147a606a8": "{", "11fcfcc5-53ab-4c40-a52d-9fefdfb704ac": "\"1\": \"What are the key steps involved in the 7.5 Annotation Post-Processing process?\",", "eb616cc4-459d-458d-b0a2-fc06107cbff0": "\"2\": \"How does the 7.5 Annotation Post-Processing improve the quality of annotations?\"", "119da9c1-ae46-4bb5-83fa-019b1f32f421": "}", "04713342-85b2-4e03-aeb0-f011c494d253": "What techniques are used to encourage high quality labels from the annotator model in the annotation process?", "563dcab8-07dd-4111-8c31-ea61214cf934": "How does the use of chain-of-thought and redundancy contribute to the annotation post-processing steps?"}, "relevant_contexts": {"6ac2ecc9-5251-40f9-b8aa-1c0b2bf2b67f": ["956979c6-68c5-497a-b5f5-c54ad7f7f9f2"], "d874aaca-b1af-460c-adda-59b66df00bc5": ["956979c6-68c5-497a-b5f5-c54ad7f7f9f2"], "b7185d80-ae31-4864-b8b4-99ded47b302d": ["d6eec980-1d67-4ee1-a91b-e6baf2cc3503"], "9df7ada3-ed92-4333-8429-313875852a6b": ["d6eec980-1d67-4ee1-a91b-e6baf2cc3503"], "668fb562-182f-4aff-bd39-27d334d0b7a6": ["c7ebb920-ec2b-4115-bbf8-3816e1bf6de5"], "ce57db82-d980-40ab-bcdd-c0f7380e66ee": ["c7ebb920-ec2b-4115-bbf8-3816e1bf6de5"], "252e2c0b-6e95-4c12-81c3-ffed2708a22c": ["c8cdf4c4-b1bc-4dcc-b145-a1a9a875f678"], "ceba922d-50b6-4906-aa71-0dd9aaf72a1a": ["c8cdf4c4-b1bc-4dcc-b145-a1a9a875f678"], "2ab191a6-8a10-4853-9b53-6b380d3c6bd5": ["1033ee79-7cd6-47e8-b05a-7f35af3aa919"], "5b9f0b7f-3ff5-4dbf-a331-ea0855cd6062": ["1033ee79-7cd6-47e8-b05a-7f35af3aa919"], "32554a71-2a14-4ead-ad12-08a29a317795": ["6dc8e9b1-7d0f-43be-8126-d3e9ccf26467"], "c3613716-29e4-4166-9235-b1b2fe4cb34d": ["6dc8e9b1-7d0f-43be-8126-d3e9ccf26467"], "347e5c6e-e7aa-4e05-b04b-504b5f7abd64": ["dffc3b77-11dc-46e7-8bdc-9c0dd9516b89"], "0c6b5d69-bbff-414c-8624-6610dd194641": ["dffc3b77-11dc-46e7-8bdc-9c0dd9516b89"], "e400ad5f-648a-4f06-aa17-403b76e124c7": ["7045ac16-3857-4288-be01-47e1c0ab5f1f"], "4ed16539-2e53-447e-886f-52de3f7f6889": ["7045ac16-3857-4288-be01-47e1c0ab5f1f"], "c22b9310-c9f4-4276-bfdc-24ce6d201332": ["fc29944f-5edf-4411-8dd7-29dca557bdc0"], "e0fa379f-fed9-4b55-a9a7-a642089dd211": ["fc29944f-5edf-4411-8dd7-29dca557bdc0"], "4aa0aff4-9fb4-4d8f-818e-6f93d421c745": ["4a942b23-800b-4ad9-94ed-42228dac0cf7"], "6fbbbf73-b9d9-44cd-9577-0b4987095367": ["4a942b23-800b-4ad9-94ed-42228dac0cf7"], "06735255-4df0-4a99-a86f-94db2cc1a935": ["9f20f85d-c3a4-421b-a855-3600c37fc2e0"], "8759b02e-9082-4a6d-8b33-f80b1c762141": ["9f20f85d-c3a4-421b-a855-3600c37fc2e0"], "45d4295c-b223-4265-944e-6ff10737e8ec": ["dab117d7-86d9-48ca-9a45-4c6cffebc846"], "3bb52b6d-a064-4eef-bba3-cc819ab75d5b": ["dab117d7-86d9-48ca-9a45-4c6cffebc846"], "7bdbfc9e-ea3f-4d5f-ae98-fd71522cd5cd": ["934f65c5-c3fa-4edb-bb97-01661a9d26cb"], "e50eeceb-7dbc-40e0-832e-4d97ab9f4f03": ["934f65c5-c3fa-4edb-bb97-01661a9d26cb"], "5ad83387-b1bb-432f-a0b0-bcdef8e51144": ["293b420d-aff1-4639-a4fb-0f7aef26edfe"], "df04320b-02e4-48ee-9dac-b541bf643f94": ["293b420d-aff1-4639-a4fb-0f7aef26edfe"], "6edc2a4c-db90-4776-a003-e7e05879830f": ["dc5a1053-d708-42e6-8973-7e52272fe7ff"], "f0eb7e48-7c9d-4f7a-9521-c7bb82800a44": ["dc5a1053-d708-42e6-8973-7e52272fe7ff"], "5dc0fbfb-3f17-47ca-920c-0c84ae485a1d": ["37460103-4729-48ed-aa63-dad126418bfe"], "8ce5a40a-639f-4f5d-b81b-a1360436fedf": ["37460103-4729-48ed-aa63-dad126418bfe"], "a0f33ca0-78a7-41c6-8d83-90da3399938c": ["59d327ae-bfc2-451d-ae47-98339c91edd5"], "27e06c27-a366-4829-86bb-c34b79efbe3d": ["59d327ae-bfc2-451d-ae47-98339c91edd5"], "4ba0f1e0-9ed9-4724-8e64-11aa41a37dd4": ["39852235-be1e-4aa3-8cbb-b8de344c1059"], "d60233bb-857d-4cc3-a8da-701175da8017": ["39852235-be1e-4aa3-8cbb-b8de344c1059"], "2da5a328-33ee-44e4-a64a-f0278d70fa26": ["3f1dd124-0e9e-42de-a541-a60f5d5e31a9"], "8d49d4e4-9399-4c3f-822c-d4c2505c7f8b": ["3f1dd124-0e9e-42de-a541-a60f5d5e31a9"], "d9165a30-0634-44df-9e58-55a8421f9dd7": ["3cec763c-ddb7-4a42-9dc0-b47ac51c175d"], "ee1e097f-a55e-40ad-84ba-a578713325e0": ["3cec763c-ddb7-4a42-9dc0-b47ac51c175d"], "2811ed4e-44ed-445f-a509-5ed1ec936ae7": ["21062649-5d07-4859-a410-94dbdadda3ff"], "7f6cf1c8-acf8-4de8-b7aa-84a72015bd21": ["21062649-5d07-4859-a410-94dbdadda3ff"], "8cabec3b-4b08-4e7a-8e89-0c2043c46319": ["ccd230ab-7900-44ea-aaeb-656e9caf4acb"], "15a652c5-57db-41a6-bfe4-05fad3055c47": ["ccd230ab-7900-44ea-aaeb-656e9caf4acb"], "ea14b712-8bf9-4c1a-a99c-0816e39de95f": ["c2674fe0-4e8c-4225-91cd-95654a656a9d"], "aecc111e-0b43-423b-93b7-9c1906d3a9aa": ["c2674fe0-4e8c-4225-91cd-95654a656a9d"], "dc64991a-e040-426e-ac26-4ea4c3086cc9": ["ec4f30f2-64fa-4083-aae4-677435fc8d0a"], "487163ae-ec6f-4643-892a-3fa87b91d9a6": ["ec4f30f2-64fa-4083-aae4-677435fc8d0a"], "8e6fba8b-b0b7-46cb-ac0e-6adbecfc7770": ["af1fd2da-bf04-415b-a420-2248fffe9f90"], "c9c0f378-8993-4b5c-9e45-1ff4e4b7caa1": ["af1fd2da-bf04-415b-a420-2248fffe9f90"], "d3cd89e5-b235-4c5b-9cd2-84c5dc530249": ["b98ea011-42b4-47bc-888b-6f34e754ee66"], "4f079def-e676-4b23-807f-bd8728a32b38": ["b98ea011-42b4-47bc-888b-6f34e754ee66"], "441d4cdf-5571-43e7-8866-9199a3f6b216": ["234fb7da-257f-4526-912a-87f2a9f2dff5"], "b152e915-7b3f-4d70-87da-a93268e8704f": ["234fb7da-257f-4526-912a-87f2a9f2dff5"], "ca11cadf-ebaf-4363-a308-aeb2f7a32c45": ["08f8b828-b7f5-42b4-b92f-18738760512b"], "82d25ff6-0290-472f-9894-89be161f33c2": ["08f8b828-b7f5-42b4-b92f-18738760512b"], "b9ba1f5a-2797-4529-8d18-aa0cccf3116b": ["c3928a27-9b3a-4b83-bdc1-9a372919c0b6"], "004a3330-e0f1-457c-9715-a6bdbe9e47b3": ["c3928a27-9b3a-4b83-bdc1-9a372919c0b6"], "eb723fa9-2339-49ff-a04c-473de1a2c2ef": ["d5a021f9-b20b-4af4-b469-169ba73dea4d"], "41af23c3-b775-4ebe-9c00-0f2bd707e134": ["d5a021f9-b20b-4af4-b469-169ba73dea4d"], "4ad9571e-25b9-46c1-8696-aae25abc895c": ["bcf36b24-871f-4155-8798-863de89f474c"], "8c90acd0-0477-4bb6-b2b1-6d1866e4ce83": ["bcf36b24-871f-4155-8798-863de89f474c"], "e75f216d-57f4-4449-9f0b-4229343079f6": ["b4463def-5da0-4a46-a2d5-7afa17fcbc51"], "1cfbc678-a87b-4f1e-b37d-516ed3bb1668": ["b4463def-5da0-4a46-a2d5-7afa17fcbc51"], "68f7f119-4ece-45a2-a9f9-ca7a284e7563": ["406a7322-e66c-4d27-abc3-f15889ef096a"], "09be2ba5-08ae-42e1-aa3b-0d27fbb7f343": ["406a7322-e66c-4d27-abc3-f15889ef096a"], "caff12e9-c83e-4cce-824d-4cfcd74df2a5": ["5cc8699f-cccb-436e-81af-7fb381af5325"], "46000b59-5502-4996-ae28-2b96c10e517f": ["5cc8699f-cccb-436e-81af-7fb381af5325"], "8bbc254c-29c6-41db-87d8-25778185a411": ["133b4d25-fd44-4ec0-ac11-dc363f85cc90"], "0c17ee53-0dcc-42f5-9ce0-ffafead3f6ae": ["133b4d25-fd44-4ec0-ac11-dc363f85cc90"], "410199f9-5727-4636-beec-7416f1c452c0": ["35b9348f-c9e7-4d47-a010-f368a807f93a"], "716163cd-820c-42ff-97de-bc00f00ab056": ["35b9348f-c9e7-4d47-a010-f368a807f93a"], "617b248e-5ff4-451e-91fd-31ea9cc6d6ff": ["d7c4c452-66b2-4a73-8eca-95c924cfabab"], "3c694402-416c-4df4-a184-b7954f08c365": ["d7c4c452-66b2-4a73-8eca-95c924cfabab"], "6fd5d27c-e7d4-47a8-8b06-f44e5e502ecc": ["6f82022a-78da-4ee0-9f2e-ce8085019eec"], "5caf4469-9d6a-47e8-8c24-6dcb0eea7389": ["6f82022a-78da-4ee0-9f2e-ce8085019eec"], "bdfecb59-fbe8-4965-a52f-5eea22771b8a": ["8675d89a-5e2a-4986-89f5-3dcf976f43ee"], "52217561-1cd9-4517-94e6-356f91fa2f2d": ["8675d89a-5e2a-4986-89f5-3dcf976f43ee"], "ba267703-5f09-4b40-a01e-91b291eff982": ["268e5170-84a9-4d69-8dfc-9eb68b9614bf"], "46e223cb-e1d0-44ab-abdb-1b1a1a5c78ee": ["268e5170-84a9-4d69-8dfc-9eb68b9614bf"], "c8e4d6b5-6b22-49f5-8bad-33acf1e51195": ["2692a231-9134-4581-a9f0-0f475238c461"], "67dabf51-c85c-43cf-9942-18356d32e362": ["2692a231-9134-4581-a9f0-0f475238c461"], "5b18f24c-04ba-4a47-841e-bce84e553cc6": ["25b55637-890f-44ed-8a09-f575c26a4640"], "c4f5733d-7f5e-4e01-91f8-a417d548aa53": ["25b55637-890f-44ed-8a09-f575c26a4640"], "42e9f803-d106-46ef-b120-3230a7814183": ["494a8a4f-c5bc-495b-9de7-8ac5e2e97179"], "744a1705-ccdb-437d-8800-08a433b4b15a": ["494a8a4f-c5bc-495b-9de7-8ac5e2e97179"], "fb0500cb-43f3-4def-8826-3e831d90a15e": ["5ce2cfac-3378-471e-9856-0f323dd37ec6"], "c580ee6b-54e6-42fd-869c-9b40634ebe68": ["5ce2cfac-3378-471e-9856-0f323dd37ec6"], "7ec1c1a0-0b45-4427-b8b1-0f7a87be1e72": ["33b554fa-4cdf-4a1e-9764-98cf051f121c"], "ea462615-b749-4a73-9fc4-590e204f29f7": ["33b554fa-4cdf-4a1e-9764-98cf051f121c"], "bb2faf40-a4c3-4e11-8ccc-95703f346739": ["516b4a68-6d6a-4969-8974-8a22c7bbd11e"], "0a914d29-a6f4-4ac9-a2cb-f092f9c57e0e": ["516b4a68-6d6a-4969-8974-8a22c7bbd11e"], "92acfa29-cfc5-4c36-9569-d05580cffadb": ["01a8c270-a8e6-4aa2-ad8d-591d09222fb2"], "282c7e45-2e2c-49ad-a309-694e051b6435": ["01a8c270-a8e6-4aa2-ad8d-591d09222fb2"], "1c99f918-8d5c-4711-a92d-357ac8fe0db7": ["b85b9def-63c2-491f-82c5-95b5569cfa28"], "0e05b57e-daaa-4eeb-9bd2-8fa307aadb35": ["b85b9def-63c2-491f-82c5-95b5569cfa28"], "eedb9202-0e56-49ff-a0d2-55aa06ebebe4": ["bef9d765-3aa1-4d8a-b45f-1d7b1c962912"], "20784a72-80e9-47a1-bbd4-0aa492f4229a": ["bef9d765-3aa1-4d8a-b45f-1d7b1c962912"], "76d1d071-8cbb-475f-bfc6-d4f09e43ac69": ["c2316c00-b3c4-4111-9a49-1772cb932c01"], "b106d2b9-3d31-42c2-a376-b3f3614dd631": ["c2316c00-b3c4-4111-9a49-1772cb932c01"], "64328b24-489b-47d1-aec3-f91068294b68": ["8c9611d8-2a15-4d19-ab4b-8dc993b55c8f"], "965014ca-d700-4000-ab9d-026e841139dd": ["8c9611d8-2a15-4d19-ab4b-8dc993b55c8f"], "a7720017-2fef-4c94-b005-b7f2a084d65c": ["9a65cdfb-9f73-4177-a048-892aec95c2f7"], "66321d2d-acc0-4dd6-b398-926ecff55100": ["9a65cdfb-9f73-4177-a048-892aec95c2f7"], "7ac6fd48-dab4-4680-ada2-940571715e22": ["733ee48e-9de4-425f-ab4a-a22d1673d993"], "cb427c27-f779-4f22-9e82-470412b39030": ["733ee48e-9de4-425f-ab4a-a22d1673d993"], "0b48a429-389d-4098-8411-587e86c7703c": ["28517ea1-930d-4f9c-87d7-442dfc0fca68"], "3aa36fba-b3f1-4186-a801-388010c00b8d": ["28517ea1-930d-4f9c-87d7-442dfc0fca68"], "b879e406-8829-491a-a270-2bf297851e32": ["56213d54-979b-417c-b141-36edbd8d6e8b"], "058d0250-9cae-4689-82d9-411d3f1179b9": ["56213d54-979b-417c-b141-36edbd8d6e8b"], "6a7a452b-b955-4539-9b4b-d4d64dd6efd0": ["06dcb043-19b1-420e-b1ba-d7b069132871"], "38e4a8f0-282d-419e-be14-153539480672": ["06dcb043-19b1-420e-b1ba-d7b069132871"], "b2878f5d-0798-4c7c-942b-1c76b69701ea": ["48bd5401-099e-4803-8aab-891131c92b0a"], "f191bce0-4d4b-4ffb-bf39-bee95933e4ff": ["48bd5401-099e-4803-8aab-891131c92b0a"], "e84911dc-1bed-48dc-aaa2-17f2ce6547bb": ["421d7118-269a-41ac-b2c2-f764bfea9f7b"], "67b4b0d9-0253-455c-a79e-8c1994e05a8f": ["421d7118-269a-41ac-b2c2-f764bfea9f7b"], "c36e56e9-f6c3-4f79-930a-b7693441c671": ["6f91c28a-746e-4502-b504-598736db89d3"], "ee54e0a7-523a-4794-9377-1181534653a7": ["6f91c28a-746e-4502-b504-598736db89d3"], "39d7fcac-1787-4321-b68b-b2e77501f077": ["3c5f5df1-3fee-4461-8b4d-7c8b7adf1d3c"], "e95447f6-a398-4210-bd07-a033cf82dfdf": ["3c5f5df1-3fee-4461-8b4d-7c8b7adf1d3c"], "33468818-829a-4161-9500-b06444180b78": ["46a37333-e815-4a6f-b625-cd644849b8f5"], "1282d0bf-37cb-4304-bcb6-ef417b1a6f79": ["46a37333-e815-4a6f-b625-cd644849b8f5"], "b4cb0f98-6d0f-4c01-b41d-c568e11198b0": ["6bdaf012-c9ef-4776-ad20-833319f643c4"], "e4555e5e-352f-4c50-aecc-3d2c0cdb3da9": ["6bdaf012-c9ef-4776-ad20-833319f643c4"], "7e8117bc-dae7-40ef-aa93-fa93c5d7e6b4": ["63cfa073-4954-4436-ac00-287c761500bd"], "98521bf8-107c-4f9c-a935-a0742ac95634": ["63cfa073-4954-4436-ac00-287c761500bd"], "895d3351-1d81-4e03-8cd5-afb58bf2c0a6": ["0d537d5c-3a75-4a28-b59b-b0a22c1beaf8"], "f933b361-594f-4647-9a43-f21b12794cee": ["0d537d5c-3a75-4a28-b59b-b0a22c1beaf8"], "6120c9b9-15dd-4bfa-a1d5-fab9a25035c2": ["4f351567-4fc8-4d87-98fb-b28f064c239e"], "b6103ef2-93cc-4c66-b5cd-089e4bc39e87": ["4f351567-4fc8-4d87-98fb-b28f064c239e"], "ffdf34b6-8a50-408b-9c21-f0ef0d5f300b": ["9314d874-dea5-4349-869b-f3cb3df4d93d"], "8d6d51b2-5fec-496e-b16d-257f3a44d3cc": ["9314d874-dea5-4349-869b-f3cb3df4d93d"], "604fb1a2-d1ef-4c9e-8f93-0edccb2b69aa": ["1e00ce4f-3040-43d3-a079-5262312ec1f9"], "47e30937-2956-4e88-8467-c2170f3d767e": ["1e00ce4f-3040-43d3-a079-5262312ec1f9"], "433412f6-1585-4a25-baaa-f2cbdbae6077": ["26d6ef3e-df4d-40f6-9a37-8c170ad97174"], "913205b9-68f0-4ad5-9fb8-b57505907c9f": ["26d6ef3e-df4d-40f6-9a37-8c170ad97174"], "ef528a66-3b50-4fd0-ab63-86367dee0020": ["cf4e5943-db15-466b-b88e-a5ec65fdce52"], "7a04d537-ab40-4b64-a320-3952bc7ba51c": ["cf4e5943-db15-466b-b88e-a5ec65fdce52"], "3c393dbf-312b-4250-81ce-31d400736f75": ["c06d058e-c144-47ba-ab39-9f202ccd7526"], "1d36b34d-628b-415c-9b9f-acc7974a067e": ["c06d058e-c144-47ba-ab39-9f202ccd7526"], "8213a1d2-5378-4b85-b9c4-726f1bf9f8db": ["550d0ce4-d554-45e3-ac67-3f5e98af18e0"], "0b84c831-9e85-46f5-8cd7-b430b71dfe68": ["550d0ce4-d554-45e3-ac67-3f5e98af18e0"], "7fcb5cde-1196-42ce-b5a4-7d1b59aec2f2": ["4646db17-7ce8-4d62-a40a-ecc09a1f4692"], "225a337a-6193-4850-83fe-59d921070d48": ["4646db17-7ce8-4d62-a40a-ecc09a1f4692"], "76c483d6-8459-48e4-83d8-4c9e48e1b7d0": ["fc363ba1-515d-43db-90f8-10517f803394"], "3e3a94d4-19a9-40ed-b4b5-39f4b311c97d": ["fc363ba1-515d-43db-90f8-10517f803394"], "daa5bfb7-7b86-434b-bb44-f3ce3ca33a75": ["1e935891-b377-4812-b1ef-48492b27d593"], "04d22d9d-73cb-4d14-a11c-ae43c9b6bb91": ["1e935891-b377-4812-b1ef-48492b27d593"], "91436727-9e85-4594-bb9f-9e6a89c47390": ["4e18da13-de46-411a-b126-44e05509079e"], "c5bea7af-6803-4331-acf5-a58752dd0d56": ["4e18da13-de46-411a-b126-44e05509079e"], "4133f339-2942-4093-8366-9b6460337619": ["d1fdb647-7f31-4cd6-ae2d-99310c19d4e5"], "8b8dc0e8-95ed-4092-ba8a-8900dbb9ed22": ["d1fdb647-7f31-4cd6-ae2d-99310c19d4e5"], "2e262bc2-7ef2-4118-969a-e2463f5b3a48": ["b9266e4c-026e-4860-9ba3-8b04f34861c1"], "b9fc7e99-996f-4994-b6cb-280121c2cc16": ["b9266e4c-026e-4860-9ba3-8b04f34861c1"], "60805b32-271f-4a21-903c-51fe9fd4ef17": ["f83eab95-fa01-4db4-ab5e-5203d796361d"], "bacb3ba9-3f7f-4f14-b845-899340495c3f": ["f83eab95-fa01-4db4-ab5e-5203d796361d"], "cde3af8d-0841-47de-bd36-571170f181dc": ["0339f981-df9a-4fb7-a4b0-86c7496804c5"], "282583e2-bc45-4c21-93c2-524e5b707237": ["0339f981-df9a-4fb7-a4b0-86c7496804c5"], "ea380e63-22fd-4eb8-a047-70349b624413": ["21c90303-40b6-4f83-9fd3-d170ccfaa6a1"], "fe323902-dbd4-4df0-88d4-fa791a78a9a0": ["21c90303-40b6-4f83-9fd3-d170ccfaa6a1"], "2305283a-c77b-45f1-9aa2-a6c54ad31141": ["279510c1-639e-442c-9445-2dc4b36e2147"], "abdc4ad1-3d89-4a04-ad1c-b8cfacffa756": ["279510c1-639e-442c-9445-2dc4b36e2147"], "19843cd0-642d-4b88-a785-5fd33670cf5b": ["6846ced7-92f6-40d3-90e0-9fc1b63f8ddd"], "f876200c-ef0f-4082-a58c-db1c8a293d39": ["6846ced7-92f6-40d3-90e0-9fc1b63f8ddd"], "5d22515e-7f30-4be8-adf5-55677c9a40ec": ["9d8b55de-f1a1-4c7f-a88b-b56cfa790198"], "849368fc-30c6-4e27-b035-ea87f6c51663": ["9d8b55de-f1a1-4c7f-a88b-b56cfa790198"], "beca93d4-fe00-4a68-a746-1f86d1dc6491": ["f2d582db-30ae-43d6-9d2c-fb93cb193b34"], "1ffd055d-cab5-4f65-a594-c3876c70e0d8": ["f2d582db-30ae-43d6-9d2c-fb93cb193b34"], "9c96eecd-540b-488c-bd8c-4b98d046ded7": ["bb1565f3-7324-4955-94f0-cb99c7819d76"], "842fe4db-8bfc-4493-a52f-b19781325557": ["bb1565f3-7324-4955-94f0-cb99c7819d76"], "c95b0bba-e2fc-4f00-a654-1e003dd0e754": ["e5f3301b-0714-4d19-8ce9-837aeab09cd3"], "22fc3488-9111-41a4-a14f-bef2744d3b69": ["e5f3301b-0714-4d19-8ce9-837aeab09cd3"], "3c1a8d5e-5357-45b1-ad2c-110d875d852c": ["99822720-1f6c-4f38-9bb3-5a4a8346aa9f"], "4943e313-241a-4f00-b5f3-33f60eb8f9a3": ["99822720-1f6c-4f38-9bb3-5a4a8346aa9f"], "ca3c3cfa-4611-43d1-949b-82909547f091": ["99da1248-4d20-4754-a339-0263e028f44d"], "8102d78a-8ce0-474e-bc45-b01741be5e48": ["99da1248-4d20-4754-a339-0263e028f44d"], "7c15ad97-116d-41bf-8889-f923a249cc74": ["9ccc1c8e-b7a4-4f13-a0ea-ea9f0e70c7a9"], "28cc727d-9b98-4f18-8288-537974a2a56c": ["9ccc1c8e-b7a4-4f13-a0ea-ea9f0e70c7a9"], "ae6896e4-d8fd-464f-b995-82c239cd42e8": ["9d9c792e-98d8-4d32-a554-8f7cd0ff4e40"], "3fdae308-438b-449c-b5a9-6d7a3d7fa42b": ["9d9c792e-98d8-4d32-a554-8f7cd0ff4e40"], "d3ac34f0-1e4d-4d4d-8c5f-4a9cdbbcf158": ["b23f3d14-4cd5-4150-b7c1-21524ff6b107"], "8b4812be-e89d-490e-98f5-96069ac50018": ["b23f3d14-4cd5-4150-b7c1-21524ff6b107"], "253aaf64-eeaa-4013-811b-0cdd9bf0a952": ["04782e90-3735-4578-ad61-6765a24fc885"], "8219c682-7a12-457a-a10b-ed0d34df44f4": ["04782e90-3735-4578-ad61-6765a24fc885"], "ec9e5cc6-a2cd-4771-a161-44068ee432a6": ["b00abeb3-d6d7-4025-90a1-f4fc15424d2c"], "923bbdd9-ffd1-4b44-860a-57a61dd5660a": ["b00abeb3-d6d7-4025-90a1-f4fc15424d2c"], "5602197b-687a-4280-a239-d3a046ecbb86": ["74fa7cc8-4e37-48e1-a627-c669402cac8e"], "5225184a-8cd1-4a75-b75a-47987d0d6703": ["74fa7cc8-4e37-48e1-a627-c669402cac8e"], "33ae719b-9c67-46ed-9640-87a0a24b413d": ["9fef63a5-c297-41d0-891f-645b713082f2"], "3fdc4738-cb41-4927-8697-f3a8e925cae5": ["9fef63a5-c297-41d0-891f-645b713082f2"], "56ded4c5-cb46-4a06-a88c-f6fb9f6da484": ["abe96608-59c9-4659-b292-0c3cc07a5826"], "5bd375e3-de45-4317-920a-bc49d22c93e2": ["abe96608-59c9-4659-b292-0c3cc07a5826"], "f7122e63-62b3-4314-99f8-6896ed8a69e1": ["4bd535c8-0572-414d-97e3-96635485eb77"], "22b5b310-1beb-47ff-a55c-3142062e84ee": ["4bd535c8-0572-414d-97e3-96635485eb77"], "4e4f9d92-9d3d-4162-a68d-68e73e7d2acf": ["6769c525-70e7-4f49-8d45-7299a919aa66"], "dbfb2f51-a154-4f2a-876a-9cf31165b750": ["6769c525-70e7-4f49-8d45-7299a919aa66"], "554de090-a6b1-4b5d-98ce-2d1a6ab7b7b6": ["4b5ca1f0-c646-4e81-96e1-cf61bd1d3438"], "c9a4cf6b-d84e-4416-bae5-d8d0aa26987e": ["4b5ca1f0-c646-4e81-96e1-cf61bd1d3438"], "efefd776-b579-497b-a2fb-492ed9ffb349": ["0eb42c88-edbe-475d-9be4-5f6ffae16e61"], "96f736ef-7955-483f-95f2-b3e68a19ea8f": ["0eb42c88-edbe-475d-9be4-5f6ffae16e61"], "3fd523b3-c944-4b66-8e03-7751e565d468": ["4fa2408c-7e4b-40bc-b95c-7c8e75677c75"], "0e71e6ba-fe56-496a-9f01-2ca54c38c29a": ["4fa2408c-7e4b-40bc-b95c-7c8e75677c75"], "748a91ee-77ff-41d5-968b-e4748a5aa383": ["1eb4b60c-ff45-41fc-aa87-2cce228d2ea2"], "bae43270-202a-4d16-b1c0-aab2480993e1": ["1eb4b60c-ff45-41fc-aa87-2cce228d2ea2"], "1d699c0f-ff02-4a93-8b1c-8887ac66be13": ["2cb52636-f080-451c-9f3d-b38f29a27d54"], "f42cc190-ac6a-4650-a806-12437a1230ae": ["2cb52636-f080-451c-9f3d-b38f29a27d54"], "f3bad1d5-6498-473f-888a-86398e7f2fe0": ["6f665ffd-41ad-432c-87a3-19541f7eb110"], "638ff4ba-38ed-4030-b51b-4fa716b9c446": ["6f665ffd-41ad-432c-87a3-19541f7eb110"], "c6d84534-77c1-4e04-baa6-8eaf5adfac8f": ["c010bd7e-0af1-4c96-94a9-4b7f9e8a9792"], "71a989bb-45f6-4031-8956-395f2437f3e9": ["c010bd7e-0af1-4c96-94a9-4b7f9e8a9792"], "059fc40d-cde2-4258-9ac0-4bf6fc9cbc07": ["c8f14e5c-671a-48f9-a27e-f816ad46bb82"], "7952a16a-0a2d-4d1d-ac36-6120336021e1": ["c8f14e5c-671a-48f9-a27e-f816ad46bb82"], "cee6fb53-cb82-4a5a-9543-cdd190499717": ["5662b505-bfe7-4764-8360-bb54f96be493"], "659aad35-f836-4edb-bd08-3cb1d10b95c1": ["5662b505-bfe7-4764-8360-bb54f96be493"], "45cf80c3-80d4-4cf1-ae58-778f3ff5e069": ["3ad94145-5e87-4f39-8c5f-110b50261589"], "6d4f2a16-6ec6-4c9c-8e8e-09af9773d1b9": ["3ad94145-5e87-4f39-8c5f-110b50261589"], "c5ba53f5-44c8-48f5-8e4f-584e5f891611": ["02d1c4ea-a5c2-4542-ae84-b2c48d7a6f67"], "30f42dda-6c2b-4e1a-b850-de1bdd2d6634": ["02d1c4ea-a5c2-4542-ae84-b2c48d7a6f67"], "e3da1f9a-2e9c-497a-8ef3-7499d7378bdc": ["d8327f76-fbd0-441a-9a00-742ccd95258e"], "579c9f1e-9356-4fac-a9a6-1868ee93a34d": ["d8327f76-fbd0-441a-9a00-742ccd95258e"], "1812cad4-d6d3-4159-a8f3-319df704776c": ["43b721df-2e96-4210-a343-8ded8758230c"], "7faa24b6-9eae-4e25-a0f6-657d2fad3feb": ["43b721df-2e96-4210-a343-8ded8758230c"], "2a19b927-72fc-4f19-94f7-2b22be9af876": ["c1728133-77cc-42e4-8889-530a24b2030c"], "ea552173-2f84-49fe-9dd2-65befd41e4cd": ["c1728133-77cc-42e4-8889-530a24b2030c"], "61b13019-448a-445a-9989-6da49473458a": ["c534c6c5-d881-48c1-956e-2e19565d9ad5"], "1d02021e-b531-4abf-8d4c-9e7ec38e0eee": ["c534c6c5-d881-48c1-956e-2e19565d9ad5"], "f8b1a899-5a3e-4d8f-9746-4babf3641836": ["d109263a-92bb-4fd6-8e21-d20acfc2c4d7"], "6cd9097f-40b4-4084-9cb8-d5ff20ef92f3": ["d109263a-92bb-4fd6-8e21-d20acfc2c4d7"], "a92480bc-e549-4043-90d9-42e5cf8e0377": ["4385d59f-e2ff-4019-8534-e3c49916e893"], "fe941c1b-a6bc-4fd3-89b8-d5a86a36af21": ["4385d59f-e2ff-4019-8534-e3c49916e893"], "34c74176-68d6-480b-b5ce-42aa91b84ca7": ["98bdfbe0-da0c-4941-912d-77bf2d5d53df"], "47d01dd3-52c1-4942-a156-86fb9c5236bd": ["98bdfbe0-da0c-4941-912d-77bf2d5d53df"], "01a852c6-2e89-4582-923c-d0129d86edd4": ["14285653-90af-4a46-aa0e-3918134ad356"], "6af626d1-9272-4798-b03f-83fe5f06b891": ["14285653-90af-4a46-aa0e-3918134ad356"], "a21def2b-99f4-4c4a-bb7d-fb3904e94b2a": ["e823eda0-a8fd-46ab-a70f-4b3748efe8b8"], "c69d1908-7a93-43f4-a04d-748d2569ffac": ["e823eda0-a8fd-46ab-a70f-4b3748efe8b8"], "c1af4256-6475-4bec-992b-aafc6fa8d86e": ["6e26b5bd-c367-40fd-8690-247614f99851"], "5da3655b-1e26-4afb-a16b-9f492f61b824": ["6e26b5bd-c367-40fd-8690-247614f99851"], "c3806fef-bf31-46ad-916e-7749ff11807e": ["5626ac12-448b-461b-86b3-b07fd3d7d933"], "301bbbaa-928a-44a6-aa1a-d3e192126399": ["5626ac12-448b-461b-86b3-b07fd3d7d933"], "02247380-8d2a-4dce-9dd4-1d65347e9f5a": ["300246a8-39b4-4000-8108-8be0b5e558ea"], "52b4abff-dcf0-4797-b0de-a3996de12e9f": ["300246a8-39b4-4000-8108-8be0b5e558ea"], "af135dd4-81fd-484b-89dc-f2d176c554c9": ["8700fae4-bd44-4d2c-ae7b-fbab95810a6c"], "a3615f51-d8e5-4b37-95f8-880e2deba530": ["8700fae4-bd44-4d2c-ae7b-fbab95810a6c"], "7c9f0047-25e1-41e1-b16d-e6ad680151ad": ["dbb28f3f-5058-4797-b418-ae8cb1e2d0d7"], "7cf6c9f3-ec51-4f4f-89a9-02778b2e4aa2": ["dbb28f3f-5058-4797-b418-ae8cb1e2d0d7"], "b8c119b1-b56b-485a-bc90-a8195b3aff3b": ["0d8a02cc-9ff6-4a74-80e7-c868d012b0a1"], "379ba222-574b-4dcc-85ad-b66b53ac2930": ["0d8a02cc-9ff6-4a74-80e7-c868d012b0a1"], "ebe38151-64c8-4f88-a7d0-a9d46a14a1e1": ["786973c5-eab4-4f9a-8408-2c2d4be513ea"], "c0c2ce4d-7db1-42c6-9334-735035443193": ["786973c5-eab4-4f9a-8408-2c2d4be513ea"], "6611a2c4-1fc5-4c62-9e13-6d3ba7675c27": ["1f3dc489-5b3a-4506-b610-a52f80b1383f"], "34588603-e82a-4df3-a089-36fba0e44966": ["1f3dc489-5b3a-4506-b610-a52f80b1383f"], "44e14305-7a7b-420c-b5ec-8732e32fffcd": ["708853d5-8696-48f8-8bea-64c868fe652e"], "a3f22266-8579-4141-8932-310e40ea0520": ["708853d5-8696-48f8-8bea-64c868fe652e"], "14930bf3-732c-44ba-a349-de234e42c012": ["ae7f8786-17d7-42de-bd71-49c0662f7310"], "329590e0-f4fe-44c9-a8c8-b8bd2e22a4dc": ["ae7f8786-17d7-42de-bd71-49c0662f7310"], "eb5bcb00-f38d-48cc-a7d1-66ebe2e347b0": ["7ce3143d-307f-4284-bb00-e2fae1bdc4eb"], "dfc9303c-55da-4fa9-a1e0-cd088a474bee": ["7ce3143d-307f-4284-bb00-e2fae1bdc4eb"], "ba146f4b-8603-49af-beaf-c928f3733a1c": ["3974c247-82b7-42cf-865b-6377f26d7c88"], "e3d23a74-f4cd-4da3-aaac-03f4d4a6cae3": ["3974c247-82b7-42cf-865b-6377f26d7c88"], "2d46db04-bd82-4ec1-910b-cf88ba4155ff": ["319131df-316d-49ea-befa-024dd698bc24"], "f50fa441-9fec-4a92-9da6-2ef28f4f5f6f": ["319131df-316d-49ea-befa-024dd698bc24"], "f258440d-b78d-4daa-bc14-71de23f2e599": ["2626dd45-199b-405b-a55a-b7ed8e95301a"], "d6b11936-ac0a-480f-a44d-a20493e7813d": ["2626dd45-199b-405b-a55a-b7ed8e95301a"], "ecbf30a2-307c-45d9-a640-2a2ff0a61862": ["ba4fdb0e-d413-47db-8ab4-58f3e3c352d0"], "ea699e13-24b2-49f8-8e22-deee382f6b25": ["ba4fdb0e-d413-47db-8ab4-58f3e3c352d0"], "e23e0c9f-5573-4aba-985b-649a6224944c": ["d4eef488-b16c-4dd6-897e-fb6509ced0a7"], "d4c2f60d-3adf-4be3-8129-2d0fe5cf1f76": ["d4eef488-b16c-4dd6-897e-fb6509ced0a7"], "864284dd-f8ce-4b5e-9e49-c8e86387e9dc": ["34b855eb-9a30-4a5d-8573-f6f928d92d64"], "ba3e8695-ee5b-4349-b97f-d564beceabbf": ["34b855eb-9a30-4a5d-8573-f6f928d92d64"], "768d9dba-da04-41a9-9b2e-c41fe0290b38": ["4303976d-fbb8-47f6-b508-7fb5247d2b96"], "1e41d031-09e0-4b99-917f-3dfa2be6c5c5": ["4303976d-fbb8-47f6-b508-7fb5247d2b96"], "bbb84668-6086-420a-bfc8-217486bd96bc": ["5592af13-7d80-4983-b3e8-42512afe8ee1"], "72441e66-e46d-47cb-bf59-f0336423bca4": ["5592af13-7d80-4983-b3e8-42512afe8ee1"], "d451e6e9-acbb-4f61-bb04-ed1b191ca96a": ["43e8680d-e613-4fbe-b6ba-80d1f32c12ea"], "64a9cd9b-b89b-438b-b875-13836fbdaaac": ["43e8680d-e613-4fbe-b6ba-80d1f32c12ea"], "d72ded49-04db-4cec-85be-ac91e65cb196": ["d7cb37fc-6dd9-4169-bcde-a0a36957402b"], "9725769a-0a86-412e-a6d5-f1958657c76b": ["d7cb37fc-6dd9-4169-bcde-a0a36957402b"], "ee57677b-75e0-40b2-b904-c8b4a0265cc8": ["12316e9d-ec0b-4365-869b-41e0b82ff35c"], "9a631343-4ad6-4694-bf2b-5386377067a6": ["12316e9d-ec0b-4365-869b-41e0b82ff35c"], "7973498e-251a-4ffb-8e1d-1fb92961255e": ["2c81ee73-e4ae-40f5-bd65-4ee991ad7ed5"], "60cdfeb3-85dd-41c3-9699-70fc9186ed05": ["2c81ee73-e4ae-40f5-bd65-4ee991ad7ed5"], "bbf78094-5694-4cb8-bcaf-06efdb59cea5": ["89837166-2a67-408e-ad35-2e24a8aa28e3"], "74c0f1ac-2fec-41f8-9207-19a8633eaa28": ["89837166-2a67-408e-ad35-2e24a8aa28e3"], "218050c1-f8a9-43a3-b6f0-06030c0e24bb": ["658c0ac1-60c7-4f2e-ba60-52528a9dda53"], "3e74bfe3-7b07-4eff-90c0-d5d384f29b51": ["658c0ac1-60c7-4f2e-ba60-52528a9dda53"], "e7e2f106-4fe0-4add-998e-39bc8966a221": ["b82a809f-5e3d-418d-b7a3-234f05ccd1dd"], "d4f18c68-638b-4cb9-80b7-bbdf7a449b38": ["b82a809f-5e3d-418d-b7a3-234f05ccd1dd"], "aa0d96f6-f87a-4dd4-9bd2-ae2e20260e50": ["de76359a-1adb-4516-a560-3e849d0895d0"], "d8c12c42-a1c8-4bb3-8ff5-3ab23df4d04e": ["de76359a-1adb-4516-a560-3e849d0895d0"], "778847b0-4ba4-4408-a39c-ba0875f02bac": ["dd41da1a-e815-4fd5-be63-15008352ba4b"], "c55576fa-06c5-4ba6-b2e6-bd64036d3fa8": ["dd41da1a-e815-4fd5-be63-15008352ba4b"], "3a8a1f80-9a6e-4813-b404-c5031999c8ac": ["069d3911-f75c-4d43-a9a5-aaebcc0810ed"], "5c0523ad-051d-4591-aa8b-ec16402cced0": ["069d3911-f75c-4d43-a9a5-aaebcc0810ed"], "75896c2c-7971-4aeb-a738-67ffeb6ef24a": ["1ecedf2a-97d5-4619-8449-eea00bb7a948"], "fe2b6901-c84f-47f2-9b82-8846fe9d6a97": ["1ecedf2a-97d5-4619-8449-eea00bb7a948"], "278b8cb4-cb33-48ae-a795-554cf5fc2c25": ["091e8798-49bb-477f-b644-714d8e44e2ac"], "93b75144-ea45-4339-b69e-7663e32dbf74": ["091e8798-49bb-477f-b644-714d8e44e2ac"], "0e98bb1a-7b7a-477d-af47-3636a32c4f0d": ["f406e70e-e18b-4843-b186-fab0a5edb175"], "fbd39819-e49d-4926-ba4e-c67503c855e6": ["f406e70e-e18b-4843-b186-fab0a5edb175"], "92c9e483-14c2-42e9-be02-5f9b5c7e0381": ["ad9a6ab6-9e2b-4382-a071-aa3ca84989c0"], "1b33fe33-904d-4b2c-bae4-3215613f1e43": ["ad9a6ab6-9e2b-4382-a071-aa3ca84989c0"], "32882d04-07ee-4aeb-8c01-b67910325003": ["cbede85b-2bbc-4554-9646-469c30e9633d"], "f14a7751-dc62-45e8-9fbc-f4fa85e6c498": ["cbede85b-2bbc-4554-9646-469c30e9633d"], "e36b433c-7ca4-45ca-aa27-f7333f7e6347": ["e37150fd-e544-4505-8163-538db0115fcf"], "0a118505-e76f-41a2-8222-664fd3c45aec": ["e37150fd-e544-4505-8163-538db0115fcf"], "47fea870-ad3f-42d9-a195-ef6243607e3c": ["7e615b69-87e5-46bc-88ef-21cf7b70a773"], "5f2cf192-1700-4465-9c79-5941f31c7f19": ["7e615b69-87e5-46bc-88ef-21cf7b70a773"], "b42d579d-839d-428d-8723-631b142afabf": ["19facdd7-9d56-42a4-8a72-9abb342ffdbf"], "e4b8b690-ea8d-4152-a64d-2e2e7e5bf331": ["19facdd7-9d56-42a4-8a72-9abb342ffdbf"], "fde76901-156d-4c29-9da6-cfaa943e8991": ["fd05a92a-c18c-4cc2-b329-3c407749a501"], "9ac9ef16-f47d-4e2e-a732-c6e05f75a92a": ["fd05a92a-c18c-4cc2-b329-3c407749a501"], "3b613bc7-0535-49fd-aa40-1161c44f84a2": ["76123283-e452-4494-b5b2-7a85a6918e31"], "49e1ea1e-7c6c-46c2-ab04-5fe0601f7f5f": ["76123283-e452-4494-b5b2-7a85a6918e31"], "fa9deb39-f804-4367-9244-d1fd4a6f6e8a": ["4b885ce6-817f-4f32-a98f-763e21c9ab26"], "a55434e8-29bd-4aed-af0f-7a7de46f4c01": ["4b885ce6-817f-4f32-a98f-763e21c9ab26"], "493471d8-3355-44ed-bf1d-5c1d92f824fc": ["4edaaea2-1585-473d-8d6b-fe80373b6ba2"], "d3ad2491-7fc8-457f-bd14-5e896a857019": ["4edaaea2-1585-473d-8d6b-fe80373b6ba2"], "47e4fe8b-ad05-4ed0-ae06-c16069e5ed3b": ["69458a0d-2806-4dcd-92f5-0ae04cdae8c8"], "3935227c-1b7c-43df-8c7d-f3fca9ce5a40": ["69458a0d-2806-4dcd-92f5-0ae04cdae8c8"], "e2de17d4-f32a-43bd-b813-b6686695e5ab": ["fd61ed2f-e5a9-451f-bfac-fbc598e3f6c1"], "a82a04e9-e3f2-4921-987f-4062cb6a9baa": ["fd61ed2f-e5a9-451f-bfac-fbc598e3f6c1"], "dccb37b3-d7ab-4188-8e84-7d2ccf3c3eae": ["70fbe16d-9e09-4f25-aa79-c5b5bf392cb9"], "423232e5-e8f8-48df-86f3-a1e2fda59b32": ["70fbe16d-9e09-4f25-aa79-c5b5bf392cb9"], "83ee71cd-f2a3-4a88-9305-1363695a46eb": ["af80ad32-7269-4568-b5a0-34ce8ec1b7c4"], "e79c09c5-fcc8-4e74-a4df-eb3127f0a4da": ["af80ad32-7269-4568-b5a0-34ce8ec1b7c4"], "47a8d295-a2e5-4ddb-94ff-9b261cea8240": ["95aea154-e498-4a97-bfc4-22796356a576"], "dc5a40ef-5d61-45f7-b197-ae256ca95d85": ["95aea154-e498-4a97-bfc4-22796356a576"], "eefdc5b7-d693-44e2-aa45-3889c537c7b4": ["21015d77-38f1-4114-9c72-a8d08a6128fe"], "a0316bdd-14c2-43d6-bb76-39f96fa3b73c": ["21015d77-38f1-4114-9c72-a8d08a6128fe"], "7b41c29c-9150-4d67-bc4d-47cd0dd3d7b5": ["9e293f9b-fa42-4677-bdac-f670f937a364"], "cb5d7311-bef1-4300-8614-2512e9167f9b": ["9e293f9b-fa42-4677-bdac-f670f937a364"], "cd177abd-bb49-4840-848d-2362761b4d9f": ["a9a8a759-f0bd-4441-96a4-81a0b663066f"], "a3e434ce-d150-4434-8cf8-fb51dad272f8": ["a9a8a759-f0bd-4441-96a4-81a0b663066f"], "70348660-830f-401b-9d11-1e6748d86482": ["8c361e55-5f4e-471c-9975-b715a76d882d"], "c4c4ce3c-3e64-4321-b6de-03ef781b378a": ["8c361e55-5f4e-471c-9975-b715a76d882d"], "69aeee48-d3db-452c-bde6-122830deeee6": ["70e43965-a335-41c4-b1d0-b5d0bb9f6299"], "d6dbe4e7-5cbc-4f0e-9ebc-a8bcdc5cfdea": ["70e43965-a335-41c4-b1d0-b5d0bb9f6299"], "67177f18-6f82-4a5f-a76e-33ebf7e24d14": ["479c1de0-13bf-4a7f-a682-ea5e01d477bb"], "fc87fa1d-2e92-47dd-b1e1-a61d60139e5f": ["479c1de0-13bf-4a7f-a682-ea5e01d477bb"], "f7bb0fd4-be52-44b5-8fcf-756fbce61b53": ["dc27e359-8247-44e7-9888-e977a603bfcf"], "58e17543-5934-4024-a2e1-a62870ba0cf6": ["dc27e359-8247-44e7-9888-e977a603bfcf"], "402eab35-8c56-42e0-a8ef-3bf5bbcc38b4": ["14c2be15-9105-43bc-b9c5-75cf102f1097"], "e99fe596-3f78-4189-8e03-cc811a73cafe": ["14c2be15-9105-43bc-b9c5-75cf102f1097"], "5aea11ab-d350-4e98-bdb9-f2cdf026b236": ["051692c2-6c10-47d3-a8c0-842efcd3b1ed"], "c635566f-3fe9-491d-b7d1-1341024c274a": ["051692c2-6c10-47d3-a8c0-842efcd3b1ed"], "a2423980-a5e4-4ec2-8795-0e3306d21007": ["7b375eec-082b-402b-aea7-689f4dcad9f4"], "9d74f1cb-c20c-40a4-b89f-65fcd6627936": ["7b375eec-082b-402b-aea7-689f4dcad9f4"], "a642746a-2327-4fc5-a0f5-1f2facaa0a86": ["c38c9a3a-26f2-4b9e-ba20-459c99405673"], "4b761c3b-28ac-46fd-a4c5-ec618993f3d2": ["c38c9a3a-26f2-4b9e-ba20-459c99405673"], "ae2e5c3a-0077-4542-b29b-88adaf5df0c9": ["cbd7a799-6819-4c1c-a4f7-4a772eacd6c4"], "c26b5f57-1ccf-4db4-9973-f0168b74d676": ["cbd7a799-6819-4c1c-a4f7-4a772eacd6c4"], "927b898d-220c-4498-a065-cc867396995e": ["bdef7f34-b785-4172-add3-b6a6f6f3abad"], "9301c9f1-2f55-479c-8a8c-56ec5a4d3b4d": ["bdef7f34-b785-4172-add3-b6a6f6f3abad"], "303433bd-fa08-4bf3-b77d-f53a28569267": ["b3ae6c07-be39-4ddd-abf0-0e639f92a336"], "83e30e59-4cd3-4e63-ab9a-14d936ec0d1e": ["b3ae6c07-be39-4ddd-abf0-0e639f92a336"], "865c9e4d-3012-4fbb-8d27-657a88712c89": ["57bd30e6-e1c1-4716-be9e-36d9983e0d36"], "e268b1e9-4e24-4a63-95c5-1962a102e56e": ["57bd30e6-e1c1-4716-be9e-36d9983e0d36"], "cab18ebb-2551-4a18-8d6c-d6bd136d98fd": ["3acdd23a-3248-4a66-bfd1-64ff1382e01d"], "bad35d02-8e3d-4866-a276-d6bf57fd862a": ["3acdd23a-3248-4a66-bfd1-64ff1382e01d"], "24ab7b2f-55ef-4ba4-9b63-76b118a96d8a": ["e1e0f6e5-efa1-4906-a97b-10043097fb31"], "221c2c7a-601a-45de-9b0c-9866a7374254": ["e1e0f6e5-efa1-4906-a97b-10043097fb31"], "348cd04d-e970-4595-918d-d4b839659c6d": ["15641f3e-b13d-413b-af82-535320c6c045"], "7e1fe631-dbe3-4d77-b1f7-82f7d19cb0dd": ["15641f3e-b13d-413b-af82-535320c6c045"], "6c08d0a7-a1a4-4a7b-94e6-a66bdaf2f274": ["e4106e27-8936-4860-911c-ba34996c5c05"], "a6751c3b-fcbb-4c10-81a1-94af9ee550d3": ["e4106e27-8936-4860-911c-ba34996c5c05"], "b005b997-3833-4b75-b880-ea68a1af0a1b": ["e4106e27-8936-4860-911c-ba34996c5c05"], "4764e9b3-9181-4199-8a67-fa589de11f74": ["e4106e27-8936-4860-911c-ba34996c5c05"], "4af5f2be-6d57-4b85-82b9-14c938c9e192": ["e4106e27-8936-4860-911c-ba34996c5c05"], "6a8206e8-0e4b-4427-81e6-c25c2d20269a": ["e4106e27-8936-4860-911c-ba34996c5c05"], "1537cabf-1df1-46fe-b1b0-90632c05c507": ["f53451e9-a22c-48c0-bdae-1f51e88ea293"], "7171c790-b172-4e22-a9ce-efb777243108": ["f53451e9-a22c-48c0-bdae-1f51e88ea293"], "e8f59a91-0e14-42cc-922f-59e385e95ae2": ["99925769-ac04-42c2-8d2b-dc7bcebfe650"], "29df27cf-4c8c-435b-b731-b3305b9b4aa9": ["99925769-ac04-42c2-8d2b-dc7bcebfe650"], "57d98179-8975-4fdc-a7f4-7208fec3dfe3": ["2a5317fb-ba64-4c07-be6d-f9873e8be355"], "6bd50e6c-cdd7-4987-bb13-d4896799cd20": ["2a5317fb-ba64-4c07-be6d-f9873e8be355"], "4e3f07d2-62f4-4ffa-9988-de84267ade58": ["416d19a0-9a5d-470a-859f-3e96b45317e2"], "0a82c02f-9444-4fbd-a8af-71675a09e907": ["416d19a0-9a5d-470a-859f-3e96b45317e2"], "ef2f6780-7d08-44df-8a5f-bb4ba346ef3b": ["8a7176e6-2e2b-467e-9985-2abe8b1a7e55"], "07942ab8-e5ae-4b29-9e57-659e306db35a": ["8a7176e6-2e2b-467e-9985-2abe8b1a7e55"], "bc6a296f-d801-4a5f-8be9-abe0fb734447": ["b3bddad1-9124-43ca-a26a-a7e325664b63"], "e4be92dd-4787-4dbc-85a6-80b162385c54": ["b3bddad1-9124-43ca-a26a-a7e325664b63"], "c17f510d-8194-4402-9902-99f0c83879ef": ["5c4e574a-c1d4-48ee-8a8f-7bdd38779cd5"], "afaef7ec-1640-4406-a738-99dfbe228fac": ["5c4e574a-c1d4-48ee-8a8f-7bdd38779cd5"], "475ee9e2-659e-4ac2-98be-486a9611d5ff": ["aa52019a-5958-416f-9de3-9a0e7d9e37ca"], "65041dbb-b55a-41fc-9fe6-333ef14a382f": ["aa52019a-5958-416f-9de3-9a0e7d9e37ca"], "832c0023-08f7-49dd-b2fd-fae4a894d9bc": ["b3cc0a8e-f60d-440c-8ad5-ad6b620a59dc"], "b93f1291-1900-4158-a5b4-591a4c476da8": ["b3cc0a8e-f60d-440c-8ad5-ad6b620a59dc"], "8454982b-d06d-487c-8204-4fb09b813c94": ["98a21eab-9cf5-4a54-a445-9ff41fa0cfe2"], "72a00814-181c-4d14-b7f2-8a64bc25360f": ["98a21eab-9cf5-4a54-a445-9ff41fa0cfe2"], "2c64e353-f778-473c-9f35-3c6651e18679": ["85d89886-5df0-4421-a2b0-2bfb9f0eebe6"], "a7379903-83ed-46be-9f8c-12b9ae402714": ["85d89886-5df0-4421-a2b0-2bfb9f0eebe6"], "691dc567-c483-4016-9d26-0da534bd096a": ["24969099-b3e1-49e2-b10a-0304f905f25e"], "630b875d-5263-4a8d-806f-54e4a253eee1": ["24969099-b3e1-49e2-b10a-0304f905f25e"], "2f3ee13c-5628-42fa-8ca1-5f7147a606a8": ["046ed474-f39e-4058-9224-c7228e631d92"], "11fcfcc5-53ab-4c40-a52d-9fefdfb704ac": ["046ed474-f39e-4058-9224-c7228e631d92"], "eb616cc4-459d-458d-b0a2-fc06107cbff0": ["046ed474-f39e-4058-9224-c7228e631d92"], "119da9c1-ae46-4bb5-83fa-019b1f32f421": ["046ed474-f39e-4058-9224-c7228e631d92"], "04713342-85b2-4e03-aeb0-f011c494d253": ["86802c2b-7d41-4a28-b840-a746cbad6bbf"], "563dcab8-07dd-4111-8c31-ea61214cf934": ["86802c2b-7d41-4a28-b840-a746cbad6bbf"]}, "corpus": {"956979c6-68c5-497a-b5f5-c54ad7f7f9f2": "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 Introduction\n\n2 Related Work\n\nRAG evaluation\nFinetuned RAG evaluation models\n\n\n\n3 RAGBench Construction\n\n\n3.1 Component Datasets\n\nSource Domains\nContext Token Length\nTask Types\nQuestion Sources\nResponse Generation\nData Splits\n\n\n\n3.2 TRACe Evaluation Framework\n\nDefinitions\nContext Relevance\nContext Utilization\nCompleteness\nAdherence\n\n\n3.3 RAGBench Statistics\n\n3.4 LLM annotator", "d6eec980-1d67-4ee1-a91b-e6baf2cc3503": "3.3 RAGBench Statistics\n\n3.4 LLM annotator\n\nAlignment with Human Judgements\n\n\n3.5 RAG Case Study\n\n\n\n4 Experiments\n\n4.1 LLM Judge\n4.2 Fine-tuned Judge\n4.3 Evaluation\n\n\n\n5 Results\n\nEstimating Context Relevance is Difficult\n\n\n6 Conclusion\n\n7 Appendix\n\n7.1 RAGBench Code and Data\n\n7.2 RAGBench Dataset Details\n\nPubMedQA [14]\nCovidQA-RAG\nHotpotQA [42]\nMS Marco [28]\nCUAD [12]\nDelucionQA [33]\nEManual [27]\nTechQA [3]\nFinQA [6]\nTAT-QA [47]\nHAGRID [15]\nExpertQA [25]", "c7ebb920-ec2b-4115-bbf8-3816e1bf6de5": "7.3 Response Generation Prompt\n7.4 GPT Labeling Prompt\n7.5 Annotation Post-Processing Steps\n\n7.6 Annotation Alignment with Human Judgements\n\n7.6.1 Adherence Alignment with DelucionQA\n7.6.2 Relevance and Utilization Alignment with DelucionQA\n7.6.3 Rank-based Alignment for Adherence and Relevance\n\n\n7.7 RAG Case Study\n7.8 DeBERTa model training\n\n\n\n\n\n\n\nRAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems", "c8cdf4c4-b1bc-4dcc-b145-a1a9a875f678": "Robert Friel\u2217\nGalileo Technologies Inc.\nrob@rungalileo.io\n&Masha Belyi\u2217\nGalileo Technologies Inc.\nmasha@rungalileo.io\n&Atindriyo Sanyal \nGalileo Technologies Inc.\natin@rungalileo.io", "1033ee79-7cd6-47e8-b05a-7f35af3aa919": "Abstract", "6dc8e9b1-7d0f-43be-8126-d3e9ccf26467": "Retrieval-Augmented Generation (RAG) has become a standard architectural pattern for incorporating domain-specific knowledge into user-facing chat applications powered by Large Language Models (LLMs). RAG systems are characterized by (1) a document retriever that queries a domain-specific corpus for context information relevant to an input query, and (2) an LLM that generates a response based on the provided query and context. However, comprehensive evaluation of RAG systems remains a challenge", "dffc3b77-11dc-46e7-8bdc-9c0dd9516b89": "evaluation of RAG systems remains a challenge due to the lack of unified evaluation criteria and annotated datasets. In response, we introduce RAGBench: the first comprehensive, large-scale RAG benchmark dataset of 100k examples. It covers five unique industry-specific domains and various RAG task types. RAGBench examples are sourced from industry corpora such as user manuals, making it particularly relevant for industry applications. Further, we formalize the TRACe evaluation framework: a set", "7045ac16-3857-4288-be01-47e1c0ab5f1f": "formalize the TRACe evaluation framework: a set of explainable and actionable RAG evaluation metrics applicable across all RAG domains. We release the labeled dataset at https://huggingface.co/datasets/rungalileo/ragbench. RAGBench explainable labels facilitate holistic evaluation of RAG systems, enabling actionable feedback for continuous improvement of production applications. Thorough extensive benchmarking, we find that LLM-based RAG evaluation methods struggle to compete with a finetuned", "fc29944f-5edf-4411-8dd7-29dca557bdc0": "methods struggle to compete with a finetuned RoBERTa model on the RAG evaluation task. We identify areas where existing approaches fall short and propose the adoption of RAGBench with TRACe towards advancing the state of RAG evaluation systems.", "4a942b23-800b-4ad9-94ed-42228dac0cf7": "**footnotetext: Equal Contributions\n\n\n1 Introduction", "9f20f85d-c3a4-421b-a855-3600c37fc2e0": "Despite remarkable reasoning and conversational abilities, out-of-the-box pre-trained Large Language Models (LLMs) struggle to reason about out-of-domain, knowledge-intensive queries [20, 13]. In response, Retriever-Augmented Generation (RAG) systems [20, 19] are becoming increasingly popular in user-facing dialogue applications [34]. Generally, RAG systems comprise a retriever component that queries relevant documents from an in-domain corpus and a downstream LLM generator model that", "dab117d7-86d9-48ca-9a45-4c6cffebc846": "corpus and a downstream LLM generator model that incorporates the retrieved documents along with the original user query to output an informed response. The additional context helps ground the LLM in factual information and has been shown to boost performance on knowledge-intensive tasks [20].", "934f65c5-c3fa-4edb-bb97-01661a9d26cb": "Still, when used in production settings, RAG systems are prone to hallucinations as the generator model struggles to retrieve relevant information from the context [1, 30, 7]. In the absence of a one-fits-all approach, application-specific RAG systems must be fine-tuned for optimal performance on domain-specific tasks. However, the choice of retriever and generator models for each application is complex and has serious implications on overall system quality and costs. With numerous commercial", "293b420d-aff1-4639-a4fb-0f7aef26edfe": "quality and costs. With numerous commercial and open-source generative LLMs readily available111https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard and many variable parameters in the RAG system design (Figure 1), tuning an optimal system for a particular RAG application involves iterative evaluation of multiple configurations. This motivates the need for automated RAG evaluation solutions.", "dc5a1053-d708-42e6-8973-7e52272fe7ff": "In response, automated RAG evaluation systems like RAGAS [9] and TruLens [36] have emerged. These systems adopt a zero-shot LLM prompt-based approach to predict a set of curated RAG evaluation metrics. However, the lack of unified RAG benchmarks makes it difficult to compare approaches against each other. Each new study designs a new dataset, often employing LLMs as generators and labelers [9, 32, 4], which renders them irreproducible. A few benchmarks like RGB [4], AttributionBench [22] and", "37460103-4729-48ed-aa63-dad126418bfe": "like RGB [4], AttributionBench [22] and RAGTruth [40] have been proposed recently, but they are small in size and target a disjoint set of labels. The exact RAG evaluation criteria also vary from study to study. ARES [32] and RAGAS [9] define a context relevance metric to evaluate the quality of the retrieved documents, along with answer relevance and faithfulness to evaluate the quality of the generative model. However, others have explored other metrics like correctness [1] noise rejection", "59d327ae-bfc2-451d-ae47-98339c91edd5": "metrics like correctness [1] noise rejection and robustness [4], to name a few. Finally, most studies evaluate on small in-domain evaluation datasets that are specific to each new application [32, 33, 9, 1, 4], leaving cross-domain generalization an open question.", "39852235-be1e-4aa3-8cbb-b8de344c1059": "In this work we propose RAGBench: a comprehensive dataset for training and benchmarking RAG evaluation models. RAGBench comprises data sourced from multiple domains along with a comprehensive suite of evaluation metrics. Specifically, we adopt existing metric definitions for context relevance, answer faithfulness [9, 32] and introduce two new metrics: context utilization and answer completeness. We argue that this new suite of metrics better describes the overall RAG system performance, with", "3f1dd124-0e9e-42de-a541-a60f5d5e31a9": "the overall RAG system performance, with the potential to provide granular, actionable insights to the RAG practitioner.", "3cec763c-ddb7-4a42-9dc0-b47ac51c175d": "We evaluate state-of-the art LLMs and existing RAG evaluation systems on RAGBench. We find that a 400M-parameter DeBERTa-large model that was fine-tuned on RAGBench outperforms few-shot LLM judges across numerous domains and task types. We highlight this result to motivate future work aimed at leveraging these data for advancing RAG evaluation models and improving on the proposed benchmark.\n\n\n\n\n2 Related Work", "21062649-5d07-4859-a410-94dbdadda3ff": "We differentiate our work from existing ground-truth RAG datasets like ChatRAGBench [23], CRAG [41], DomainRAG [38], and ALCE [10]. These datasets contain RAG samples with ground-truth responses and are used for end-to-end evaluation of RAG systems via response-level metrics like exact match or ROUGE scores. In contrast, we design RAGBench to enable development of more mature evaluation systems that effectively evaluate different parts of the RAG system along multiple dimensions like retriever", "ccd230ab-7900-44ea-aaeb-656e9caf4acb": "system along multiple dimensions like retriever relevance, adherence and completeness of the response.", "c2674fe0-4e8c-4225-91cd-95654a656a9d": "Various RAG benchmarks focus specifically on hallucination detection [40, 33, 21, 5].", "ec4f30f2-64fa-4083-aae4-677435fc8d0a": "FELM [5] is a multi-domain and task dataset with factuality labels for open domain QA. RAGTruth [40], DelucionQA [33], and HaluEval [21] are RAG-specific datasets with both synthetic as well as human-annotated labels for hallucinations in LLM reseponses. While these are appropriate benchmarks for hallucination detection models, they do not offer the level of granularity we offer with RAGBench that is necessary to understand the RAG system as a whole.", "af1fd2da-bf04-415b-a420-2248fffe9f90": "RAG evaluation", "b98ea011-42b4-47bc-888b-6f34e754ee66": "Recently, several parallel efforts have proposed approaches to automated RAG evaluation. In RAGAS [9], the authors query an LLM-judge (GPT-3.5) with a curated prompt to evaluate context relevance, answer relevance and faithfulness of a RAG response. Next, Saad-Falcon et\u00a0al. [32] propose ARES, a framework for fine-tuning smaller NLI models to predict the same metrics. In parallel, Chen et\u00a0al. [4] develop a heuristic system to probe LLM\u2019s robustness to noisy and irrelevant context documents, and", "234fb7da-257f-4526-912a-87f2a9f2dff5": "to noisy and irrelevant context documents, and Adlakha et\u00a0al. [1] explore heuristic algorithms to estimate RAG correctness and faithfulness. The lack of established RAG benchmarks makes it difficult to compare these approaches against each other. We aim to address this limitation by introducing RAGBench.", "08f8b828-b7f5-42b4-b92f-18738760512b": "Finetuned RAG evaluation models", "c3928a27-9b3a-4b83-bdc1-9a372919c0b6": "Fine-tuned LLM judges are another a common way to approach the LLM evaluation task [16, 44, 40]. A number of studies also leverage small, fine-tuned Natural Language Inference (NLI) models for RAG hallucination detection [2, 22, 32]. NLI models measure the degree of entailment between a premise and a hypothesis, which has been successfully repurposed for evaluating LLM response attribution in RAG setting. In this work, we train and evaluate an NLI model for RAG evaluation using RAGBench. The", "d5a021f9-b20b-4af4-b469-169ba73dea4d": "NLI model for RAG evaluation using RAGBench. The fine-tuned model not only outperforms LLM judges in hallucination/attribution detection but also excels on the new RAG evaluation metrics we propose.", "bcf36b24-871f-4155-8798-863de89f474c": "Figure 1: RAG system workflow, with highlighted variable parameters: (1) Context format and length, (2) retriever model, (3) number of retrieved documents, and (4) generation model.\n\n\n\n\n\n3 RAGBench Construction\n\n\n3.1 Component Datasets", "b4463def-5da0-4a46-a2d5-7afa17fcbc51": "RAGBench is a collection of real-world datasets that span different domains and RAG task types. We source data from open-book Question-Answer (QA) datasets (CovidQA [26], PubmedQA [14], HotpotQA [42], MS Marco [28], CUAD [12], EManual [27], TechQA [3], FinQA [6], TAT-QA [47], ExpertQA [25], HAGRID [15]), as well one that was specifically adapted for RAG (DelucionQA [33]). We transform all 12 component datasets to a standardized RAG format with consistent annotations. To best represent", "406a7322-e66c-4d27-abc3-f15889ef096a": "with consistent annotations. To best represent real-world RAG scenarios, we vary a number parameters to construct the benchmark: the source domain, number of context documents, context token length, and the response generator model Figure 1 illustrates where these variable parameters fall in the RAG pipeline.", "5cc8699f-cccb-436e-81af-7fb381af5325": "Source Domains\n\nRAGBench comprises five distinct domains: bio-medical research (PubmedQA, CovidQA), general knowledge (HotpotQA, MS Marco, HAGRID, ExperQA), legal contracts (CuAD), customer support (DelucionQA, EManual, TechQA), and finance (FinBench, TAT-QA). We select these specific domains based on availability of data, and applicability to real-world RAG applications across different industry verticals. For detailed descriptions of each component data source, refer to Appendix 7.2.", "133b4d25-fd44-4ec0-ac11-dc363f85cc90": "Context Token Length\n\nContext token length in RAGBench ranges from 100 to 11k tokens, which we report in Table 1. Notably, CUAD documents feature long contexts of up to 11k tokens each, compared to the relatively short context in PubMedQA.\n\n\nTable 1: RAGBench component datasets.\n\n\n\nDataset\n\n\nDomain\n\n\n\n\nDocument Source\n\n\n\n\nQuestion Source\n\n\n\n\n#docs\n\n\n\n\ndoc length\n\n\n\n\n#Train\n\n\n\n\n#Dev\n\n\n\n\n#Test\n\n\n\n\n\n\nPubMedQA\n\n\nbiomedical \nresearch\n\n\n\n\nresearch \nabstracts\n\n\n\n\nautomated heuristics\n\n\n\n\n4\n\n\n\n\n99", "35b9348f-c9e7-4d47-a010-f368a807f93a": "automated heuristics\n\n\n\n\n4\n\n\n\n\n99\n\n\n\n\n19.5k\n\n\n\n\n2.5k\n\n\n\n\n2.5k\n\n\n\n\nCovidQA-RAG\n\n\nbiomedical \nresearch\n\n\n\n\nresearch \npapers\n\n\n\n\nexpert\n\n\n\n\n4\n\n\n\n\n122\n\n\n\n\n2.5k\n\n\n\n\n534\n\n\n\n\n492\n\n\n\n\nHotpotQA\n\n\ngeneral \nknowledge\n\n\n\n\nwikipedia\n\n\n\n\ncrowd-sourced\n\n\n\n\n4\n\n\n\n\n126\n\n\n\n\n3.7k\n\n\n\n\n847\n\n\n\n\n776\n\n\n\n\nMS Marco\n\n\ngeneral \nknowledge\n\n\n\n\nweb pages\n\n\n\n\nuser \nweb queries\n\n\n\n\n10\n\n\n\n\n94\n\n\n\n\n3.7k\n\n\n\n\n790\n\n\n\n\n839\n\n\n\n\nHAGRID\n\n\ngeneral knowledge\n\n\n\n\nwikipedia\n\n\n\n\nexpert\n\n\n\n\n3\n\n\n\n\n153\n\n\n\n\n2.0k\n\n\n\n\n322\n\n\n\n\n1.3k\n\n\n\n\nExpertQA", "d7c4c452-66b2-4a73-8eca-95c924cfabab": "153\n\n\n\n\n2.0k\n\n\n\n\n322\n\n\n\n\n1.3k\n\n\n\n\nExpertQA\n\n\ngeneral knowledge\n\n\n\n\ngoogle search\n\n\n\n\nexpert\n\n\n\n\n3\n\n\n\n\n548\n\n\n\n\n1.6k\n\n\n\n\n202\n\n\n\n\n203\n\n\n\n\nCUAD\n\n\nlegal\n\n\n\n\nlegal \ncontracts\n\n\n\n\nexpert\n\n\n\n\n1\n\n\n\n\n11k\n\n\n\n\n1.5k\n\n\n\n\n506\n\n\n\n\n508\n\n\n\n\nDelucionQA\n\n\ncustomer\nsupport\n\n\n\n\nJeep manual\n\n\n\n\nLLM\n\n\n\n\n3\n\n\n\n\n296\n\n\n\n\n1.5k\n\n\n\n\n177\n\n\n\n\n182\n\n\n\n\nEManual\n\n\ncustomer\nsupport\n\n\n\n\nTV manual\n\n\n\n\nannotator\n\n\n\n\n3\n\n\n\n\n165\n\n\n\n\n1k\n\n\n\n\n132\n\n\n\n\n132\n\n\n\n\nTechQA\n\n\ncustomer\nsupport\n\n\n\n\nTechnotes\n\n\n\n\ntech forums\n\n\n\n\n5\n\n\n\n\n1.8k\n\n\n\n\n1.2k", "6f82022a-78da-4ee0-9f2e-ce8085019eec": "tech forums\n\n\n\n\n5\n\n\n\n\n1.8k\n\n\n\n\n1.2k\n\n\n\n\n302\n\n\n\n\n310\n\n\n\n\nFinQA\n\n\nfinance\n\n\n\n\nearning \nreports\n\n\n\n\nexpert\n\n\n\n\n3\n\n\n\n\n310\n\n\n\n\n12k\n\n\n\n\n1.7k\n\n\n\n\n2.2k\n\n\n\n\nTAT-QA\n\n\nfinance\n\n\n\n\nfinancial \nreports\n\n\n\n\nexpert\n\n\n\n\n5\n\n\n\n\n96\n\n\n\n\n26k\n\n\n\n\n3.2k\n\n\n\n\n3.2k\n\n\n\n\nTotal\n\n\n\n\n\n\n\n78k\n\n\n\n\n12k\n\n\n\n\n11k\n\n\n\n\n\n\n\n\nTask Types", "8675d89a-5e2a-4986-89f5-3dcf976f43ee": "We curate RAGBench to inlcude a variety of difficult RAG task types. Customer support datasets simulate a common application of RAG in industry settings. FinQA and TAT-QA require numerical reasoning over hybrid tabular and text data. HotpotQA, CovidQA, and PubMedQA necessitate retrieval and reasoning over multiple context docs. The CUAD dataset is a challenging addition to RAGBench for several reasons: (i) it represents a difficult and highly-specialized real-world domain in which of-the-shelf", "268e5170-84a9-4d69-8dfc-9eb68b9614bf": "real-world domain in which of-the-shelf pre-trained LLM models struggle to perform well [24], and (ii) it is equally challenging in RAG context due to very long context lengths of legal contract documents.", "2692a231-9134-4581-a9f0-0f475238c461": "Question Sources", "25b55637-890f-44ed-8a09-f575c26a4640": "All component datasets include domain-specific questions that represent real-world user queries about various topics. Questions for DelucionQA, HotpotQA, and EManual are crowd-sourced; questions for CovidQA, CUAD, HAGRID, ExpertQA, and FinQA are composed by domain experts; MS Marco is sourced from real-world user web search queries; likewise, TechQA questions are user queries posted on IBM technical forums; PubMedQA is the only dataset with automatically-generated questions from research", "494a8a4f-c5bc-495b-9de7-8ac5e2e97179": "automatically-generated questions from research article titles.", "5ce2cfac-3378-471e-9856-0f323dd37ec6": "Response Generation", "33b554fa-4cdf-4a1e-9764-98cf051f121c": "For each component dataset we generate responses with LLMs. Exceptions to this are HAGRID and ExpertQA datasets, which contain LLM-generated responses in the original data. To introduce variability into the dataset, we generate two responses per input with different modes: GPT-3.5 (gpt-3.5-0125) and Claude 3 Haiku. Both are proprietary models that are offered at a reasonable price point222https://openai.com/api/pricing/, https://www.anthropic.com/api, which we believe make them suitable", "516b4a68-6d6a-4969-8974-8a22c7bbd11e": "which we believe make them suitable candidates for generating real-world RAG responses. For CUAD we only generate responses with Claude 3 Haiku due to prohibitively long context lengths that exceed the GPT-3.5 16k token limit. To encourage a diverse distribution of labels in RAGBench, we use a basic prompt (Appendix 7.3) that does not explicitly require the model to stick to the provided context when generating the response. We set the temperature to 1.0 for generation.", "01a8c270-a8e6-4aa2-ad8d-591d09222fb2": "Data Splits\n\nWe split each component dataset into train, validation, and test sets, ensuring there is no overlap in queries across splits from the same data source. RAGBench totals 100k samples, split across train, validation, and test sets. Component dataset statistics are reported in Table 1.\n\n\n\n\n\n3.2 TRACe Evaluation Framework", "b85b9def-63c2-491f-82c5-95b5569cfa28": "We propose a suite of four comprehensive metrics to evaluate the quality of the retriever and the response generator components of RAG. An optimal RAG system must balance accuracy and efficiency. The retriever should precisely return all the necessary information to address the user query, avoiding any superfluous data. The generator must effectively utilize the retrieved information, ensuring the response is strictly based on the provided context without introducing any hallucinations in the", "bef9d765-3aa1-4d8a-b45f-1d7b1c962912": "without introducing any hallucinations in the output.", "c2316c00-b3c4-4111-9a49-1772cb932c01": "Towards comprehensive evaluation of the abovementioned criteria, we introduce the TRACe evaluation framework to measure uTilization, Relevance, Adherence, and Completeness of a RAG system. Utilization, Adherence, and Completeness measure the quality of the generator. Adherence here is synonymous with previously proposed answer faithfullness, groundednes, and attribution, all terms used in literature to measure how well an LLM output adheres to a source of factual information. Relevance measures", "8c9611d8-2a15-4d19-ab4b-8dc993b55c8f": "source of factual information. Relevance measures the quality of the retriever output with respect to the query. Below we formalize the definition of each metric.", "9a65cdfb-9f73-4177-a048-892aec95c2f7": "Definitions", "733ee48e-9de4-425f-ab4a-a22d1673d993": "Let D\ud835\udc37Ditalic_D be a set of context documents {d1\u2062\u2026\u2062dn}subscript\ud835\udc511\u2026subscript\ud835\udc51\ud835\udc5b\\{d_{1}...d_{n}\\}{ italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT \u2026 italic_d start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } retrieved for a RAG input query. We define a set of relevant tokens in disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as Ri={t1,\u2026\u2062tr}subscript\ud835\udc45\ud835\udc56subscript\ud835\udc611\u2026subscript\ud835\udc61\ud835\udc5fR_{i}=\\{t_{1},...t_{r}\\}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t", "28517ea1-930d-4f9c-87d7-442dfc0fca68": "italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 italic_t start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT }. Risubscript\ud835\udc45\ud835\udc56R_{i}italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT encodes information in context document disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT that is useful for answering the query. Similarly, we define Ui={t1,\u2026\u2062tu}subscript\ud835\udc48\ud835\udc56subscript\ud835\udc611\u2026subscript\ud835\udc61\ud835\udc62U_{i}=\\{t_{1},...t_{u}\\}italic_U start_POSTSUBSCRIPT italic_i", "56213d54-979b-417c-b141-36edbd8d6e8b": "start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 italic_t start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT } as the set of utilized tokens in document disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, which reflect information that the generation model is using to produce a response. Refer to Figure 2 for a visual representation of relevant and utilized spans. L\u2062e\u2062n\u2062(x)\ud835\udc3f\ud835\udc52\ud835\udc5b\ud835\udc65Len(x)italic_L italic_e italic_n ( italic_x )", "06dcb043-19b1-420e-b1ba-d7b069132871": "italic_e italic_n ( italic_x ) measures the length of strings in x\ud835\udc65xitalic_x, which can be interpreted as character length, token length, or sentence length. For calculating ground-truth metrics, we employ sentence-length, since it aligns best with our annotation schema (Section 3.4). However, token or character length may also be suitable for other use cases.", "48bd5401-099e-4803-8aab-891131c92b0a": "Figure 2: Example of RAG Question, Context, and Response. Relevant context spans are highlighted, and utilized spans are underlined.\n\n\n\nContext Relevance", "421d7118-269a-41ac-b2c2-f764bfea9f7b": "Context Relevance\n\nContext Relevance is defined in [9, 32] as the fraction of the retrieved context that is relevant to the input query. Low relevance points to an inefficient retriever that supplies excess information to the generation model. Long context inputs into the generator may accrue unnecessary costs, as well as compromise the quality of the generated output. We measure relevance of context document disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT as:", "6f91c28a-746e-4502-b504-598736db89d3": "document relevance=L\u2062e\u2062n\u2062(Ri)L\u2062e\u2062n\u2062(di)document relevance\ud835\udc3f\ud835\udc52\ud835\udc5bsubscript\ud835\udc45\ud835\udc56\ud835\udc3f\ud835\udc52\ud835\udc5bsubscript\ud835\udc51\ud835\udc56\\text{document relevance}=\\frac{{Len(R_{i})}}{Len(d_{i})}document relevance = divide start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG\n\n(1)\n\n\nExample-level relevance can be aggregated over all context documents in the example as:", "3c5f5df1-3fee-4461-8b4d-7c8b7adf1d3c": "example relevance=\u2211i=1|D|L\u2062e\u2062n\u2062(Ri)\u2211i=1|D|L\u2062e\u2062n\u2062(di)example relevancesuperscriptsubscript\ud835\udc561\ud835\udc37\ud835\udc3f\ud835\udc52\ud835\udc5bsubscript\ud835\udc45\ud835\udc56superscriptsubscript\ud835\udc561\ud835\udc37\ud835\udc3f\ud835\udc52\ud835\udc5bsubscript\ud835\udc51\ud835\udc56\\text{example relevance}=\\frac{\\sum_{i=1}^{|D|}{Len(R_{i})}}{\\sum_{i=1}^{|D|}%", "46a37333-e815-4a6f-b625-cd644849b8f5": "Len(d_{i})}example relevance = divide start_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG", "6bdaf012-c9ef-4776-ad20-833319f643c4": "(2)\n\n\n\n\n\nContext Utilization\n\nContext Utilization is a new metric introduced in TRACe. We aim to measure the the fraction of the retrieved context that is used by the generator to produce the response.\nLow Utilization in combination with low Relevance points to a greedy retriever, while low Utilization alone points to a weak generator that fails to leverage the provided context efficiently. Document-level and example-level Utilization are defined as:", "63cfa073-4954-4436-ac00-287c761500bd": "document utilization=L\u2062e\u2062n\u2062(Ui)L\u2062e\u2062n\u2062(di)example utilization=\u2211i=1|D|L\u2062e\u2062n\u2062(Ui)\u2211i=1|D|L\u2062e\u2062n\u2062(di)formulae-sequencedocument utilization\ud835\udc3f\ud835\udc52\ud835\udc5bsubscript\ud835\udc48\ud835\udc56\ud835\udc3f\ud835\udc52\ud835\udc5bsubscript\ud835\udc51\ud835\udc56example utilizationsuperscriptsubscript\ud835\udc561\ud835\udc37\ud835\udc3f\ud835\udc52\ud835\udc5bsubscript\ud835\udc48\ud835\udc56superscriptsubscript\ud835\udc561\ud835\udc37\ud835\udc3f\ud835\udc52\ud835\udc5bsubscript\ud835\udc51\ud835\udc56\\text{document utilization}=\\frac{{Len(U_{i})}}{Len(d_{i})}\\quad\\text{example %", "0d537d5c-3a75-4a28-b59b-b0a22c1beaf8": "utilization}=\\frac{\\sum_{i=1}^{|D|}{Len(U_{i})}}{\\sum_{i=1}^{|D|}Len(d_{i})}document utilization = divide start_ARG italic_L italic_e italic_n ( italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG example utilization = divide start_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n (", "4f351567-4fc8-4d87-98fb-b28f064c239e": "end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG \u2211 start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | italic_D | end_POSTSUPERSCRIPT italic_L italic_e italic_n ( italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG", "9314d874-dea5-4349-869b-f3cb3df4d93d": "(3)\n\n\n\n\n\nCompleteness", "1e00ce4f-3040-43d3-a079-5262312ec1f9": "Completeness is another new metrics we introduce to measure how well the response incorporates all the relevant information in the context. Note that this is different from Utilization; it is possible to have high Relevance and high Utilization, but low Completeness when the generator utilizes irrelevant information in the context to produce a low quality response. Completeness for document disubscript\ud835\udc51\ud835\udc56d_{i}italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT is calculated as the fraction", "26d6ef3e-df4d-40f6-9a37-8c170ad97174": "end_POSTSUBSCRIPT is calculated as the fraction of utilized substrings among all relevant substrings:", "cf4e5943-db15-466b-b88e-a5ec65fdce52": "completeness=L\u2062e\u2062n\u2062(Ri\u2229Ui)L\u2062e\u2062n\u2062(Ri)completeness\ud835\udc3f\ud835\udc52\ud835\udc5bsubscript\ud835\udc45\ud835\udc56subscript\ud835\udc48\ud835\udc56\ud835\udc3f\ud835\udc52\ud835\udc5bsubscript\ud835\udc45\ud835\udc56\\text{completeness}=\\frac{Len(R_{i}\\cap U_{i})}{Len(R_{i})}completeness = divide start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT \u2229 italic_U start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG start_ARG italic_L italic_e italic_n ( italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) end_ARG\n\n(4)", "c06d058e-c144-47ba-ab39-9f202ccd7526": "(4)\n\n\nAnd can be extended to example-level by considering all relevant and utilized substrings across all context documents.\n\n\n\nAdherence", "550d0ce4-d554-45e3-ac67-3f5e98af18e0": "Adherence is designed to detect hallucinations in RAG responses. Our definition of Adherence is synonymous with answer faithfullness [9, 32], groundednes [36], and attribution [31]. For alignment with existing hallucination detection approaches, we define example-level adherence as a boolean indicating whether or not all parts of the response are grounded in the context. However, in our annotation schema (Section 3.4) we also define", "4646db17-7ce8-4d62-a40a-ecc09a1f4692": "annotation schema (Section 3.4) we also define Ai={t1,\u2026\u2062ta}subscript\ud835\udc34\ud835\udc56subscript\ud835\udc611\u2026subscript\ud835\udc61\ud835\udc4eA_{i}=\\{t_{1},...t_{a}\\}italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , \u2026 italic_t start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT } as the set of response tokens that are supported by the context to enable granular Adherence evaluation.", "fc363ba1-515d-43db-90f8-10517f803394": "Figure 3: Distributions of relevance, utilization, and completeness labels in RAGBench. Y-axis is normalized to visualize densities.\n\n\n\n\n\n3.3 RAGBench Statistics", "1e935891-b377-4812-b1ef-48492b27d593": "3.3 RAGBench Statistics\n\nRAGBench component datasets contain between 1% - 20% hallucinations. ExpertQA, CovidQA, and MS Marco contain the highest fraction of hallucinated responses (12%, 16%, and 13%, respectively), while Cuad, FinQA, and TAT-QA contain the least (about 1% for each). We visualize distributions of relevance, utilization, and completeness scores in Figure 3.\n\n\n\n\n3.4 LLM annotator", "4e18da13-de46-411a-b126-44e05509079e": "3.4 LLM annotator\n\nWe prompt GPT-4 (gpt-4-0125-preview) to produce ground truth Adherence, Relevance, and Utilization labels for input (documents, query, response) tuples in RAGBench. Completeness is easily derived from span-level Relevance and Utilization annotations, thus we don\u2019t request explicit annotations for it.", "d1fdb647-7f31-4cd6-ae2d-99310c19d4e5": "For high quality labels, we use proven techniques like chain of thought [39] that have been shown to maximize the correlation between GPT-4 and human judgements [43, 46].\nFor relevance and utilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the input documents.", "b9266e4c-026e-4860-9ba3-8b04f34861c1": "For adherence, we instruct the LLM to identify which response sentences, if any, are supported by the provided context. We can then derive an example-level boolean adherence label by checking if all response sentences are supported. The exact prompt used for annotation is provided in Appendix 7.4. We apply post-processing steps to ensure high quality, reliable annotations from our GPT-labeler, which we outline in Appendix 7.5.", "f83eab95-fa01-4db4-ab5e-5203d796361d": "Table 2: GPT-4 annotator achieves high alignment with human judgements. We report F1 and Accuracy alignment metrics on human annotated subsets of DelucionQA. Adherence annotations are evaluated against DelucionQA human-annotated labels. Relevance and Utilization are evaluated against DelucionQA(40): a subset of 40 examples randomly sampled from the DelucionQA test set and annotated by the authors.\n\n\n\nTest Set\nMetric\nF1\nAccuracy\n\n\n\n\nDelucionQA - example level\nAdherence\n0.96\n0.93", "0339f981-df9a-4fb7-a4b0-86c7496804c5": "DelucionQA - example level\nAdherence\n0.96\n0.93\n\n\nDelucionQA - span level\nAdherence\n0.97\n0.95\n\n\nDelucionQA(40) - span level\nUtilization\n0.92\n0.94\n\n\nDelucionQA(40) - span level\nRelevance\n0.76\n0.78\n\n\n\n\n\nAlignment with Human Judgements", "21c90303-40b6-4f83-9fd3-d170ccfaa6a1": "We validate our metric formulations and labeling approach on a human annotated benchmark. DelucionQA [33] is a curated collection of user queries on the operation of Jeep\u2019s 2023 Gladiator model. Natural language queries are first generated by an LLM, then reviewed and filtered by human annotators. Context documents are sourced from Jeep\u2019s Gladiator User Manual, and responses are generated by various LLMs. Combined with example-level and span-level annotations for hallucination, DelucionQA", "279510c1-639e-442c-9445-2dc4b36e2147": "annotations for hallucination, DelucionQA represents a realistic distribution of real-world user queries and RAG responses. We find that our GPT annotator achieves 93% and 95% example- and span-level agreement with human judgements on the DelucionQA test split (Table 2).", "6846ced7-92f6-40d3-90e0-9fc1b63f8ddd": "To validate relevance and utilization annotations, we also annotate a small subset of DelucionQA with granular relevance and utilization labels. We refer to this subset as DelucionQA(40) in Table 2. Similar to adherence, we observe high correlation between relevance and utilization judgements from GPT-4 and humans. Details of the annotation process and additional validations are found in Appendix 7.6.\n\n\n\n\n\n(a) Retriever vs. Relevance\n\n\n\n\n(b) LLM vs. Hallucination", "9d8b55de-f1a1-4c7f-a88b-b56cfa790198": "(b) LLM vs. Hallucination\n\n\n\n\n\n(c) Prompt vs. Utilization\n\n\n\n\n(d) Prompt vs. Completeness", "f2d582db-30ae-43d6-9d2c-fb93cb193b34": "Figure 4: Relationship between RAG system configuration and TRACe metrics. (a) The choice and configuration of RAG retriever component affects the average relevance of the retrieved context. In this example, a dense retriever with a low number of documents per query (k=2) yields the highest average context relevance. (b) The choice of LLM and generation prompt affect how well the RAG system utilizes the provided context. Prompting the LLM with a detailed chain-of-thought prompt leads to reduced", "bb1565f3-7324-4955-94f0-cb99c7819d76": "detailed chain-of-thought prompt leads to reduced hallucinations, as well as higher response utilization (c) and completeness (d) rates.", "e5f3301b-0714-4d19-8ce9-837aeab09cd3": "3.5 RAG Case Study", "99822720-1f6c-4f38-9bb3-5a4a8346aa9f": "We design a case study to further motivate and validate the proposed TRACe framework. We sample 100 realistic world knowledge queries from the RAGTruth [40] QA training set, along with 2,512 unique context document chunks from the same dataset to use as inputs into our mock RAG systems. We simulate RAG setups of varying quality by controlling four configurable parameters: the retriever (sparse TF-IDF vs. dense), number of retrieved documents (2 vs. 4), generation model (GPT-3.5-turbo vs.", "99da1248-4d20-4754-a339-0263e028f44d": "(2 vs. 4), generation model (GPT-3.5-turbo vs. GPT-4o), and 4 versions of the generation prompt template. For illustrative purposes, we evaluate one prompt template comprising of only the question (no context) vs. three RAG-style templates that encourage the LLM to utilize the provided context and/or utilize chain-of-thought (CoT). The full generation templates are provided in Appendix 7.7. We generate responses for the 100 RAGTruth queries with each of the 32 resulting RAG systems and use our", "9ccc1c8e-b7a4-4f13-a0ea-ea9f0e70c7a9": "each of the 32 resulting RAG systems and use our LLM annotation prompt to evaluate the TRACe metrics.", "9d9c792e-98d8-4d32-a554-8f7cd0ff4e40": "Figure 4 demonstrates how the different RAG configurations influence TRACe metrics. For example, we confirm that the choice of the generative LLM model affects the amount of hallucinations (or non-adherent responses) in the generated text. Not surprisingly, GPT-4o combined with a chain-of-thought prompt yields the lowest amount of hallucination compared to the weaker GPT-3.5 model and less detailed prompts. Curiously, we find that GPT-3.5 hallucinates more when prompted with CoT, which may", "b23f3d14-4cd5-4150-b7c1-21524ff6b107": "more when prompted with CoT, which may point to limitations in the model\u2019s ability to reason about complex concepts 4(b). Overall, we find that prompting the LLM to think step-by-step and explain its reasoning leads to higher utilization of the provided context and more complete responses (4(c), 4(d)). Finally, we show that the choice of the retriever affects average relevance of the retrieved context documents per query 4(a).", "04782e90-3735-4578-ad61-6765a24fc885": "4 Experiments\n\n\n4.1 LLM Judge\n\nWe benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query GPT-3.5 with our annotation prompt, (2) RAGAS [9], and (3) TruLens [36]. RAGAS employs a series of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context Relevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness (Adherence) and Context Relevance.\n\n\n\n\n4.2 Fine-tuned Judge", "b00abeb3-d6d7-4025-90a1-f4fc15424d2c": "We fine-tune a DeBERTa-v3-Large [11] NLI checkpoint333https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli from Laurer et\u00a0al. [18] with one key architecture modification: we add a shallow prediction head for each of the output RAG metrics, which allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective and enables transfer learning from head to head through back-propagation down to the shared base layers. Each prediction head is a", "74fa7cc8-4e37-48e1-a627-c669402cac8e": "the shared base layers. Each prediction head is a single layer feed-forward net that acts on the token-level output of the last DeBERTa layer.", "9fef63a5-c297-41d0-891f-645b713082f2": "We attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and another head on the response tokens to estimate Adherence. For training, we broadcast sentence-level annotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and Adherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized spans and Adherent spans and calculate TRACe metrics using equations 2, 3, and 4. For comparison with", "abe96608-59c9-4659-b292-0c3cc07a5826": "using equations 2, 3, and 4. For comparison with existing hallucination detection approaches, we also aggregate Adherence probabilities across the entire response to produce an example-level response adherence label. For details about training and hyperparameters, refer to Appendix 7.8.", "4bd535c8-0572-414d-97e3-96635485eb77": "4.3 Evaluation\n\nOur granular annotation schema allows for various evaluation setups. For example, we could evaluate either span-level or example/response-level predictions. For easy comparison with existing RAG evaluation approaches that are less granular, we report area under the receiver-operator curve (AUROC) on the response-level hallucination detection task, and root mean squared error (RMSE) for example-level context Relevance and Utilization predictions.", "6769c525-70e7-4f49-8d45-7299a919aa66": "Table 3: Benchmark evaluation on test splits. Reporting AUROC for predicting hallucinated responses (Hal), RMSE for predicting Context Relevance (Rel) and utilization (Util). \u2217 indicates statistical significance at 95% confidence intervals, measured by bootstrap comparing the top and second-best results. RAGAS and Trulens do not evaluate Utilization.\n\n\n\n\nGPT-3.5\nRAGAS\nTruLens\nDeBERTA\n\n\nDataset\n\n\nHal\u2191\u2191\\uparrow\u2191\n\n\n\n\nRel\u2193\u2193\\downarrow\u2193\n\n\nUtil\u2193\u2193\\downarrow\u2193\n\n\n\nHal\u2191\u2191\\uparrow\u2191\n\n\n\n\nRel\u2193\u2193\\downarrow\u2193", "4b5ca1f0-c646-4e81-96e1-cf61bd1d3438": "Hal\u2191\u2191\\uparrow\u2191\n\n\n\n\nRel\u2193\u2193\\downarrow\u2193\n\n\nUtil\u2193\u2193\\downarrow\u2193\n\n\n\nHal\u2191\u2191\\uparrow\u2191\n\n\n\n\nRel\u2193\u2193\\downarrow\u2193\n\n\nUtil\u2193\u2193\\downarrow\u2193\n\n\n\nHal\u2191\u2191\\uparrow\u2191\n\n\n\n\nRel\u2193\u2193\\downarrow\u2193\n\n\nUtil\u2193\u2193\\downarrow\u2193\n\n\n\nPubMedQA\n\n\n0.51\n\n\n\n\n0.21\u2217\n\n\n0.16\n\n\n0.54\n\n\n\n\n0.37\n\n\n-\n\n\n0.62\n\n\n\n\n0.45\n\n\n-\n\n\n0.80\u2217\n\n\n\n\n0.26\n\n\n0.17\n\n\nCovidQA-RAG\n\n\n0.57\n\n\n\n\n0.18\n\n\n0.11\n\n\n0.58\n\n\n\n\n0.17\n\n\n-\n\n\n0.62\n\n\n\n\n0.58\n\n\n-\n\n\n0.77\u2217\n\n\n\n\n0.19\n\n\n0.11\n\n\nHotpotQA\n\n\n0.59\n\n\n\n\n0.11\n\n\n0.08\n\n\n0.62\n\n\n\n\n0.14\n\n\n-\n\n\n0.64\n\n\n\n\n0.73\n\n\n-\n\n\n0.85\u2217\n\n\n\n\n0.11\n\n\n0.08\n\n\nMS Marco\n\n\n0.65", "0eb42c88-edbe-475d-9be4-5f6ffae16e61": "-\n\n\n0.85\u2217\n\n\n\n\n0.11\n\n\n0.08\n\n\nMS Marco\n\n\n0.65\n\n\n\n\n0.23\n\n\n0.11\n\n\n0.63\n\n\n\n\n0.25\n\n\n-\n\n\n0.62\n\n\n\n\n0.61\n\n\n-\n\n\n0.70\n\n\n\n\n0.22\n\n\n0.10\n\n\nHAGRID\n\n\n0.58\n\n\n\n\n0.22\n\n\n0.15\n\n\n0.62\n\n\n\n\n0.22\n\n\n-\n\n\n0.67\n\n\n\n\n0.69\n\n\n-\n\n\n0.81\u2217\n\n\n\n\n0.20\u2217\n\n\n0.13\n\n\nExpertQA\n\n\n0.55\n\n\n\n\n0.31\n\n\n0.23\n\n\n0.57\n\n\n\n\n0.28\n\n\n-\n\n\n0.70\n\n\n\n\n0.60\n\n\n-\n\n\n0.87\u2217\n\n\n\n\n0.18\u2217\n\n\n0.11\u2217\n\n\nDelucionQA\n\n\n0.57\n\n\n\n\n0.18\n\n\n0.10\n\n\n0.70\u2217\n\n\n\n\n0.22\n\n\n-\n\n\n0.55\n\n\n\n\n0.64\n\n\n-\n\n\n0.64\n\n\n\n\n0.15\u2217\n\n\n0.10\n\n\nEManual\n\n\n0.54\n\n\n\n\n0.17\n\n\n0.11\u2217\n\n\n0.57\n\n\n\n\n0.27\n\n\n-\n\n\n0.61\n\n\n\n\n0.64\n\n\n-", "4fa2408c-7e4b-40bc-b95c-7c8e75677c75": "0.11\u2217\n\n\n0.57\n\n\n\n\n0.27\n\n\n-\n\n\n0.61\n\n\n\n\n0.64\n\n\n-\n\n\n0.76\u2217\n\n\n\n\n0.13\u2217\n\n\n0.13\n\n\nTechQA\n\n\n0.51\n\n\n\n\n0.10\n\n\n0.05\n\n\n0.52\n\n\n\n\n0.12\n\n\n-\n\n\n0.57\n\n\n\n\n0.70\n\n\n-\n\n\n0.86\u2217\n\n\n\n\n0.08\u2217\n\n\n0.04\u2217\n\n\nFinQA\n\n\n0.57\n\n\n\n\n0.10\n\n\n0.13\n\n\n0.57\n\n\n\n\n0.06\u2217\n\n\n-\n\n\n0.53\n\n\n\n\n0.79\n\n\n-\n\n\n0.81\u2217\n\n\n\n\n0.10\n\n\n0.10\n\n\nTAT-QA\n\n\n0.52\n\n\n\n\n0.20\n\n\n0.17\u2217\n\n\n0.63\n\n\n\n\n0.18\u2217\n\n\n-\n\n\n0.59\n\n\n\n\n0.72\n\n\n-\n\n\n0.83\u2217\n\n\n\n\n0.27\n\n\n0.23\n\n\nCUAD\n\n\n0.51\n\n\n\n\n0.27\n\n\n0.11\n\n\n0.66\n\n\n\n\n0.19\u2217\n\n\n-\n\n\n0.40\n\n\n\n\n0.66\n\n\n-\n\n\n0.80\u2217\n\n\n\n\n0.24\n\n\n0.10\n\n\n\n\n\n\n\n\n5 Results", "1eb4b60c-ff45-41fc-aa87-2cce228d2ea2": "-\n\n\n0.80\u2217\n\n\n\n\n0.24\n\n\n0.10\n\n\n\n\n\n\n\n\n5 Results\n\nTable 3 reports results on test splits of each RAGBench component dataset. We compare baseline LLM methods with a finetunes DeBERTA encoder that trained on the full RAGBench train split.", "2cb52636-f080-451c-9f3d-b38f29a27d54": "We observe that the finetuned DeBERTa model trained on RAGBench achieves competitive performance with billion-parameter LLM judges across numerous domain subsets of the RAGBench test set. On the hallucination detection task, DeBERTA AUROC scores range from 0.64 to 0.86. While RMSE for relevance and utilization range from 0.04 to 0.26, depending on the domain and task.\n\n\nEstimating Context Relevance is Difficult", "6f665ffd-41ad-432c-87a3-19541f7eb110": "As shown in Table 3, Relevance RMSE scores are generally higher than those for Utilization, indicating a greater difficulty in the relevance prediction task. Utilization can be assessed through a straightforward semantic comparison between the context and the response. In contrast, relevance is a more intricate metric. Due to the nature of RAG, the majority of retrieved documents are semantically related to the query. However, mere semantic similarity is insufficient. The model must ascertain", "c010bd7e-0af1-4c96-94a9-4b7f9e8a9792": "is insufficient. The model must ascertain whether the provided context includes specific information necessary to accurately answer the question. Thus, the task inherently involves deriving the correct answer, followed by assessing what information in the context may be used to arrive at that answer.", "c8f14e5c-671a-48f9-a27e-f816ad46bb82": "6 Conclusion", "5662b505-bfe7-4764-8360-bb54f96be493": "In this paper we introduce RAGBench, a large-scale dataset composed of real-world RAG examples intended for training and benchmarking powerful RAG evaluation models. To this end, we formulate TRACe, a RAG evaluation framework comprising four metrics: uTilization, Relevance, Adherence, and Completeness. TRACe standardizes the evaluation process, offering a consistent and systematic approach to measuring RAG system performance across various dimensions. We propose an automated approach to", "3ad94145-5e87-4f39-8c5f-110b50261589": "dimensions. We propose an automated approach to generate TRACe labels for RAGBench with an LLM and demonstrate high correlation between our approach and human judgements.", "02d1c4ea-a5c2-4542-ae84-b2c48d7a6f67": "We benchmark existing RAG evaluation frameworks on RAGBench and demonstrate that a 400M parameter DeBERTa model finetuned on RAGBench data performs competitively with billion-parameter LLM Judges and commercial RAG evaluation systems. Though, the gap between the best-performing RAG evaluator and ground truth is still large. We motivate future work to leverage RAGBench toward fine-tuning more powerful evaluation models to explore the potential for narrowing the performance gap between these", "d8327f76-fbd0-441a-9a00-742ccd95258e": "for narrowing the performance gap between these models and the ground truth.", "43b721df-2e96-4210-a343-8ded8758230c": "Our contributions address the need for standardized benchmarks and methodologies, enabling more precise and actionable insights into the strengths and weaknesses of different RAG systems. This, in turn, will facilitate iterative improvement of RAG models, driving forward the capabilities of retrieval-augmented generation in real-world applications.\n\n\n\nReferences\n\n\nAdlakha et\u00a0al. [2023]\n\nV.\u00a0Adlakha, P.\u00a0BehnamGhader, X.\u00a0H. Lu, N.\u00a0Meade, and S.\u00a0Reddy.", "c1728133-77cc-42e4-8889-530a24b2030c": "Evaluating correctness and faithfulness of instruction-following models for question answering.\n\n\narXiv preprint arXiv:2307.16877v1, 2023.\n\n\n\n\nBohnet et\u00a0al. [2023]\n\nB.\u00a0Bohnet, V.\u00a0Q. Tran, P.\u00a0Verga, R.\u00a0Aharoni, D.\u00a0Andor, L.\u00a0B. Soares, M.\u00a0Ciaramita, J.\u00a0Eisenstein, K.\u00a0Ganchev, J.\u00a0Herzig, K.\u00a0Hui, T.\u00a0Kwiatkowski, J.\u00a0Ma, J.\u00a0Ni, L.\u00a0S. Saralegui, T.\u00a0Schuster, W.\u00a0W. Cohen, M.\u00a0Collins, D.\u00a0Das, D.\u00a0Metzler, S.\u00a0Petrov, and K.\u00a0Webster.", "c534c6c5-d881-48c1-956e-2e19565d9ad5": "Attributed question answering: Evaluation and modeling for attributed large language models, 2023.\n\n\n\n\nCastelli et\u00a0al. [2020]\n\nV.\u00a0Castelli, R.\u00a0Chakravarti, S.\u00a0Dana, A.\u00a0Ferritto, R.\u00a0Florian, M.\u00a0Franz, D.\u00a0Garg, D.\u00a0Khandelwal, S.\u00a0McCarley, M.\u00a0McCawley, M.\u00a0Nasr, L.\u00a0Pan, C.\u00a0Pendus, J.\u00a0Pitrelli, S.\u00a0Pujar, S.\u00a0Roukos, A.\u00a0Sakrajda, A.\u00a0Sil, R.\u00a0Uceda-Sosa, T.\u00a0Ward, and R.\u00a0Zhang.\n\n\nThe TechQA dataset.", "d109263a-92bb-4fd6-8e21-d20acfc2c4d7": "The TechQA dataset.\n\n\nIn D.\u00a0Jurafsky, J.\u00a0Chai, N.\u00a0Schluter, and J.\u00a0Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1269\u20131278, Online, July 2020. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2020.acl-main.117.\n\n\nURL https://aclanthology.org/2020.acl-main.117.\n\n\n\n\nChen et\u00a0al. [2023]\n\nJ.\u00a0Chen, H.\u00a0Lin, X.\u00a0Han, and L.\u00a0Sun.\n\n\nBenchmarking large language models in retrieval-augmented generation.", "4385d59f-e2ff-4019-8534-e3c49916e893": "arXiv preprint arXiv:2309.01431, 2023.\n\n\n\n\nchen et\u00a0al. [2023]\n\ns.\u00a0chen, Y.\u00a0Zhao, J.\u00a0Zhang, I.-C. Chern, S.\u00a0Gao, P.\u00a0Liu, and J.\u00a0He.\n\n\nFelm: Benchmarking factuality evaluation of large language models.\n\n\nIn A.\u00a0Oh, T.\u00a0Naumann, A.\u00a0Globerson, K.\u00a0Saenko, M.\u00a0Hardt, and S.\u00a0Levine, editors, Advances in Neural Information Processing Systems, volume\u00a036, pages 44502\u201344523. Curran Associates, Inc., 2023.", "98bdfbe0-da0c-4941-912d-77bf2d5d53df": "URL https://proceedings.neurips.cc/paper_files/paper/2023/file/8b8a7960d343e023a6a0afe37eee6022-Paper-Datasets_and_Benchmarks.pdf.\n\n\n\n\nChen et\u00a0al. [2021]\n\nZ.\u00a0Chen, W.\u00a0Chen, C.\u00a0Smiley, S.\u00a0Shah, I.\u00a0Borova, D.\u00a0Langdon, R.\u00a0Moussa, M.\u00a0Beane, T.-H. Huang, B.\u00a0Routledge, and W.\u00a0Y. Wang.\n\n\nFinQA: A dataset of numerical reasoning over financial data.", "14285653-90af-4a46-aa0e-3918134ad356": "In M.-F. Moens, X.\u00a0Huang, L.\u00a0Specia, and S.\u00a0W.-t. Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3697\u20133711, Online and Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2021.emnlp-main.300.\n\n\nURL https://aclanthology.org/2021.emnlp-main.300.\n\n\n\n\nChiesurin et\u00a0al. [2023]", "e823eda0-a8fd-46ab-a70f-4b3748efe8b8": "Chiesurin et\u00a0al. [2023]\n\nS.\u00a0Chiesurin, D.\u00a0Dimakopoulos, M.\u00a0A. Sobrevilla\u00a0Cabezudo, A.\u00a0Eshghi, I.\u00a0Papaioannou, V.\u00a0Rieser, and I.\u00a0Konstas.\n\n\nThe dangers of trusting stochastic parrots: Faithfulness and trust in open-domain conversational question answering.\n\n\nIn A.\u00a0Rogers, J.\u00a0Boyd-Graber, and N.\u00a0Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 947\u2013959, Toronto, Canada, July 2023. Association for Computational Linguistics.", "6e26b5bd-c367-40fd-8690-247614f99851": "doi: 10.18653/v1/2023.findings-acl.60.\n\n\nURL https://aclanthology.org/2023.findings-acl.60.\n\n\n\n\nDinan et\u00a0al. [2019]\n\nE.\u00a0Dinan, S.\u00a0Roller, K.\u00a0Shuster, A.\u00a0Fan, M.\u00a0Auli, and J.\u00a0Weston.\n\n\nWizard of wikipedia: Knowledge-powered conversational agents, 2019.\n\n\n\n\nEs et\u00a0al. [2024]\n\nS.\u00a0Es, J.\u00a0James, L.\u00a0Espinosa\u00a0Anke, and S.\u00a0Schockaert.\n\n\nRAGAs: Automated evaluation of retrieval augmented generation.", "5626ac12-448b-461b-86b3-b07fd3d7d933": "In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations. Association for Computational Linguistics, Mar. 2024.\n\n\n\n\nGao et\u00a0al. [2023]\n\nT.\u00a0Gao, H.\u00a0Yen, J.\u00a0Yu, and D.\u00a0Chen.\n\n\nEnabling large language models to generate text with citations.", "300246a8-39b4-4000-8108-8be0b5e558ea": "In H.\u00a0Bouamor, J.\u00a0Pino, and K.\u00a0Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6465\u20136488, Singapore, Dec. 2023. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2023.emnlp-main.398.\n\n\nURL https://aclanthology.org/2023.emnlp-main.398.\n\n\n\n\nHe et\u00a0al. [2023]\n\nP.\u00a0He, J.\u00a0Gao, and W.\u00a0Chen.\n\n\nDeBERTav3: Improving deBERTa using ELECTRA-style pre-training with gradient-disentangled embedding sharing.", "8700fae4-bd44-4d2c-ae7b-fbab95810a6c": "In The Eleventh International Conference on Learning Representations, 2023.\n\n\nURL https://openreview.net/forum?id=sE7-XhLxHA.\n\n\n\n\nHendrycks et\u00a0al. [2021]\n\nD.\u00a0Hendrycks, C.\u00a0Burns, A.\u00a0Chen, and S.\u00a0Ball.\n\n\nCuad: An expert-annotated nlp dataset for legal contract review.\n\n\nNeurIPS, 2021.\n\n\n\n\nHuang and Huang [2024]\n\nY.\u00a0Huang and J.\u00a0Huang.\n\n\nA survey on retrieval-augmented text generation for large language models, 2024.\n\n\n\n\nJin et\u00a0al. [2019]\n\nQ.\u00a0Jin, B.\u00a0Dhingra, Z.\u00a0Liu, W.\u00a0Cohen, and X.\u00a0Lu.", "dbb28f3f-5058-4797-b418-ae8cb1e2d0d7": "Q.\u00a0Jin, B.\u00a0Dhingra, Z.\u00a0Liu, W.\u00a0Cohen, and X.\u00a0Lu.\n\n\nPubMedQA: A dataset for biomedical research question answering.\n\n\nIn K.\u00a0Inui, J.\u00a0Jiang, V.\u00a0Ng, and X.\u00a0Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2567\u20132577, Hong Kong, China, Nov. 2019. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/D19-1259.", "0d8a02cc-9ff6-4a74-80e7-c868d012b0a1": "doi: 10.18653/v1/D19-1259.\n\n\nURL https://aclanthology.org/D19-1259.\n\n\n\n\nKamalloo et\u00a0al. [2023]\n\nE.\u00a0Kamalloo, A.\u00a0Jafari, X.\u00a0Zhang, N.\u00a0Thakur, and J.\u00a0Lin.\n\n\nHagrid: A human-llm collaborative dataset for generative information-seeking with attribution, 2023.\n\n\n\n\nKim et\u00a0al. [2024]\n\nS.\u00a0Kim, J.\u00a0Suk, S.\u00a0Longpre, B.\u00a0Y. Lin, J.\u00a0Shin, S.\u00a0Welleck, G.\u00a0Neubig, M.\u00a0Lee, K.\u00a0Lee, and M.\u00a0Seo.\n\n\nPrometheus 2: An open source language model specialized in evaluating other language models, 2024.", "786973c5-eab4-4f9a-8408-2c2d4be513ea": "Kwiatkowski et\u00a0al. [2019]\n\nT.\u00a0Kwiatkowski, J.\u00a0Palomaki, O.\u00a0Redfield, M.\u00a0Collins, A.\u00a0Parikh, C.\u00a0Alberti, D.\u00a0Epstein, I.\u00a0Polosukhin, M.\u00a0Kelcey, J.\u00a0Devlin, K.\u00a0Lee, K.\u00a0N. Toutanova, L.\u00a0Jones, M.-W. Chang, A.\u00a0Dai, J.\u00a0Uszkoreit, Q.\u00a0Le, and S.\u00a0Petrov.\n\n\nNatural questions: a benchmark for question answering research.\n\n\nTransactions of the Association of Computational Linguistics, 2019.\n\n\n\n\nLaurer et\u00a0al. [2022]\n\nM.\u00a0Laurer, W.\u00a0van Atteveldt, A.\u00a0Casas, and K.\u00a0Welbers.", "1f3dc489-5b3a-4506-b610-a52f80b1383f": "Less annotating, more classifying \u2013 addressing the data scarcity issue of supervised machine learning with deep transfer learning and bert - nli.\n\n\nOpen Science Framework Preprint, 2022.\n\n\nURL https://osf.io/74b8k.\n\n\n\n\nLee et\u00a0al. [2019]\n\nK.\u00a0Lee, M.-W. Chang, and K.\u00a0Toutanova.\n\n\nLatent retrieval for weakly supervised open domain question answering.", "708853d5-8696-48f8-8bea-64c868fe652e": "In A.\u00a0Korhonen, D.\u00a0Traum, and L.\u00a0M\u00e0rquez, editors, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6086\u20136096, Florence, Italy, July 2019. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/P19-1612.\n\n\nURL https://aclanthology.org/P19-1612.\n\n\n\n\nLewis et\u00a0al. [2020]\n\nP.\u00a0Lewis, E.\u00a0Perez, A.\u00a0Piktus, F.\u00a0Petroni, V.\u00a0Karpukhin, N.\u00a0Goyal, H.\u00a0K\u00fcttler, M.\u00a0Lewis, W.-t. Yih, T.\u00a0Rockt\u00e4schel, S.\u00a0Riedel, and D.\u00a0Kiela.", "ae7f8786-17d7-42de-bd71-49c0662f7310": "Retrieval-augmented generation for knowledge-intensive nlp tasks.\n\n\nIn Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS \u201920, Red Hook, NY, USA, 2020. Curran Associates Inc.\n\n\nISBN 9781713829546.\n\n\n\n\nLi et\u00a0al. [2023]\n\nJ.\u00a0Li, X.\u00a0Cheng, X.\u00a0Zhao, J.-Y. Nie, and J.-R. Wen.\n\n\nHaluEval: A large-scale hallucination evaluation benchmark for large language models.", "7ce3143d-307f-4284-bb00-e2fae1bdc4eb": "In H.\u00a0Bouamor, J.\u00a0Pino, and K.\u00a0Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 6449\u20136464, Singapore, Dec. 2023. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2023.emnlp-main.397.\n\n\nURL https://aclanthology.org/2023.emnlp-main.397.\n\n\n\n\nLi et\u00a0al. [2024]\n\nY.\u00a0Li, X.\u00a0Yue, Z.\u00a0Liao, and H.\u00a0Sun.\n\n\nAttributionbench: How hard is automatic attribution evaluation?\n\n\narXiv preprint arXiv:2402.15089v1, 2024.\n\n\n\n\nLiu et\u00a0al. [2024]", "3974c247-82b7-42cf-865b-6377f26d7c88": "Liu et\u00a0al. [2024]\n\nZ.\u00a0Liu, W.\u00a0Ping, R.\u00a0Roy, P.\u00a0Xu, C.\u00a0Lee, M.\u00a0Shoeybi, and B.\u00a0Catanzaro.\n\n\nChatqa: Building gpt-4 level conversational qa models.\n\n\narXiv preprint arXiv:2401.10225, 2024.\n\n\n\n\nMagesh et\u00a0al. [2024]\n\nV.\u00a0Magesh, F.\u00a0Surani, M.\u00a0Dahl, M.\u00a0Suzgun, C.\u00a0D. Manning, and D.\u00a0E. Ho.\n\n\nHallucination-free? assessing the reliability of leading ai legal research tools, 2024.\n\n\n\n\nMalaviya et\u00a0al. [2024]\n\nC.\u00a0Malaviya, S.\u00a0Lee, S.\u00a0Chen, E.\u00a0Sieber, M.\u00a0Yatskar, and D.\u00a0Roth.", "319131df-316d-49ea-befa-024dd698bc24": "Expertqa: Expert-curated questions and attributed answers, 2024.\n\n\n\n\nM\u00f6ller et\u00a0al. [2020]\n\nT.\u00a0M\u00f6ller, A.\u00a0Reina, R.\u00a0Jayakumar, and M.\u00a0Pietsch.\n\n\nCOVID-QA: A question answering dataset for COVID-19.\n\n\nIn Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020, Online, July 2020. Association for Computational Linguistics.\n\n\nURL https://aclanthology.org/2020.nlpcovid19-acl.18.\n\n\n\n\nNandy et\u00a0al. [2021]\n\nA.\u00a0Nandy, S.\u00a0Sharma, S.\u00a0Maddhashiya, K.\u00a0Sachdeva, P.\u00a0Goyal, and N.\u00a0Ganguly.", "2626dd45-199b-405b-a55a-b7ed8e95301a": "Question answering over electronic devices: A new benchmark dataset and a multi-task learning based QA framework.\n\n\nIn M.-F. Moens, X.\u00a0Huang, L.\u00a0Specia, and S.\u00a0W.-t. Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4600\u20134609, Punta Cana, Dominican Republic, Nov. 2021. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2021.findings-emnlp.392.\n\n\nURL https://aclanthology.org/2021.findings-emnlp.392.\n\n\n\n\nNguyen et\u00a0al. [2016]", "ba4fdb0e-d413-47db-8ab4-58f3e3c352d0": "Nguyen et\u00a0al. [2016]\n\nT.\u00a0Nguyen, M.\u00a0Rosenberg, X.\u00a0Song, J.\u00a0Gao, S.\u00a0Tiwary, R.\u00a0Majumder, and L.\u00a0Deng.\n\n\nMs marco: A human generated machine reading comprehension dataset.\n\n\nNovember 2016.\n\n\nURL https://www.microsoft.com/en-us/research/publication/ms-marco-human-generated-machine-reading-comprehension-dataset/.\n\n\n\n\nPetroni et\u00a0al. [2021]", "d4eef488-b16c-4dd6-897e-fb6509ced0a7": "Petroni et\u00a0al. [2021]\n\nF.\u00a0Petroni, A.\u00a0Piktus, A.\u00a0Fan, P.\u00a0Lewis, M.\u00a0Yazdani, N.\u00a0De\u00a0Cao, J.\u00a0Thorne, Y.\u00a0Jernite, V.\u00a0Karpukhin, J.\u00a0Maillard, V.\u00a0Plachouras, T.\u00a0Rockt\u00e4schel, and S.\u00a0Riedel.\n\n\nKILT: a benchmark for knowledge intensive language tasks.", "34b855eb-9a30-4a5d-8573-f6f928d92d64": "In K.\u00a0Toutanova, A.\u00a0Rumshisky, L.\u00a0Zettlemoyer, D.\u00a0Hakkani-Tur, I.\u00a0Beltagy, S.\u00a0Bethard, R.\u00a0Cotterell, T.\u00a0Chakraborty, and Y.\u00a0Zhou, editors, Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523\u20132544, Online, June 2021. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2021.naacl-main.200.\n\n\nURL https://aclanthology.org/2021.naacl-main.200.\n\n\n\n\nRashkin et\u00a0al. [2021]", "4303976d-fbb8-47f6-b508-7fb5247d2b96": "Rashkin et\u00a0al. [2021]\n\nH.\u00a0Rashkin, D.\u00a0Reitter, G.\u00a0S. Tomar, and D.\u00a0Das.\n\n\nIncreasing faithfulness in knowledge-grounded dialogue with controllable features.\n\n\nIn C.\u00a0Zong, F.\u00a0Xia, W.\u00a0Li, and R.\u00a0Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 704\u2013718, Online, Aug. 2021. Association for Computational Linguistics.", "5592af13-7d80-4983-b3e8-42512afe8ee1": "doi: 10.18653/v1/2021.acl-long.58.\n\n\nURL https://aclanthology.org/2021.acl-long.58.\n\n\n\n\nRashkin et\u00a0al. [2023]\n\nH.\u00a0Rashkin, V.\u00a0Nikolaev, M.\u00a0Lamm, L.\u00a0Aroyo, M.\u00a0Collins, D.\u00a0Das, S.\u00a0Petrov, G.\u00a0S. Tomar, I.\u00a0Turc, and D.\u00a0Reitter.\n\n\nMeasuring attribution in natural language generation models.\n\n\nComputational Linguistics, 49(4):777\u2013840, 12 2023.\n\n\n\n\nSaad-Falcon et\u00a0al. [2024]\n\nJ.\u00a0Saad-Falcon, O.\u00a0Khattab, C.\u00a0Potts, and M.\u00a0Zaharia.", "43e8680d-e613-4fbe-b6ba-80d1f32c12ea": "Ares: An automated evaluation framework for retrieval-augmented generation systems.\n\n\narXiv preprint arXiv:2311.09476v2, 2024.\n\n\n\n\nSadat et\u00a0al. [2023]\n\nM.\u00a0Sadat, Z.\u00a0Zhou, L.\u00a0Lange, J.\u00a0Araki, A.\u00a0Gundroo, B.\u00a0Wang, R.\u00a0Menon, M.\u00a0Parvez, and Z.\u00a0Feng.\n\n\nDelucionqa: Detecting hallucinations in domain-specific question answering.\n\n\npages 822\u2013835, 01 2023.\n\n\ndoi: 10.18653/v1/2023.findings-emnlp.59.\n\n\n\n\nSiriwardhana et\u00a0al. [2023]", "d7cb37fc-6dd9-4169-bcde-a0a36957402b": "Siriwardhana et\u00a0al. [2023]\n\nS.\u00a0Siriwardhana, R.\u00a0Weerasekera, E.\u00a0Wen, T.\u00a0Kaluarachchi, R.\u00a0Rana, and S.\u00a0Nanayakkara.\n\n\nImproving the domain adaptation of retrieval augmented generation (RAG) models for open domain question answering.\n\n\nTransactions of the Association for Computational Linguistics, 11:1\u201317, 2023.\n\n\ndoi: 10.1162/tacl_a_00530.\n\n\nURL https://aclanthology.org/2023.tacl-1.1.\n\n\n\n\nThorne et\u00a0al. [2018]\n\nJ.\u00a0Thorne, A.\u00a0Vlachos, C.\u00a0Christodoulopoulos, and A.\u00a0Mittal.", "12316e9d-ec0b-4365-869b-41e0b82ff35c": "FEVER: a large-scale dataset for fact extraction and VERification.\n\n\nIn M.\u00a0Walker, H.\u00a0Ji, and A.\u00a0Stent, editors, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809\u2013819, New Orleans, Louisiana, June 2018. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/N18-1074.\n\n\nURL https://aclanthology.org/N18-1074.\n\n\n\n\nTrulens [2023]\n\nTrulens, 2023.", "2c81ee73-e4ae-40f5-bd65-4ee991ad7ed5": "Trulens [2023]\n\nTrulens, 2023.\n\n\nhttps://www.trulens.org/.\n\n\n\n\nWang et\u00a0al. [2020]\n\nL.\u00a0L. Wang, K.\u00a0Lo, Y.\u00a0Chandrasekhar, R.\u00a0Reas, J.\u00a0Yang, D.\u00a0Burdick, D.\u00a0Eide, K.\u00a0Funk, Y.\u00a0Katsis, R.\u00a0M. Kinney, Y.\u00a0Li, Z.\u00a0Liu, W.\u00a0Merrill, P.\u00a0Mooney, D.\u00a0A. Murdick, D.\u00a0Rishi, J.\u00a0Sheehan, Z.\u00a0Shen, B.\u00a0Stilson, A.\u00a0D. Wade, K.\u00a0Wang, N.\u00a0X.\u00a0R. Wang, C.\u00a0Wilhelm, B.\u00a0Xie, D.\u00a0M. Raymond, D.\u00a0S. Weld, O.\u00a0Etzioni, and S.\u00a0Kohlmeier.\n\n\nCORD-19: The COVID-19 open research dataset.", "89837166-2a67-408e-ad35-2e24a8aa28e3": "CORD-19: The COVID-19 open research dataset.\n\n\nIn K.\u00a0Verspoor, K.\u00a0B. Cohen, M.\u00a0Dredze, E.\u00a0Ferrara, J.\u00a0May, R.\u00a0Munro, C.\u00a0Paris, and B.\u00a0Wallace, editors, Proceedings of the 1st Workshop on NLP for COVID-19 at ACL 2020, Online, July 2020. Association for Computational Linguistics.\n\n\nURL https://aclanthology.org/2020.nlpcovid19-acl.1.\n\n\n\n\nWang et\u00a0al. [2024]\n\nS.\u00a0Wang, J.\u00a0Liu, S.\u00a0Song, J.\u00a0Cheng, Y.\u00a0Fu, P.\u00a0Guo, K.\u00a0Fang, Y.\u00a0Zhu, and Z.\u00a0Dou.", "658c0ac1-60c7-4f2e-ba60-52528a9dda53": "Domainrag: A chinese benchmark for evaluating domain-specific retrieval-augmented generation, 2024.\n\n\nURL https://arxiv.org/abs/2406.05654.\n\n\n\n\nWei et\u00a0al. [2022]\n\nJ.\u00a0Wei, X.\u00a0Wang, D.\u00a0Schuurmans, M.\u00a0Bosma, b.\u00a0ichter, F.\u00a0Xia, E.\u00a0Chi, Q.\u00a0V. Le, and D.\u00a0Zhou.\n\n\nChain-of-thought prompting elicits reasoning in large language models.", "b82a809f-5e3d-418d-b7a3-234f05ccd1dd": "In S.\u00a0Koyejo, S.\u00a0Mohamed, A.\u00a0Agarwal, D.\u00a0Belgrave, K.\u00a0Cho, and A.\u00a0Oh, editors, Advances in Neural Information Processing Systems, volume\u00a035, pages 24824\u201324837. Curran Associates, Inc., 2022.\n\n\nURL https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf.\n\n\n\n\nWu et\u00a0al. [2023]\n\nY.\u00a0Wu, J.\u00a0Zhu, S.\u00a0Xu, K.\u00a0Shum, C.\u00a0Niu, R.\u00a0Zhong, J.\u00a0Song, and T.\u00a0Zhang.", "de76359a-1adb-4516-a560-3e849d0895d0": "Ragtruth: A hallucination corpus for developing trustworthy retrieval-augmented language models, 2023.\n\n\n\n\nYang et\u00a0al. [2024]\n\nX.\u00a0Yang, K.\u00a0Sun, H.\u00a0Xin, Y.\u00a0Sun, N.\u00a0Bhalla, X.\u00a0Chen, S.\u00a0Choudhary, R.\u00a0D. Gui, Z.\u00a0W. Jiang, Z.\u00a0Jiang, L.\u00a0Kong, B.\u00a0Moran, J.\u00a0Wang, Y.\u00a0E. Xu, A.\u00a0Yan, C.\u00a0Yang, E.\u00a0Yuan, H.\u00a0Zha, N.\u00a0Tang, L.\u00a0Chen, N.\u00a0Scheffer, Y.\u00a0Liu, N.\u00a0Shah, R.\u00a0Wanga, A.\u00a0Kumar, W.\u00a0tau Yih, and X.\u00a0L. Dong.\n\n\nCrag \u2013 comprehensive rag benchmark, 2024.\n\n\nURL https://arxiv.org/abs/2406.04744.", "dd41da1a-e815-4fd5-be63-15008352ba4b": "URL https://arxiv.org/abs/2406.04744.\n\n\n\n\nYang et\u00a0al. [2018]\n\nZ.\u00a0Yang, P.\u00a0Qi, S.\u00a0Zhang, Y.\u00a0Bengio, W.\u00a0W. Cohen, R.\u00a0Salakhutdinov, and C.\u00a0D. Manning.\n\n\nHotpotQA: A dataset for diverse, explainable multi-hop question answering.\n\n\nIn Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.\n\n\n\n\nYe et\u00a0al. [2024]\n\nS.\u00a0Ye, D.\u00a0Kim, S.\u00a0Kim, H.\u00a0Hwang, S.\u00a0Kim, Y.\u00a0Jo, J.\u00a0Thorne, J.\u00a0Kim, and M.\u00a0Seo.\n\n\nFLASK: Fine-grained language model evaluation based on alignment skill sets.", "069d3911-f75c-4d43-a9a5-aaebcc0810ed": "In The Twelfth International Conference on Learning Representations, 2024.\n\n\nURL https://openreview.net/forum?id=CYmF38ysDa.\n\n\n\n\nYue et\u00a0al. [2023]\n\nX.\u00a0Yue, B.\u00a0Wang, Z.\u00a0Chen, K.\u00a0Zhang, Y.\u00a0Su, and H.\u00a0Sun.\n\n\nAutomatic evaluation of attribution by large language models.\n\n\nIn H.\u00a0Bouamor, J.\u00a0Pino, and K.\u00a0Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 4615\u20134635, Singapore, Dec. 2023. Association for Computational Linguistics.", "1ecedf2a-97d5-4619-8449-eea00bb7a948": "doi: 10.18653/v1/2023.findings-emnlp.307.\n\n\nURL https://aclanthology.org/2023.findings-emnlp.307.\n\n\n\n\nZhang et\u00a0al. [2022]\n\nX.\u00a0Zhang, N.\u00a0Thakur, O.\u00a0Ogundepo, E.\u00a0Kamalloo, D.\u00a0Alfonso-Hermelo, X.\u00a0Li, Q.\u00a0Liu, M.\u00a0Rezagholizadeh, and J.\u00a0Lin.\n\n\nMaking a miracl: Multilingual information retrieval across a continuum of languages, 2022.\n\n\n\n\nZheng et\u00a0al. [2023]\n\nL.\u00a0Zheng, W.-L. Chiang, Y.\u00a0Sheng, S.\u00a0Zhuang, Z.\u00a0Wu, Y.\u00a0Zhuang, Z.\u00a0Lin, Z.\u00a0Li, D.\u00a0Li, E.\u00a0Xing, H.\u00a0Zhang, J.\u00a0E. Gonzalez, and I.\u00a0Stoica.", "091e8798-49bb-477f-b644-714d8e44e2ac": "Judging LLM-as-a-judge with MT-bench and chatbot arena.\n\n\nIn Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023.\n\n\nURL https://openreview.net/forum?id=uccHPGDlao.\n\n\n\n\nZhu et\u00a0al. [2021]\n\nF.\u00a0Zhu, W.\u00a0Lei, Y.\u00a0Huang, C.\u00a0Wang, S.\u00a0Zhang, J.\u00a0Lv, F.\u00a0Feng, and T.-S. Chua.\n\n\nTAT-QA: A question answering benchmark on a hybrid of tabular and textual content in finance.", "f406e70e-e18b-4843-b186-fab0a5edb175": "In C.\u00a0Zong, F.\u00a0Xia, W.\u00a0Li, and R.\u00a0Navigli, editors, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 3277\u20133287, Online, Aug. 2021. Association for Computational Linguistics.\n\n\ndoi: 10.18653/v1/2021.acl-long.254.\n\n\nURL https://aclanthology.org/2021.acl-long.254.\n\n\n\n\n\n\n\n7 Appendix\n\n\n7.1 RAGBench Code and Data", "ad9a6ab6-9e2b-4382-a071-aa3ca84989c0": "7 Appendix\n\n\n7.1 RAGBench Code and Data\n\nWe release RAGBench data on Hugginggface: https://huggingface.co/datasets/rungalileo/ragbench. Refer to model card and documentation there.\n\n\nWe publish our inferfence and evaluation code on Gihub: https://github.com/rungalileo/ragbench/tree/main/ragbench.\n\n\n\n\n7.2 RAGBench Dataset Details", "cbede85b-2bbc-4554-9646-469c30e9633d": "7.2 RAGBench Dataset Details\n\nRAGBench is sourced from publicly released acadmic and industry datasets. As far as we know, none of the component datasets contain personally identifiable information or offensive content.\n\n\nPubMedQA [14]", "e37150fd-e544-4505-8163-538db0115fcf": "PubMedQA is a collection of PubMed research abstracts with corresponding yes/no/maybe questions paired with each abstract. The original dataset comprises 3 subsets: PQA-L, PQA-U, and PQA-A, with 1k, 60k, and 210k abstracts, respectively. For all subsets, the question is derived from the title of the PubMed article using rule-based heuristics. Long answers are automatically derived from the last sentence of the abstract for PQA-L and PQA-U, and QA-L answers are further reviewed by expert", "7e615b69-87e5-46bc-88ef-21cf7b70a773": "and QA-L answers are further reviewed by expert annotators and annotated as yes/no/maybe. PQA-A comprises exclusively automatically generated questions and short answers.", "19facdd7-9d56-42a4-8a72-9abb342ffdbf": "For RAGBench we utilize the PQA-U subset and re-frame it from QA into a RAG task. To simulate RAG, we leverage already segmented PQA-U abstracts context chunks and we encode them into a vector DB with OpenAI embeddings. The size of the resulting DB is 200k. We retrieve 4 chunks for each PQA-U question using FAISS with eucledian distance as the similarity function. We ignore the responses and labels in the original dataset and generate new responses with an LLM.\n\n\n\nCovidQA-RAG", "fd05a92a-c18c-4cc2-b329-3c407749a501": "CovidQA-RAG\n\nCovidQA-RAG is a combination of 2k expert-annotated questions sourced from COVID-QA [26] and a vector database of 250,000 100-word passages built by Siriwardhana et\u00a0al. [34]. Both questions and answers are sourced from CORD-19 [37] collection of research articles about COVID-19.", "76123283-e452-4494-b5b2-7a85a6918e31": "We embed the questions and database passages with OpenAI embeddings and retrieve up to N passages for each COVID-QA question from the vector database using FAISS with eucledian distance as the similarity function and max_distance=0.25. We generate responses for each resulting RAG (context, question) instance with an LLM.\n\n\n\nHotpotQA [42]", "4b885ce6-817f-4f32-a98f-763e21c9ab26": "HotpotQA comprises 113K crowd-sourced question-answer pairs sourced from Wikipedia. Each pair is associated with a set of related context passages from one or multiple Wikipedia pages. The dataset is constructed in a way that requires multi-hop reasoning over multiple context documents to arrive at the answer, which renders it a valuable candidate for our benchmark. We sample data from the dev-distractor split, which contains up to 8 distractor context documents per sample. We downsample the", "4edaaea2-1585-473d-8d6b-fe80373b6ba2": "context documents per sample. We downsample the context documents to 4 per example, making sure to include the document containing the response. We treat the context passages in HotpotQA as RAG context documents, and generate responses for each (context, question) instance with an LLM.", "69458a0d-2806-4dcd-92f5-0ae04cdae8c8": "MS Marco [28]", "fd61ed2f-e5a9-451f-bfac-fbc598e3f6c1": "MS Marco is an open-domain question answering dataset sourced from Bing search engine user query logs. Each question is associated with 10 context passages retrieved via Bing web search. Human annotators compose a response based on the provided context documents, and label the documents utilized in the response as relevant. We sample data from the original version of the dataset, comprising 80k train, 10k validation, and 10k test samples. As with other datasets, we ignore the human annotated", "70fbe16d-9e09-4f25-aa79-c5b5bf392cb9": "other datasets, we ignore the human annotated answers and generate responses with an LLM in RAG setting.", "af80ad32-7269-4568-b5a0-34ce8ec1b7c4": "CUAD [12]", "95aea154-e498-4a97-bfc4-22796356a576": "CUAD is a collection of commercial legal contracts with expert annotated questions and responses. The contracts are sourced from a public legal contract library(EDGAR) and range from 1-100 pages in length. Experts in the legal domain compose multiple questions per contract and label the relevant parts of the contract that are useful for answering the questions. There are 21k questions pertaining to 510 documents in total. The questions are very specific to each contract, thus we don\u2019t perform", "21015d77-38f1-4114-9c72-a8d08a6128fe": "specific to each contract, thus we don\u2019t perform additional retrieval over the contract corpus, and form RAG examples with 1 context contract each for our benchmark. Due to high anntoation costs associated with long-context RAG, we sample 5 question per doc. As with other datasets, we generate responses with an LLM in RAG setting.", "9e293f9b-fa42-4677-bdac-f670f937a364": "DelucionQA [33]\n\n\nDelucionQA is a domain-specific RAG dataset leveraging Jeep\u2019s 2023 Gladiator model manual as the source of knowledge. The questions and answers are automatically generated by large language models. RAG context passages are retrieved from the Jeep car manual via both sparse and dense retrieval methods to add variance in the sample distribution. Further, MTurk workers annotate whether or not responses are supported by the context.", "a9a8a759-f0bd-4441-96a4-81a0b663066f": "Upon closer inspection, we found only 1 relevant passage associated with each question in the DelucionQA dataset. To make the dataset more challenging for RAGBench, we build a vector database from the 1,046 context passages in DelucionQA and and retrieve up to 3 context documents per question from it. We use text-embedding-ada-002 embeddings from OpenAI to build the database. There are 913 unique questions in DelucionQA. For each resulting (context, question) sample, we generate responses with", "8c361e55-5f4e-471c-9975-b715a76d882d": "question) sample, we generate responses with an LLM.", "70e43965-a335-41c4-b1d0-b5d0bb9f6299": "EManual [27]", "479c1de0-13bf-4a7f-a682-ea5e01d477bb": "EManual is a question answer dataset comprising consumer electronic device manuals and realistic questions about them composed by human annotators. The subset made available at the time of writing amounts to 659 unique questions about the Samsung Smart TV/remote and the accompanying user manual, segmented into 261 chunks. To form a RAG dataset, we embed the manual segments into a vector database with OpenAI embedding and retrieve up to 3 context documents per question from it. For each", "dc27e359-8247-44e7-9888-e977a603bfcf": "context documents per question from it. For each resulting (context, question) sample, we generate responses with an LLM.", "14c2be15-9105-43bc-b9c5-75cf102f1097": "TechQA [3]", "051692c2-6c10-47d3-a8c0-842efcd3b1ed": "TechQA is a collection of real-world user questions posted on IBMDeveloper and DeveloperWorks forums, along with 50 technical support documents relating to each question. The documents are sourced from database of 800k technical documents that support accepted answers on the tech forums. The authors release 1.4k questions, split between train, validation, and test sets. The data are curated such that fractions on the each split unanswerable given the information in the linked documents, which", "7b375eec-082b-402b-aea7-689f4dcad9f4": "the information in the linked documents, which makes it a good candidate for RAGBench. To reduce annotation costs, we sub-sample the data down to 10 documents per question, making sure to include the document containing the answer, when applicable. We use the provided splits with (context document, question) examples and generate responses for each with an LLM.", "c38c9a3a-26f2-4b9e-ba20-459c99405673": "FinQA [6]\n\n\nFinQA is a QA dataset of financial report passages and associated questions. Questions are curated such that numerical reasoning over multiple unstructured and tabular inputs is required to arrive at the answer. FinQA totals 8,281 financial QA pairs, split between train, validation, and test splits. We retain the original splits and generate 2 LLM responses per each context-query example in FinQA.\n\n\n\nTAT-QA [47]", "cbd7a799-6819-4c1c-a4f7-4a772eacd6c4": "TAT-QA [47]\n\n\nTAT-QA is another financial QA dataset that requires numerical reasoning over tables and text. The data are sourced from 500 financial reports released on https://www.annualreports.com/. Expert annotators with background in finance annotate question-answer pairs based on the available documents. We leverage the full dataset (13k train, 1.6k validation and test) but generate new responses with LLMs for RAGBench.\n\n\n\nHAGRID [15]", "bdef7f34-b785-4172-add3-b6a6f6f3abad": "HAGRID is a QA dataset built on top of MIRACL [45], a multi-lingual information-retrieval dataset. HAGRID passes questions and relevant context documents from MIRACLE through an LLM to produce a response for each example in the dataset. Annotors then rate the response on informativeness and attribution dimensions. The original context documents are sourced from Wikipedia and associated questions are generated by expert annotators. Since HAGRID already contains LLM-generated responses, we", "b3ae6c07-be39-4ddd-abf0-0e639f92a336": "already contains LLM-generated responses, we directly use them and don\u2019t generate additional responses for RAGBench.", "57bd30e6-e1c1-4716-be9e-36d9983e0d36": "ExpertQA [25]\n\n\nExpertQA is a collection of curated questions from domain-experts in various fields of sicence, arts, and law. The dataset also contains expert curated passsages relevant to each question, alongside LLM-generated responses. As with HAGRID, we leverage the LLM-generated responses in ExpertQA directly for our RAG dataset.\n\n\n\n\n\n7.3 Response Generation Prompt", "3acdd23a-3248-4a66-bfd1-64ff1382e01d": "7.3 Response Generation Prompt\n\nWe use the following prompt template to generate LLM responses for each sample in RAGBench. Context documents, separated by line breaks, along with the question are slotted in for each generation sample.\n\n    Use the following pieces of context to answer the question.\n\n    {documents}\n\n    Question: {question}\n\n\n\n\n\n7.4 GPT Labeling Prompt\n\nWe use the following prompt template to generate annotations with GPT-4", "e1e0f6e5-efa1-4906-a97b-10043097fb31": "I asked someone to answer a question based on one or more documents.\nYour task is to review their response and assess whether or not each sentence\nin that response is supported by text in the documents. And if so, which\nsentences in the documents provide that support. You will also tell me which\nof the documents contain useful information for answering the question, and\nwhich of the documents the answer was sourced from.", "15641f3e-b13d-413b-af82-535320c6c045": "Here are the documents, each of which is split into sentences. Alongside each\nsentence is associated key, such as \u20190a.\u2019 or \u20190b.\u2019 that you can use to refer\nto it:\n\n\u2018\u2018\u2018\n{documents}\n\u2018\u2018\u2018\n\nThe question was:\n\u2018\u2018\u2018\n{question}\n\u2018\u2018\u2018\n\nHere is their response, split into sentences. Alongside each sentence is\nassociated key, such as \u2019a.\u2019 or \u2019b.\u2019 that you can use to refer to it. Note\nthat these keys are unique to the response, and are not related to the keys\nin the documents:\n\n\u2018\u2018\u2018\n{answer}\n\u2018\u2018\u2018", "e4106e27-8936-4860-911c-ba34996c5c05": "\u2018\u2018\u2018\n{answer}\n\u2018\u2018\u2018\n\nYou must respond with a JSON object matching this schema:", "f53451e9-a22c-48c0-bdae-1f51e88ea293": "\u2018\u2018\u2018\n{{\n  \"relevance_explanation\": string,\n  \"all_relevant_sentence_keys\": [string],\n  \"overall_supported_explanation\": string,\n  \"overall_supported\": boolean,\n  \"sentence_support_information\": [\n    {{\n      \"response_sentence_key\": string,\n      \"explanation\": string,\n      \"supporting_sentence_keys\": [string],\n      \"fully_supported\": boolean\n    }},\n  ],\n  \"all_utilized_sentence_keys\": [string]\n}}\n\u2018\u2018\u2018\nThe relevance_explanation field is a string explaining which documents", "99925769-ac04-42c2-8d2b-dc7bcebfe650": "contain useful information for answering the question. Provide a step-by-step\nbreakdown of information provided in the documents and how it is useful for\nanswering the question.", "2a5317fb-ba64-4c07-be6d-f9873e8be355": "The all_relevant_sentence_keys field is a list of all document sentences keys\n(e.g. \u20190a\u2019) that are revant to the question. Include every sentence that is\nuseful and relevant to the question, even if it was not used in the response,\nor if only parts of the sentence are useful. Ignore the provided response when\nmaking this judgement and base your judgement solely on the provided documents\nand question. Omit sentences that, if removed from the document, would not", "416d19a0-9a5d-470a-859f-3e96b45317e2": "impact someone\u2019s ability to answer the question.", "8a7176e6-2e2b-467e-9985-2abe8b1a7e55": "The overall_supported_explanation field is a string explaining why the response\n*as a whole* is or is not supported by the documents. In this field, provide a\nstep-by-step breakdown of the claims made in the response and the support (or\nlack thereof) for those claims in the documents. Begin by assessing each claim\nseparately, one by one; don\u2019t make any remarks about the response as a whole\nuntil you have assessed all the claims in isolation.", "b3bddad1-9124-43ca-a26a-a7e325664b63": "The overall_supported field is a boolean indicating whether the response as a\nwhole is supported by the documents. This value should reflect the conclusion\nyou drew at the end of your step-by-step breakdown in overall_supported_explanation.\n\nIn the sentence_support_information field, provide information about the support\n*for each sentence* in the response.", "5c4e574a-c1d4-48ee-8a8f-7bdd38779cd5": "The sentence_support_information field is a list of objects, one for each sentence\nin the response. Each object MUST have the following fields:\n- response_sentence_key: a string identifying the sentence in the response.\nThis key is the same as the one used in the response above.\n- explanation: a string explaining why the sentence is or is not supported by the\ndocuments.\n- supporting_sentence_keys: keys (e.g. \u20190a\u2019) of sentences from the documents that", "aa52019a-5958-416f-9de3-9a0e7d9e37ca": "support the response sentence. If the sentence is not supported, this list MUST\nbe empty. If the sentence is supported, this list MUST contain one or more keys.\nIn special cases where the sentence is supported, but not by any specific sentence,\nyou can use the string \"supported_without_sentence\" to indicate that the sentence\nis generally supported by the documents. Consider cases where the sentence is\nexpressing inability to answer the question due to lack of relevant information in", "b3cc0a8e-f60d-440c-8ad5-ad6b620a59dc": "the provided contex as \"supported_without_sentence\". In cases where the sentence\nis making a general statement (e.g. outlining the steps to produce an answer, or\nsummarizing previously stated sentences, or a transition sentence), use the\nsting \"general\".In cases where the sentence is correctly stating a well-known fact,\nlike a mathematical formula, use the string \"well_known_fact\". In cases where the\nsentence is performing numerical reasoning (e.g. addition, multiplication), use", "98a21eab-9cf5-4a54-a445-9ff41fa0cfe2": "the string \"numerical_reasoning\".\n- fully_supported: a boolean indicating whether the sentence is fully supported by\nthe documents.\n  - This value should reflect the conclusion you drew at the end of your step-by-step\n  breakdown in explanation.\n  - If supporting_sentence_keys is an empty list, then fully_supported must be false.\n  - Otherwise, use fully_supported to clarify whether everything in the response", "85d89886-5df0-4421-a2b0-2bfb9f0eebe6": "sentence is fully supported by the document text indicated in supporting_sentence_keys\n  (fully_supported = true), or whether the sentence is only partially or incompletely\n  supported by that document text (fully_supported = false).", "24969099-b3e1-49e2-b10a-0304f905f25e": "The all_utilized_sentence_keys field is a list of all sentences keys (e.g. \u20190a\u2019) that\nwere used to construct the answer. Include every sentence that either directly supported\nthe answer, or was implicitly used to construct the answer, even if it was not used\nin its entirety. Omit sentences that were not used, and could have been removed from\nthe documents without affecting the answer.", "046ed474-f39e-4058-9224-c7228e631d92": "You must respond with a valid JSON string.  Use escapes for quotes, e.g. \u2018\\\\\"\u2018, and\nnewlines, e.g. \u2018\\\\n\u2018. Do not write anything before or after the JSON string. Do not\nwrap the JSON string in backticks like \u2018\u2018\u2018 or \u2018\u2018\u2018json.\n\nAs a reminder: your task is to review the response and assess which documents contain\nuseful information pertaining to the question, and how each sentence in the response\nis supported by the text in the documents.\\\n\n\n\n\n\n7.5 Annotation Post-Processing Steps", "86802c2b-7d41-4a28-b840-a746cbad6bbf": "7.5 Annotation Post-Processing Steps\n\nAs shown in Appendix 7.4, we request very detailed annotations with explanations from GPT-4-turbo. We pivot on chain-of-thought [39] and redundancy to encourage high quality labels from the annotator model."}}