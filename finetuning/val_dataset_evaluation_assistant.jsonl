{"questions": {"10c826f0-e59a-4039-b5a4-bfb31fd04b89": "What types of annotations are requested for assessing Adherence according to the context?", "4f7c6b92-346b-4431-96f6-607b5eb51ad2": "What happens if GPT-4's response-level and sentence-level annotations are found to be inconsistent?", "ef859b58-a234-4a96-8951-9200ae9848ea": "What is the main cause of the data conflicts identified after manual inspection?", "954af320-7a92-4f01-bc35-1a2a3152461e": "How does the annotation schema treat partially supported sentences?", "986625d8-4130-448f-99a6-4eca88151c54": "How does taking extra measures for Adherence impact the quality and stability of relevance and utilization labels?", "d2015441-9346-4acb-9a98-57687f22d582": "Why are all TRACe metrics considered related according to the context?", "a218e2c0-019d-4051-b9d4-dafd138451ce": "What types of off-schema keys does GPT-4-turbo sometimes inject into the response during the final post-processing step?", "d5801a78-b99e-492c-8d30-13e618f6a443": "How are annotation errors such as misspelled or new fields handled in the post-processing step?", "43968561-5dfd-449e-a782-cce3e54a17ed": "What is the source of the context documents used in the DelucionQA benchmark?", "f3e1d220-620b-492e-b5c1-cb45ab485fc6": "How are the natural language queries in DelucionQA generated and processed before use?", "9d40ac34-84a8-45a8-8d03-16319b482fb8": "How are example-level labels determined based on span-level annotations in the given context?", "b0ca169a-68eb-4fe9-8b77-bc3ee4eea7d1": "Under what condition does a whole response receive a Hallucinated label according to the context?", "c15a98f6-5836-4bd7-af5b-f4fb837bb69a": "What are the two categories that sentences labeled as \"Neither\" by DelucionQA typically fall into?", "8eae5388-c416-4063-a801-e7073108eb24": "Why are examples with any \"Neither\" labels removed according to the annotation schema?", "ba62d7d7-b1b2-4005-8b5d-8cf671e8d82f": "How many examples were annotated with the LLM annotator after removing those with \"Neither\" sentence annotations?", "47d26ac3-3ba7-4f7f-b25d-7a48a29af26b": "Where can the alignment between the LLM annotator and human annotations be found?", "d5b32c0a-8a78-4b0c-8eb0-f7c3ddbd5326": "What are the F1 and Accuracy scores for Adherence on the DelucionQA example level test set?", "15c61360-645d-4808-bcb4-f37d5f67ad5d": "How do the F1 and Accuracy metrics change when the \"Neither\" category is removed from the DelucionQA example and span level test sets?", "b8336030-e761-480c-abef-acce21671fa0": "What are the utilization values reported for DelucionQA at the sapen level?", "e204d625-6c73-4720-90bf-af9e3f15363f": "How does the relevance at the span level compare between the two reported values for DelucionQA?", "bea7160b-31f9-4540-b488-050f18403dc5": "How did the authors validate the Relevance and Utilization annotations in their study?", "4b0e35e2-af0c-455e-a18d-1863ea139626": "What metrics are reported in Table 4 to evaluate the annotation quality?", "59c0dcd9-3682-48e1-b059-a7c6fd78b53f": "What datasets are included in the RAG validation set sampled from KILT?", "a7fb4465-b822-4ca8-88f1-526e3cba8eeb": "How do Saad-Falcon et al. generate systems of varying quality in their mock RAG datasets?", "5da39be3-3e84-4cae-8c0f-b9622234a01b": "What metric is used to compare the rankings of mock systems to the ground truth?", "7ec7ab5e-b599-416e-bf88-457817b2877e": "How are the average annotated context relevance and adherence scores utilized in the evaluation process?", "c4aa534f-7709-4376-bf3c-9d42f1d5ffc4": "What range of Kendall\u2019s \u03c4 values do the GPT-4 annotations achieve according to Table 5?", "8b9a6d52-2921-456c-a2fa-900c1e034518": "How are binary context relevance and labels derived from the GPT-4 annotations for comparison with ground truth labels?"}, "relevant_contexts": {"10c826f0-e59a-4039-b5a4-bfb31fd04b89": ["970523e6-fa1f-4c2f-9e60-dd56f235a958"], "4f7c6b92-346b-4431-96f6-607b5eb51ad2": ["970523e6-fa1f-4c2f-9e60-dd56f235a958"], "ef859b58-a234-4a96-8951-9200ae9848ea": ["2da5cdbd-155e-43db-8fa8-2d74f1508fa8"], "954af320-7a92-4f01-bc35-1a2a3152461e": ["2da5cdbd-155e-43db-8fa8-2d74f1508fa8"], "986625d8-4130-448f-99a6-4eca88151c54": ["95e9a536-e22b-4a57-86b6-738d93aeb8b4"], "d2015441-9346-4acb-9a98-57687f22d582": ["95e9a536-e22b-4a57-86b6-738d93aeb8b4"], "a218e2c0-019d-4051-b9d4-dafd138451ce": ["c7a8cd54-1756-4f71-b744-da01c6fff9b2"], "d5801a78-b99e-492c-8d30-13e618f6a443": ["c7a8cd54-1756-4f71-b744-da01c6fff9b2"], "43968561-5dfd-449e-a782-cce3e54a17ed": ["b57ab10d-0766-4e53-aed5-a629d9505bab"], "f3e1d220-620b-492e-b5c1-cb45ab485fc6": ["b57ab10d-0766-4e53-aed5-a629d9505bab"], "9d40ac34-84a8-45a8-8d03-16319b482fb8": ["4854af46-f82a-4709-a274-ed6f65d04b65"], "b0ca169a-68eb-4fe9-8b77-bc3ee4eea7d1": ["4854af46-f82a-4709-a274-ed6f65d04b65"], "c15a98f6-5836-4bd7-af5b-f4fb837bb69a": ["b71ef7c3-c9d8-4794-8a4f-390cdd15033d"], "8eae5388-c416-4063-a801-e7073108eb24": ["b71ef7c3-c9d8-4794-8a4f-390cdd15033d"], "ba62d7d7-b1b2-4005-8b5d-8cf671e8d82f": ["f3610261-aa72-4f1e-95e3-4314c56ed2a9"], "47d26ac3-3ba7-4f7f-b25d-7a48a29af26b": ["f3610261-aa72-4f1e-95e3-4314c56ed2a9"], "d5b32c0a-8a78-4b0c-8eb0-f7c3ddbd5326": ["92735f65-a46e-4200-aa1d-84d10f2a5c14"], "15c61360-645d-4808-bcb4-f37d5f67ad5d": ["92735f65-a46e-4200-aa1d-84d10f2a5c14"], "b8336030-e761-480c-abef-acce21671fa0": ["d5087560-b95b-4d37-8235-aee00b368aeb"], "e204d625-6c73-4720-90bf-af9e3f15363f": ["d5087560-b95b-4d37-8235-aee00b368aeb"], "bea7160b-31f9-4540-b488-050f18403dc5": ["f95262ac-893e-4e63-91bb-567114217715"], "4b0e35e2-af0c-455e-a18d-1863ea139626": ["f95262ac-893e-4e63-91bb-567114217715"], "59c0dcd9-3682-48e1-b059-a7c6fd78b53f": ["9bb50650-825e-41f0-9322-29e8ff8a7476"], "a7fb4465-b822-4ca8-88f1-526e3cba8eeb": ["9bb50650-825e-41f0-9322-29e8ff8a7476"], "5da39be3-3e84-4cae-8c0f-b9622234a01b": ["983416dd-0d14-4fe2-8e3b-5d17fa795c81"], "7ec7ab5e-b599-416e-bf88-457817b2877e": ["983416dd-0d14-4fe2-8e3b-5d17fa795c81"], "c4aa534f-7709-4376-bf3c-9d42f1d5ffc4": ["629f488e-00d1-421d-b6ad-7005bacb8804"], "8b9a6d52-2921-456c-a2fa-900c1e034518": ["629f488e-00d1-421d-b6ad-7005bacb8804"]}, "corpus": {"970523e6-fa1f-4c2f-9e60-dd56f235a958": "For Adherence, we request both response-level and sentence-level annotations that we compare in post-processing to identify inconsistencies where GPT-4 disagrees with its own judgements. For example, if GPT-4 claims a response as supported by the context as a whole, but identifies no supporting information for one or more claims in the response, we send the example for re-annotation. We re-annotate all data up to 3 times, after which a fraction (<2%) of the data are still conflicting. After", "2da5cdbd-155e-43db-8fa8-2d74f1508fa8": "(<2%) of the data are still conflicting. After manual inspection, we find that the majority of the conflicts arise from partially hallucinated sentences that are somewhat, but not fully, grounded in the context. We leverage a sentence-level \"fully_supported\" boolean annotation to identify and resolve such cases. According to our annotation schema, we treat all partially supported sentences as hallucinations.", "95e9a536-e22b-4a57-86b6-738d93aeb8b4": "Since all TRACe metrics are related, we qualitatively observe that taking the extra measures for Adherence also positively impacts the quality and stability of the relevance and utilization labels.", "c7a8cd54-1756-4f71-b744-da01c6fff9b2": "In the final post-processing step, we remove any off-schema keys that GPT-4-turbo sometimes injects into the response. For example, it will occasionally misspell \"supporting_sentence_keys\" as \"supported_sentence_keys\" and/or introduce completely new fields into the output json. We algorithmically find and remove/replace such annotation errors.\n\n\n\n\n7.6 Annotation Alignment with Human Judgements\n\n\n7.6.1 Adherence Alignment with DelucionQA", "b57ab10d-0766-4e53-aed5-a629d9505bab": "We validate our metric formulations and labeling approach on a human annotated benchmark. DelucionQA [33] is a curated collection of user queries on the operation of Jeep\u2019s 2023 Gladiator model. Natural language queries are first generated by an LLM, then reviewed and filtered by human annotators. Context documents are sourced from Jeep\u2019s Gladiator User Manual, and responses are generated by various LLMs. Human annotators label each response sentence as \"Supported\" by the context documents,", "4854af46-f82a-4709-a274-ed6f65d04b65": "sentence as \"Supported\" by the context documents, \"Conflicted\", or \"Neither\". Example-level labels are derived from the span-level annotation as follows: if at least one response sentence is annotated as \"Conflicted\" or \"Neither\", the whole response receives a Hallucinated label.", "b71ef7c3-c9d8-4794-8a4f-390cdd15033d": "In our initial investigation, we found that sentences that DelucionQA labels as \"Neither\" often fall into one of two categories: (1) general filler statements (e.g. \"Here are the steps:\"), (2) claims of missing information (e.g. \"There is no mention of any problem with engine start-up in freezing weather related to DEF.\"). According to our annotation schema, these types of statements are generally grounded in the context and not hallucinations. Thus, we remove examples with any \"Neither\"", "f3610261-aa72-4f1e-95e3-4314c56ed2a9": "Thus, we remove examples with any \"Neither\" sentence annotations for our analysis. We annotate the remaining 421 examples with our LLM annotator and report alignment with human annotations in Table 4.", "92735f65-a46e-4200-aa1d-84d10f2a5c14": "Table 4: Annotation Alignment with DelucionQA. We report F1 and Accuracy metrics on human annotated subsets of DelucionQA. DelucionQA(40) is a subset of 40 examples randomly sampled from the DelucionQA test set and annotated for Relevance and Utilization by the authors.\n\n\n\nTest Set\nMetric\nF1\nAccuracy\n\n\n\n\nDelucionQA - example level\nAdherence\n0.83\n0.76\n\n\nDelucionQA - example level - remove \"Neither\"\nAdherence\n0.96\n0.93\n\n\nDelucionQA - span level - remove \"Neither\"\nAdherence\n0.97\n0.95", "d5087560-b95b-4d37-8235-aee00b368aeb": "DelucionQA(40) - sapen level\nUtilization\n0.92\n0.94\n\n\nDelucionQA(40) - span level\nRelevance\n0.76\n0.78\n\n\n\n\n\n\n\n7.6.2 Relevance and Utilization Alignment with DelucionQA", "f95262ac-893e-4e63-91bb-567114217715": "To validate Relevance and Utilization annotations, the authors annotate a small set of 40 randomly samples examples from the DelucionQA test set. We follow the same instructions as in out annotation prompt 7.4 to label relevant and utilized context sentences, given the context, query, and response. We report sentence-level F1 and overall alignment (Accuracy) scores in Table 4.\n\n\n\n\n7.6.3 Rank-based Alignment for Adherence and Relevance", "9bb50650-825e-41f0-9322-29e8ff8a7476": "We use mock RAG datasets generated by Saad-Falcon et\u00a0al. [32] for this analysis. Their RAG validation set is sampled from KILT [29], including Natural Questions (NQ)[17], HotpotQA[42], FEVER[35], and Wizards of Wikipedia (WoW) [8] datasets. The authors synthetically generate systems of varying quality by adjusting the ratio of relevant documents and responses in the data. We sample 500 examples from each simulated RAG dataset and annotated them as described in section 3.4. Next, we calculate", "983416dd-0d14-4fe2-8e3b-5d17fa795c81": "as described in section 3.4. Next, we calculate average annotated context relevance and adherence scores for each dataset and use those to rank the mock systems. We compare our rankings to ground truth with the Kendall rank correlation (Kendall\u2019s \u03c4\ud835\udf0f\\tauitalic_\u03c4) metric, which evaluates the agreement between two sets of ranks on a scale from 0 (no agreement) to 1 (perfect agreement).", "629f488e-00d1-421d-b6ad-7005bacb8804": "As shown in Table 5, the GPT-4 annotations achieve high Kendall\u2019s \u03c4\ud835\udf0f\\tauitalic_\u03c4 ranging from 0.78 to 1. For a fair comparison with the ground truth labels, we derive binary context relevance and labels from the GPT-4 annotations by thresholding the example Relevance score (equation 2) at 0. For comparison, we also report ranking results with our more granular example-level Relevance scores that range from 0-1. We find that these metrics produce a different ranking (see lower Kendall\u2019s"}}