{"questions": {"d8955940-fe76-4e9f-93ec-9a3d7477c84f": "What is the main concept introduced in \"Direct preference optimization: Your language model is secretly a reward model\"?", "066289b9-7486-496b-9c43-7a2240bc5596": "How do Raghubir and Valenzuela (2006) describe the impact of position biases in decision-making?", "d5eafd09-4ebb-479a-9bac-169b4fdb0e34": "What are the main findings of Raina et al. (2024) regarding the robustness of LLMs when used as judges against universal adversarial attacks?", "1b2ca034-1714-487f-a861-1f40d1f57fdf": "How do Raju et al. (2024) approach the construction of domain-specific evaluation sets for LLM-as-a-judge?", "9ca92a83-163b-4f18-98ea-078af1b2c0e7": "What is the focus of the study conducted by Ryu et al. (2023) in the context of Korean legal question answering?", "f47c1ed4-6bf6-4e64-93d0-e2606b14fb60": "What contribution do Saad-Falcon et al. (2023) make to the evaluation of retrieval-augmented generation systems?", "fef42546-382c-4029-a320-be0bc625238b": "What are the key techniques and applications discussed in the systematic survey of prompt engineering in large language models presented in arXiv preprint arXiv:2402.07927 (2024)?", "c5efe173-86cf-4d32-88a8-0fc825deb1aa": "How do the statistical methods such as Spearman\u2019s rank correlation coefficient and Kendall\u2019s tau regression coefficient estimates relate to the analysis or evaluation of prompt engineering techniques in large language models?", "29eef44c-ef4e-471b-ade6-b474ae0f78c5": "What methods are proposed in \"Who validates the validators?\" for aligning LLM-assisted evaluation of LLM outputs with human preferences?", "d6eb4ec2-ad08-4875-bb6d-c4649546330a": "How do Shen et al. (2023) characterize and evaluate in-the-wild jailbreak prompts on large language models?", "e5cdf0c7-bc78-4553-a386-a22ff1be8ce7": "What is the main focus of the research conducted by Shi et al. (2023) as presented at the International Conference on Machine Learning?", "2985d9e9-a15c-4a11-91c3-718ac3a3122b": "What type of attack is discussed in the 2024b paper by Jiawen Shi and colleagues regarding large language models?", "50e7e979-fffb-4c03-82bf-d441e1df3372": "What is the main focus of the study titled \"Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by LLMs\"?", "ad5666d2-038e-4a97-801e-bb6648bca831": "How does the Fusion-Eval approach integrate assistant evaluators with large language models according to Shu et al. (2024)?", "455ae496-7989-4748-a907-bd0da1811815": "What is the main focus of the paper titled \"Beyond human data: Scaling self-training for problem-solving with language models\"?", "ff4e5750-6134-4c7c-af33-e5777eaca28e": "How do Son et al. (2024a) propose to use large language models in the creation of financial benchmarks according to their work on KRX Bench?", "d151aaa3-4c5b-4c11-8bce-e238a90a2655": "What is the main focus of the MM-Eval benchmark introduced by Son et al. (2024b)?", "9db5e6f9-bb8d-4338-b6b1-fa434f106d47": "How does FineSurE, proposed by Song et al. (2024a), utilize large language models for summarization evaluation?", "0e3b6bf7-b0a3-4e61-9cbd-e8781c3c02df": "What is the main research question addressed by Mingyang Song, Mao Zheng, and Xuan Luo in their 2024 paper on many-shot in-context learning for long-context LLM judges?", "e420a59e-ab7b-4a30-8356-d876d6c8a02e": "How do Yishen Song and colleagues propose to use open-source large language models for automated essay scoring and revising in their 2024 study?", "85710fa4-8c31-4616-abb0-2e76a6c1a8d6": "What are the main findings of Stephan et al. (2024) regarding the performance of large language models as judges on mathematical reasoning tasks?", "55dd04fa-1aab-4146-af64-58e80c0c5555": "According to Stureborg et al. (2024), in what ways are large language models inconsistent and biased evaluators?", "bbf58cda-f585-4fd3-b898-96641d5a4476": "What are the main findings of Lichao Sun's 2020 paper on natural backdoor attacks on text data?", "67698a45-174c-41e2-a1f5-32a69ed74059": "How do Szymanski et al. (2024) describe the limitations of using large language models as judges for evaluating LLM outputs in expert knowledge tasks?", "c71c693a-04e2-4772-bff7-3bf2359d0501": "What is the main focus of the paper \"JudgeBench: A Benchmark for Evaluating LLM-based Judges\" by Tan et al. (2024)?", "e395a9c8-2e6d-4d4a-a02f-536194e8f9c5": "How do Tessler et al. (2024) propose AI can assist humans in democratic deliberation according to their Science publication?", "43f29890-9c6a-4d7d-bf35-684176f365a8": "What are the main findings of Thakur et al. (2024) regarding the alignment and vulnerabilities of large language models when used as judges?", "36f7ae67-a527-4119-938e-6e694ca3b118": "According to Tonmoy et al. (2024), what are some key techniques for mitigating hallucinations in large language models?", "e393b554-e28e-4d5d-9007-ed6089ca5aaf": "What are the main contributions of Touvron et al. (2023) in their work on Llama 2 models?", "97a5f27f-79da-4fe5-a9f3-9d797c6b1d70": "How does the Appworld framework introduced by Trivedi et al. (2024b) facilitate benchmarking interactive coding agents?", "c8d20f17-0833-4fdd-bc77-85bcd75c46c2": "What is the main contribution of Trivedi et al. (2024a) regarding large language models (LLMs)?", "302d4a6e-42ef-4978-853c-b3c8e2478d48": "What research question do Tseng et al. (2024) explore about expert-level language models?", "ef227a61-f1b3-4621-81c3-a32a9c452aa0": "What is the main finding of Tyen et al. (2023) regarding large language models and reasoning errors?", "d202e65d-639b-4be1-9fb4-4ce888bfc146": "What question do Valmeekam et al. (2023) explore about large language models and self-critiquing?", "db38cd94-cd46-46ae-bb40-f78a11e3443f": "What are the main approaches discussed in the papers for improving the evaluation of large language model generations?", "1dd5d426-2e2a-4489-9a29-b699a5696b03": "How do the concepts of \"Foundational autoraters\" and \"Halu-j\" contribute to addressing challenges in automatic evaluation and hallucination detection in LLMs?", "47491fc9-23ce-4ed4-87fe-6518f6a22b1d": "What are the main contributions of the paper titled \"Automated Genre-Aware Article Scoring and Feedback Using Large Language Models\"?", "80040dc3-133d-4e9a-b06f-3869684cb2f3": "How do Wang et al. (2023e) approach learning evaluation models from large language models for sequence generation?", "fc72155b-d079-4a84-8e54-f01ba234f5e4": "What is the main contribution of the paper \"Self-taught evaluators\" by Wang et al. (2024e)?", "f4982dd3-c308-4dda-80c5-afec47600cdb": "How does the \"Shepherd\" critic introduced by Wang et al. (2023c) contribute to language model generation evaluation?", "61f2c0d8-0514-494c-a2b2-bc7cb6cd5987": "What is the main focus of the paper titled \"Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic Evaluation Framework for LLMs\"?", "1bbcaa96-82be-45bf-b6e1-d89e8a64f906": "How does the concept of self-consistency contribute to chain of thought reasoning in language models according to Wang et al. (2022)?", "498a14de-9191-4b4e-b301-38c72c837e6d": "What is the focus of the paper titled \"Pandalm: An automatic evaluation benchmark for llm instruction tuning optimization\" by Wang et al. (2023d)?", "a3da478b-d652-4b10-a4b2-dc22a21ab7b8": "How does the work \"HelpSteer2-Preference: Complementing Ratings with Preferences\" by Wang et al. (2024a) contribute to evaluation methods?", "42bc723d-e630-40b0-823d-9d78ef2e652e": "What is the main contribution of the paper \"Helpsteer: Multi-attribute helpfulness dataset for steerlm\" by Wang et al. (2023a)?", "6cfe013b-b00c-486c-b59c-2ef8ea017497": "How does the \"Cream: Consistency regularized self-rewarding language models\" approach proposed by Wang et al. (2024d) aim to improve language models?", "a3a72af5-cdb4-4bb8-ac4d-144e451dd99e": "What is the main focus of the study conducted by Watts et al. (2024) in their paper titled \"PARIKSHA\"?", "bbd8091e-7b76-4a83-9964-221a497df696": "How does the work of Wei et al. (2022) contribute to understanding reasoning in large language models?", "252b2fd3-d7cf-488f-987a-ec1f431c5dac": "What is the main focus of the CodeUltraFeedback dataset mentioned in the context?", "b2577a23-3e78-42e9-8fd8-bccf483ec478": "How do Wu et al. (2024b) propose to improve language model alignment according to the provided information?", "3d168f24-ff41-47a3-a88d-1f2c950866fb": "What is the main focus of the paper by Xia et al. (2024a) titled \"Evaluating Mathematical Reasoning Beyond Accuracy\"?", "84d4bb8a-c6cc-47f1-b908-d3a101066fe7": "How do Xia et al. (2024b) propose that language models can evaluate themselves according to their paper \"Language Models can Evaluate Themselves via Probability Discrepancy\"?", "29bd8a11-d9f2-4117-a4f7-38ce0d0a55da": "What is the main focus of the paper \"Sorry-bench\" by Xie et al. (2024b)?", "740f6353-df76-470b-b283-dbf3734e83ea": "How does the work by Yuxi Xie et al. (2024a) contribute to reasoning in large language models?", "2d8ebba8-8ee9-40dd-9c5e-7ef60e2302c0": "What is the main focus of the DOCLENS framework introduced by Xie et al. (2024c) in the evaluation of medical text generation?", "c597cbc8-786d-442a-a930-544369ac874c": "How does the fine-grained critique-based evaluator proposed by Xie et al. (2024d) aim to improve model factuality?", "f6584af3-fc03-4462-beaf-399a38b64bea": "What is the main focus of the paper \"Llava-critic: Learning to evaluate multimodal models\" by Xiong et al. (2024)?", "ed0229bf-b8ac-4e93-8c62-596f30489775": "How does the work \"Wizardlm: Empowering large language models to follow complex instructions\" by Xu et al. (2023d) contribute to the development of large language models?", "ddf44a0d-e0bc-4b8c-acc6-872da86075f6": "What aspects of Chinese large language models are evaluated in the study titled \"Cvalues: Measuring the values of Chinese large language models from safety to responsibility\"?", "7c93d7e4-a8e8-4aea-a455-806f190a7b92": "How do Xu et al. (2024b) describe the role of large language models in natural language generation (NLG) evaluation?", "b5845615-aaa2-45f0-a0ba-2b99bd1e22c5": "What is the main focus of the paper \"INSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback\" by Xu et al. (2023e)?", "1b291a96-43c8-487d-8f53-7c3670afb888": "According to Xu et al. (2024c), how does a large language model (LLM) affect self-bias in self-refinement?", "8896259d-bfc6-40cc-a0f0-44bf34d9ff42": "What is the main focus of the paper titled \"An LLM can Fool Itself: A Prompt-Based Adversarial Attack\"?", "a817a041-03ee-4d65-83b6-f540b394b713": "How do Xu et al. (2023c) propose to enhance reasoning in large language models?", "c4ded33e-3165-4ca3-8fb4-2d9b755f11a3": "What is the main focus of the paper by Yan et al. (2024b) presented at the 2024 Conference on Empirical Methods in Natural Language Processing?", "5a7b3592-556c-4f3f-a341-ca766cb7096a": "Who are the authors of the study titled \"Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing\"?", "6102a609-ffef-4ee7-8fdb-ce802354fa44": "What method is proposed for mitigating biases in instruction-following language models according to the 62nd Annual Meeting of the Association for Computational Linguistics paper?", "d649a9c4-41b6-4a82-a771-a016fafcbb34": "How do Ye and Ng (2024) approach selective instruction following in their work titled \"Self-Judge: Selective Instruction Following with Alignment Self-Evaluation\"?", "0efb3d15-2567-4676-aa0f-7447f11c383f": "What is the main focus of the study conducted by Ye et al. (2024b) titled \"Justice or prejudice? quantifying biases in llm-as-a-judge\"?", "bb22dcd6-b098-43d0-a75b-8de4c44117cb": "How does the 2023a work by Ye et al. describe the process of iterative self-revising in large language models?", "4efd31a0-bee1-4110-a077-207e3470581e": "What is the main focus of the Flask framework mentioned in the context?", "f5688e1b-a724-4dc3-9790-ce90fb5e41af": "Which paper introduces the concept of learning a generative judge from preference data?", "9d671597-cb9f-439f-a3cf-4e0c1e7171c2": "What is the main focus of the work by Yoshino et al. (2023) as mentioned in the context?", "d10c7313-b6cc-425d-bebe-d92369d27b35": "What contribution do Yu et al. (2024) make to the evaluation of large language models according to the context?", "044a79dc-98a3-4044-8e53-e0692d0a8522": "What is the main focus of the paper by Yuan et al. (2024) titled \"Self-rewarding language models\"?", "7162212b-c51b-462d-88fe-9c6a72367003": "How do Yue et al. (2023a) contribute to the field of legal services with their work on Disc-lawllm?", "0f22b5eb-953c-4bde-adfc-4a2259eb9d7b": "What is the main contribution of the paper \"STaR: Self-taught reasoner bootstrapping reasoning with reasoning\" by Zelikman et al. (2024)?", "f4027b8c-7c37-430e-8c3f-27ba4a60c2fb": "What topic do Zeng et al. (2024) address in their paper titled \"Automatic Instruction Evolving for Large Language Models\"?", "e1bb7837-003d-4aea-997f-24f403b042d4": "What is the main focus of the paper titled \"TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot\" by Kaiqi Zhang, Shuai Yuan, and Honghan Zhao (2024c)?", "08aee59a-6ada-490b-ba39-1a204a2b5a2e": "How does the work \"RevisEval: Improving LLM-as-a-Judge via Response-Adapted References\" by Qiyuan Zhang et al. (2024b) aim to enhance the evaluation capabilities of large language models?", "cf397ba3-ea14-4978-b965-ef48da09c231": "What is the main focus of the paper titled \"Llmaaa: Making large language models as active annotators\"?", "a134cef3-680b-4753-9909-54eaf7cfb7cd": "How do Zhang et al. (2024a) utilize large language models in the context of recommendation systems?", "44cf1687-2737-4774-9b1c-fe6b67bb92f4": "According to the context, what characteristics make LLM networks fairer evaluators?", "e8471963-9d6c-4128-9f63-f9c26c1119fa": "What methods are discussed by Zhao et al. (2024b) for automating LLM evaluations?", "637ceb52-dbba-434f-b4d4-396cb934a5f5": "What are the main topics addressed by the works of Xiutian Zhao, Ke Wang, and Wei Peng (2024a) and Yachao Zhao et al. (2023a) regarding large language models?", "48b6e28a-ba24-41ee-a898-444c09d034a4": "How does the 2021 study by Zihao Zhao et al. propose to improve the few-shot performance of language models?", "d64d4436-0e69-4007-9b70-6317e8923157": "What is the main finding of Zheng et al. (2023b) regarding large language models and multiple choice selection?", "95b54551-b498-4b84-8e26-4c26be32811b": "What evaluation methods are discussed by Zheng et al. (2023a) for assessing large language models as judges?", "79ba5689-2db1-4cc6-a336-e6b9dc883322": "What issue is highlighted by the finding that null models achieve high win rates in automatic LLM benchmarks?", "662fbb0a-712e-4a43-80ca-8fb1cdcfcf7e": "How do the works by Zhou et al. (2024c) and Zhou et al. (2024e) contribute to improving the evaluation and alignment of large language models?", "232fee96-1129-4e75-a749-4c33aeffd264": "What is the main focus of the paper \"Batch calibration: Rethinking calibration for in-context learning and prompt engineering\" by Han Zhou et al. (2023a)?", "6168eec4-1f1f-46e5-9afc-124b1fc48404": "How do Ruiyang Zhou, Lu Chen, and Kai Yu (2024a) evaluate the reliability of large language models (LLMs) in automatic paper reviewing tasks?", "b25e5991-7be9-4b7e-971e-138b6073d4eb": "What is the main focus of the paper \"Sotopia: Interactive evaluation for social intelligence in language agents\" by Zhou et al. (2023b)?", "2846543e-c72f-4e5a-abcc-b1b4719255a4": "What topic is addressed in the 2024b paper by Zhou et al. regarding vision language models?", "36ba3948-7389-45f8-9c62-b0bad29ef926": "What methodologies do Zhu et al. (2023) propose for fine-tuning large language models to act as scalable judges?", "3490ab7b-be84-4010-a120-c7fb3d5ebca3": "How does the setwise approach introduced by Zhuang et al. (2024) improve zero-shot ranking efficiency with large language models?", "174d2522-b139-437a-b23f-290d16755663": "What methodology do Zhuge et al. (2024) propose for evaluating agents using other agents in their \"Agent-as-a-Judge\" framework?", "ac489873-0cb2-472f-90f4-6244f7d4c3e5": "How does Terry Yue Zhuo's (2023) ICE-Score approach instruct large language models to evaluate code effectively?", "3100447f-c29d-42ff-8fb2-7053f5ec18ca": "What is the significance of the arXiv preprint with the identifier arXiv:2307.15043 published in 2023?", "2f399cce-ec1a-41e7-ba29-5cf1c81d1e3e": "How does the LaTeXML system contribute to the generation of documents like the one created on Tue Dec 10 05:49:16 2024?"}, "relevant_contexts": {"d8955940-fe76-4e9f-93ec-9a3d7477c84f": ["e78d6df8-1c5c-4c12-9c0c-609d422b879c"], "066289b9-7486-496b-9c43-7a2240bc5596": ["e78d6df8-1c5c-4c12-9c0c-609d422b879c"], "d5eafd09-4ebb-479a-9bac-169b4fdb0e34": ["0c14b5ee-95ec-4868-84b1-173fe33160f6"], "1b2ca034-1714-487f-a861-1f40d1f57fdf": ["0c14b5ee-95ec-4868-84b1-173fe33160f6"], "9ca92a83-163b-4f18-98ea-078af1b2c0e7": ["6c831f3c-d151-4a71-9ee3-aad5aaaa1057"], "f47c1ed4-6bf6-4e64-93d0-e2606b14fb60": ["6c831f3c-d151-4a71-9ee3-aad5aaaa1057"], "fef42546-382c-4029-a320-be0bc625238b": ["591e0507-858a-427c-bd0a-8d2e0ef180a0"], "c5efe173-86cf-4d32-88a8-0fc825deb1aa": ["591e0507-858a-427c-bd0a-8d2e0ef180a0"], "29eef44c-ef4e-471b-ade6-b474ae0f78c5": ["adb4004e-b73d-4041-9f65-0e1b47ad800f"], "d6eb4ec2-ad08-4875-bb6d-c4649546330a": ["adb4004e-b73d-4041-9f65-0e1b47ad800f"], "e5cdf0c7-bc78-4553-a386-a22ff1be8ce7": ["6116542b-1255-440b-8126-7caf2cfc4e1e"], "2985d9e9-a15c-4a11-91c3-718ac3a3122b": ["6116542b-1255-440b-8126-7caf2cfc4e1e"], "50e7e979-fffb-4c03-82bf-d441e1df3372": ["44869b98-db05-4684-bc71-8db6b8489d40"], "ad5666d2-038e-4a97-801e-bb6648bca831": ["44869b98-db05-4684-bc71-8db6b8489d40"], "455ae496-7989-4748-a907-bd0da1811815": ["b6918911-989b-46d4-a064-43d5fcafa30d"], "ff4e5750-6134-4c7c-af33-e5777eaca28e": ["b6918911-989b-46d4-a064-43d5fcafa30d"], "d151aaa3-4c5b-4c11-8bce-e238a90a2655": ["18e8954f-b542-46ab-be84-ca252da72511"], "9db5e6f9-bb8d-4338-b6b1-fa434f106d47": ["18e8954f-b542-46ab-be84-ca252da72511"], "0e3b6bf7-b0a3-4e61-9cbd-e8781c3c02df": ["d9a00185-1e55-4b10-9981-82e097aab845"], "e420a59e-ab7b-4a30-8356-d876d6c8a02e": ["d9a00185-1e55-4b10-9981-82e097aab845"], "85710fa4-8c31-4616-abb0-2e76a6c1a8d6": ["fd0b5dab-398e-4964-b9a5-813fddb26481"], "55dd04fa-1aab-4146-af64-58e80c0c5555": ["fd0b5dab-398e-4964-b9a5-813fddb26481"], "bbf58cda-f585-4fd3-b898-96641d5a4476": ["46e493c8-5d81-41d2-9c06-5c1c6f5285fb"], "67698a45-174c-41e2-a1f5-32a69ed74059": ["46e493c8-5d81-41d2-9c06-5c1c6f5285fb"], "c71c693a-04e2-4772-bff7-3bf2359d0501": ["7b105afb-7fd0-416c-8e96-64bec451f18d"], "e395a9c8-2e6d-4d4a-a02f-536194e8f9c5": ["7b105afb-7fd0-416c-8e96-64bec451f18d"], "43f29890-9c6a-4d7d-bf35-684176f365a8": ["2c061971-f636-43a8-8339-38bb01596809"], "36f7ae67-a527-4119-938e-6e694ca3b118": ["2c061971-f636-43a8-8339-38bb01596809"], "e393b554-e28e-4d5d-9007-ed6089ca5aaf": ["b10fffbc-c202-4ec0-8874-38d437b353c7"], "97a5f27f-79da-4fe5-a9f3-9d797c6b1d70": ["b10fffbc-c202-4ec0-8874-38d437b353c7"], "c8d20f17-0833-4fdd-bc77-85bcd75c46c2": ["87811c5e-da4f-496f-93ad-a20246e270eb"], "302d4a6e-42ef-4978-853c-b3c8e2478d48": ["87811c5e-da4f-496f-93ad-a20246e270eb"], "ef227a61-f1b3-4621-81c3-a32a9c452aa0": ["56e5b03c-9607-42d5-8c15-15854381602b"], "d202e65d-639b-4be1-9fb4-4ce888bfc146": ["56e5b03c-9607-42d5-8c15-15854381602b"], "db38cd94-cd46-46ae-bb40-f78a11e3443f": ["9244969f-c68f-4e00-b4e1-9107c3ac2c72"], "1dd5d426-2e2a-4489-9a29-b699a5696b03": ["9244969f-c68f-4e00-b4e1-9107c3ac2c72"], "47491fc9-23ce-4ed4-87fe-6518f6a22b1d": ["d68a2c52-287e-45f3-8d60-7dc6bd48e7cd"], "80040dc3-133d-4e9a-b06f-3869684cb2f3": ["d68a2c52-287e-45f3-8d60-7dc6bd48e7cd"], "fc72155b-d079-4a84-8e54-f01ba234f5e4": ["5d459b8c-f8a5-43b2-8e14-153cc9eb42c7"], "f4982dd3-c308-4dda-80c5-afec47600cdb": ["5d459b8c-f8a5-43b2-8e14-153cc9eb42c7"], "61f2c0d8-0514-494c-a2b2-bc7cb6cd5987": ["2d16e6bd-ab89-4e40-a124-797dc7ee7e48"], "1bbcaa96-82be-45bf-b6e1-d89e8a64f906": ["2d16e6bd-ab89-4e40-a124-797dc7ee7e48"], "498a14de-9191-4b4e-b301-38c72c837e6d": ["fdfc99de-47fc-4c99-b74b-3ed9248039cc"], "a3da478b-d652-4b10-a4b2-dc22a21ab7b8": ["fdfc99de-47fc-4c99-b74b-3ed9248039cc"], "42bc723d-e630-40b0-823d-9d78ef2e652e": ["67d44499-ee06-4efa-9760-e7cf98b0ba5d"], "6cfe013b-b00c-486c-b59c-2ef8ea017497": ["67d44499-ee06-4efa-9760-e7cf98b0ba5d"], "a3a72af5-cdb4-4bb8-ac4d-144e451dd99e": ["19b7e9e2-2be5-4250-9030-5dd3abdfadc7"], "bbd8091e-7b76-4a83-9964-221a497df696": ["19b7e9e2-2be5-4250-9030-5dd3abdfadc7"], "252b2fd3-d7cf-488f-987a-ec1f431c5dac": ["ec8253fe-14dc-461e-a309-16d9522c0928"], "b2577a23-3e78-42e9-8fd8-bccf483ec478": ["ec8253fe-14dc-461e-a309-16d9522c0928"], "3d168f24-ff41-47a3-a88d-1f2c950866fb": ["c8d5fdf0-69a6-4567-9377-996212c361b7"], "84d4bb8a-c6cc-47f1-b908-d3a101066fe7": ["c8d5fdf0-69a6-4567-9377-996212c361b7"], "29bd8a11-d9f2-4117-a4f7-38ce0d0a55da": ["41c998f9-0a1f-4ba2-9b99-65103d2e2842"], "740f6353-df76-470b-b283-dbf3734e83ea": ["41c998f9-0a1f-4ba2-9b99-65103d2e2842"], "2d8ebba8-8ee9-40dd-9c5e-7ef60e2302c0": ["a6dbc108-52a9-4f77-a889-faec899a8f73"], "c597cbc8-786d-442a-a930-544369ac874c": ["a6dbc108-52a9-4f77-a889-faec899a8f73"], "f6584af3-fc03-4462-beaf-399a38b64bea": ["2f5d6ecb-2550-411e-bac8-795ee844534e"], "ed0229bf-b8ac-4e93-8c62-596f30489775": ["2f5d6ecb-2550-411e-bac8-795ee844534e"], "ddf44a0d-e0bc-4b8c-acc6-872da86075f6": ["b781c1b2-2c5e-47cd-bee3-ad85e572b100"], "7c93d7e4-a8e8-4aea-a455-806f190a7b92": ["b781c1b2-2c5e-47cd-bee3-ad85e572b100"], "b5845615-aaa2-45f0-a0ba-2b99bd1e22c5": ["f737d982-ce34-4896-8cfc-7265230b6800"], "1b291a96-43c8-487d-8f53-7c3670afb888": ["f737d982-ce34-4896-8cfc-7265230b6800"], "8896259d-bfc6-40cc-a0f0-44bf34d9ff42": ["508f011d-7ae0-4b08-9cd0-04af9de56d94"], "a817a041-03ee-4d65-83b6-f540b394b713": ["508f011d-7ae0-4b08-9cd0-04af9de56d94"], "c4ded33e-3165-4ca3-8fb4-2d9b755f11a3": ["db80c688-8fa1-47b2-b559-ab0392941c5b"], "5a7b3592-556c-4f3f-a341-ca766cb7096a": ["db80c688-8fa1-47b2-b559-ab0392941c5b"], "6102a609-ffef-4ee7-8fdb-ce802354fa44": ["86e2a2a5-c7ca-43af-89f2-0b6abdf235fc"], "d649a9c4-41b6-4a82-a771-a016fafcbb34": ["86e2a2a5-c7ca-43af-89f2-0b6abdf235fc"], "0efb3d15-2567-4676-aa0f-7447f11c383f": ["c849c39b-893e-4c50-8486-38245b485205"], "bb22dcd6-b098-43d0-a75b-8de4c44117cb": ["c849c39b-893e-4c50-8486-38245b485205"], "4efd31a0-bee1-4110-a077-207e3470581e": ["e9f568e9-aabb-464f-b40a-203d64695b40"], "f5688e1b-a724-4dc3-9790-ce90fb5e41af": ["e9f568e9-aabb-464f-b40a-203d64695b40"], "9d671597-cb9f-439f-a3cf-4e0c1e7171c2": ["51bd0f1a-31f3-496e-baf4-97603ed2ba87"], "d10c7313-b6cc-425d-bebe-d92369d27b35": ["51bd0f1a-31f3-496e-baf4-97603ed2ba87"], "044a79dc-98a3-4044-8e53-e0692d0a8522": ["4a7bfeb0-61a6-49f2-8cfe-bc445faffad2"], "7162212b-c51b-462d-88fe-9c6a72367003": ["4a7bfeb0-61a6-49f2-8cfe-bc445faffad2"], "0f22b5eb-953c-4bde-adfc-4a2259eb9d7b": ["3bfb2a8c-757b-4401-a306-bee62e99be41"], "f4027b8c-7c37-430e-8c3f-27ba4a60c2fb": ["3bfb2a8c-757b-4401-a306-bee62e99be41"], "e1bb7837-003d-4aea-997f-24f403b042d4": ["7a80d397-a47a-4cd1-af51-2d54a79d15cb"], "08aee59a-6ada-490b-ba39-1a204a2b5a2e": ["7a80d397-a47a-4cd1-af51-2d54a79d15cb"], "cf397ba3-ea14-4978-b965-ef48da09c231": ["2367e4b5-9835-4a56-95bf-cb3a1f361a8d"], "a134cef3-680b-4753-9909-54eaf7cfb7cd": ["2367e4b5-9835-4a56-95bf-cb3a1f361a8d"], "44cf1687-2737-4774-9b1c-fe6b67bb92f4": ["29547f60-ca53-4cd5-b089-7e6679d04fad"], "e8471963-9d6c-4128-9f63-f9c26c1119fa": ["29547f60-ca53-4cd5-b089-7e6679d04fad"], "637ceb52-dbba-434f-b4d4-396cb934a5f5": ["6671814f-e937-4489-84f1-4f6a4e3ef880"], "48b6e28a-ba24-41ee-a898-444c09d034a4": ["6671814f-e937-4489-84f1-4f6a4e3ef880"], "d64d4436-0e69-4007-9b70-6317e8923157": ["356cbf6b-490c-4fe9-b952-74e400fd6f12"], "95b54551-b498-4b84-8e26-4c26be32811b": ["356cbf6b-490c-4fe9-b952-74e400fd6f12"], "79ba5689-2db1-4cc6-a336-e6b9dc883322": ["238b6bc5-c656-4da6-8157-e80f3ad38cec"], "662fbb0a-712e-4a43-80ca-8fb1cdcfcf7e": ["238b6bc5-c656-4da6-8157-e80f3ad38cec"], "232fee96-1129-4e75-a749-4c33aeffd264": ["2a5a1b83-d8d7-421a-a37b-c05baf20adfa"], "6168eec4-1f1f-46e5-9afc-124b1fc48404": ["2a5a1b83-d8d7-421a-a37b-c05baf20adfa"], "b25e5991-7be9-4b7e-971e-138b6073d4eb": ["66b023d1-6e8c-4772-8a95-536c67a6b1df"], "2846543e-c72f-4e5a-abcc-b1b4719255a4": ["66b023d1-6e8c-4772-8a95-536c67a6b1df"], "36ba3948-7389-45f8-9c62-b0bad29ef926": ["b084203d-5488-496c-aa5d-619e07f77bf3"], "3490ab7b-be84-4010-a120-c7fb3d5ebca3": ["b084203d-5488-496c-aa5d-619e07f77bf3"], "174d2522-b139-437a-b23f-290d16755663": ["dceca76d-9523-479c-95d7-61acd5965ee4"], "ac489873-0cb2-472f-90f4-6244f7d4c3e5": ["dceca76d-9523-479c-95d7-61acd5965ee4"], "3100447f-c29d-42ff-8fb2-7053f5ec18ca": ["16d00206-0871-421a-9b79-7783667f15f9"], "2f399cce-ec1a-41e7-ba29-5cf1c81d1e3e": ["16d00206-0871-421a-9b79-7783667f15f9"]}, "corpus": {"e78d6df8-1c5c-4c12-9c0c-609d422b879c": "Direct preference optimization: Your language model is secretly a reward model.\n\n\nAdvances in Neural Information Processing Systems 36 (2024).\n\n\n\n\n\n\nRaghubir and Valenzuela (2006)\n\nPriya Raghubir and Ana Valenzuela. 2006.\n\n\nCenter-of-inattention: Position biases in decision-making.\n\n\nOrganizational Behavior and Human Decision Processes 99, 1 (2006), 66\u201380.\n\n\n\n\n\n\nRahmani et\u00a0al. (2024)\n\nHossein\u00a0A Rahmani, Emine Yilmaz, Nick Craswell, Bhaskar Mitra, Paul Thomas, Charles\u00a0LA Clarke, Mohammad Aliannejadi, Clemencia Siro, and Guglielmo Faggioli. 2024.\n\n\nLLMJudge: LLMs for Relevance Judgments.\n\n\narXiv preprint arXiv:2408.08896 (2024).\n\n\n\n\n\n\nRaina et\u00a0al. (2024)", "0c14b5ee-95ec-4868-84b1-173fe33160f6": "Raina et\u00a0al. (2024)\n\nVyas Raina, Adian Liusie, and Mark Gales. 2024.\n\n\nIs LLM-as-a-Judge Robust? Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment.\n\n\narXiv preprint arXiv:2402.14016 (2024).\n\n\n\n\n\n\nRaju et\u00a0al. (2024)\n\nRavi Raju, Swayambhoo Jain, Bo Li, Jonathan Li, and Urmish Thakker. 2024.\n\n\nConstructing domain-specific evaluation sets for llm-as-a-judge.\n\n\narXiv preprint arXiv:2408.08808 (2024).\n\n\n\n\n\n\nRen et\u00a0al. (2023)\n\nJie Ren, Yao Zhao, Tu Vu, Peter\u00a0J Liu, and Balaji Lakshminarayanan. 2023.\n\n\nSelf-evaluation improves selective generation in large language models. In Proceedings on. PMLR, 49\u201364.\n\n\n\n\n\n\nRyu et\u00a0al. (2023)", "6c831f3c-d151-4a71-9ee3-aad5aaaa1057": "Ryu et\u00a0al. (2023)\n\nCheol Ryu, Seolhwa Lee, Subeen Pang, Chanyeol Choi, Hojun Choi, Myeonggee Min, and Jy-Yong Sohn. 2023.\n\n\nRetrieval-based Evaluation for LLMs: A Case Study in Korean Legal QA. In Proceedings of the Natural Legal Language Processing Workshop 2023. 132\u2013137.\n\n\n\n\n\n\nSaad-Falcon et\u00a0al. (2023)\n\nJon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2023.\n\n\nAres: An automated evaluation framework for retrieval-augmented generation systems.\n\n\narXiv preprint arXiv:2311.09476 (2023).\n\n\n\n\n\n\nSahoo et\u00a0al. (2024)\n\nPranab Sahoo, Ayush\u00a0Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024.", "591e0507-858a-427c-bd0a-8d2e0ef180a0": "A systematic survey of prompt engineering in large language models: Techniques and applications.\n\n\narXiv preprint arXiv:2402.07927 (2024).\n\n\n\n\n\n\nSedgwick (2014)\n\nPhilip Sedgwick. 2014.\n\n\nSpearman\u2019s rank correlation coefficient.\n\n\nBmj 349 (2014).\n\n\n\n\n\n\nSen (1968)\n\nPranab\u00a0Kumar Sen. 1968.\n\n\nEstimates of the regression coefficient based on Kendall\u2019s tau.\n\n\nJournal of the American statistical association 63, 324 (1968), 1379\u20131389.\n\n\n\n\n\n\nShankar et\u00a0al. (2024)\n\nShreya Shankar, JD Zamfirescu-Pereira, Bj\u00f6rn Hartmann, Aditya Parameswaran, and Ian Arawjo. 2024.", "adb4004e-b73d-4041-9f65-0e1b47ad800f": "Who validates the validators? aligning llm-assisted evaluation of llm outputs with human preferences. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology. 1\u201314.\n\n\n\n\n\n\nShen et\u00a0al. (2023)\n\nXinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang. 2023.\n\n\n\" do anything now\": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.\n\n\narXiv preprint arXiv:2308.03825 (2023).\n\n\n\n\n\n\nShen and Wan (2023)\n\nYuchen Shen and Xiaojun Wan. 2023.\n\n\nOpinsummeval: Revisiting automated evaluation for opinion summarization.\n\n\narXiv preprint arXiv:2310.18122 (2023).\n\n\n\n\n\n\nShi et\u00a0al. (2023)", "6116542b-1255-440b-8126-7caf2cfc4e1e": "Shi et\u00a0al. (2023)\n\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed\u00a0H Chi, Nathanael Sch\u00e4rli, and Denny Zhou. 2023.\n\n\nLarge language models can be easily distracted by irrelevant context. In International Conference on Machine Learning. PMLR, 31210\u201331227.\n\n\n\n\n\n\nShi et\u00a0al. (2024b)\n\nJiawen Shi, Zenghui Yuan, Yinuo Liu, Yue Huang, Pan Zhou, Lichao Sun, and Neil\u00a0Zhenqiang Gong. 2024b.\n\n\nOptimization-based Prompt Injection Attack to LLM-as-a-Judge.\n\n\narXiv preprint arXiv:2403.17710 (2024).\n\n\n\n\n\n\nShi et\u00a0al. (2024a)\n\nLin Shi, Chiyu Ma, Wenhua Liang, Weicheng Ma, and Soroush Vosoughi. 2024a.", "44869b98-db05-4684-bc71-8db6b8489d40": "Judging the judges: A systematic investigation of position bias in pairwise comparative assessments by llms.\n\n\narXiv preprint arXiv:2406.07791 (2024).\n\n\n\n\n\n\nShu et\u00a0al. (2024)\n\nLei Shu, Nevan Wichers, Liangchen Luo, Yun Zhu, Yinxiao Liu, Jindong Chen, and Lei Meng. 2024.\n\n\nFusion-Eval: Integrating Assistant Evaluators with LLMs. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track. 225\u2013238.\n\n\n\n\n\n\nSingh et\u00a0al. (2023)\n\nAvi Singh, John\u00a0D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Xavier Garcia, Peter\u00a0J Liu, James Harrison, Jaehoon Lee, Kelvin Xu, et\u00a0al. 2023.", "b6918911-989b-46d4-a064-43d5fcafa30d": "Beyond human data: Scaling self-training for problem-solving with language models.\n\n\narXiv preprint arXiv:2312.06585 (2023).\n\n\n\n\n\n\nSoboroff (2024)\n\nIan Soboroff. 2024.\n\n\nDon\u2019t Use LLMs to Make Relevance Judgments.\n\n\narXiv preprint arXiv:2409.15133 (2024).\n\n\n\n\n\n\nSon et\u00a0al. (2024a)\n\nGuijin Son, Hyunjun Jeon, Chami Hwang, and Hanearl Jung. 2024a.\n\n\nKRX Bench: Automating Financial Benchmark Creation via Large Language Models. In Proceedings of the Joint Workshop of the 7th Financial Technology and Natural Language Processing, the 5th Knowledge Discovery from Unstructured Data in Financial Services, and the 4th Workshop on Economics and Natural Language Processing@ LREC-COLING 2024. 10\u201320.", "18e8954f-b542-46ab-be84-ca252da72511": "Son et\u00a0al. (2024b)\n\nGuijin Son, Dongkeun Yoon, Juyoung Suk, Javier Aula-Blasco, Mano Aslan, Vu\u00a0Trong Kim, Shayekh\u00a0Bin Islam, Jaume Prats-Cristi\u00e0, Luc\u00eda Tormo-Ba\u00f1uelos, and Seungone Kim. 2024b.\n\n\nMM-Eval: A Multilingual Meta-Evaluation Benchmark for LLM-as-a-Judge and Reward Models.\n\n\narXiv preprint arXiv:2410.17578 (2024).\n\n\n\n\n\n\nSong et\u00a0al. (2024a)\n\nHwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, and Saab Mansour. 2024a.\n\n\nFineSurE: Fine-grained summarization evaluation using LLMs.\n\n\narXiv preprint arXiv:2407.00908 (2024).\n\n\n\n\n\n\nSong et\u00a0al. (2024b)\n\nMingyang Song, Mao Zheng, and Xuan Luo. 2024b.", "d9a00185-1e55-4b10-9981-82e097aab845": "Mingyang Song, Mao Zheng, and Xuan Luo. 2024b.\n\n\nCan Many-Shot In-Context Learning Help Long-Context LLM Judges? See More, Judge Better!\n\n\narXiv preprint arXiv:2406.11629 (2024).\n\n\n\n\n\n\nSong et\u00a0al. (2024c)\n\nYishen Song, Qianta Zhu, Huaibo Wang, and Qinhua Zheng. 2024c.\n\n\nAutomated Essay Scoring and Revising Based on Open-Source Large Language Models.\n\n\nIEEE Transactions on Learning Technologies (2024).\n\n\n\n\n\n\nSottana et\u00a0al. (2023)\n\nAndrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan. 2023.\n\n\nEvaluation metrics in the era of GPT-4: reliably evaluating large language models on sequence to sequence tasks.\n\n\narXiv preprint arXiv:2310.13800 (2023).\n\n\n\n\n\n\nStephan et\u00a0al. (2024)", "fd0b5dab-398e-4964-b9a5-813fddb26481": "Stephan et\u00a0al. (2024)\n\nAndreas Stephan, Dawei Zhu, Matthias A\u00dfenmacher, Xiaoyu Shen, and Benjamin Roth. 2024.\n\n\nFrom calculation to adjudication: Examining llm judges on mathematical reasoning tasks.\n\n\narXiv preprint arXiv:2409.04168 (2024).\n\n\n\n\n\n\nStureborg et\u00a0al. (2024)\n\nRickard Stureborg, Dimitris Alikaniotis, and Yoshi Suhara. 2024.\n\n\nLarge language models are inconsistent and biased evaluators.\n\n\narXiv preprint arXiv:2405.01724 (2024).\n\n\n\n\n\n\nSun et\u00a0al. (2024)\n\nHanshi Sun, Momin Haider, Ruiqi Zhang, Huitao Yang, Jiahao Qiu, Ming Yin, Mengdi Wang, Peter Bartlett, and Andrea Zanette. 2024.\n\n\nFast Best-of-N Decoding via Speculative Rejection.\n\n\narXiv preprint arXiv:2410.20290 (2024).", "46e493c8-5d81-41d2-9c06-5c1c6f5285fb": "arXiv preprint arXiv:2410.20290 (2024).\n\n\n\n\n\n\nSun (2020)\n\nLichao Sun. 2020.\n\n\nNatural backdoor attack on text data.\n\n\narXiv preprint arXiv:2006.16176 (2020).\n\n\n\n\n\n\nSun et\u00a0al. (2020)\n\nLichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari Asai, Jia Li, Philip Yu, and Caiming Xiong. 2020.\n\n\nAdv-bert: Bert is not robust on misspellings! generating nature adversarial samples on bert.\n\n\narXiv preprint arXiv:2003.04985 (2020).\n\n\n\n\n\n\nSzymanski et\u00a0al. (2024)\n\nAnnalisa Szymanski, Noah Ziems, Heather\u00a0A Eicher-Miller, Toby Jia-Jun Li, Meng Jiang, and Ronald\u00a0A Metoyer. 2024.\n\n\nLimitations of the LLM-as-a-Judge Approach for Evaluating LLM Outputs in Expert Knowledge Tasks.", "7b105afb-7fd0-416c-8e96-64bec451f18d": "arXiv preprint arXiv:2410.20266 (2024).\n\n\n\n\n\n\nTan et\u00a0al. (2024)\n\nSijun Tan, Siyuan Zhuang, Kyle Montgomery, William\u00a0Y Tang, Alejandro Cuadron, Chenguang Wang, Raluca\u00a0Ada Popa, and Ion Stoica. 2024.\n\n\nJudgeBench: A Benchmark for Evaluating LLM-based Judges.\n\n\narXiv preprint arXiv:2410.12784 (2024).\n\n\n\n\n\n\nTessler et\u00a0al. (2024)\n\nMichael\u00a0Henry Tessler, Michiel\u00a0A Bakker, Daniel Jarrett, Hannah Sheahan, Martin\u00a0J Chadwick, Raphael Koster, Georgina Evans, Lucy Campbell-Gillingham, Tantum Collins, David\u00a0C Parkes, et\u00a0al. 2024.\n\n\nAI can help humans find common ground in democratic deliberation.\n\n\nScience 386, 6719 (2024), eadq2852.\n\n\n\n\n\n\nThakur et\u00a0al. (2024)", "2c061971-f636-43a8-8339-38bb01596809": "Thakur et\u00a0al. (2024)\n\nAman\u00a0Singh Thakur, Kartik Choudhary, Venkat\u00a0Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. 2024.\n\n\nJudging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges.\n\n\narXiv preprint arXiv:2406.12624 (2024).\n\n\n\n\n\n\nTonmoy et\u00a0al. (2024)\n\nSM Tonmoy, SM Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, and Amitava Das. 2024.\n\n\nA comprehensive survey of hallucination mitigation techniques in large language models.\n\n\narXiv preprint arXiv:2401.01313 (2024).\n\n\n\n\n\n\nT\u00f6rnberg (2023)\n\nPetter T\u00f6rnberg. 2023.\n\n\nChatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning.", "b10fffbc-c202-4ec0-8874-38d437b353c7": "arXiv preprint arXiv:2304.06588 (2023).\n\n\n\n\n\n\nTouvron et\u00a0al. (2023)\n\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et\u00a0al. 2023.\n\n\nLlama 2: Open foundation and fine-tuned chat models.\n\n\narXiv preprint arXiv:2307.09288 (2023).\n\n\n\n\n\n\nTrivedi et\u00a0al. (2024b)\n\nHarsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward Li, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. 2024b.\n\n\nAppworld: A controllable world of apps and people for benchmarking interactive coding agents.\n\n\narXiv preprint arXiv:2407.18901 (2024).\n\n\n\n\n\n\nTrivedi et\u00a0al. (2024a)", "87811c5e-da4f-496f-93ad-a20246e270eb": "Trivedi et\u00a0al. (2024a)\n\nPrapti Trivedi, Aditya Gulati, Oliver Molenschot, Meghana\u00a0Arakkal Rajeev, Rajkumar Ramamurthy, Keith Stevens, Tanveesh\u00a0Singh Chaudhery, Jahnavi Jambholkar, James Zou, and Nazneen Rajani. 2024a.\n\n\nSelf-rationalization improves LLM as a fine-grained judge.\n\n\narXiv preprint arXiv:2410.05495 (2024).\n\n\n\n\n\n\nTseng et\u00a0al. (2024)\n\nYu-Min Tseng, Wei-Lin Chen, Chung-Chi Chen, and Hsin-Hsi Chen. 2024.\n\n\nAre Expert-Level Language Models Expert-Level Annotators?\n\n\narXiv preprint arXiv:2410.03254 (2024).\n\n\n\n\n\n\nTuring (2009)\n\nAlan\u00a0M Turing. 2009.\n\n\nComputing machinery and intelligence.\n\n\nSpringer.\n\n\n\n\n\n\nTyen et\u00a0al. (2023)", "56e5b03c-9607-42d5-8c15-15854381602b": "Springer.\n\n\n\n\n\n\nTyen et\u00a0al. (2023)\n\nGladys Tyen, Hassan Mansoor, Peter Chen, Tony Mak, and Victor C\u0103rbune. 2023.\n\n\nLLMs cannot find reasoning errors, but can correct them!\n\n\narXiv preprint arXiv:2311.08516 (2023).\n\n\n\n\n\n\nValmeekam et\u00a0al. (2023)\n\nKarthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. 2023.\n\n\nCan large language models really improve by self-critiquing their own plans?\n\n\narXiv preprint arXiv:2310.08118 (2023).\n\n\n\n\n\n\nVerga et\u00a0al. (2024)\n\nPat Verga, Sebastian Hofstatter, Sophia Althammer, Yixuan Su, Aleksandra Piktus, Arkady Arkhangorodsky, Minjie Xu, Naomi White, and Patrick Lewis. 2024.", "9244969f-c68f-4e00-b4e1-9107c3ac2c72": "Replacing Judges with Juries: Evaluating LLM Generations with a Panel of Diverse Models.\n\n\narXiv preprint arXiv:2404.18796 (2024).\n\n\n\n\n\n\nVu et\u00a0al. (2024)\n\nTu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, and Yun-Hsuan Sung. 2024.\n\n\nFoundational autoraters: Taming large language models for better automatic evaluation.\n\n\narXiv preprint arXiv:2407.10817 (2024).\n\n\n\n\n\n\nWang et\u00a0al. (2024b)\n\nBinjie Wang, Steffi Chern, Ethan Chern, and Pengfei Liu. 2024b.\n\n\nHalu-j: Critique-based hallucination judge.\n\n\narXiv preprint arXiv:2407.12943 (2024).\n\n\n\n\n\n\nWang et\u00a0al. (2024c)\n\nChihang Wang, Yuxin Dong, Zhenhong Zhang, Ruotong Wang, Shuo Wang, and Jiajing Chen. 2024c.", "d68a2c52-287e-45f3-8d60-7dc6bd48e7cd": "Automated Genre-Aware Article Scoring and Feedback Using Large Language Models.\n\n\narXiv preprint arXiv:2410.14165 (2024).\n\n\n\n\n\n\nWang et\u00a0al. (2023e)\n\nChenglong Wang, Hang Zhou, Kaiyan Chang, Tongran Liu, Chunliang Zhang, Quan Du, Tong Xiao, and Jingbo Zhu. 2023e.\n\n\nLearning Evaluation Models from Large Language Models for Sequence Generation.\n\n\narXiv preprint arXiv:2308.04386 (2023).\n\n\n\n\n\n\nWang et\u00a0al. (2023b)\n\nPeiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023b.\n\n\nLarge language models are not fair evaluators.\n\n\narXiv preprint arXiv:2305.17926 (2023).\n\n\n\n\n\n\nWang et\u00a0al. (2024e)", "5d459b8c-f8a5-43b2-8e14-153cc9eb42c7": "Wang et\u00a0al. (2024e)\n\nTianlu Wang, Ilia Kulikov, Olga Golovneva, Ping Yu, Weizhe Yuan, Jane Dwivedi-Yu, Richard\u00a0Yuanzhe Pang, Maryam Fazel-Zarandi, Jason Weston, and Xian Li. 2024e.\n\n\nSelf-taught evaluators.\n\n\narXiv preprint arXiv:2408.02666 (2024).\n\n\n\n\n\n\nWang et\u00a0al. (2023c)\n\nTianlu Wang, Ping Yu, Xiaoqing\u00a0Ellen Tan, Sean O\u2019Brien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023c.\n\n\nShepherd: A critic for language model generation.\n\n\narXiv preprint arXiv:2308.04592 (2023).\n\n\n\n\n\n\nWang et\u00a0al. (2024f)\n\nWanying Wang, Zeyu Ma, Pengfei Liu, and Mingang Chen. 2024f.", "2d16e6bd-ab89-4e40-a124-797dc7ee7e48": "Revisiting Benchmark and Assessment: An Agent-based Exploratory Dynamic Evaluation Framework for LLMs.\n\n\narXiv preprint arXiv:2410.11507 (2024).\n\n\n\n\n\n\nWang et\u00a0al. (2018)\n\nXuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc Najork. 2018.\n\n\nPosition bias estimation for unbiased learning to rank in personal search. In Proceedings of the eleventh ACM international conference on web search and data mining. 610\u2013618.\n\n\n\n\n\n\nWang et\u00a0al. (2022)\n\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022.\n\n\nSelf-consistency improves chain of thought reasoning in language models.\n\n\narXiv preprint arXiv:2203.11171 (2022).", "fdfc99de-47fc-4c99-b74b-3ed9248039cc": "arXiv preprint arXiv:2203.11171 (2022).\n\n\n\n\n\n\nWang et\u00a0al. (2023d)\n\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, et\u00a0al. 2023d.\n\n\nPandalm: An automatic evaluation benchmark for llm instruction tuning optimization.\n\n\narXiv preprint arXiv:2306.05087 (2023).\n\n\n\n\n\n\nWang et\u00a0al. (2024a)\n\nZhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, and Yi Dong. 2024a.\n\n\nHelpSteer2-Preference: Complementing Ratings with Preferences.\n\n\narXiv preprint arXiv:2410.01257 (2024).\n\n\n\n\n\n\nWang et\u00a0al. (2023a)", "67d44499-ee06-4efa-9760-e7cf98b0ba5d": "Wang et\u00a0al. (2023a)\n\nZhilin Wang, Yi Dong, Jiaqi Zeng, Virginia Adams, Makesh\u00a0Narsimhan Sreedhar, Daniel Egert, Olivier Delalleau, Jane\u00a0Polak Scowcroft, Neel Kant, Aidan Swope, et\u00a0al. 2023a.\n\n\nHelpsteer: Multi-attribute helpfulness dataset for steerlm.\n\n\narXiv preprint arXiv:2311.09528 (2023).\n\n\n\n\n\n\nWang et\u00a0al. (2024d)\n\nZhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, and Huaxiu Yao. 2024d.\n\n\nCream: Consistency regularized self-rewarding language models.\n\n\narXiv preprint arXiv:2410.12735 (2024).\n\n\n\n\n\n\nWarrens (2015)\n\nMatthijs\u00a0J Warrens. 2015.\n\n\nFive ways to look at Cohen\u2019s kappa.\n\n\nJournal of Psychology & Psychotherapy 5 (2015).", "19b7e9e2-2be5-4250-9030-5dd3abdfadc7": "Watts et\u00a0al. (2024)\n\nIshaan Watts, Varun Gumma, Aditya Yadavalli, Vivek Seshadri, Manohar Swaminathan, and Sunayana Sitaram. 2024.\n\n\nPARIKSHA: A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data.\n\n\narXiv preprint arXiv:2406.15053 (2024).\n\n\n\n\n\n\nWei et\u00a0al. (2022)\n\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc\u00a0V Le, Denny Zhou, et\u00a0al. 2022.\n\n\nChain-of-thought prompting elicits reasoning in large language models.\n\n\nAdvances in neural information processing systems 35 (2022), 24824\u201324837.\n\n\n\n\n\n\nWeyssow et\u00a0al. (2024)\n\nMartin Weyssow, Aton Kamanda, and Houari Sahraoui. 2024.", "ec8253fe-14dc-461e-a309-16d9522c0928": "CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences.\n\n\narXiv preprint arXiv:2403.09032 (2024).\n\n\n\n\n\n\nWu et\u00a0al. (2024a)\n\nTongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. 2024a.\n\n\nContinual learning for large language models: A survey.\n\n\narXiv preprint arXiv:2402.01364 (2024).\n\n\n\n\n\n\nWu et\u00a0al. (2024b)\n\nTianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. 2024b.\n\n\nMeta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge.\n\n\narXiv preprint arXiv:2407.19594 (2024).\n\n\n\n\n\n\nXia et\u00a0al. (2024a)", "c8d5fdf0-69a6-4567-9377-996212c361b7": "Xia et\u00a0al. (2024a)\n\nShijie Xia, Xuefeng Li, Yixin Liu, Tongshuang Wu, and Pengfei Liu. 2024a.\n\n\nEvaluating Mathematical Reasoning Beyond Accuracy.\n\n\narXiv preprint arXiv:2404.05692 (2024).\n\n\n\n\n\n\nXia et\u00a0al. (2024b)\n\nTingyu Xia, Bowen Yu, Yuan Wu, Yi Chang, and Chang Zhou. 2024b.\n\n\nLanguage Models can Evaluate Themselves via Probability Discrepancy.\n\n\narXiv preprint arXiv:2405.10516 (2024).\n\n\n\n\n\n\nXie et\u00a0al. (2023)\n\nQianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023.\n\n\nPixiu: A large language model, instruction data and evaluation benchmark for finance.\n\n\narXiv preprint arXiv:2306.05443 (2023).\n\n\n\n\n\n\nXie et\u00a0al. (2024b)", "41c998f9-0a1f-4ba2-9b99-65103d2e2842": "Xie et\u00a0al. (2024b)\n\nTinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari\u00a0Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, et\u00a0al. 2024b.\n\n\nSorry-bench: Systematically evaluating large language model safety refusal behaviors.\n\n\narXiv preprint arXiv:2406.14598 (2024).\n\n\n\n\n\n\nXie et\u00a0al. (2024a)\n\nYuxi Xie, Kenji Kawaguchi, Yiran Zhao, James\u00a0Xu Zhao, Min-Yen Kan, Junxian He, and Michael Xie. 2024a.\n\n\nSelf-evaluation guided beam search for reasoning.\n\n\nAdvances in Neural Information Processing Systems 36 (2024).\n\n\n\n\n\n\nXie et\u00a0al. (2024c)", "a6dbc108-52a9-4f77-a889-faec899a8f73": "Xie et\u00a0al. (2024c)\n\nYiqing Xie, Sheng Zhang, Hao Cheng, Pengfei Liu, Zelalem Gero, Cliff Wong, Tristan Naumann, Hoifung Poon, and Carolyn Rose. 2024c.\n\n\nDOCLENS: Multi-aspect fine-grained evaluation for medical text generation.. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics.\n\n\n\n\n\n\nXie et\u00a0al. (2024d)\n\nYiqing Xie, Wenxuan Zhou, Pradyot Prakash, Di Jin, Yuning Mao, Quintin Fettes, Arya Talebzadeh, Sinong Wang, Han Fang, Carolyn Rose, et\u00a0al. 2024d.\n\n\nImproving Model Factuality with Fine-grained Critique-based Evaluator.\n\n\narXiv preprint arXiv:2410.18359 (2024).\n\n\n\n\n\n\nXiong et\u00a0al. (2024)", "2f5d6ecb-2550-411e-bac8-795ee844534e": "Xiong et\u00a0al. (2024)\n\nTianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye, Haoqi Fan, Quanquan Gu, Heng Huang, and Chunyuan Li. 2024.\n\n\nLlava-critic: Learning to evaluate multimodal models.\n\n\narXiv preprint arXiv:2410.02712 (2024).\n\n\n\n\n\n\nXu et\u00a0al. (2023d)\n\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023d.\n\n\nWizardlm: Empowering large language models to follow complex instructions.\n\n\narXiv preprint arXiv:2304.12244 (2023).\n\n\n\n\n\n\nXu et\u00a0al. (2023b)\n\nGuohai Xu, Jiayi Liu, Ming Yan, Haotian Xu, Jinghui Si, Zhuoran Zhou, Peng Yi, Xing Gao, Jitao Sang, Rong Zhang, et\u00a0al. 2023b.", "b781c1b2-2c5e-47cd-bee3-ad85e572b100": "Cvalues: Measuring the values of chinese large language models from safety to responsibility.\n\n\narXiv preprint arXiv:2307.09705 (2023).\n\n\n\n\n\n\nXu et\u00a0al. (2024b)\n\nShuying Xu, Junjie Hu, and Ming Jiang. 2024b.\n\n\nLarge Language Models Are Active Critics in NLG Evaluation.\n\n\narXiv preprint arXiv:2410.10724 (2024).\n\n\n\n\n\n\nXu et\u00a0al. (2024a)\n\nTengyu Xu, Eryk Helenowski, Karthik\u00a0Abinav Sankararaman, Di Jin, Kaiyan Peng, Eric Han, Shaoliang Nie, Chen Zhu, Hejia Zhang, Wenxuan Zhou, et\u00a0al. 2024a.\n\n\nThe perfect blend: Redefining RLHF with mixture of judges.\n\n\narXiv preprint arXiv:2409.20370 (2024).\n\n\n\n\n\n\nXu et\u00a0al. (2023e)", "f737d982-ce34-4896-8cfc-7265230b6800": "Xu et\u00a0al. (2023e)\n\nWenda Xu, Danqing Wang, Liangming Pan, Zhenqiao Song, Markus Freitag, William\u00a0Yang Wang, and Lei Li. 2023e.\n\n\nINSTRUCTSCORE: Explainable Text Generation Evaluation with Finegrained Feedback.\n\n\narXiv preprint arXiv:2305.14282 (2023).\n\n\n\n\n\n\nXu et\u00a0al. (2024c)\n\nWenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, and William Wang. 2024c.\n\n\nPride and prejudice: LLM amplifies self-bias in self-refinement. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 15474\u201315492.\n\n\n\n\n\n\nXu et\u00a0al. (2023a)\n\nXilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang, and Mohan Kankanhalli. 2023a.", "508f011d-7ae0-4b08-9cd0-04af9de56d94": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack.\n\n\narXiv preprint arXiv:2310.13345 (2023).\n\n\n\n\n\n\nXu et\u00a0al. (2023c)\n\nZhenran Xu, Senbao Shi, Baotian Hu, Jindi Yu, Dongfang Li, Min Zhang, and Yuxiang Wu. 2023c.\n\n\nTowards reasoning in large language models via multi-agent peer review collaboration.\n\n\narXiv preprint arXiv:2311.08152 (2023).\n\n\n\n\n\n\nYan et\u00a0al. (2024a)\n\nLe Yan, Zhen Qin, Honglei Zhuang, Rolf Jagerman, Xuanhui Wang, Michael Bendersky, and Harrie Oosterhuis. 2024a.\n\n\nConsolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing.\n\n\narXiv preprint arXiv:2404.11791 (2024).\n\n\n\n\n\n\nYan et\u00a0al. (2024b)", "db80c688-8fa1-47b2-b559-ab0392941c5b": "Yan et\u00a0al. (2024b)\n\nLe Yan, Zhen Qin, Honglei Zhuang, Rolf Jagerman, Xuanhui Wang, Michael Bendersky, and Harrie Oosterhuis. 2024b.\n\n\nConsolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 410\u2013423.\n\n\n\nhttps://doi.org/10.18653/v1/2024.emnlp-main.25\n\n\n\nYang et\u00a0al. (2024)\n\nNakyeong Yang, Taegwan Kang, Stanley\u00a0Jungkyu Choi, Honglak Lee, and Kyomin Jung. 2024.", "86e2a2a5-c7ca-43af-89f2-0b6abdf235fc": "Mitigating biases for instruction-following language models via bias neurons elimination. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 9061\u20139073.\n\n\n\n\n\n\nYao et\u00a0al. (2024)\n\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. 2024.\n\n\nTree of thoughts: Deliberate problem solving with large language models.\n\n\nAdvances in Neural Information Processing Systems 36 (2024).\n\n\n\n\n\n\nYe and Ng (2024)\n\nHai Ye and Hwee\u00a0Tou Ng. 2024.\n\n\nSelf-Judge: Selective Instruction Following with Alignment Self-Evaluation.\n\n\narXiv preprint arXiv:2409.00935 (2024).\n\n\n\n\n\n\nYe et\u00a0al. (2024b)", "c849c39b-893e-4c50-8486-38245b485205": "Ye et\u00a0al. (2024b)\n\nJiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen, Qihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer, Chao Huang, Pin-Yu Chen, et\u00a0al. 2024b.\n\n\nJustice or prejudice? quantifying biases in llm-as-a-judge.\n\n\narXiv preprint arXiv:2410.02736 (2024).\n\n\n\n\n\n\nYe et\u00a0al. (2023a)\n\nSeonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, and Minjoon Seo. 2023a.\n\n\nSelfee: Iterative self-revising llm empowered by self-feedback generation.\n\n\nBlog post (2023).\n\n\n\n\n\n\nYe et\u00a0al. (2023b)\n\nSeonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. 2023b.", "e9f568e9-aabb-464f-b40a-203d64695b40": "Flask: Fine-grained language model evaluation based on alignment skill sets.\n\n\narXiv preprint arXiv:2307.10928 (2023).\n\n\n\n\n\n\nYe et\u00a0al. (2024a)\n\nZiyi Ye, Xiangsheng Li, Qiuchi Li, Qingyao Ai, Yujia Zhou, Wei Shen, Dong Yan, and Yiqun Liu. 2024a.\n\n\nBeyond Scalar Reward Model: Learning Generative Judge from Preference Data.\n\n\narXiv preprint arXiv:2410.03742 (2024).\n\n\n\n\n\n\nYi et\u00a0al. (2024)\n\nSeungjun Yi, Jaeyoung Lim, and Juyong Yoon. 2024.\n\n\nProtocoLLM: Automatic Evaluation Framework of LLMs on Domain-Specific Scientific Protocol Formulation Tasks.\n\n\narXiv preprint arXiv:2410.04601 (2024).\n\n\n\n\n\n\nYoshino et\u00a0al. (2023)", "51bd0f1a-31f3-496e-baf4-97603ed2ba87": "Yoshino et\u00a0al. (2023)\n\nKoichiro Yoshino, Yun-Nung Chen, Paul Crook, Satwik Kottur, Jinchao Li, Behnam Hedayatnia, Seungwhan Moon, Zhengcong Fei, Zekang Li, Jinchao Zhang, et\u00a0al. 2023.\n\n\nOverview of the Tenth Dialog System Technology Challenge: DSTC10.\n\n\nIEEE/ACM Transactions on Audio, Speech, and Language Processing (2023).\n\n\n\n\n\n\nYu et\u00a0al. (2024)\n\nZhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, and Shikun Zhang. 2024.\n\n\nKieval: A knowledge-grounded interactive evaluation framework for large language models.\n\n\narXiv preprint arXiv:2402.15043 (2024).\n\n\n\n\n\n\nYuan et\u00a0al. (2024)", "4a7bfeb0-61a6-49f2-8cfe-bc445faffad2": "Yuan et\u00a0al. (2024)\n\nWeizhe Yuan, Richard\u00a0Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. 2024.\n\n\nSelf-rewarding language models.\n\n\narXiv preprint arXiv:2401.10020 (2024).\n\n\n\n\n\n\nYue et\u00a0al. (2023a)\n\nShengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Xuanjing Huang, et\u00a0al. 2023a.\n\n\nDisc-lawllm: Fine-tuning large language models for intelligent legal services.\n\n\narXiv preprint arXiv:2309.11325 (2023).\n\n\n\n\n\n\nYue et\u00a0al. (2023b)\n\nXiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su, and Huan Sun. 2023b.\n\n\nAutomatic evaluation of attribution by large language models.", "3bfb2a8c-757b-4401-a306-bee62e99be41": "arXiv preprint arXiv:2305.06311 (2023).\n\n\n\n\n\n\nZelikman et\u00a0al. (2024)\n\nEric Zelikman, YH Wu, Jesse Mu, and Noah\u00a0D Goodman. 2024.\n\n\nSTaR: Self-taught reasoner bootstrapping reasoning with reasoning. In Proc. the 36th International Conference on Neural Information Processing Systems, Vol.\u00a01126.\n\n\n\n\n\n\nZeng et\u00a0al. (2024)\n\nWeihao Zeng, Can Xu, Yingxiu Zhao, Jian-Guang Lou, and Weizhu Chen. 2024.\n\n\nAutomatic Instruction Evolving for Large Language Models.\n\n\narXiv preprint arXiv:2406.00770 (2024).\n\n\n\n\n\n\nZhang et\u00a0al. (2021)\n\nChen Zhang, Jo\u00e3o Sedoc, Luis\u00a0Fernando D\u2019Haro, Rafael Banchs, and Alexander Rudnicky. 2021.\n\n\nAutomatic evaluation and moderation of open-domain dialogue systems.", "7a80d397-a47a-4cd1-af51-2d54a79d15cb": "arXiv preprint arXiv:2111.02110 (2021).\n\n\n\n\n\n\nZhang et\u00a0al. (2024c)\n\nKaiqi Zhang, Shuai Yuan, and Honghan Zhao. 2024c.\n\n\nTALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot.\n\n\narXiv preprint arXiv:2407.10999 (2024).\n\n\n\n\n\n\nZhang et\u00a0al. (2024b)\n\nQiyuan Zhang, Yufei Wang, Tiezheng Yu, Yuxin Jiang, Chuhan Wu, Liangyou Li, Yasheng Wang, Xin Jiang, Lifeng Shang, Ruiming Tang, et\u00a0al. 2024b.\n\n\nRevisEval: Improving LLM-as-a-Judge via Response-Adapted References.\n\n\narXiv preprint arXiv:2410.05193 (2024).\n\n\n\n\n\n\nZhang et\u00a0al. (2023a)\n\nRuoyu Zhang, Yanzeng Li, Yongliang Ma, Ming Zhou, and Lei Zou. 2023a.", "2367e4b5-9835-4a56-95bf-cb3a1f361a8d": "Llmaaa: Making large language models as active annotators.\n\n\narXiv preprint arXiv:2310.19596 (2023).\n\n\n\n\n\n\nZhang (2018)\n\nSaizheng Zhang. 2018.\n\n\nPersonalizing dialogue agents: I have a dog, do you have pets too.\n\n\narXiv preprint arXiv:1801.07243 (2018).\n\n\n\n\n\n\nZhang et\u00a0al. (2024a)\n\nXiaoyu Zhang, Yishan Li, Jiayin Wang, Bowen Sun, Weizhi Ma, Peijie Sun, and Min Zhang. 2024a.\n\n\nLarge language models as evaluators for recommendation explanations. In Proceedings of the 18th ACM Conference on Recommender Systems. 33\u201342.\n\n\n\n\n\n\nZhang et\u00a0al. (2023b)\n\nXinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023b.", "29547f60-ca53-4cd5-b089-7e6679d04fad": "Wider and deeper llm networks are fairer llm evaluators.\n\n\narXiv preprint arXiv:2308.01862 (2023).\n\n\n\n\n\n\nZhao et\u00a0al. (2024b)\n\nRuochen Zhao, Wenxuan Zhang, Yew\u00a0Ken Chia, Deli Zhao, and Lidong Bing. 2024b.\n\n\nAuto Arena of LLMs: Automating LLM Evaluations with Agent Peer-battles and Committee Discussions.\n\n\narXiv preprint arXiv:2405.20267 (2024).\n\n\n\n\n\n\nZhao et\u00a0al. (2023b)\n\nWayne\u00a0Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et\u00a0al. 2023b.\n\n\nA survey of large language models.\n\n\narXiv preprint arXiv:2303.18223 (2023).\n\n\n\n\n\n\nZhao et\u00a0al. (2024a)\n\nXiutian Zhao, Ke Wang, and Wei Peng. 2024a.", "6671814f-e937-4489-84f1-4f6a4e3ef880": "Xiutian Zhao, Ke Wang, and Wei Peng. 2024a.\n\n\nMeasuring the inconsistency of large language models in preferential ranking.\n\n\narXiv preprint arXiv:2410.08851 (2024).\n\n\n\n\n\n\nZhao et\u00a0al. (2023a)\n\nYachao Zhao, Bo Wang, Dongming Zhao, Kun Huang, Yan Wang, Ruifang He, and Yuexian Hou. 2023a.\n\n\nMind vs. Mouth: On Measuring Re-judge Inconsistency of Social Bias in Large Language Models.\n\n\narXiv preprint arXiv:2308.12578 (2023).\n\n\n\n\n\n\nZhao et\u00a0al. (2021)\n\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021.\n\n\nCalibrate before use: Improving few-shot performance of language models. In International conference on machine learning. PMLR, 12697\u201312706.\n\n\n\n\n\n\nZheng et\u00a0al. (2023b)", "356cbf6b-490c-4fe9-b952-74e400fd6f12": "Zheng et\u00a0al. (2023b)\n\nChujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. 2023b.\n\n\nLarge language models are not robust multiple choice selectors. In The Twelfth International Conference on Learning Representations.\n\n\n\n\n\n\nZheng et\u00a0al. (2023a)\n\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et\u00a0al. 2023a.\n\n\nJudging llm-as-a-judge with mt-bench and chatbot arena.\n\n\nAdvances in Neural Information Processing Systems 36 (2023), 46595\u201346623.\n\n\n\n\n\n\nZheng et\u00a0al. (2024)\n\nXiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, and Min Lin. 2024.", "238b6bc5-c656-4da6-8157-e80f3ad38cec": "Cheating automatic llm benchmarks: Null models achieve high win rates.\n\n\narXiv preprint arXiv:2410.07137 (2024).\n\n\n\n\n\n\nZhou et\u00a0al. (2024c)\n\nHongli Zhou, Hui Huang, Yunfei Long, Bing Xu, Conghui Zhu, Hailong Cao, Muyun Yang, and Tiejun Zhao. 2024c.\n\n\nMitigating the Bias of Large Language Model Evaluation.\n\n\narXiv preprint arXiv:2409.16788 (2024).\n\n\n\n\n\n\nZhou et\u00a0al. (2024e)\n\nHan Zhou, Xingchen Wan, Yinhong Liu, Nigel Collier, Ivan Vuli\u0107, and Anna Korhonen. 2024e.\n\n\nFairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments.\n\n\narXiv preprint arXiv:2406.11370 (2024).\n\n\n\n\n\n\nZhou et\u00a0al. (2023a)", "2a5a1b83-d8d7-421a-a37b-c05baf20adfa": "Zhou et\u00a0al. (2023a)\n\nHan Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, and Subhrajit Roy. 2023a.\n\n\nBatch calibration: Rethinking calibration for in-context learning and prompt engineering.\n\n\narXiv preprint arXiv:2309.17249 (2023).\n\n\n\n\n\n\nZhou et\u00a0al. (2024a)\n\nRuiyang Zhou, Lu Chen, and Kai Yu. 2024a.\n\n\nIs LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). 9340\u20139351.\n\n\n\n\n\n\nZhou et\u00a0al. (2023b)", "66b023d1-6e8c-4772-8a95-536c67a6b1df": "Zhou et\u00a0al. (2023b)\n\nXuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et\u00a0al. 2023b.\n\n\nSotopia: Interactive evaluation for social intelligence in language agents.\n\n\narXiv preprint arXiv:2310.11667 (2023).\n\n\n\n\n\n\nZhou et\u00a0al. (2024b)\n\nYiyang Zhou, Zhiyuan Fan, Dongjie Cheng, Sihan Yang, Zhaorun Chen, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao. 2024b.\n\n\nCalibrated self-rewarding vision language models.\n\n\narXiv preprint arXiv:2405.14622 (2024).\n\n\n\n\n\n\nZhou et\u00a0al. (2024d)\n\nYuhang Zhou, Yuchen Ni, Xiang Liu, Jian Zhang, Sen Liu, Guangnan Ye, and Hongfeng Chai. 2024d.", "b084203d-5488-496c-aa5d-619e07f77bf3": "Are Large Language Models Rational Investors?\n\n\narXiv preprint arXiv:2402.12713 (2024).\n\n\n\n\n\n\nZhu et\u00a0al. (2023)\n\nLianghui Zhu, Xinggang Wang, and Xinlong Wang. 2023.\n\n\nJudgelm: Fine-tuned large language models are scalable judges.\n\n\narXiv preprint arXiv:2310.17631 (2023).\n\n\n\n\n\n\nZhuang et\u00a0al. (2024)\n\nShengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido Zuccon. 2024.\n\n\nA Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval (Washington DC, USA) (SIGIR \u201924). Association for Computing Machinery, New York, NY, USA, 38\u201347.", "dceca76d-9523-479c-95d7-61acd5965ee4": "https://doi.org/10.1145/3626772.3657813\n\n\n\nZhuge et\u00a0al. (2024)\n\nMingchen Zhuge, Changsheng Zhao, Dylan Ashley, Wenyi Wang, Dmitrii Khizbullin, Yunyang Xiong, Zechun Liu, Ernie Chang, Raghuraman Krishnamoorthi, Yuandong Tian, et\u00a0al. 2024.\n\n\nAgent-as-a-Judge: Evaluate Agents with Agents.\n\n\narXiv preprint arXiv:2410.10934 (2024).\n\n\n\n\n\n\nZhuo (2023)\n\nTerry\u00a0Yue Zhuo. 2023.\n\n\nICE-Score: Instructing Large Language Models to Evaluate Code.\n\n\narXiv preprint arXiv:2304.14317 (2023).\n\n\n\n\n\n\nZou et\u00a0al. (2023)\n\nAndy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J\u00a0Zico Kolter, and Matt Fredrikson. 2023.\n\n\nUniversal and transferable adversarial attacks on aligned language models.", "16d00206-0871-421a-9b79-7783667f15f9": "arXiv preprint arXiv:2307.15043 (2023).\n\n\n\n\n\n\n\n\n\n\n\nGenerated  on Tue Dec 10 05:49:16 2024 by LaTeXML"}}